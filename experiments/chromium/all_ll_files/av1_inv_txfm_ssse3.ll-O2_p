; ModuleID = '../../third_party/libaom/source/libaom/av1/common/x86/av1_inv_txfm_ssse3.c'
source_filename = "../../third_party/libaom/source/libaom/av1/common/x86/av1_inv_txfm_ssse3.c"
target datalayout = "e-m:e-i64:64-f80:128-n8:16:32:64-S128"
target triple = "x86_64-unknown-linux-gnu"

module asm ".symver exp, exp@GLIBC_2.2.5"
module asm ".symver exp2, exp2@GLIBC_2.2.5"
module asm ".symver exp2f, exp2f@GLIBC_2.2.5"
module asm ".symver expf, expf@GLIBC_2.2.5"
module asm ".symver lgamma, lgamma@GLIBC_2.2.5"
module asm ".symver lgammaf, lgammaf@GLIBC_2.2.5"
module asm ".symver lgammal, lgammal@GLIBC_2.2.5"
module asm ".symver log, log@GLIBC_2.2.5"
module asm ".symver log2, log2@GLIBC_2.2.5"
module asm ".symver log2f, log2f@GLIBC_2.2.5"
module asm ".symver logf, logf@GLIBC_2.2.5"
module asm ".symver pow, pow@GLIBC_2.2.5"
module asm ".symver powf, powf@GLIBC_2.2.5"

%struct.txfm_param = type { i8, i8, i32, i32, i32, i8, i32 }

@av1_inv_txfm_shift_ls = external local_unnamed_addr global [19 x i8*], align 16
@av1_inv_cos_bit_row = external local_unnamed_addr constant [5 x [5 x i8]], align 16
@av1_inv_cos_bit_col = external local_unnamed_addr constant [5 x [5 x i8]], align 16
@tx_size_wide = internal unnamed_addr constant [19 x i32] [i32 4, i32 8, i32 16, i32 32, i32 64, i32 4, i32 8, i32 8, i32 16, i32 16, i32 32, i32 32, i32 64, i32 4, i32 16, i32 8, i32 32, i32 16, i32 64], align 16
@tx_size_high = internal unnamed_addr constant [19 x i32] [i32 4, i32 8, i32 16, i32 32, i32 64, i32 8, i32 4, i32 16, i32 8, i32 32, i32 16, i32 64, i32 32, i32 16, i32 4, i32 32, i32 8, i32 64, i32 16], align 16
@lowbd_txfm_all_1d_w4_arr = internal unnamed_addr constant [5 x [3 x void (<2 x i64>*, <2 x i64>*, i8)*]] [[3 x void (<2 x i64>*, <2 x i64>*, i8)*] [void (<2 x i64>*, <2 x i64>*, i8)* @idct4_w4_sse2, void (<2 x i64>*, <2 x i64>*, i8)* @iadst4_w4_sse2, void (<2 x i64>*, <2 x i64>*, i8)* @iidentity4_ssse3], [3 x void (<2 x i64>*, <2 x i64>*, i8)*] [void (<2 x i64>*, <2 x i64>*, i8)* @idct8_w4_sse2, void (<2 x i64>*, <2 x i64>*, i8)* @iadst8_w4_sse2, void (<2 x i64>*, <2 x i64>*, i8)* @iidentity8_sse2], [3 x void (<2 x i64>*, <2 x i64>*, i8)*] [void (<2 x i64>*, <2 x i64>*, i8)* @idct16_w4_sse2, void (<2 x i64>*, <2 x i64>*, i8)* @iadst16_w4_sse2, void (<2 x i64>*, <2 x i64>*, i8)* @iidentity16_ssse3], [3 x void (<2 x i64>*, <2 x i64>*, i8)*] zeroinitializer, [3 x void (<2 x i64>*, <2 x i64>*, i8)*] zeroinitializer], align 16
@hitx_1d_tab = internal unnamed_addr constant [16 x i8] c"\00\00\01\01\00\01\01\01\01\02\02\00\02\01\02\01", align 16
@vitx_1d_tab = internal unnamed_addr constant [16 x i8] c"\00\01\00\01\01\00\01\01\01\02\00\02\01\02\01\02", align 16
@tx_size_wide_log2 = internal unnamed_addr constant [19 x i32] [i32 2, i32 3, i32 4, i32 5, i32 6, i32 2, i32 3, i32 3, i32 4, i32 4, i32 5, i32 5, i32 6, i32 2, i32 4, i32 3, i32 5, i32 4, i32 6], align 16
@tx_size_high_log2 = internal unnamed_addr constant [19 x i32] [i32 2, i32 3, i32 4, i32 5, i32 6, i32 3, i32 2, i32 4, i32 3, i32 5, i32 4, i32 6, i32 5, i32 4, i32 2, i32 5, i32 3, i32 6, i32 4], align 16
@av1_cospi_arr_data = external local_unnamed_addr constant [7 x [64 x i32]], align 16
@av1_sinpi_arr_data = external local_unnamed_addr constant [7 x [5 x i32]], align 16
@lowbd_txfm_all_1d_w8_arr = internal unnamed_addr constant [5 x [3 x void (<2 x i64>*, <2 x i64>*, i8)*]] [[3 x void (<2 x i64>*, <2 x i64>*, i8)*] [void (<2 x i64>*, <2 x i64>*, i8)* @idct4_sse2, void (<2 x i64>*, <2 x i64>*, i8)* @iadst4_sse2, void (<2 x i64>*, <2 x i64>*, i8)* @iidentity4_ssse3], [3 x void (<2 x i64>*, <2 x i64>*, i8)*] [void (<2 x i64>*, <2 x i64>*, i8)* @idct8_sse2, void (<2 x i64>*, <2 x i64>*, i8)* @iadst8_sse2, void (<2 x i64>*, <2 x i64>*, i8)* @iidentity8_sse2], [3 x void (<2 x i64>*, <2 x i64>*, i8)*] [void (<2 x i64>*, <2 x i64>*, i8)* @idct16_sse2, void (<2 x i64>*, <2 x i64>*, i8)* @iadst16_sse2, void (<2 x i64>*, <2 x i64>*, i8)* @iidentity16_ssse3], [3 x void (<2 x i64>*, <2 x i64>*, i8)*] [void (<2 x i64>*, <2 x i64>*, i8)* @idct32_sse2, void (<2 x i64>*, <2 x i64>*, i8)* null, void (<2 x i64>*, <2 x i64>*, i8)* null], [3 x void (<2 x i64>*, <2 x i64>*, i8)*] [void (<2 x i64>*, <2 x i64>*, i8)* @idct64_low32_ssse3, void (<2 x i64>*, <2 x i64>*, i8)* null, void (<2 x i64>*, <2 x i64>*, i8)* null]], align 16
@lowbd_txfm_all_1d_zeros_idx = internal unnamed_addr constant [32 x i32] [i32 0, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 2, i32 2, i32 2, i32 2, i32 2, i32 2, i32 2, i32 2, i32 3, i32 3, i32 3, i32 3, i32 3, i32 3, i32 3, i32 3, i32 3, i32 3, i32 3, i32 3, i32 3, i32 3, i32 3, i32 3], align 16
@lowbd_txfm_all_1d_zeros_w8_arr = internal unnamed_addr constant [5 x [3 x [4 x void (<2 x i64>*, <2 x i64>*, i8)*]]] [[3 x [4 x void (<2 x i64>*, <2 x i64>*, i8)*]] [[4 x void (<2 x i64>*, <2 x i64>*, i8)*] [void (<2 x i64>*, <2 x i64>*, i8)* @idct4_sse2, void (<2 x i64>*, <2 x i64>*, i8)* @idct4_sse2, void (<2 x i64>*, <2 x i64>*, i8)* null, void (<2 x i64>*, <2 x i64>*, i8)* null], [4 x void (<2 x i64>*, <2 x i64>*, i8)*] [void (<2 x i64>*, <2 x i64>*, i8)* @iadst4_sse2, void (<2 x i64>*, <2 x i64>*, i8)* @iadst4_sse2, void (<2 x i64>*, <2 x i64>*, i8)* null, void (<2 x i64>*, <2 x i64>*, i8)* null], [4 x void (<2 x i64>*, <2 x i64>*, i8)*] [void (<2 x i64>*, <2 x i64>*, i8)* @iidentity4_ssse3, void (<2 x i64>*, <2 x i64>*, i8)* @iidentity4_ssse3, void (<2 x i64>*, <2 x i64>*, i8)* null, void (<2 x i64>*, <2 x i64>*, i8)* null]], [3 x [4 x void (<2 x i64>*, <2 x i64>*, i8)*]] [[4 x void (<2 x i64>*, <2 x i64>*, i8)*] [void (<2 x i64>*, <2 x i64>*, i8)* @idct8_low1_ssse3, void (<2 x i64>*, <2 x i64>*, i8)* @idct8_sse2, void (<2 x i64>*, <2 x i64>*, i8)* null, void (<2 x i64>*, <2 x i64>*, i8)* null], [4 x void (<2 x i64>*, <2 x i64>*, i8)*] [void (<2 x i64>*, <2 x i64>*, i8)* @iadst8_low1_ssse3, void (<2 x i64>*, <2 x i64>*, i8)* @iadst8_sse2, void (<2 x i64>*, <2 x i64>*, i8)* null, void (<2 x i64>*, <2 x i64>*, i8)* null], [4 x void (<2 x i64>*, <2 x i64>*, i8)*] [void (<2 x i64>*, <2 x i64>*, i8)* @iidentity8_sse2, void (<2 x i64>*, <2 x i64>*, i8)* @iidentity8_sse2, void (<2 x i64>*, <2 x i64>*, i8)* null, void (<2 x i64>*, <2 x i64>*, i8)* null]], [3 x [4 x void (<2 x i64>*, <2 x i64>*, i8)*]] [[4 x void (<2 x i64>*, <2 x i64>*, i8)*] [void (<2 x i64>*, <2 x i64>*, i8)* @idct16_low1_ssse3, void (<2 x i64>*, <2 x i64>*, i8)* @idct16_low8_ssse3, void (<2 x i64>*, <2 x i64>*, i8)* @idct16_sse2, void (<2 x i64>*, <2 x i64>*, i8)* null], [4 x void (<2 x i64>*, <2 x i64>*, i8)*] [void (<2 x i64>*, <2 x i64>*, i8)* @iadst16_low1_ssse3, void (<2 x i64>*, <2 x i64>*, i8)* @iadst16_low8_ssse3, void (<2 x i64>*, <2 x i64>*, i8)* @iadst16_sse2, void (<2 x i64>*, <2 x i64>*, i8)* null], [4 x void (<2 x i64>*, <2 x i64>*, i8)*] zeroinitializer], [3 x [4 x void (<2 x i64>*, <2 x i64>*, i8)*]] [[4 x void (<2 x i64>*, <2 x i64>*, i8)*] [void (<2 x i64>*, <2 x i64>*, i8)* @idct32_low1_ssse3, void (<2 x i64>*, <2 x i64>*, i8)* @idct32_low8_ssse3, void (<2 x i64>*, <2 x i64>*, i8)* @idct32_low16_ssse3, void (<2 x i64>*, <2 x i64>*, i8)* @idct32_sse2], [4 x void (<2 x i64>*, <2 x i64>*, i8)*] zeroinitializer, [4 x void (<2 x i64>*, <2 x i64>*, i8)*] zeroinitializer], [3 x [4 x void (<2 x i64>*, <2 x i64>*, i8)*]] [[4 x void (<2 x i64>*, <2 x i64>*, i8)*] [void (<2 x i64>*, <2 x i64>*, i8)* @idct64_low1_ssse3, void (<2 x i64>*, <2 x i64>*, i8)* @idct64_low8_ssse3, void (<2 x i64>*, <2 x i64>*, i8)* @idct64_low16_ssse3, void (<2 x i64>*, <2 x i64>*, i8)* @idct64_low32_ssse3], [4 x void (<2 x i64>*, <2 x i64>*, i8)*] zeroinitializer, [4 x void (<2 x i64>*, <2 x i64>*, i8)*] zeroinitializer]], align 16
@tx_size_wide_log2_eob = internal unnamed_addr constant [19 x i32] [i32 2, i32 3, i32 4, i32 5, i32 5, i32 2, i32 3, i32 3, i32 4, i32 4, i32 5, i32 5, i32 5, i32 2, i32 4, i32 3, i32 5, i32 4, i32 5], align 16
@av1_eob_to_eobxy_default = internal unnamed_addr constant [19 x i16*] [i16* null, i16* getelementptr inbounds ([8 x i16], [8 x i16]* @av1_eob_to_eobxy_8x8_default, i32 0, i32 0), i16* getelementptr inbounds ([16 x i16], [16 x i16]* @av1_eob_to_eobxy_16x16_default, i32 0, i32 0), i16* getelementptr inbounds ([32 x i16], [32 x i16]* @av1_eob_to_eobxy_32x32_default, i32 0, i32 0), i16* getelementptr inbounds ([32 x i16], [32 x i16]* @av1_eob_to_eobxy_32x32_default, i32 0, i32 0), i16* null, i16* null, i16* getelementptr inbounds ([16 x i16], [16 x i16]* @av1_eob_to_eobxy_8x16_default, i32 0, i32 0), i16* getelementptr inbounds ([8 x i16], [8 x i16]* @av1_eob_to_eobxy_16x8_default, i32 0, i32 0), i16* getelementptr inbounds ([32 x i16], [32 x i16]* @av1_eob_to_eobxy_16x32_default, i32 0, i32 0), i16* getelementptr inbounds ([16 x i16], [16 x i16]* @av1_eob_to_eobxy_32x16_default, i32 0, i32 0), i16* getelementptr inbounds ([32 x i16], [32 x i16]* @av1_eob_to_eobxy_32x32_default, i32 0, i32 0), i16* getelementptr inbounds ([32 x i16], [32 x i16]* @av1_eob_to_eobxy_32x32_default, i32 0, i32 0), i16* null, i16* null, i16* getelementptr inbounds ([32 x i16], [32 x i16]* @av1_eob_to_eobxy_8x32_default, i32 0, i32 0), i16* getelementptr inbounds ([8 x i16], [8 x i16]* @av1_eob_to_eobxy_32x8_default, i32 0, i32 0), i16* getelementptr inbounds ([32 x i16], [32 x i16]* @av1_eob_to_eobxy_16x32_default, i32 0, i32 0), i16* getelementptr inbounds ([16 x i16], [16 x i16]* @av1_eob_to_eobxy_32x16_default, i32 0, i32 0)], align 16
@av1_eob_to_eobxy_8x8_default = internal constant [8 x i16] [i16 1799, i16 1799, i16 1799, i16 1799, i16 1799, i16 1799, i16 1799, i16 1799], align 16
@av1_eob_to_eobxy_16x16_default = internal constant [16 x i16] [i16 1799, i16 1799, i16 3855, i16 3855, i16 3855, i16 3855, i16 3855, i16 3855, i16 3855, i16 3855, i16 3855, i16 3855, i16 3855, i16 3855, i16 3855, i16 3855], align 16
@av1_eob_to_eobxy_32x32_default = internal constant [32 x i16] [i16 1799, i16 3855, i16 3855, i16 3855, i16 7967, i16 7967, i16 7967, i16 7967, i16 7967, i16 7967, i16 7967, i16 7967, i16 7967, i16 7967, i16 7967, i16 7967, i16 7967, i16 7967, i16 7967, i16 7967, i16 7967, i16 7967, i16 7967, i16 7967, i16 7967, i16 7967, i16 7967, i16 7967, i16 7967, i16 7967, i16 7967, i16 7967], align 16
@av1_eob_to_eobxy_8x16_default = internal constant [16 x i16] [i16 1799, i16 1799, i16 1799, i16 1799, i16 1799, i16 3847, i16 3847, i16 3847, i16 3847, i16 3847, i16 3847, i16 3847, i16 3847, i16 3847, i16 3847, i16 3847], align 16
@av1_eob_to_eobxy_16x8_default = internal constant [8 x i16] [i16 1799, i16 1799, i16 1807, i16 1807, i16 1807, i16 1807, i16 1807, i16 1807], align 16
@av1_eob_to_eobxy_16x32_default = internal constant [32 x i16] [i16 1799, i16 1799, i16 3855, i16 3855, i16 3855, i16 3855, i16 3855, i16 3855, i16 3855, i16 7951, i16 7951, i16 7951, i16 7951, i16 7951, i16 7951, i16 7951, i16 7951, i16 7951, i16 7951, i16 7951, i16 7951, i16 7951, i16 7951, i16 7951, i16 7951, i16 7951, i16 7951, i16 7951, i16 7951, i16 7951, i16 7951, i16 7951], align 16
@av1_eob_to_eobxy_32x16_default = internal constant [16 x i16] [i16 1799, i16 3855, i16 3855, i16 3855, i16 3871, i16 3871, i16 3871, i16 3871, i16 3871, i16 3871, i16 3871, i16 3871, i16 3871, i16 3871, i16 3871, i16 3871], align 16
@av1_eob_to_eobxy_8x32_default = internal constant [32 x i16] [i16 1799, i16 1799, i16 1799, i16 1799, i16 1799, i16 3847, i16 3847, i16 3847, i16 3847, i16 3847, i16 3847, i16 3847, i16 3847, i16 7943, i16 7943, i16 7943, i16 7943, i16 7943, i16 7943, i16 7943, i16 7943, i16 7943, i16 7943, i16 7943, i16 7943, i16 7943, i16 7943, i16 7943, i16 7943, i16 7943, i16 7943, i16 7943], align 16
@av1_eob_to_eobxy_32x8_default = internal constant [8 x i16] [i16 1799, i16 1807, i16 1807, i16 1823, i16 1823, i16 1823, i16 1823, i16 1823], align 16
@NewSqrt2list = internal unnamed_addr constant [5 x i32] [i32 5793, i32 8192, i32 11586, i32 16384, i32 23172], align 16
@eob_fill = internal unnamed_addr constant [32 x i32] [i32 0, i32 7, i32 7, i32 7, i32 7, i32 7, i32 7, i32 7, i32 15, i32 15, i32 15, i32 15, i32 15, i32 15, i32 15, i32 15, i32 31, i32 31, i32 31, i32 31, i32 31, i32 31, i32 31, i32 31, i32 31, i32 31, i32 31, i32 31, i32 31, i32 31, i32 31, i32 31], align 16
@switch.table.av1_lowbd_inv_txfm2d_add_ssse3.2 = private unnamed_addr constant [6 x i32] [i32 1, i32 1, i32 1, i32 0, i32 0, i32 1], align 4
@switch.table.lowbd_inv_txfm2d_add_no_identity_ssse3 = private unnamed_addr constant [12 x i32] [i32 1, i32 0, i32 1, i32 0, i32 1, i32 0, i32 0, i32 0, i32 0, i32 0, i32 1, i32 0], align 4
@switch.table.lowbd_inv_txfm2d_add_no_identity_ssse3.3 = private unnamed_addr constant [12 x i32] [i32 0, i32 1, i32 1, i32 1, i32 0, i32 0, i32 0, i32 0, i32 0, i32 0, i32 0, i32 1], align 4

; Function Attrs: nounwind ssp uwtable
define hidden void @av1_lowbd_inv_txfm2d_add_ssse3(i32* readonly, i8*, i32, i8 zeroext, i8 zeroext, i32) local_unnamed_addr #0 {
  %7 = alloca [64 x <2 x i64>], align 16
  %8 = alloca [64 x <2 x i64>], align 16
  %9 = alloca [32 x <2 x i64>], align 16
  %10 = alloca [16 x <2 x i64>], align 16
  %11 = alloca [8 x <2 x i64>], align 16
  %12 = alloca [4 x <2 x i64>], align 16
  switch i8 %4, label %1780 [
    i8 0, label %13
    i8 5, label %195
    i8 6, label %516
    i8 13, label %774
    i8 14, label %1169
  ]

13:                                               ; preds = %6
  %14 = bitcast [4 x <2 x i64>]* %12 to i8*
  call void @llvm.lifetime.start.p0i8(i64 64, i8* nonnull %14) #9
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %14, i8 -86, i64 64, i1 false) #9
  %15 = load i8*, i8** getelementptr inbounds ([19 x i8*], [19 x i8*]* @av1_inv_txfm_shift_ls, i64 0, i64 0), align 16
  %16 = load i8, i8* getelementptr inbounds ([5 x [5 x i8]], [5 x [5 x i8]]* @av1_inv_cos_bit_row, i64 0, i64 0, i64 0), align 16
  %17 = load i8, i8* getelementptr inbounds ([5 x [5 x i8]], [5 x [5 x i8]]* @av1_inv_cos_bit_col, i64 0, i64 0, i64 0), align 16
  %18 = zext i8 %3 to i64
  %19 = getelementptr inbounds [16 x i8], [16 x i8]* @hitx_1d_tab, i64 0, i64 %18
  %20 = load i8, i8* %19, align 1
  %21 = zext i8 %20 to i64
  %22 = getelementptr inbounds [5 x [3 x void (<2 x i64>*, <2 x i64>*, i8)*]], [5 x [3 x void (<2 x i64>*, <2 x i64>*, i8)*]]* @lowbd_txfm_all_1d_w4_arr, i64 0, i64 0, i64 %21
  %23 = load void (<2 x i64>*, <2 x i64>*, i8)*, void (<2 x i64>*, <2 x i64>*, i8)** %22, align 8
  %24 = getelementptr inbounds [16 x i8], [16 x i8]* @vitx_1d_tab, i64 0, i64 %18
  %25 = load i8, i8* %24, align 1
  %26 = zext i8 %25 to i64
  %27 = getelementptr inbounds [5 x [3 x void (<2 x i64>*, <2 x i64>*, i8)*]], [5 x [3 x void (<2 x i64>*, <2 x i64>*, i8)*]]* @lowbd_txfm_all_1d_w4_arr, i64 0, i64 0, i64 %26
  %28 = load void (<2 x i64>*, <2 x i64>*, i8)*, void (<2 x i64>*, <2 x i64>*, i8)** %27, align 8
  switch i8 %3, label %32 [
    i8 6, label %31
    i8 15, label %30
    i8 7, label %30
    i8 5, label %30
    i8 14, label %29
    i8 8, label %29
    i8 4, label %29
  ]

29:                                               ; preds = %13, %13, %13
  br label %32

30:                                               ; preds = %13, %13, %13
  br label %32

31:                                               ; preds = %13
  br label %32

32:                                               ; preds = %31, %30, %29, %13
  %33 = phi i32 [ 1, %31 ], [ 0, %30 ], [ 1, %29 ], [ 0, %13 ]
  %34 = phi i1 [ false, %31 ], [ false, %30 ], [ true, %29 ], [ true, %13 ]
  %35 = bitcast i32* %0 to <4 x i32>*
  %36 = load <4 x i32>, <4 x i32>* %35, align 16
  %37 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %36, <4 x i32> undef) #9
  %38 = getelementptr inbounds [4 x <2 x i64>], [4 x <2 x i64>]* %12, i64 0, i64 0
  %39 = bitcast [4 x <2 x i64>]* %12 to <8 x i16>*
  %40 = getelementptr inbounds i32, i32* %0, i64 4
  %41 = bitcast i32* %40 to <4 x i32>*
  %42 = load <4 x i32>, <4 x i32>* %41, align 16
  %43 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %42, <4 x i32> undef) #9
  %44 = getelementptr inbounds [4 x <2 x i64>], [4 x <2 x i64>]* %12, i64 0, i64 1
  %45 = bitcast <2 x i64>* %44 to <8 x i16>*
  %46 = getelementptr inbounds i32, i32* %0, i64 8
  %47 = bitcast i32* %46 to <4 x i32>*
  %48 = load <4 x i32>, <4 x i32>* %47, align 16
  %49 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %48, <4 x i32> undef) #9
  %50 = getelementptr inbounds [4 x <2 x i64>], [4 x <2 x i64>]* %12, i64 0, i64 2
  %51 = bitcast <2 x i64>* %50 to <8 x i16>*
  %52 = getelementptr inbounds i32, i32* %0, i64 12
  %53 = bitcast i32* %52 to <4 x i32>*
  %54 = load <4 x i32>, <4 x i32>* %53, align 16
  %55 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %54, <4 x i32> undef) #9
  %56 = getelementptr inbounds [4 x <2 x i64>], [4 x <2 x i64>]* %12, i64 0, i64 3
  %57 = bitcast <2 x i64>* %56 to <8 x i16>*
  %58 = shufflevector <8 x i16> %37, <8 x i16> %43, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %59 = shufflevector <8 x i16> %49, <8 x i16> %55, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %60 = bitcast <8 x i16> %58 to <4 x i32>
  %61 = bitcast <8 x i16> %59 to <4 x i32>
  %62 = shufflevector <4 x i32> %60, <4 x i32> %61, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %63 = bitcast [4 x <2 x i64>]* %12 to <4 x i32>*
  store <4 x i32> %62, <4 x i32>* %63, align 16
  %64 = bitcast <4 x i32> %62 to <16 x i8>
  %65 = shufflevector <16 x i8> %64, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23>
  %66 = bitcast <2 x i64>* %44 to <16 x i8>*
  store <16 x i8> %65, <16 x i8>* %66, align 16
  %67 = shufflevector <4 x i32> %60, <4 x i32> %61, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %68 = bitcast <2 x i64>* %50 to <4 x i32>*
  store <4 x i32> %67, <4 x i32>* %68, align 16
  %69 = bitcast <4 x i32> %67 to <16 x i8>
  %70 = shufflevector <16 x i8> %69, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23>
  %71 = bitcast <2 x i64>* %56 to <16 x i8>*
  store <16 x i8> %70, <16 x i8>* %71, align 16
  call void %23(<2 x i64>* nonnull %38, <2 x i64>* nonnull %38, i8 signext %16) #9
  %72 = load <8 x i16>, <8 x i16>* %39, align 16
  %73 = load <8 x i16>, <8 x i16>* %45, align 16
  br i1 %34, label %85, label %74

74:                                               ; preds = %32
  %75 = load <8 x i16>, <8 x i16>* %51, align 16
  %76 = load <8 x i16>, <8 x i16>* %57, align 16
  %77 = shufflevector <8 x i16> %76, <8 x i16> %75, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %78 = shufflevector <8 x i16> %73, <8 x i16> %72, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %79 = bitcast <8 x i16> %77 to <4 x i32>
  %80 = bitcast <8 x i16> %78 to <4 x i32>
  %81 = shufflevector <4 x i32> %79, <4 x i32> %80, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  store <4 x i32> %81, <4 x i32>* %63, align 16
  %82 = bitcast <4 x i32> %81 to <16 x i8>
  %83 = shufflevector <16 x i8> %82, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23>
  store <16 x i8> %83, <16 x i8>* %66, align 16
  %84 = shufflevector <4 x i32> %79, <4 x i32> %80, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  br label %96

85:                                               ; preds = %32
  %86 = shufflevector <8 x i16> %72, <8 x i16> %73, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %87 = load <8 x i16>, <8 x i16>* %51, align 16
  %88 = load <8 x i16>, <8 x i16>* %57, align 16
  %89 = shufflevector <8 x i16> %87, <8 x i16> %88, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %90 = bitcast <8 x i16> %86 to <4 x i32>
  %91 = bitcast <8 x i16> %89 to <4 x i32>
  %92 = shufflevector <4 x i32> %90, <4 x i32> %91, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  store <4 x i32> %92, <4 x i32>* %63, align 16
  %93 = bitcast <4 x i32> %92 to <16 x i8>
  %94 = shufflevector <16 x i8> %93, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23>
  store <16 x i8> %94, <16 x i8>* %66, align 16
  %95 = shufflevector <4 x i32> %90, <4 x i32> %91, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  br label %96

96:                                               ; preds = %85, %74
  %97 = phi <4 x i32> [ %95, %85 ], [ %84, %74 ]
  store <4 x i32> %97, <4 x i32>* %68, align 16
  %98 = bitcast <4 x i32> %97 to <16 x i8>
  %99 = shufflevector <16 x i8> %98, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23>
  store <16 x i8> %99, <16 x i8>* %71, align 16
  call void %28(<2 x i64>* nonnull %38, <2 x i64>* nonnull %38, i8 signext %17) #9
  %100 = getelementptr inbounds i8, i8* %15, i64 1
  %101 = load i8, i8* %100, align 1
  %102 = sext i8 %101 to i32
  %103 = icmp slt i8 %101, 0
  br i1 %103, label %104, label %118

104:                                              ; preds = %96
  %105 = add nsw i32 %102, 15
  %106 = shl i32 1, %105
  %107 = trunc i32 %106 to i16
  %108 = insertelement <8 x i16> undef, i16 %107, i32 0
  %109 = shufflevector <8 x i16> %108, <8 x i16> undef, <8 x i32> zeroinitializer
  %110 = load <8 x i16>, <8 x i16>* %39, align 16
  %111 = call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %110, <8 x i16> %109) #9
  store <8 x i16> %111, <8 x i16>* %39, align 16
  %112 = load <8 x i16>, <8 x i16>* %45, align 16
  %113 = call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %112, <8 x i16> %109) #9
  store <8 x i16> %113, <8 x i16>* %45, align 16
  %114 = load <8 x i16>, <8 x i16>* %51, align 16
  %115 = call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %114, <8 x i16> %109) #9
  store <8 x i16> %115, <8 x i16>* %51, align 16
  %116 = load <8 x i16>, <8 x i16>* %57, align 16
  %117 = call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %116, <8 x i16> %109) #9
  br label %129

118:                                              ; preds = %96
  %119 = icmp eq i8 %101, 0
  br i1 %119, label %131, label %120

120:                                              ; preds = %118
  %121 = load <8 x i16>, <8 x i16>* %39, align 16
  %122 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %121, i32 %102) #9
  store <8 x i16> %122, <8 x i16>* %39, align 16
  %123 = load <8 x i16>, <8 x i16>* %45, align 16
  %124 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %123, i32 %102) #9
  store <8 x i16> %124, <8 x i16>* %45, align 16
  %125 = load <8 x i16>, <8 x i16>* %51, align 16
  %126 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %125, i32 %102) #9
  store <8 x i16> %126, <8 x i16>* %51, align 16
  %127 = load <8 x i16>, <8 x i16>* %57, align 16
  %128 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %127, i32 %102) #9
  br label %129

129:                                              ; preds = %120, %104
  %130 = phi <8 x i16> [ %117, %104 ], [ %128, %120 ]
  store <8 x i16> %130, <8 x i16>* %57, align 16
  br label %131

131:                                              ; preds = %129, %118
  %132 = icmp ne i32 %33, 0
  %133 = select i1 %132, i64 3, i64 0
  %134 = sext i32 %2 to i64
  %135 = bitcast i8* %1 to i32*
  %136 = load i32, i32* %135, align 4
  %137 = insertelement <4 x i32> <i32 undef, i32 0, i32 undef, i32 undef>, i32 %136, i32 0
  %138 = getelementptr inbounds [4 x <2 x i64>], [4 x <2 x i64>]* %12, i64 0, i64 %133
  %139 = bitcast <2 x i64>* %138 to <8 x i16>*
  %140 = load <8 x i16>, <8 x i16>* %139, align 16
  %141 = bitcast <4 x i32> %137 to <16 x i8>
  %142 = shufflevector <16 x i8> %141, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %143 = bitcast <16 x i8> %142 to <8 x i16>
  %144 = call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %140, <8 x i16> %143) #9
  %145 = call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %144, <8 x i16> undef) #9
  %146 = bitcast <16 x i8> %145 to <4 x i32>
  %147 = extractelement <4 x i32> %146, i32 0
  store i32 %147, i32* %135, align 4
  %148 = select i1 %132, i64 2, i64 1
  %149 = getelementptr inbounds i8, i8* %1, i64 %134
  %150 = bitcast i8* %149 to i32*
  %151 = load i32, i32* %150, align 4
  %152 = insertelement <4 x i32> <i32 undef, i32 0, i32 undef, i32 undef>, i32 %151, i32 0
  %153 = getelementptr inbounds [4 x <2 x i64>], [4 x <2 x i64>]* %12, i64 0, i64 %148
  %154 = bitcast <2 x i64>* %153 to <8 x i16>*
  %155 = load <8 x i16>, <8 x i16>* %154, align 16
  %156 = bitcast <4 x i32> %152 to <16 x i8>
  %157 = shufflevector <16 x i8> %156, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %158 = bitcast <16 x i8> %157 to <8 x i16>
  %159 = call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %155, <8 x i16> %158) #9
  %160 = call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %159, <8 x i16> undef) #9
  %161 = bitcast <16 x i8> %160 to <4 x i32>
  %162 = extractelement <4 x i32> %161, i32 0
  store i32 %162, i32* %150, align 4
  %163 = select i1 %132, i64 1, i64 2
  %164 = shl nsw i64 %134, 1
  %165 = getelementptr inbounds i8, i8* %1, i64 %164
  %166 = bitcast i8* %165 to i32*
  %167 = load i32, i32* %166, align 4
  %168 = insertelement <4 x i32> <i32 undef, i32 0, i32 undef, i32 undef>, i32 %167, i32 0
  %169 = getelementptr inbounds [4 x <2 x i64>], [4 x <2 x i64>]* %12, i64 0, i64 %163
  %170 = bitcast <2 x i64>* %169 to <8 x i16>*
  %171 = load <8 x i16>, <8 x i16>* %170, align 16
  %172 = bitcast <4 x i32> %168 to <16 x i8>
  %173 = shufflevector <16 x i8> %172, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %174 = bitcast <16 x i8> %173 to <8 x i16>
  %175 = call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %171, <8 x i16> %174) #9
  %176 = call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %175, <8 x i16> undef) #9
  %177 = bitcast <16 x i8> %176 to <4 x i32>
  %178 = extractelement <4 x i32> %177, i32 0
  store i32 %178, i32* %166, align 4
  %179 = select i1 %132, i64 0, i64 3
  %180 = mul nsw i64 %134, 3
  %181 = getelementptr inbounds i8, i8* %1, i64 %180
  %182 = bitcast i8* %181 to i32*
  %183 = load i32, i32* %182, align 4
  %184 = insertelement <4 x i32> <i32 undef, i32 0, i32 undef, i32 undef>, i32 %183, i32 0
  %185 = getelementptr inbounds [4 x <2 x i64>], [4 x <2 x i64>]* %12, i64 0, i64 %179
  %186 = bitcast <2 x i64>* %185 to <8 x i16>*
  %187 = load <8 x i16>, <8 x i16>* %186, align 16
  %188 = bitcast <4 x i32> %184 to <16 x i8>
  %189 = shufflevector <16 x i8> %188, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %190 = bitcast <16 x i8> %189 to <8 x i16>
  %191 = call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %187, <8 x i16> %190) #9
  %192 = call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %191, <8 x i16> undef) #9
  %193 = bitcast <16 x i8> %192 to <4 x i32>
  %194 = extractelement <4 x i32> %193, i32 0
  store i32 %194, i32* %182, align 4
  call void @llvm.lifetime.end.p0i8(i64 64, i8* nonnull %14) #9
  br label %2784

195:                                              ; preds = %6
  %196 = bitcast [8 x <2 x i64>]* %11 to i8*
  call void @llvm.lifetime.start.p0i8(i64 128, i8* nonnull %196) #9
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %196, i8 -86, i64 128, i1 false) #9
  %197 = load i8*, i8** getelementptr inbounds ([19 x i8*], [19 x i8*]* @av1_inv_txfm_shift_ls, i64 0, i64 5), align 8
  %198 = load i8, i8* getelementptr inbounds ([5 x [5 x i8]], [5 x [5 x i8]]* @av1_inv_cos_bit_row, i64 0, i64 0, i64 1), align 1
  %199 = load i8, i8* getelementptr inbounds ([5 x [5 x i8]], [5 x [5 x i8]]* @av1_inv_cos_bit_col, i64 0, i64 0, i64 1), align 1
  %200 = zext i8 %3 to i64
  %201 = getelementptr inbounds [16 x i8], [16 x i8]* @hitx_1d_tab, i64 0, i64 %200
  %202 = load i8, i8* %201, align 1
  %203 = zext i8 %202 to i64
  %204 = getelementptr inbounds [5 x [3 x void (<2 x i64>*, <2 x i64>*, i8)*]], [5 x [3 x void (<2 x i64>*, <2 x i64>*, i8)*]]* @lowbd_txfm_all_1d_w8_arr, i64 0, i64 0, i64 %203
  %205 = load void (<2 x i64>*, <2 x i64>*, i8)*, void (<2 x i64>*, <2 x i64>*, i8)** %204, align 8
  %206 = getelementptr inbounds [16 x i8], [16 x i8]* @vitx_1d_tab, i64 0, i64 %200
  %207 = load i8, i8* %206, align 1
  %208 = zext i8 %207 to i64
  %209 = getelementptr inbounds [5 x [3 x void (<2 x i64>*, <2 x i64>*, i8)*]], [5 x [3 x void (<2 x i64>*, <2 x i64>*, i8)*]]* @lowbd_txfm_all_1d_w4_arr, i64 0, i64 1, i64 %208
  %210 = load void (<2 x i64>*, <2 x i64>*, i8)*, void (<2 x i64>*, <2 x i64>*, i8)** %209, align 8
  switch i8 %3, label %214 [
    i8 6, label %213
    i8 15, label %212
    i8 7, label %212
    i8 5, label %212
    i8 14, label %211
    i8 8, label %211
    i8 4, label %211
  ]

211:                                              ; preds = %195, %195, %195
  br label %214

212:                                              ; preds = %195, %195, %195
  br label %214

213:                                              ; preds = %195
  br label %214

214:                                              ; preds = %213, %212, %211, %195
  %215 = phi i32 [ 1, %213 ], [ 0, %212 ], [ 1, %211 ], [ 0, %195 ]
  %216 = phi i1 [ false, %213 ], [ false, %212 ], [ true, %211 ], [ true, %195 ]
  %217 = bitcast i32* %0 to <4 x i32>*
  %218 = load <4 x i32>, <4 x i32>* %217, align 16
  %219 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %218, <4 x i32> undef) #9
  %220 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %11, i64 0, i64 0
  %221 = bitcast [8 x <2 x i64>]* %11 to <8 x i16>*
  %222 = getelementptr inbounds i32, i32* %0, i64 4
  %223 = bitcast i32* %222 to <4 x i32>*
  %224 = load <4 x i32>, <4 x i32>* %223, align 16
  %225 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %224, <4 x i32> undef) #9
  %226 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %11, i64 0, i64 1
  %227 = bitcast <2 x i64>* %226 to <8 x i16>*
  %228 = getelementptr inbounds i32, i32* %0, i64 8
  %229 = bitcast i32* %228 to <4 x i32>*
  %230 = load <4 x i32>, <4 x i32>* %229, align 16
  %231 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %230, <4 x i32> undef) #9
  %232 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %11, i64 0, i64 2
  %233 = bitcast <2 x i64>* %232 to <8 x i16>*
  %234 = getelementptr inbounds i32, i32* %0, i64 12
  %235 = bitcast i32* %234 to <4 x i32>*
  %236 = load <4 x i32>, <4 x i32>* %235, align 16
  %237 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %236, <4 x i32> undef) #9
  %238 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %11, i64 0, i64 3
  %239 = bitcast <2 x i64>* %238 to <8 x i16>*
  %240 = getelementptr inbounds i32, i32* %0, i64 16
  %241 = bitcast i32* %240 to <4 x i32>*
  %242 = load <4 x i32>, <4 x i32>* %241, align 16
  %243 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %242, <4 x i32> %242) #9
  %244 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %11, i64 0, i64 4
  %245 = bitcast <2 x i64>* %244 to <8 x i16>*
  store <8 x i16> %243, <8 x i16>* %245, align 16
  %246 = getelementptr inbounds i32, i32* %0, i64 20
  %247 = bitcast i32* %246 to <4 x i32>*
  %248 = load <4 x i32>, <4 x i32>* %247, align 16
  %249 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %248, <4 x i32> %248) #9
  %250 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %11, i64 0, i64 5
  %251 = bitcast <2 x i64>* %250 to <8 x i16>*
  store <8 x i16> %249, <8 x i16>* %251, align 16
  %252 = getelementptr inbounds i32, i32* %0, i64 24
  %253 = bitcast i32* %252 to <4 x i32>*
  %254 = load <4 x i32>, <4 x i32>* %253, align 16
  %255 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %254, <4 x i32> %254) #9
  %256 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %11, i64 0, i64 6
  %257 = bitcast <2 x i64>* %256 to <8 x i16>*
  store <8 x i16> %255, <8 x i16>* %257, align 16
  %258 = getelementptr inbounds i32, i32* %0, i64 28
  %259 = bitcast i32* %258 to <4 x i32>*
  %260 = load <4 x i32>, <4 x i32>* %259, align 16
  %261 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %260, <4 x i32> %260) #9
  %262 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %11, i64 0, i64 7
  %263 = bitcast <2 x i64>* %262 to <8 x i16>*
  store <8 x i16> %261, <8 x i16>* %263, align 16
  %264 = shufflevector <8 x i16> %219, <8 x i16> %225, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %265 = shufflevector <8 x i16> %231, <8 x i16> %237, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %266 = shufflevector <8 x i16> %243, <8 x i16> %249, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %267 = shufflevector <8 x i16> %255, <8 x i16> %261, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %268 = bitcast <8 x i16> %264 to <4 x i32>
  %269 = bitcast <8 x i16> %265 to <4 x i32>
  %270 = shufflevector <4 x i32> %268, <4 x i32> %269, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %271 = bitcast <4 x i32> %270 to <2 x i64>
  %272 = bitcast <8 x i16> %266 to <4 x i32>
  %273 = bitcast <8 x i16> %267 to <4 x i32>
  %274 = shufflevector <4 x i32> %272, <4 x i32> %273, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %275 = bitcast <4 x i32> %274 to <2 x i64>
  %276 = shufflevector <4 x i32> %268, <4 x i32> %269, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %277 = bitcast <4 x i32> %276 to <2 x i64>
  %278 = shufflevector <4 x i32> %272, <4 x i32> %273, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %279 = bitcast <4 x i32> %278 to <2 x i64>
  %280 = shufflevector <2 x i64> %271, <2 x i64> %275, <2 x i32> <i32 0, i32 2>
  %281 = shufflevector <2 x i64> %271, <2 x i64> %275, <2 x i32> <i32 1, i32 3>
  %282 = shufflevector <2 x i64> %277, <2 x i64> %279, <2 x i32> <i32 0, i32 2>
  %283 = shufflevector <2 x i64> %277, <2 x i64> %279, <2 x i32> <i32 1, i32 3>
  %284 = bitcast <2 x i64> %280 to <8 x i16>
  %285 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %284, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #9
  store <8 x i16> %285, <8 x i16>* %221, align 16
  %286 = bitcast <2 x i64> %281 to <8 x i16>
  %287 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %286, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #9
  store <8 x i16> %287, <8 x i16>* %227, align 16
  %288 = bitcast <2 x i64> %282 to <8 x i16>
  %289 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %288, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #9
  store <8 x i16> %289, <8 x i16>* %233, align 16
  %290 = bitcast <2 x i64> %283 to <8 x i16>
  %291 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %290, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #9
  store <8 x i16> %291, <8 x i16>* %239, align 16
  call void %205(<2 x i64>* nonnull %220, <2 x i64>* nonnull %220, i8 signext %198) #9
  %292 = load <8 x i16>, <8 x i16>* %221, align 16
  %293 = load <8 x i16>, <8 x i16>* %227, align 16
  br i1 %216, label %309, label %294

294:                                              ; preds = %214
  %295 = load <8 x i16>, <8 x i16>* %233, align 16
  %296 = load <8 x i16>, <8 x i16>* %239, align 16
  %297 = shufflevector <8 x i16> %296, <8 x i16> %295, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %298 = shufflevector <8 x i16> %293, <8 x i16> %292, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %299 = shufflevector <8 x i16> %296, <8 x i16> %295, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %300 = shufflevector <8 x i16> %293, <8 x i16> %292, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %301 = bitcast <8 x i16> %297 to <4 x i32>
  %302 = bitcast <8 x i16> %298 to <4 x i32>
  %303 = shufflevector <4 x i32> %301, <4 x i32> %302, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %304 = bitcast <8 x i16> %299 to <4 x i32>
  %305 = bitcast <8 x i16> %300 to <4 x i32>
  %306 = shufflevector <4 x i32> %304, <4 x i32> %305, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %307 = shufflevector <4 x i32> %301, <4 x i32> %302, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %308 = shufflevector <4 x i32> %304, <4 x i32> %305, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  br label %324

309:                                              ; preds = %214
  %310 = shufflevector <8 x i16> %292, <8 x i16> %293, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %311 = load <8 x i16>, <8 x i16>* %233, align 16
  %312 = load <8 x i16>, <8 x i16>* %239, align 16
  %313 = shufflevector <8 x i16> %311, <8 x i16> %312, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %314 = shufflevector <8 x i16> %292, <8 x i16> %293, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %315 = shufflevector <8 x i16> %311, <8 x i16> %312, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %316 = bitcast <8 x i16> %310 to <4 x i32>
  %317 = bitcast <8 x i16> %313 to <4 x i32>
  %318 = shufflevector <4 x i32> %316, <4 x i32> %317, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %319 = bitcast <8 x i16> %314 to <4 x i32>
  %320 = bitcast <8 x i16> %315 to <4 x i32>
  %321 = shufflevector <4 x i32> %319, <4 x i32> %320, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %322 = shufflevector <4 x i32> %316, <4 x i32> %317, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %323 = shufflevector <4 x i32> %319, <4 x i32> %320, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  br label %324

324:                                              ; preds = %309, %294
  %325 = phi <4 x i32> [ %323, %309 ], [ %308, %294 ]
  %326 = phi <4 x i32> [ %318, %309 ], [ %303, %294 ]
  %327 = phi <4 x i32> [ %322, %309 ], [ %307, %294 ]
  %328 = phi <4 x i32> [ %321, %309 ], [ %306, %294 ]
  %329 = bitcast <4 x i32> %328 to <2 x i64>
  %330 = bitcast <4 x i32> %327 to <2 x i64>
  %331 = bitcast <4 x i32> %326 to <2 x i64>
  %332 = bitcast <4 x i32> %325 to <2 x i64>
  %333 = insertelement <2 x i64> %331, i64 0, i32 1
  store <2 x i64> %333, <2 x i64>* %220, align 16
  %334 = shufflevector <2 x i64> %331, <2 x i64> <i64 undef, i64 0>, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %334, <2 x i64>* %226, align 16
  %335 = insertelement <2 x i64> %330, i64 0, i32 1
  store <2 x i64> %335, <2 x i64>* %232, align 16
  %336 = shufflevector <2 x i64> %330, <2 x i64> <i64 undef, i64 0>, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %336, <2 x i64>* %238, align 16
  %337 = insertelement <2 x i64> %329, i64 0, i32 1
  store <2 x i64> %337, <2 x i64>* %244, align 16
  %338 = shufflevector <2 x i64> %329, <2 x i64> <i64 undef, i64 0>, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %338, <2 x i64>* %250, align 16
  %339 = insertelement <2 x i64> %332, i64 0, i32 1
  store <2 x i64> %339, <2 x i64>* %256, align 16
  %340 = shufflevector <2 x i64> %332, <2 x i64> <i64 undef, i64 0>, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %340, <2 x i64>* %262, align 16
  call void %210(<2 x i64>* nonnull %220, <2 x i64>* nonnull %220, i8 signext %199) #9
  %341 = getelementptr inbounds i8, i8* %197, i64 1
  %342 = load i8, i8* %341, align 1
  %343 = sext i8 %342 to i32
  %344 = icmp slt i8 %342, 0
  br i1 %344, label %345, label %367

345:                                              ; preds = %324
  %346 = add nsw i32 %343, 15
  %347 = shl i32 1, %346
  %348 = trunc i32 %347 to i16
  %349 = insertelement <8 x i16> undef, i16 %348, i32 0
  %350 = shufflevector <8 x i16> %349, <8 x i16> undef, <8 x i32> zeroinitializer
  %351 = load <8 x i16>, <8 x i16>* %221, align 16
  %352 = call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %351, <8 x i16> %350) #9
  store <8 x i16> %352, <8 x i16>* %221, align 16
  %353 = load <8 x i16>, <8 x i16>* %227, align 16
  %354 = call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %353, <8 x i16> %350) #9
  store <8 x i16> %354, <8 x i16>* %227, align 16
  %355 = load <8 x i16>, <8 x i16>* %233, align 16
  %356 = call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %355, <8 x i16> %350) #9
  store <8 x i16> %356, <8 x i16>* %233, align 16
  %357 = load <8 x i16>, <8 x i16>* %239, align 16
  %358 = call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %357, <8 x i16> %350) #9
  store <8 x i16> %358, <8 x i16>* %239, align 16
  %359 = load <8 x i16>, <8 x i16>* %245, align 16
  %360 = call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %359, <8 x i16> %350) #9
  store <8 x i16> %360, <8 x i16>* %245, align 16
  %361 = load <8 x i16>, <8 x i16>* %251, align 16
  %362 = call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %361, <8 x i16> %350) #9
  store <8 x i16> %362, <8 x i16>* %251, align 16
  %363 = load <8 x i16>, <8 x i16>* %257, align 16
  %364 = call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %363, <8 x i16> %350) #9
  store <8 x i16> %364, <8 x i16>* %257, align 16
  %365 = load <8 x i16>, <8 x i16>* %263, align 16
  %366 = call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %365, <8 x i16> %350) #9
  br label %386

367:                                              ; preds = %324
  %368 = icmp eq i8 %342, 0
  br i1 %368, label %388, label %369

369:                                              ; preds = %367
  %370 = load <8 x i16>, <8 x i16>* %221, align 16
  %371 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %370, i32 %343) #9
  store <8 x i16> %371, <8 x i16>* %221, align 16
  %372 = load <8 x i16>, <8 x i16>* %227, align 16
  %373 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %372, i32 %343) #9
  store <8 x i16> %373, <8 x i16>* %227, align 16
  %374 = load <8 x i16>, <8 x i16>* %233, align 16
  %375 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %374, i32 %343) #9
  store <8 x i16> %375, <8 x i16>* %233, align 16
  %376 = load <8 x i16>, <8 x i16>* %239, align 16
  %377 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %376, i32 %343) #9
  store <8 x i16> %377, <8 x i16>* %239, align 16
  %378 = load <8 x i16>, <8 x i16>* %245, align 16
  %379 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %378, i32 %343) #9
  store <8 x i16> %379, <8 x i16>* %245, align 16
  %380 = load <8 x i16>, <8 x i16>* %251, align 16
  %381 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %380, i32 %343) #9
  store <8 x i16> %381, <8 x i16>* %251, align 16
  %382 = load <8 x i16>, <8 x i16>* %257, align 16
  %383 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %382, i32 %343) #9
  store <8 x i16> %383, <8 x i16>* %257, align 16
  %384 = load <8 x i16>, <8 x i16>* %263, align 16
  %385 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %384, i32 %343) #9
  br label %386

386:                                              ; preds = %369, %345
  %387 = phi <8 x i16> [ %366, %345 ], [ %385, %369 ]
  store <8 x i16> %387, <8 x i16>* %263, align 16
  br label %388

388:                                              ; preds = %386, %367
  %389 = icmp ne i32 %215, 0
  %390 = select i1 %389, i64 7, i64 0
  %391 = sext i32 %2 to i64
  %392 = bitcast i8* %1 to i32*
  %393 = load i32, i32* %392, align 4
  %394 = insertelement <4 x i32> <i32 undef, i32 0, i32 undef, i32 undef>, i32 %393, i32 0
  %395 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %11, i64 0, i64 %390
  %396 = bitcast <2 x i64>* %395 to <8 x i16>*
  %397 = load <8 x i16>, <8 x i16>* %396, align 16
  %398 = bitcast <4 x i32> %394 to <16 x i8>
  %399 = shufflevector <16 x i8> %398, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %400 = bitcast <16 x i8> %399 to <8 x i16>
  %401 = call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %397, <8 x i16> %400) #9
  %402 = call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %401, <8 x i16> undef) #9
  %403 = bitcast <16 x i8> %402 to <4 x i32>
  %404 = extractelement <4 x i32> %403, i32 0
  store i32 %404, i32* %392, align 4
  %405 = select i1 %389, i64 6, i64 1
  %406 = getelementptr inbounds i8, i8* %1, i64 %391
  %407 = bitcast i8* %406 to i32*
  %408 = load i32, i32* %407, align 4
  %409 = insertelement <4 x i32> <i32 undef, i32 0, i32 undef, i32 undef>, i32 %408, i32 0
  %410 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %11, i64 0, i64 %405
  %411 = bitcast <2 x i64>* %410 to <8 x i16>*
  %412 = load <8 x i16>, <8 x i16>* %411, align 16
  %413 = bitcast <4 x i32> %409 to <16 x i8>
  %414 = shufflevector <16 x i8> %413, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %415 = bitcast <16 x i8> %414 to <8 x i16>
  %416 = call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %412, <8 x i16> %415) #9
  %417 = call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %416, <8 x i16> undef) #9
  %418 = bitcast <16 x i8> %417 to <4 x i32>
  %419 = extractelement <4 x i32> %418, i32 0
  store i32 %419, i32* %407, align 4
  %420 = select i1 %389, i64 5, i64 2
  %421 = shl nsw i64 %391, 1
  %422 = getelementptr inbounds i8, i8* %1, i64 %421
  %423 = bitcast i8* %422 to i32*
  %424 = load i32, i32* %423, align 4
  %425 = insertelement <4 x i32> <i32 undef, i32 0, i32 undef, i32 undef>, i32 %424, i32 0
  %426 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %11, i64 0, i64 %420
  %427 = bitcast <2 x i64>* %426 to <8 x i16>*
  %428 = load <8 x i16>, <8 x i16>* %427, align 16
  %429 = bitcast <4 x i32> %425 to <16 x i8>
  %430 = shufflevector <16 x i8> %429, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %431 = bitcast <16 x i8> %430 to <8 x i16>
  %432 = call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %428, <8 x i16> %431) #9
  %433 = call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %432, <8 x i16> undef) #9
  %434 = bitcast <16 x i8> %433 to <4 x i32>
  %435 = extractelement <4 x i32> %434, i32 0
  store i32 %435, i32* %423, align 4
  %436 = select i1 %389, i64 4, i64 3
  %437 = mul nsw i64 %391, 3
  %438 = getelementptr inbounds i8, i8* %1, i64 %437
  %439 = bitcast i8* %438 to i32*
  %440 = load i32, i32* %439, align 4
  %441 = insertelement <4 x i32> <i32 undef, i32 0, i32 undef, i32 undef>, i32 %440, i32 0
  %442 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %11, i64 0, i64 %436
  %443 = bitcast <2 x i64>* %442 to <8 x i16>*
  %444 = load <8 x i16>, <8 x i16>* %443, align 16
  %445 = bitcast <4 x i32> %441 to <16 x i8>
  %446 = shufflevector <16 x i8> %445, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %447 = bitcast <16 x i8> %446 to <8 x i16>
  %448 = call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %444, <8 x i16> %447) #9
  %449 = call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %448, <8 x i16> undef) #9
  %450 = bitcast <16 x i8> %449 to <4 x i32>
  %451 = extractelement <4 x i32> %450, i32 0
  store i32 %451, i32* %439, align 4
  %452 = select i1 %389, i64 3, i64 4
  %453 = shl nsw i64 %391, 2
  %454 = getelementptr inbounds i8, i8* %1, i64 %453
  %455 = bitcast i8* %454 to i32*
  %456 = load i32, i32* %455, align 4
  %457 = insertelement <4 x i32> <i32 undef, i32 0, i32 undef, i32 undef>, i32 %456, i32 0
  %458 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %11, i64 0, i64 %452
  %459 = bitcast <2 x i64>* %458 to <8 x i16>*
  %460 = load <8 x i16>, <8 x i16>* %459, align 16
  %461 = bitcast <4 x i32> %457 to <16 x i8>
  %462 = shufflevector <16 x i8> %461, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %463 = bitcast <16 x i8> %462 to <8 x i16>
  %464 = call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %460, <8 x i16> %463) #9
  %465 = call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %464, <8 x i16> undef) #9
  %466 = bitcast <16 x i8> %465 to <4 x i32>
  %467 = extractelement <4 x i32> %466, i32 0
  store i32 %467, i32* %455, align 4
  %468 = select i1 %389, i64 2, i64 5
  %469 = mul nsw i64 %391, 5
  %470 = getelementptr inbounds i8, i8* %1, i64 %469
  %471 = bitcast i8* %470 to i32*
  %472 = load i32, i32* %471, align 4
  %473 = insertelement <4 x i32> <i32 undef, i32 0, i32 undef, i32 undef>, i32 %472, i32 0
  %474 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %11, i64 0, i64 %468
  %475 = bitcast <2 x i64>* %474 to <8 x i16>*
  %476 = load <8 x i16>, <8 x i16>* %475, align 16
  %477 = bitcast <4 x i32> %473 to <16 x i8>
  %478 = shufflevector <16 x i8> %477, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %479 = bitcast <16 x i8> %478 to <8 x i16>
  %480 = call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %476, <8 x i16> %479) #9
  %481 = call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %480, <8 x i16> undef) #9
  %482 = bitcast <16 x i8> %481 to <4 x i32>
  %483 = extractelement <4 x i32> %482, i32 0
  store i32 %483, i32* %471, align 4
  %484 = select i1 %389, i64 1, i64 6
  %485 = mul nsw i64 %391, 6
  %486 = getelementptr inbounds i8, i8* %1, i64 %485
  %487 = bitcast i8* %486 to i32*
  %488 = load i32, i32* %487, align 4
  %489 = insertelement <4 x i32> <i32 undef, i32 0, i32 undef, i32 undef>, i32 %488, i32 0
  %490 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %11, i64 0, i64 %484
  %491 = bitcast <2 x i64>* %490 to <8 x i16>*
  %492 = load <8 x i16>, <8 x i16>* %491, align 16
  %493 = bitcast <4 x i32> %489 to <16 x i8>
  %494 = shufflevector <16 x i8> %493, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %495 = bitcast <16 x i8> %494 to <8 x i16>
  %496 = call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %492, <8 x i16> %495) #9
  %497 = call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %496, <8 x i16> undef) #9
  %498 = bitcast <16 x i8> %497 to <4 x i32>
  %499 = extractelement <4 x i32> %498, i32 0
  store i32 %499, i32* %487, align 4
  %500 = select i1 %389, i64 0, i64 7
  %501 = mul nsw i64 %391, 7
  %502 = getelementptr inbounds i8, i8* %1, i64 %501
  %503 = bitcast i8* %502 to i32*
  %504 = load i32, i32* %503, align 4
  %505 = insertelement <4 x i32> <i32 undef, i32 0, i32 undef, i32 undef>, i32 %504, i32 0
  %506 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %11, i64 0, i64 %500
  %507 = bitcast <2 x i64>* %506 to <8 x i16>*
  %508 = load <8 x i16>, <8 x i16>* %507, align 16
  %509 = bitcast <4 x i32> %505 to <16 x i8>
  %510 = shufflevector <16 x i8> %509, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %511 = bitcast <16 x i8> %510 to <8 x i16>
  %512 = call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %508, <8 x i16> %511) #9
  %513 = call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %512, <8 x i16> undef) #9
  %514 = bitcast <16 x i8> %513 to <4 x i32>
  %515 = extractelement <4 x i32> %514, i32 0
  store i32 %515, i32* %503, align 4
  call void @llvm.lifetime.end.p0i8(i64 128, i8* nonnull %196) #9
  br label %2784

516:                                              ; preds = %6
  %517 = bitcast [8 x <2 x i64>]* %11 to i8*
  call void @llvm.lifetime.start.p0i8(i64 128, i8* nonnull %517) #9
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %517, i8 -86, i64 128, i1 false) #9
  %518 = load i8*, i8** getelementptr inbounds ([19 x i8*], [19 x i8*]* @av1_inv_txfm_shift_ls, i64 0, i64 6), align 16
  %519 = load i8, i8* getelementptr inbounds ([5 x [5 x i8]], [5 x [5 x i8]]* @av1_inv_cos_bit_row, i64 0, i64 1, i64 0), align 1
  %520 = load i8, i8* getelementptr inbounds ([5 x [5 x i8]], [5 x [5 x i8]]* @av1_inv_cos_bit_col, i64 0, i64 1, i64 0), align 1
  %521 = zext i8 %3 to i64
  %522 = getelementptr inbounds [16 x i8], [16 x i8]* @hitx_1d_tab, i64 0, i64 %521
  %523 = load i8, i8* %522, align 1
  %524 = zext i8 %523 to i64
  %525 = getelementptr inbounds [5 x [3 x void (<2 x i64>*, <2 x i64>*, i8)*]], [5 x [3 x void (<2 x i64>*, <2 x i64>*, i8)*]]* @lowbd_txfm_all_1d_w4_arr, i64 0, i64 1, i64 %524
  %526 = load void (<2 x i64>*, <2 x i64>*, i8)*, void (<2 x i64>*, <2 x i64>*, i8)** %525, align 8
  %527 = getelementptr inbounds [16 x i8], [16 x i8]* @vitx_1d_tab, i64 0, i64 %521
  %528 = load i8, i8* %527, align 1
  %529 = zext i8 %528 to i64
  %530 = getelementptr inbounds [5 x [3 x void (<2 x i64>*, <2 x i64>*, i8)*]], [5 x [3 x void (<2 x i64>*, <2 x i64>*, i8)*]]* @lowbd_txfm_all_1d_w8_arr, i64 0, i64 0, i64 %529
  %531 = load void (<2 x i64>*, <2 x i64>*, i8)*, void (<2 x i64>*, <2 x i64>*, i8)** %530, align 8
  switch i8 %3, label %535 [
    i8 6, label %534
    i8 15, label %533
    i8 7, label %533
    i8 5, label %533
    i8 14, label %532
    i8 8, label %532
    i8 4, label %532
  ]

532:                                              ; preds = %516, %516, %516
  br label %535

533:                                              ; preds = %516, %516, %516
  br label %535

534:                                              ; preds = %516
  br label %535

535:                                              ; preds = %534, %533, %532, %516
  %536 = phi i32 [ 1, %534 ], [ 0, %533 ], [ 1, %532 ], [ 0, %516 ]
  %537 = phi i1 [ false, %534 ], [ false, %533 ], [ true, %532 ], [ true, %516 ]
  %538 = bitcast i32* %0 to <4 x i32>*
  %539 = load <4 x i32>, <4 x i32>* %538, align 16
  %540 = getelementptr inbounds i32, i32* %0, i64 4
  %541 = bitcast i32* %540 to <4 x i32>*
  %542 = load <4 x i32>, <4 x i32>* %541, align 16
  %543 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %539, <4 x i32> %542) #9
  %544 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %11, i64 0, i64 0
  %545 = bitcast [8 x <2 x i64>]* %11 to <8 x i16>*
  %546 = getelementptr inbounds i32, i32* %0, i64 8
  %547 = bitcast i32* %546 to <4 x i32>*
  %548 = load <4 x i32>, <4 x i32>* %547, align 16
  %549 = getelementptr inbounds i32, i32* %0, i64 12
  %550 = bitcast i32* %549 to <4 x i32>*
  %551 = load <4 x i32>, <4 x i32>* %550, align 16
  %552 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %548, <4 x i32> %551) #9
  %553 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %11, i64 0, i64 1
  %554 = bitcast <2 x i64>* %553 to <8 x i16>*
  %555 = getelementptr inbounds i32, i32* %0, i64 16
  %556 = bitcast i32* %555 to <4 x i32>*
  %557 = load <4 x i32>, <4 x i32>* %556, align 16
  %558 = getelementptr inbounds i32, i32* %0, i64 20
  %559 = bitcast i32* %558 to <4 x i32>*
  %560 = load <4 x i32>, <4 x i32>* %559, align 16
  %561 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %557, <4 x i32> %560) #9
  %562 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %11, i64 0, i64 2
  %563 = bitcast <2 x i64>* %562 to <8 x i16>*
  %564 = getelementptr inbounds i32, i32* %0, i64 24
  %565 = bitcast i32* %564 to <4 x i32>*
  %566 = load <4 x i32>, <4 x i32>* %565, align 16
  %567 = getelementptr inbounds i32, i32* %0, i64 28
  %568 = bitcast i32* %567 to <4 x i32>*
  %569 = load <4 x i32>, <4 x i32>* %568, align 16
  %570 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %566, <4 x i32> %569) #9
  %571 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %11, i64 0, i64 3
  %572 = bitcast <2 x i64>* %571 to <8 x i16>*
  %573 = shufflevector <8 x i16> %543, <8 x i16> %552, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %574 = shufflevector <8 x i16> %561, <8 x i16> %570, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %575 = shufflevector <8 x i16> %543, <8 x i16> %552, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %576 = shufflevector <8 x i16> %561, <8 x i16> %570, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %577 = bitcast <8 x i16> %573 to <4 x i32>
  %578 = bitcast <8 x i16> %574 to <4 x i32>
  %579 = shufflevector <4 x i32> %577, <4 x i32> %578, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %580 = bitcast <4 x i32> %579 to <2 x i64>
  %581 = bitcast <8 x i16> %575 to <4 x i32>
  %582 = bitcast <8 x i16> %576 to <4 x i32>
  %583 = shufflevector <4 x i32> %581, <4 x i32> %582, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %584 = bitcast <4 x i32> %583 to <2 x i64>
  %585 = shufflevector <4 x i32> %577, <4 x i32> %578, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %586 = bitcast <4 x i32> %585 to <2 x i64>
  %587 = shufflevector <4 x i32> %581, <4 x i32> %582, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %588 = bitcast <4 x i32> %587 to <2 x i64>
  %589 = insertelement <2 x i64> %580, i64 0, i32 1
  %590 = shufflevector <2 x i64> %580, <2 x i64> <i64 undef, i64 0>, <2 x i32> <i32 1, i32 3>
  %591 = insertelement <2 x i64> %586, i64 0, i32 1
  %592 = shufflevector <2 x i64> %586, <2 x i64> <i64 undef, i64 0>, <2 x i32> <i32 1, i32 3>
  %593 = insertelement <2 x i64> %584, i64 0, i32 1
  %594 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %11, i64 0, i64 4
  %595 = shufflevector <2 x i64> %584, <2 x i64> <i64 undef, i64 0>, <2 x i32> <i32 1, i32 3>
  %596 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %11, i64 0, i64 5
  %597 = insertelement <2 x i64> %588, i64 0, i32 1
  %598 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %11, i64 0, i64 6
  %599 = shufflevector <2 x i64> %588, <2 x i64> <i64 undef, i64 0>, <2 x i32> <i32 1, i32 3>
  %600 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %11, i64 0, i64 7
  %601 = bitcast <2 x i64> %589 to <8 x i16>
  %602 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %601, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #9
  store <8 x i16> %602, <8 x i16>* %545, align 16
  %603 = bitcast <2 x i64> %590 to <8 x i16>
  %604 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %603, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #9
  store <8 x i16> %604, <8 x i16>* %554, align 16
  %605 = bitcast <2 x i64> %591 to <8 x i16>
  %606 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %605, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #9
  store <8 x i16> %606, <8 x i16>* %563, align 16
  %607 = bitcast <2 x i64> %592 to <8 x i16>
  %608 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %607, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #9
  store <8 x i16> %608, <8 x i16>* %572, align 16
  %609 = bitcast <2 x i64>* %594 to <8 x i16>*
  %610 = bitcast <2 x i64> %593 to <8 x i16>
  %611 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %610, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #9
  store <8 x i16> %611, <8 x i16>* %609, align 16
  %612 = bitcast <2 x i64>* %596 to <8 x i16>*
  %613 = bitcast <2 x i64> %595 to <8 x i16>
  %614 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %613, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #9
  store <8 x i16> %614, <8 x i16>* %612, align 16
  %615 = bitcast <2 x i64>* %598 to <8 x i16>*
  %616 = bitcast <2 x i64> %597 to <8 x i16>
  %617 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %616, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #9
  store <8 x i16> %617, <8 x i16>* %615, align 16
  %618 = bitcast <2 x i64>* %600 to <8 x i16>*
  %619 = bitcast <2 x i64> %599 to <8 x i16>
  %620 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %619, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #9
  store <8 x i16> %620, <8 x i16>* %618, align 16
  call void %526(<2 x i64>* nonnull %544, <2 x i64>* nonnull %544, i8 signext %519) #9
  %621 = load <8 x i16>, <8 x i16>* %545, align 16
  %622 = load <8 x i16>, <8 x i16>* %554, align 16
  br i1 %537, label %650, label %623

623:                                              ; preds = %535
  %624 = load <8 x i16>, <8 x i16>* %563, align 16
  %625 = load <8 x i16>, <8 x i16>* %572, align 16
  %626 = load <8 x i16>, <8 x i16>* %609, align 16
  %627 = load <8 x i16>, <8 x i16>* %612, align 16
  %628 = load <8 x i16>, <8 x i16>* %615, align 16
  %629 = load <8 x i16>, <8 x i16>* %618, align 16
  %630 = shufflevector <8 x i16> %629, <8 x i16> %628, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %631 = shufflevector <8 x i16> %627, <8 x i16> %626, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %632 = shufflevector <8 x i16> %625, <8 x i16> %624, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %633 = shufflevector <8 x i16> %622, <8 x i16> %621, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %634 = bitcast <8 x i16> %630 to <4 x i32>
  %635 = bitcast <8 x i16> %631 to <4 x i32>
  %636 = shufflevector <4 x i32> %634, <4 x i32> %635, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %637 = bitcast <4 x i32> %636 to <2 x i64>
  %638 = bitcast <8 x i16> %632 to <4 x i32>
  %639 = bitcast <8 x i16> %633 to <4 x i32>
  %640 = shufflevector <4 x i32> %638, <4 x i32> %639, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %641 = bitcast <4 x i32> %640 to <2 x i64>
  %642 = shufflevector <4 x i32> %634, <4 x i32> %635, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %643 = bitcast <4 x i32> %642 to <2 x i64>
  %644 = shufflevector <4 x i32> %638, <4 x i32> %639, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %645 = bitcast <4 x i32> %644 to <2 x i64>
  %646 = shufflevector <2 x i64> %637, <2 x i64> %641, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %646, <2 x i64>* %544, align 16
  %647 = shufflevector <2 x i64> %637, <2 x i64> %641, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %647, <2 x i64>* %553, align 16
  %648 = shufflevector <2 x i64> %643, <2 x i64> %645, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %648, <2 x i64>* %562, align 16
  %649 = shufflevector <2 x i64> %643, <2 x i64> %645, <2 x i32> <i32 1, i32 3>
  br label %677

650:                                              ; preds = %535
  %651 = shufflevector <8 x i16> %621, <8 x i16> %622, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %652 = load <8 x i16>, <8 x i16>* %563, align 16
  %653 = load <8 x i16>, <8 x i16>* %572, align 16
  %654 = shufflevector <8 x i16> %652, <8 x i16> %653, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %655 = load <8 x i16>, <8 x i16>* %609, align 16
  %656 = load <8 x i16>, <8 x i16>* %612, align 16
  %657 = shufflevector <8 x i16> %655, <8 x i16> %656, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %658 = load <8 x i16>, <8 x i16>* %615, align 16
  %659 = load <8 x i16>, <8 x i16>* %618, align 16
  %660 = shufflevector <8 x i16> %658, <8 x i16> %659, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %661 = bitcast <8 x i16> %651 to <4 x i32>
  %662 = bitcast <8 x i16> %654 to <4 x i32>
  %663 = shufflevector <4 x i32> %661, <4 x i32> %662, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %664 = bitcast <4 x i32> %663 to <2 x i64>
  %665 = bitcast <8 x i16> %657 to <4 x i32>
  %666 = bitcast <8 x i16> %660 to <4 x i32>
  %667 = shufflevector <4 x i32> %665, <4 x i32> %666, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %668 = bitcast <4 x i32> %667 to <2 x i64>
  %669 = shufflevector <4 x i32> %661, <4 x i32> %662, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %670 = bitcast <4 x i32> %669 to <2 x i64>
  %671 = shufflevector <4 x i32> %665, <4 x i32> %666, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %672 = bitcast <4 x i32> %671 to <2 x i64>
  %673 = shufflevector <2 x i64> %664, <2 x i64> %668, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %673, <2 x i64>* %544, align 16
  %674 = shufflevector <2 x i64> %664, <2 x i64> %668, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %674, <2 x i64>* %553, align 16
  %675 = shufflevector <2 x i64> %670, <2 x i64> %672, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %675, <2 x i64>* %562, align 16
  %676 = shufflevector <2 x i64> %670, <2 x i64> %672, <2 x i32> <i32 1, i32 3>
  br label %677

677:                                              ; preds = %650, %623
  %678 = phi <2 x i64> [ %649, %623 ], [ %676, %650 ]
  store <2 x i64> %678, <2 x i64>* %571, align 16
  call void %531(<2 x i64>* nonnull %544, <2 x i64>* nonnull %544, i8 signext %520) #9
  %679 = getelementptr inbounds i8, i8* %518, i64 1
  %680 = load i8, i8* %679, align 1
  %681 = sext i8 %680 to i32
  %682 = icmp slt i8 %680, 0
  br i1 %682, label %683, label %697

683:                                              ; preds = %677
  %684 = add nsw i32 %681, 15
  %685 = shl i32 1, %684
  %686 = trunc i32 %685 to i16
  %687 = insertelement <8 x i16> undef, i16 %686, i32 0
  %688 = shufflevector <8 x i16> %687, <8 x i16> undef, <8 x i32> zeroinitializer
  %689 = load <8 x i16>, <8 x i16>* %545, align 16
  %690 = call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %689, <8 x i16> %688) #9
  store <8 x i16> %690, <8 x i16>* %545, align 16
  %691 = load <8 x i16>, <8 x i16>* %554, align 16
  %692 = call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %691, <8 x i16> %688) #9
  store <8 x i16> %692, <8 x i16>* %554, align 16
  %693 = load <8 x i16>, <8 x i16>* %563, align 16
  %694 = call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %693, <8 x i16> %688) #9
  store <8 x i16> %694, <8 x i16>* %563, align 16
  %695 = load <8 x i16>, <8 x i16>* %572, align 16
  %696 = call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %695, <8 x i16> %688) #9
  br label %708

697:                                              ; preds = %677
  %698 = icmp eq i8 %680, 0
  br i1 %698, label %710, label %699

699:                                              ; preds = %697
  %700 = load <8 x i16>, <8 x i16>* %545, align 16
  %701 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %700, i32 %681) #9
  store <8 x i16> %701, <8 x i16>* %545, align 16
  %702 = load <8 x i16>, <8 x i16>* %554, align 16
  %703 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %702, i32 %681) #9
  store <8 x i16> %703, <8 x i16>* %554, align 16
  %704 = load <8 x i16>, <8 x i16>* %563, align 16
  %705 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %704, i32 %681) #9
  store <8 x i16> %705, <8 x i16>* %563, align 16
  %706 = load <8 x i16>, <8 x i16>* %572, align 16
  %707 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %706, i32 %681) #9
  br label %708

708:                                              ; preds = %699, %683
  %709 = phi <8 x i16> [ %696, %683 ], [ %707, %699 ]
  store <8 x i16> %709, <8 x i16>* %572, align 16
  br label %710

710:                                              ; preds = %708, %697
  %711 = icmp ne i32 %536, 0
  %712 = select i1 %711, i64 3, i64 0
  %713 = sext i32 %2 to i64
  %714 = bitcast i8* %1 to i64*
  %715 = load i64, i64* %714, align 1
  %716 = insertelement <2 x i64> undef, i64 %715, i32 0
  %717 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %11, i64 0, i64 %712
  %718 = bitcast <2 x i64>* %717 to <8 x i16>*
  %719 = load <8 x i16>, <8 x i16>* %718, align 16
  %720 = bitcast <2 x i64> %716 to <16 x i8>
  %721 = shufflevector <16 x i8> %720, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %722 = bitcast <16 x i8> %721 to <8 x i16>
  %723 = call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %719, <8 x i16> %722) #9
  %724 = call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %723, <8 x i16> undef) #9
  %725 = bitcast <16 x i8> %724 to <2 x i64>
  %726 = extractelement <2 x i64> %725, i32 0
  store i64 %726, i64* %714, align 1
  %727 = select i1 %711, i64 2, i64 1
  %728 = getelementptr inbounds i8, i8* %1, i64 %713
  %729 = bitcast i8* %728 to i64*
  %730 = load i64, i64* %729, align 1
  %731 = insertelement <2 x i64> undef, i64 %730, i32 0
  %732 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %11, i64 0, i64 %727
  %733 = bitcast <2 x i64>* %732 to <8 x i16>*
  %734 = load <8 x i16>, <8 x i16>* %733, align 16
  %735 = bitcast <2 x i64> %731 to <16 x i8>
  %736 = shufflevector <16 x i8> %735, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %737 = bitcast <16 x i8> %736 to <8 x i16>
  %738 = call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %734, <8 x i16> %737) #9
  %739 = call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %738, <8 x i16> undef) #9
  %740 = bitcast <16 x i8> %739 to <2 x i64>
  %741 = extractelement <2 x i64> %740, i32 0
  store i64 %741, i64* %729, align 1
  %742 = select i1 %711, i64 1, i64 2
  %743 = shl nsw i64 %713, 1
  %744 = getelementptr inbounds i8, i8* %1, i64 %743
  %745 = bitcast i8* %744 to i64*
  %746 = load i64, i64* %745, align 1
  %747 = insertelement <2 x i64> undef, i64 %746, i32 0
  %748 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %11, i64 0, i64 %742
  %749 = bitcast <2 x i64>* %748 to <8 x i16>*
  %750 = load <8 x i16>, <8 x i16>* %749, align 16
  %751 = bitcast <2 x i64> %747 to <16 x i8>
  %752 = shufflevector <16 x i8> %751, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %753 = bitcast <16 x i8> %752 to <8 x i16>
  %754 = call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %750, <8 x i16> %753) #9
  %755 = call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %754, <8 x i16> undef) #9
  %756 = bitcast <16 x i8> %755 to <2 x i64>
  %757 = extractelement <2 x i64> %756, i32 0
  store i64 %757, i64* %745, align 1
  %758 = select i1 %711, i64 0, i64 3
  %759 = mul nsw i64 %713, 3
  %760 = getelementptr inbounds i8, i8* %1, i64 %759
  %761 = bitcast i8* %760 to i64*
  %762 = load i64, i64* %761, align 1
  %763 = insertelement <2 x i64> undef, i64 %762, i32 0
  %764 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %11, i64 0, i64 %758
  %765 = bitcast <2 x i64>* %764 to <8 x i16>*
  %766 = load <8 x i16>, <8 x i16>* %765, align 16
  %767 = bitcast <2 x i64> %763 to <16 x i8>
  %768 = shufflevector <16 x i8> %767, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %769 = bitcast <16 x i8> %768 to <8 x i16>
  %770 = call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %766, <8 x i16> %769) #9
  %771 = call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %770, <8 x i16> undef) #9
  %772 = bitcast <16 x i8> %771 to <2 x i64>
  %773 = extractelement <2 x i64> %772, i32 0
  store i64 %773, i64* %761, align 1
  call void @llvm.lifetime.end.p0i8(i64 128, i8* nonnull %517) #9
  br label %2784

774:                                              ; preds = %6
  %775 = bitcast [16 x <2 x i64>]* %10 to i8*
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %775) #9
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %775, i8 -86, i64 256, i1 false) #9
  %776 = load i8*, i8** getelementptr inbounds ([19 x i8*], [19 x i8*]* @av1_inv_txfm_shift_ls, i64 0, i64 13), align 8
  %777 = load i8, i8* getelementptr inbounds ([5 x [5 x i8]], [5 x [5 x i8]]* @av1_inv_cos_bit_row, i64 0, i64 0, i64 2), align 2
  %778 = load i8, i8* getelementptr inbounds ([5 x [5 x i8]], [5 x [5 x i8]]* @av1_inv_cos_bit_col, i64 0, i64 0, i64 2), align 2
  %779 = zext i8 %3 to i64
  %780 = getelementptr inbounds [16 x i8], [16 x i8]* @hitx_1d_tab, i64 0, i64 %779
  %781 = load i8, i8* %780, align 1
  %782 = zext i8 %781 to i64
  %783 = getelementptr inbounds [5 x [3 x void (<2 x i64>*, <2 x i64>*, i8)*]], [5 x [3 x void (<2 x i64>*, <2 x i64>*, i8)*]]* @lowbd_txfm_all_1d_w8_arr, i64 0, i64 0, i64 %782
  %784 = load void (<2 x i64>*, <2 x i64>*, i8)*, void (<2 x i64>*, <2 x i64>*, i8)** %783, align 8
  %785 = getelementptr inbounds [16 x i8], [16 x i8]* @vitx_1d_tab, i64 0, i64 %779
  %786 = load i8, i8* %785, align 1
  %787 = zext i8 %786 to i64
  %788 = getelementptr inbounds [5 x [3 x void (<2 x i64>*, <2 x i64>*, i8)*]], [5 x [3 x void (<2 x i64>*, <2 x i64>*, i8)*]]* @lowbd_txfm_all_1d_w4_arr, i64 0, i64 2, i64 %787
  %789 = load void (<2 x i64>*, <2 x i64>*, i8)*, void (<2 x i64>*, <2 x i64>*, i8)** %788, align 8
  switch i8 %3, label %793 [
    i8 6, label %792
    i8 15, label %791
    i8 7, label %791
    i8 5, label %791
    i8 14, label %790
    i8 8, label %790
    i8 4, label %790
  ]

790:                                              ; preds = %774, %774, %774
  br label %793

791:                                              ; preds = %774, %774, %774
  br label %793

792:                                              ; preds = %774
  br label %793

793:                                              ; preds = %792, %791, %790, %774
  %794 = phi i32 [ 1, %792 ], [ 0, %791 ], [ 1, %790 ], [ 0, %774 ]
  %795 = phi i1 [ false, %792 ], [ false, %791 ], [ true, %790 ], [ true, %774 ]
  %796 = icmp eq void (<2 x i64>*, <2 x i64>*, i8)* %784, @iidentity4_ssse3
  br label %964

797:                                              ; preds = %1149
  %798 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %10, i64 0, i64 0
  call void %789(<2 x i64>* nonnull %798, <2 x i64>* nonnull %798, i8 signext %778) #9
  %799 = getelementptr inbounds i8, i8* %776, i64 1
  %800 = load i8, i8* %799, align 1
  %801 = sext i8 %800 to i32
  %802 = icmp slt i8 %800, 0
  br i1 %802, label %803, label %872

803:                                              ; preds = %797
  %804 = add nsw i32 %801, 15
  %805 = shl i32 1, %804
  %806 = trunc i32 %805 to i16
  %807 = insertelement <8 x i16> undef, i16 %806, i32 0
  %808 = shufflevector <8 x i16> %807, <8 x i16> undef, <8 x i32> zeroinitializer
  %809 = bitcast [16 x <2 x i64>]* %10 to <8 x i16>*
  %810 = load <8 x i16>, <8 x i16>* %809, align 16
  %811 = call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %810, <8 x i16> %808) #9
  store <8 x i16> %811, <8 x i16>* %809, align 16
  %812 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %10, i64 0, i64 1
  %813 = bitcast <2 x i64>* %812 to <8 x i16>*
  %814 = load <8 x i16>, <8 x i16>* %813, align 16
  %815 = call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %814, <8 x i16> %808) #9
  store <8 x i16> %815, <8 x i16>* %813, align 16
  %816 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %10, i64 0, i64 2
  %817 = bitcast <2 x i64>* %816 to <8 x i16>*
  %818 = load <8 x i16>, <8 x i16>* %817, align 16
  %819 = call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %818, <8 x i16> %808) #9
  store <8 x i16> %819, <8 x i16>* %817, align 16
  %820 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %10, i64 0, i64 3
  %821 = bitcast <2 x i64>* %820 to <8 x i16>*
  %822 = load <8 x i16>, <8 x i16>* %821, align 16
  %823 = call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %822, <8 x i16> %808) #9
  store <8 x i16> %823, <8 x i16>* %821, align 16
  %824 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %10, i64 0, i64 4
  %825 = bitcast <2 x i64>* %824 to <8 x i16>*
  %826 = load <8 x i16>, <8 x i16>* %825, align 16
  %827 = call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %826, <8 x i16> %808) #9
  store <8 x i16> %827, <8 x i16>* %825, align 16
  %828 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %10, i64 0, i64 5
  %829 = bitcast <2 x i64>* %828 to <8 x i16>*
  %830 = load <8 x i16>, <8 x i16>* %829, align 16
  %831 = call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %830, <8 x i16> %808) #9
  store <8 x i16> %831, <8 x i16>* %829, align 16
  %832 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %10, i64 0, i64 6
  %833 = bitcast <2 x i64>* %832 to <8 x i16>*
  %834 = load <8 x i16>, <8 x i16>* %833, align 16
  %835 = call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %834, <8 x i16> %808) #9
  store <8 x i16> %835, <8 x i16>* %833, align 16
  %836 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %10, i64 0, i64 7
  %837 = bitcast <2 x i64>* %836 to <8 x i16>*
  %838 = load <8 x i16>, <8 x i16>* %837, align 16
  %839 = call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %838, <8 x i16> %808) #9
  store <8 x i16> %839, <8 x i16>* %837, align 16
  %840 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %10, i64 0, i64 8
  %841 = bitcast <2 x i64>* %840 to <8 x i16>*
  %842 = load <8 x i16>, <8 x i16>* %841, align 16
  %843 = call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %842, <8 x i16> %808) #9
  store <8 x i16> %843, <8 x i16>* %841, align 16
  %844 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %10, i64 0, i64 9
  %845 = bitcast <2 x i64>* %844 to <8 x i16>*
  %846 = load <8 x i16>, <8 x i16>* %845, align 16
  %847 = call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %846, <8 x i16> %808) #9
  store <8 x i16> %847, <8 x i16>* %845, align 16
  %848 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %10, i64 0, i64 10
  %849 = bitcast <2 x i64>* %848 to <8 x i16>*
  %850 = load <8 x i16>, <8 x i16>* %849, align 16
  %851 = call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %850, <8 x i16> %808) #9
  store <8 x i16> %851, <8 x i16>* %849, align 16
  %852 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %10, i64 0, i64 11
  %853 = bitcast <2 x i64>* %852 to <8 x i16>*
  %854 = load <8 x i16>, <8 x i16>* %853, align 16
  %855 = call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %854, <8 x i16> %808) #9
  store <8 x i16> %855, <8 x i16>* %853, align 16
  %856 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %10, i64 0, i64 12
  %857 = bitcast <2 x i64>* %856 to <8 x i16>*
  %858 = load <8 x i16>, <8 x i16>* %857, align 16
  %859 = call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %858, <8 x i16> %808) #9
  store <8 x i16> %859, <8 x i16>* %857, align 16
  %860 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %10, i64 0, i64 13
  %861 = bitcast <2 x i64>* %860 to <8 x i16>*
  %862 = load <8 x i16>, <8 x i16>* %861, align 16
  %863 = call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %862, <8 x i16> %808) #9
  store <8 x i16> %863, <8 x i16>* %861, align 16
  %864 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %10, i64 0, i64 14
  %865 = bitcast <2 x i64>* %864 to <8 x i16>*
  %866 = load <8 x i16>, <8 x i16>* %865, align 16
  %867 = call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %866, <8 x i16> %808) #9
  store <8 x i16> %867, <8 x i16>* %865, align 16
  %868 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %10, i64 0, i64 15
  %869 = bitcast <2 x i64>* %868 to <8 x i16>*
  %870 = load <8 x i16>, <8 x i16>* %869, align 16
  %871 = call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %870, <8 x i16> %808) #9
  store <8 x i16> %871, <8 x i16>* %869, align 16
  br label %938

872:                                              ; preds = %797
  %873 = icmp eq i8 %800, 0
  br i1 %873, label %938, label %874

874:                                              ; preds = %872
  %875 = bitcast [16 x <2 x i64>]* %10 to <8 x i16>*
  %876 = load <8 x i16>, <8 x i16>* %875, align 16
  %877 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %876, i32 %801) #9
  store <8 x i16> %877, <8 x i16>* %875, align 16
  %878 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %10, i64 0, i64 1
  %879 = bitcast <2 x i64>* %878 to <8 x i16>*
  %880 = load <8 x i16>, <8 x i16>* %879, align 16
  %881 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %880, i32 %801) #9
  store <8 x i16> %881, <8 x i16>* %879, align 16
  %882 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %10, i64 0, i64 2
  %883 = bitcast <2 x i64>* %882 to <8 x i16>*
  %884 = load <8 x i16>, <8 x i16>* %883, align 16
  %885 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %884, i32 %801) #9
  store <8 x i16> %885, <8 x i16>* %883, align 16
  %886 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %10, i64 0, i64 3
  %887 = bitcast <2 x i64>* %886 to <8 x i16>*
  %888 = load <8 x i16>, <8 x i16>* %887, align 16
  %889 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %888, i32 %801) #9
  store <8 x i16> %889, <8 x i16>* %887, align 16
  %890 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %10, i64 0, i64 4
  %891 = bitcast <2 x i64>* %890 to <8 x i16>*
  %892 = load <8 x i16>, <8 x i16>* %891, align 16
  %893 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %892, i32 %801) #9
  store <8 x i16> %893, <8 x i16>* %891, align 16
  %894 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %10, i64 0, i64 5
  %895 = bitcast <2 x i64>* %894 to <8 x i16>*
  %896 = load <8 x i16>, <8 x i16>* %895, align 16
  %897 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %896, i32 %801) #9
  store <8 x i16> %897, <8 x i16>* %895, align 16
  %898 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %10, i64 0, i64 6
  %899 = bitcast <2 x i64>* %898 to <8 x i16>*
  %900 = load <8 x i16>, <8 x i16>* %899, align 16
  %901 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %900, i32 %801) #9
  store <8 x i16> %901, <8 x i16>* %899, align 16
  %902 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %10, i64 0, i64 7
  %903 = bitcast <2 x i64>* %902 to <8 x i16>*
  %904 = load <8 x i16>, <8 x i16>* %903, align 16
  %905 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %904, i32 %801) #9
  store <8 x i16> %905, <8 x i16>* %903, align 16
  %906 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %10, i64 0, i64 8
  %907 = bitcast <2 x i64>* %906 to <8 x i16>*
  %908 = load <8 x i16>, <8 x i16>* %907, align 16
  %909 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %908, i32 %801) #9
  store <8 x i16> %909, <8 x i16>* %907, align 16
  %910 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %10, i64 0, i64 9
  %911 = bitcast <2 x i64>* %910 to <8 x i16>*
  %912 = load <8 x i16>, <8 x i16>* %911, align 16
  %913 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %912, i32 %801) #9
  store <8 x i16> %913, <8 x i16>* %911, align 16
  %914 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %10, i64 0, i64 10
  %915 = bitcast <2 x i64>* %914 to <8 x i16>*
  %916 = load <8 x i16>, <8 x i16>* %915, align 16
  %917 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %916, i32 %801) #9
  store <8 x i16> %917, <8 x i16>* %915, align 16
  %918 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %10, i64 0, i64 11
  %919 = bitcast <2 x i64>* %918 to <8 x i16>*
  %920 = load <8 x i16>, <8 x i16>* %919, align 16
  %921 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %920, i32 %801) #9
  store <8 x i16> %921, <8 x i16>* %919, align 16
  %922 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %10, i64 0, i64 12
  %923 = bitcast <2 x i64>* %922 to <8 x i16>*
  %924 = load <8 x i16>, <8 x i16>* %923, align 16
  %925 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %924, i32 %801) #9
  store <8 x i16> %925, <8 x i16>* %923, align 16
  %926 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %10, i64 0, i64 13
  %927 = bitcast <2 x i64>* %926 to <8 x i16>*
  %928 = load <8 x i16>, <8 x i16>* %927, align 16
  %929 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %928, i32 %801) #9
  store <8 x i16> %929, <8 x i16>* %927, align 16
  %930 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %10, i64 0, i64 14
  %931 = bitcast <2 x i64>* %930 to <8 x i16>*
  %932 = load <8 x i16>, <8 x i16>* %931, align 16
  %933 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %932, i32 %801) #9
  store <8 x i16> %933, <8 x i16>* %931, align 16
  %934 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %10, i64 0, i64 15
  %935 = bitcast <2 x i64>* %934 to <8 x i16>*
  %936 = load <8 x i16>, <8 x i16>* %935, align 16
  %937 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %936, i32 %801) #9
  store <8 x i16> %937, <8 x i16>* %935, align 16
  br label %938

938:                                              ; preds = %874, %872, %803
  %939 = icmp ne i32 %794, 0
  %940 = select i1 %939, i64 -1, i64 1
  %941 = select i1 %939, i64 15, i64 0
  %942 = sext i32 %2 to i64
  br label %943

943:                                              ; preds = %943, %938
  %944 = phi i64 [ 0, %938 ], [ %961, %943 ]
  %945 = phi i64 [ %941, %938 ], [ %962, %943 ]
  %946 = mul nsw i64 %944, %942
  %947 = getelementptr inbounds i8, i8* %1, i64 %946
  %948 = bitcast i8* %947 to i32*
  %949 = load i32, i32* %948, align 4
  %950 = insertelement <4 x i32> <i32 undef, i32 0, i32 undef, i32 undef>, i32 %949, i32 0
  %951 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %10, i64 0, i64 %945
  %952 = bitcast <2 x i64>* %951 to <8 x i16>*
  %953 = load <8 x i16>, <8 x i16>* %952, align 16
  %954 = bitcast <4 x i32> %950 to <16 x i8>
  %955 = shufflevector <16 x i8> %954, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %956 = bitcast <16 x i8> %955 to <8 x i16>
  %957 = call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %953, <8 x i16> %956) #9
  %958 = call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %957, <8 x i16> undef) #9
  %959 = bitcast <16 x i8> %958 to <4 x i32>
  %960 = extractelement <4 x i32> %959, i32 0
  store i32 %960, i32* %948, align 4
  %961 = add nuw nsw i64 %944, 1
  %962 = add nsw i64 %945, %940
  %963 = icmp eq i64 %961, 16
  br i1 %963, label %1168, label %943

964:                                              ; preds = %1149, %793
  %965 = phi i64 [ 0, %793 ], [ %1166, %1149 ]
  %966 = shl nsw i64 %965, 5
  %967 = getelementptr inbounds i32, i32* %0, i64 %966
  %968 = shl nsw i64 %965, 3
  %969 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %10, i64 0, i64 %968
  %970 = bitcast i32* %967 to <4 x i32>*
  %971 = load <4 x i32>, <4 x i32>* %970, align 16
  %972 = call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %971, <4 x i32> undef) #9
  %973 = bitcast <2 x i64>* %969 to <8 x i16>*
  %974 = getelementptr inbounds i32, i32* %967, i64 4
  %975 = bitcast i32* %974 to <4 x i32>*
  %976 = load <4 x i32>, <4 x i32>* %975, align 16
  %977 = call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %976, <4 x i32> undef) #9
  %978 = getelementptr inbounds <2 x i64>, <2 x i64>* %969, i64 1
  %979 = bitcast <2 x i64>* %978 to <8 x i16>*
  %980 = getelementptr inbounds i32, i32* %967, i64 8
  %981 = bitcast i32* %980 to <4 x i32>*
  %982 = load <4 x i32>, <4 x i32>* %981, align 16
  %983 = call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %982, <4 x i32> undef) #9
  %984 = getelementptr inbounds <2 x i64>, <2 x i64>* %969, i64 2
  %985 = bitcast <2 x i64>* %984 to <8 x i16>*
  %986 = getelementptr inbounds i32, i32* %967, i64 12
  %987 = bitcast i32* %986 to <4 x i32>*
  %988 = load <4 x i32>, <4 x i32>* %987, align 16
  %989 = call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %988, <4 x i32> undef) #9
  %990 = getelementptr inbounds <2 x i64>, <2 x i64>* %969, i64 3
  %991 = bitcast <2 x i64>* %990 to <8 x i16>*
  %992 = getelementptr inbounds i32, i32* %967, i64 16
  %993 = bitcast i32* %992 to <4 x i32>*
  %994 = load <4 x i32>, <4 x i32>* %993, align 16
  %995 = call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %994, <4 x i32> %994) #9
  %996 = getelementptr inbounds <2 x i64>, <2 x i64>* %969, i64 4
  %997 = bitcast <2 x i64>* %996 to <8 x i16>*
  store <8 x i16> %995, <8 x i16>* %997, align 16
  %998 = getelementptr inbounds i32, i32* %967, i64 20
  %999 = bitcast i32* %998 to <4 x i32>*
  %1000 = load <4 x i32>, <4 x i32>* %999, align 16
  %1001 = call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1000, <4 x i32> %1000) #9
  %1002 = getelementptr inbounds <2 x i64>, <2 x i64>* %969, i64 5
  %1003 = bitcast <2 x i64>* %1002 to <8 x i16>*
  store <8 x i16> %1001, <8 x i16>* %1003, align 16
  %1004 = getelementptr inbounds i32, i32* %967, i64 24
  %1005 = bitcast i32* %1004 to <4 x i32>*
  %1006 = load <4 x i32>, <4 x i32>* %1005, align 16
  %1007 = call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1006, <4 x i32> %1006) #9
  %1008 = getelementptr inbounds <2 x i64>, <2 x i64>* %969, i64 6
  %1009 = bitcast <2 x i64>* %1008 to <8 x i16>*
  store <8 x i16> %1007, <8 x i16>* %1009, align 16
  %1010 = getelementptr inbounds i32, i32* %967, i64 28
  %1011 = bitcast i32* %1010 to <4 x i32>*
  %1012 = load <4 x i32>, <4 x i32>* %1011, align 16
  %1013 = call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1012, <4 x i32> %1012) #9
  %1014 = getelementptr inbounds <2 x i64>, <2 x i64>* %969, i64 7
  %1015 = bitcast <2 x i64>* %1014 to <8 x i16>*
  store <8 x i16> %1013, <8 x i16>* %1015, align 16
  %1016 = shufflevector <8 x i16> %972, <8 x i16> %977, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1017 = shufflevector <8 x i16> %983, <8 x i16> %989, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1018 = shufflevector <8 x i16> %995, <8 x i16> %1001, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1019 = shufflevector <8 x i16> %1007, <8 x i16> %1013, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1020 = bitcast <8 x i16> %1016 to <4 x i32>
  %1021 = bitcast <8 x i16> %1017 to <4 x i32>
  %1022 = shufflevector <4 x i32> %1020, <4 x i32> %1021, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1023 = bitcast <4 x i32> %1022 to <2 x i64>
  %1024 = bitcast <8 x i16> %1018 to <4 x i32>
  %1025 = bitcast <8 x i16> %1019 to <4 x i32>
  %1026 = shufflevector <4 x i32> %1024, <4 x i32> %1025, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1027 = bitcast <4 x i32> %1026 to <2 x i64>
  %1028 = shufflevector <4 x i32> %1020, <4 x i32> %1021, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1029 = bitcast <4 x i32> %1028 to <2 x i64>
  %1030 = shufflevector <4 x i32> %1024, <4 x i32> %1025, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1031 = bitcast <4 x i32> %1030 to <2 x i64>
  %1032 = shufflevector <2 x i64> %1023, <2 x i64> %1027, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %1032, <2 x i64>* %969, align 16
  %1033 = shufflevector <2 x i64> %1023, <2 x i64> %1027, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %1033, <2 x i64>* %978, align 16
  %1034 = shufflevector <2 x i64> %1029, <2 x i64> %1031, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %1034, <2 x i64>* %984, align 16
  %1035 = shufflevector <2 x i64> %1029, <2 x i64> %1031, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %1035, <2 x i64>* %990, align 16
  %1036 = bitcast <2 x i64> %1032 to <8 x i16>
  %1037 = bitcast <2 x i64> %1033 to <8 x i16>
  %1038 = bitcast <2 x i64> %1034 to <8 x i16>
  %1039 = bitcast <2 x i64> %1035 to <8 x i16>
  br i1 %796, label %1040, label %1069

1040:                                             ; preds = %964
  %1041 = shufflevector <8 x i16> %1036, <8 x i16> <i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef>, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1042 = shufflevector <8 x i16> %1036, <8 x i16> <i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1>, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1043 = call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1041, <8 x i16> <i16 5793, i16 6144, i16 5793, i16 6144, i16 5793, i16 6144, i16 5793, i16 6144>) #9
  %1044 = ashr <4 x i32> %1043, <i32 13, i32 13, i32 13, i32 13>
  %1045 = call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1042, <8 x i16> <i16 5793, i16 6144, i16 5793, i16 6144, i16 5793, i16 6144, i16 5793, i16 6144>) #9
  %1046 = ashr <4 x i32> %1045, <i32 13, i32 13, i32 13, i32 13>
  %1047 = call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1044, <4 x i32> %1046) #9
  store <8 x i16> %1047, <8 x i16>* %973, align 16
  %1048 = shufflevector <8 x i16> %1037, <8 x i16> <i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef>, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1049 = shufflevector <8 x i16> %1037, <8 x i16> <i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1>, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1050 = call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1048, <8 x i16> <i16 5793, i16 6144, i16 5793, i16 6144, i16 5793, i16 6144, i16 5793, i16 6144>) #9
  %1051 = ashr <4 x i32> %1050, <i32 13, i32 13, i32 13, i32 13>
  %1052 = call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1049, <8 x i16> <i16 5793, i16 6144, i16 5793, i16 6144, i16 5793, i16 6144, i16 5793, i16 6144>) #9
  %1053 = ashr <4 x i32> %1052, <i32 13, i32 13, i32 13, i32 13>
  %1054 = call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1051, <4 x i32> %1053) #9
  store <8 x i16> %1054, <8 x i16>* %979, align 16
  %1055 = shufflevector <8 x i16> %1038, <8 x i16> <i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef>, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1056 = shufflevector <8 x i16> %1038, <8 x i16> <i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1>, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1057 = call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1055, <8 x i16> <i16 5793, i16 6144, i16 5793, i16 6144, i16 5793, i16 6144, i16 5793, i16 6144>) #9
  %1058 = ashr <4 x i32> %1057, <i32 13, i32 13, i32 13, i32 13>
  %1059 = call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1056, <8 x i16> <i16 5793, i16 6144, i16 5793, i16 6144, i16 5793, i16 6144, i16 5793, i16 6144>) #9
  %1060 = ashr <4 x i32> %1059, <i32 13, i32 13, i32 13, i32 13>
  %1061 = call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1058, <4 x i32> %1060) #9
  store <8 x i16> %1061, <8 x i16>* %985, align 16
  %1062 = shufflevector <8 x i16> %1039, <8 x i16> <i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef>, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1063 = shufflevector <8 x i16> %1039, <8 x i16> <i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1>, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1064 = call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1062, <8 x i16> <i16 5793, i16 6144, i16 5793, i16 6144, i16 5793, i16 6144, i16 5793, i16 6144>) #9
  %1065 = ashr <4 x i32> %1064, <i32 13, i32 13, i32 13, i32 13>
  %1066 = call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1063, <8 x i16> <i16 5793, i16 6144, i16 5793, i16 6144, i16 5793, i16 6144, i16 5793, i16 6144>) #9
  %1067 = ashr <4 x i32> %1066, <i32 13, i32 13, i32 13, i32 13>
  %1068 = call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1065, <4 x i32> %1067) #9
  store <8 x i16> %1068, <8 x i16>* %991, align 16
  br label %1116

1069:                                             ; preds = %964
  call void %784(<2 x i64>* %969, <2 x i64>* %969, i8 signext %777) #9
  %1070 = load i8, i8* %776, align 1
  %1071 = sext i8 %1070 to i32
  %1072 = icmp slt i8 %1070, 0
  br i1 %1072, label %1073, label %1095

1073:                                             ; preds = %1069
  %1074 = add nsw i32 %1071, 15
  %1075 = shl i32 1, %1074
  %1076 = trunc i32 %1075 to i16
  %1077 = insertelement <8 x i16> undef, i16 %1076, i32 0
  %1078 = shufflevector <8 x i16> %1077, <8 x i16> undef, <8 x i32> zeroinitializer
  %1079 = load <8 x i16>, <8 x i16>* %973, align 16
  %1080 = call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %1079, <8 x i16> %1078) #9
  store <8 x i16> %1080, <8 x i16>* %973, align 16
  %1081 = load <8 x i16>, <8 x i16>* %979, align 16
  %1082 = call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %1081, <8 x i16> %1078) #9
  store <8 x i16> %1082, <8 x i16>* %979, align 16
  %1083 = load <8 x i16>, <8 x i16>* %985, align 16
  %1084 = call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %1083, <8 x i16> %1078) #9
  store <8 x i16> %1084, <8 x i16>* %985, align 16
  %1085 = load <8 x i16>, <8 x i16>* %991, align 16
  %1086 = call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %1085, <8 x i16> %1078) #9
  store <8 x i16> %1086, <8 x i16>* %991, align 16
  %1087 = load <8 x i16>, <8 x i16>* %997, align 16
  %1088 = call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %1087, <8 x i16> %1078) #9
  store <8 x i16> %1088, <8 x i16>* %997, align 16
  %1089 = load <8 x i16>, <8 x i16>* %1003, align 16
  %1090 = call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %1089, <8 x i16> %1078) #9
  store <8 x i16> %1090, <8 x i16>* %1003, align 16
  %1091 = load <8 x i16>, <8 x i16>* %1009, align 16
  %1092 = call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %1091, <8 x i16> %1078) #9
  store <8 x i16> %1092, <8 x i16>* %1009, align 16
  %1093 = load <8 x i16>, <8 x i16>* %1015, align 16
  %1094 = call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %1093, <8 x i16> %1078) #9
  store <8 x i16> %1094, <8 x i16>* %1015, align 16
  br label %1116

1095:                                             ; preds = %1069
  %1096 = icmp eq i8 %1070, 0
  %1097 = load <8 x i16>, <8 x i16>* %973, align 16
  br i1 %1096, label %1098, label %1100

1098:                                             ; preds = %1095
  %1099 = load <8 x i16>, <8 x i16>* %979, align 16
  br label %1116

1100:                                             ; preds = %1095
  %1101 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %1097, i32 %1071) #9
  store <8 x i16> %1101, <8 x i16>* %973, align 16
  %1102 = load <8 x i16>, <8 x i16>* %979, align 16
  %1103 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %1102, i32 %1071) #9
  store <8 x i16> %1103, <8 x i16>* %979, align 16
  %1104 = load <8 x i16>, <8 x i16>* %985, align 16
  %1105 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %1104, i32 %1071) #9
  store <8 x i16> %1105, <8 x i16>* %985, align 16
  %1106 = load <8 x i16>, <8 x i16>* %991, align 16
  %1107 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %1106, i32 %1071) #9
  store <8 x i16> %1107, <8 x i16>* %991, align 16
  %1108 = load <8 x i16>, <8 x i16>* %997, align 16
  %1109 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %1108, i32 %1071) #9
  store <8 x i16> %1109, <8 x i16>* %997, align 16
  %1110 = load <8 x i16>, <8 x i16>* %1003, align 16
  %1111 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %1110, i32 %1071) #9
  store <8 x i16> %1111, <8 x i16>* %1003, align 16
  %1112 = load <8 x i16>, <8 x i16>* %1009, align 16
  %1113 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %1112, i32 %1071) #9
  store <8 x i16> %1113, <8 x i16>* %1009, align 16
  %1114 = load <8 x i16>, <8 x i16>* %1015, align 16
  %1115 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %1114, i32 %1071) #9
  store <8 x i16> %1115, <8 x i16>* %1015, align 16
  br label %1116

1116:                                             ; preds = %1098, %1100, %1073, %1040
  %1117 = phi <8 x i16> [ %1099, %1098 ], [ %1103, %1100 ], [ %1082, %1073 ], [ %1054, %1040 ]
  %1118 = phi <8 x i16> [ %1097, %1098 ], [ %1101, %1100 ], [ %1080, %1073 ], [ %1047, %1040 ]
  br i1 %795, label %1134, label %1119

1119:                                             ; preds = %1116
  %1120 = load <8 x i16>, <8 x i16>* %985, align 16
  %1121 = load <8 x i16>, <8 x i16>* %991, align 16
  %1122 = shufflevector <8 x i16> %1121, <8 x i16> %1120, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1123 = shufflevector <8 x i16> %1117, <8 x i16> %1118, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1124 = shufflevector <8 x i16> %1121, <8 x i16> %1120, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1125 = shufflevector <8 x i16> %1117, <8 x i16> %1118, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1126 = bitcast <8 x i16> %1122 to <4 x i32>
  %1127 = bitcast <8 x i16> %1123 to <4 x i32>
  %1128 = shufflevector <4 x i32> %1126, <4 x i32> %1127, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1129 = bitcast <8 x i16> %1124 to <4 x i32>
  %1130 = bitcast <8 x i16> %1125 to <4 x i32>
  %1131 = shufflevector <4 x i32> %1129, <4 x i32> %1130, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1132 = shufflevector <4 x i32> %1126, <4 x i32> %1127, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1133 = shufflevector <4 x i32> %1129, <4 x i32> %1130, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  br label %1149

1134:                                             ; preds = %1116
  %1135 = shufflevector <8 x i16> %1118, <8 x i16> %1117, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1136 = load <8 x i16>, <8 x i16>* %985, align 16
  %1137 = load <8 x i16>, <8 x i16>* %991, align 16
  %1138 = shufflevector <8 x i16> %1136, <8 x i16> %1137, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1139 = shufflevector <8 x i16> %1118, <8 x i16> %1117, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1140 = shufflevector <8 x i16> %1136, <8 x i16> %1137, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1141 = bitcast <8 x i16> %1135 to <4 x i32>
  %1142 = bitcast <8 x i16> %1138 to <4 x i32>
  %1143 = shufflevector <4 x i32> %1141, <4 x i32> %1142, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1144 = bitcast <8 x i16> %1139 to <4 x i32>
  %1145 = bitcast <8 x i16> %1140 to <4 x i32>
  %1146 = shufflevector <4 x i32> %1144, <4 x i32> %1145, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1147 = shufflevector <4 x i32> %1141, <4 x i32> %1142, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1148 = shufflevector <4 x i32> %1144, <4 x i32> %1145, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  br label %1149

1149:                                             ; preds = %1134, %1119
  %1150 = phi <4 x i32> [ %1148, %1134 ], [ %1133, %1119 ]
  %1151 = phi <4 x i32> [ %1143, %1134 ], [ %1128, %1119 ]
  %1152 = phi <4 x i32> [ %1147, %1134 ], [ %1132, %1119 ]
  %1153 = phi <4 x i32> [ %1146, %1134 ], [ %1131, %1119 ]
  %1154 = bitcast <4 x i32> %1153 to <2 x i64>
  %1155 = bitcast <4 x i32> %1152 to <2 x i64>
  %1156 = bitcast <4 x i32> %1151 to <2 x i64>
  %1157 = bitcast <4 x i32> %1150 to <2 x i64>
  %1158 = insertelement <2 x i64> %1156, i64 0, i32 1
  store <2 x i64> %1158, <2 x i64>* %969, align 16
  %1159 = shufflevector <2 x i64> %1156, <2 x i64> <i64 undef, i64 0>, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %1159, <2 x i64>* %978, align 16
  %1160 = insertelement <2 x i64> %1155, i64 0, i32 1
  store <2 x i64> %1160, <2 x i64>* %984, align 16
  %1161 = shufflevector <2 x i64> %1155, <2 x i64> <i64 undef, i64 0>, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %1161, <2 x i64>* %990, align 16
  %1162 = insertelement <2 x i64> %1154, i64 0, i32 1
  store <2 x i64> %1162, <2 x i64>* %996, align 16
  %1163 = shufflevector <2 x i64> %1154, <2 x i64> <i64 undef, i64 0>, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %1163, <2 x i64>* %1002, align 16
  %1164 = insertelement <2 x i64> %1157, i64 0, i32 1
  store <2 x i64> %1164, <2 x i64>* %1008, align 16
  %1165 = shufflevector <2 x i64> %1157, <2 x i64> <i64 undef, i64 0>, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %1165, <2 x i64>* %1014, align 16
  %1166 = add nuw nsw i64 %965, 1
  %1167 = icmp eq i64 %1166, 2
  br i1 %1167, label %797, label %964

1168:                                             ; preds = %943
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %775) #9
  br label %2784

1169:                                             ; preds = %6
  %1170 = bitcast [16 x <2 x i64>]* %10 to i8*
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %1170) #9
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %1170, i8 -86, i64 256, i1 false) #9
  %1171 = load i8*, i8** getelementptr inbounds ([19 x i8*], [19 x i8*]* @av1_inv_txfm_shift_ls, i64 0, i64 14), align 16
  %1172 = load i8, i8* getelementptr inbounds ([5 x [5 x i8]], [5 x [5 x i8]]* @av1_inv_cos_bit_row, i64 0, i64 2, i64 0), align 2
  %1173 = load i8, i8* getelementptr inbounds ([5 x [5 x i8]], [5 x [5 x i8]]* @av1_inv_cos_bit_col, i64 0, i64 2, i64 0), align 2
  %1174 = zext i8 %3 to i64
  %1175 = getelementptr inbounds [16 x i8], [16 x i8]* @hitx_1d_tab, i64 0, i64 %1174
  %1176 = load i8, i8* %1175, align 1
  %1177 = zext i8 %1176 to i64
  %1178 = getelementptr inbounds [5 x [3 x void (<2 x i64>*, <2 x i64>*, i8)*]], [5 x [3 x void (<2 x i64>*, <2 x i64>*, i8)*]]* @lowbd_txfm_all_1d_w4_arr, i64 0, i64 2, i64 %1177
  %1179 = load void (<2 x i64>*, <2 x i64>*, i8)*, void (<2 x i64>*, <2 x i64>*, i8)** %1178, align 8
  %1180 = getelementptr inbounds [16 x i8], [16 x i8]* @vitx_1d_tab, i64 0, i64 %1174
  %1181 = load i8, i8* %1180, align 1
  %1182 = zext i8 %1181 to i64
  %1183 = getelementptr inbounds [5 x [3 x void (<2 x i64>*, <2 x i64>*, i8)*]], [5 x [3 x void (<2 x i64>*, <2 x i64>*, i8)*]]* @lowbd_txfm_all_1d_w8_arr, i64 0, i64 0, i64 %1182
  %1184 = load void (<2 x i64>*, <2 x i64>*, i8)*, void (<2 x i64>*, <2 x i64>*, i8)** %1183, align 8
  %1185 = add i8 %3, -4
  %1186 = icmp ult i8 %1185, 12
  br i1 %1186, label %1187, label %1194

1187:                                             ; preds = %1169
  %1188 = sext i8 %1185 to i64
  %1189 = getelementptr inbounds [12 x i32], [12 x i32]* @switch.table.lowbd_inv_txfm2d_add_no_identity_ssse3, i64 0, i64 %1188
  %1190 = load i32, i32* %1189, align 4
  %1191 = sext i8 %1185 to i64
  %1192 = getelementptr inbounds [12 x i32], [12 x i32]* @switch.table.lowbd_inv_txfm2d_add_no_identity_ssse3.3, i64 0, i64 %1191
  %1193 = load i32, i32* %1192, align 4
  br label %1194

1194:                                             ; preds = %1187, %1169
  %1195 = phi i32 [ 0, %1169 ], [ %1190, %1187 ]
  %1196 = phi i32 [ 0, %1169 ], [ %1193, %1187 ]
  %1197 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %10, i64 0, i64 0
  %1198 = bitcast i32* %0 to <4 x i32>*
  %1199 = load <4 x i32>, <4 x i32>* %1198, align 16
  %1200 = getelementptr inbounds i32, i32* %0, i64 4
  %1201 = bitcast i32* %1200 to <4 x i32>*
  %1202 = load <4 x i32>, <4 x i32>* %1201, align 16
  %1203 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1199, <4 x i32> %1202) #9
  %1204 = bitcast [16 x <2 x i64>]* %10 to <8 x i16>*
  %1205 = getelementptr inbounds i32, i32* %0, i64 16
  %1206 = bitcast i32* %1205 to <4 x i32>*
  %1207 = load <4 x i32>, <4 x i32>* %1206, align 16
  %1208 = getelementptr inbounds i32, i32* %0, i64 20
  %1209 = bitcast i32* %1208 to <4 x i32>*
  %1210 = load <4 x i32>, <4 x i32>* %1209, align 16
  %1211 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1207, <4 x i32> %1210) #9
  %1212 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %10, i64 0, i64 1
  %1213 = bitcast <2 x i64>* %1212 to <8 x i16>*
  %1214 = getelementptr inbounds i32, i32* %0, i64 32
  %1215 = bitcast i32* %1214 to <4 x i32>*
  %1216 = load <4 x i32>, <4 x i32>* %1215, align 16
  %1217 = getelementptr inbounds i32, i32* %0, i64 36
  %1218 = bitcast i32* %1217 to <4 x i32>*
  %1219 = load <4 x i32>, <4 x i32>* %1218, align 16
  %1220 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1216, <4 x i32> %1219) #9
  %1221 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %10, i64 0, i64 2
  %1222 = bitcast <2 x i64>* %1221 to <8 x i16>*
  %1223 = getelementptr inbounds i32, i32* %0, i64 48
  %1224 = bitcast i32* %1223 to <4 x i32>*
  %1225 = load <4 x i32>, <4 x i32>* %1224, align 16
  %1226 = getelementptr inbounds i32, i32* %0, i64 52
  %1227 = bitcast i32* %1226 to <4 x i32>*
  %1228 = load <4 x i32>, <4 x i32>* %1227, align 16
  %1229 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1225, <4 x i32> %1228) #9
  %1230 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %10, i64 0, i64 3
  %1231 = bitcast <2 x i64>* %1230 to <8 x i16>*
  %1232 = shufflevector <8 x i16> %1203, <8 x i16> %1211, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1233 = shufflevector <8 x i16> %1220, <8 x i16> %1229, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1234 = shufflevector <8 x i16> %1203, <8 x i16> %1211, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1235 = shufflevector <8 x i16> %1220, <8 x i16> %1229, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1236 = bitcast <8 x i16> %1232 to <4 x i32>
  %1237 = bitcast <8 x i16> %1233 to <4 x i32>
  %1238 = shufflevector <4 x i32> %1236, <4 x i32> %1237, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1239 = bitcast <4 x i32> %1238 to <2 x i64>
  %1240 = bitcast <8 x i16> %1234 to <4 x i32>
  %1241 = bitcast <8 x i16> %1235 to <4 x i32>
  %1242 = shufflevector <4 x i32> %1240, <4 x i32> %1241, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1243 = bitcast <4 x i32> %1242 to <2 x i64>
  %1244 = shufflevector <4 x i32> %1236, <4 x i32> %1237, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1245 = bitcast <4 x i32> %1244 to <2 x i64>
  %1246 = shufflevector <4 x i32> %1240, <4 x i32> %1241, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1247 = bitcast <4 x i32> %1246 to <2 x i64>
  %1248 = insertelement <2 x i64> %1239, i64 0, i32 1
  store <2 x i64> %1248, <2 x i64>* %1197, align 16
  %1249 = shufflevector <2 x i64> %1239, <2 x i64> <i64 undef, i64 0>, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %1249, <2 x i64>* %1212, align 16
  %1250 = insertelement <2 x i64> %1245, i64 0, i32 1
  store <2 x i64> %1250, <2 x i64>* %1221, align 16
  %1251 = shufflevector <2 x i64> %1245, <2 x i64> <i64 undef, i64 0>, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %1251, <2 x i64>* %1230, align 16
  %1252 = insertelement <2 x i64> %1243, i64 0, i32 1
  %1253 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %10, i64 0, i64 4
  store <2 x i64> %1252, <2 x i64>* %1253, align 16
  %1254 = shufflevector <2 x i64> %1243, <2 x i64> <i64 undef, i64 0>, <2 x i32> <i32 1, i32 3>
  %1255 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %10, i64 0, i64 5
  store <2 x i64> %1254, <2 x i64>* %1255, align 16
  %1256 = insertelement <2 x i64> %1247, i64 0, i32 1
  %1257 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %10, i64 0, i64 6
  store <2 x i64> %1256, <2 x i64>* %1257, align 16
  %1258 = shufflevector <2 x i64> %1247, <2 x i64> <i64 undef, i64 0>, <2 x i32> <i32 1, i32 3>
  %1259 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %10, i64 0, i64 7
  store <2 x i64> %1258, <2 x i64>* %1259, align 16
  %1260 = getelementptr inbounds i32, i32* %0, i64 8
  %1261 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %10, i64 0, i64 8
  %1262 = bitcast i32* %1260 to <4 x i32>*
  %1263 = load <4 x i32>, <4 x i32>* %1262, align 16
  %1264 = getelementptr inbounds i32, i32* %0, i64 12
  %1265 = bitcast i32* %1264 to <4 x i32>*
  %1266 = load <4 x i32>, <4 x i32>* %1265, align 16
  %1267 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1263, <4 x i32> %1266) #9
  %1268 = bitcast <2 x i64>* %1261 to <8 x i16>*
  %1269 = getelementptr inbounds i32, i32* %0, i64 24
  %1270 = bitcast i32* %1269 to <4 x i32>*
  %1271 = load <4 x i32>, <4 x i32>* %1270, align 16
  %1272 = getelementptr inbounds i32, i32* %0, i64 28
  %1273 = bitcast i32* %1272 to <4 x i32>*
  %1274 = load <4 x i32>, <4 x i32>* %1273, align 16
  %1275 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1271, <4 x i32> %1274) #9
  %1276 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %10, i64 0, i64 9
  %1277 = bitcast <2 x i64>* %1276 to <8 x i16>*
  %1278 = getelementptr inbounds i32, i32* %0, i64 40
  %1279 = bitcast i32* %1278 to <4 x i32>*
  %1280 = load <4 x i32>, <4 x i32>* %1279, align 16
  %1281 = getelementptr inbounds i32, i32* %0, i64 44
  %1282 = bitcast i32* %1281 to <4 x i32>*
  %1283 = load <4 x i32>, <4 x i32>* %1282, align 16
  %1284 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1280, <4 x i32> %1283) #9
  %1285 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %10, i64 0, i64 10
  %1286 = bitcast <2 x i64>* %1285 to <8 x i16>*
  %1287 = getelementptr inbounds i32, i32* %0, i64 56
  %1288 = bitcast i32* %1287 to <4 x i32>*
  %1289 = load <4 x i32>, <4 x i32>* %1288, align 16
  %1290 = getelementptr inbounds i32, i32* %0, i64 60
  %1291 = bitcast i32* %1290 to <4 x i32>*
  %1292 = load <4 x i32>, <4 x i32>* %1291, align 16
  %1293 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1289, <4 x i32> %1292) #9
  %1294 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %10, i64 0, i64 11
  %1295 = bitcast <2 x i64>* %1294 to <8 x i16>*
  %1296 = shufflevector <8 x i16> %1267, <8 x i16> %1275, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1297 = shufflevector <8 x i16> %1284, <8 x i16> %1293, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1298 = shufflevector <8 x i16> %1267, <8 x i16> %1275, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1299 = shufflevector <8 x i16> %1284, <8 x i16> %1293, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1300 = bitcast <8 x i16> %1296 to <4 x i32>
  %1301 = bitcast <8 x i16> %1297 to <4 x i32>
  %1302 = shufflevector <4 x i32> %1300, <4 x i32> %1301, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1303 = bitcast <4 x i32> %1302 to <2 x i64>
  %1304 = bitcast <8 x i16> %1298 to <4 x i32>
  %1305 = bitcast <8 x i16> %1299 to <4 x i32>
  %1306 = shufflevector <4 x i32> %1304, <4 x i32> %1305, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1307 = bitcast <4 x i32> %1306 to <2 x i64>
  %1308 = shufflevector <4 x i32> %1300, <4 x i32> %1301, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1309 = bitcast <4 x i32> %1308 to <2 x i64>
  %1310 = shufflevector <4 x i32> %1304, <4 x i32> %1305, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1311 = bitcast <4 x i32> %1310 to <2 x i64>
  %1312 = insertelement <2 x i64> %1303, i64 0, i32 1
  store <2 x i64> %1312, <2 x i64>* %1261, align 16
  %1313 = shufflevector <2 x i64> %1303, <2 x i64> <i64 undef, i64 0>, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %1313, <2 x i64>* %1276, align 16
  %1314 = insertelement <2 x i64> %1309, i64 0, i32 1
  store <2 x i64> %1314, <2 x i64>* %1285, align 16
  %1315 = shufflevector <2 x i64> %1309, <2 x i64> <i64 undef, i64 0>, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %1315, <2 x i64>* %1294, align 16
  %1316 = insertelement <2 x i64> %1307, i64 0, i32 1
  %1317 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %10, i64 0, i64 12
  store <2 x i64> %1316, <2 x i64>* %1317, align 16
  %1318 = shufflevector <2 x i64> %1307, <2 x i64> <i64 undef, i64 0>, <2 x i32> <i32 1, i32 3>
  %1319 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %10, i64 0, i64 13
  store <2 x i64> %1318, <2 x i64>* %1319, align 16
  %1320 = insertelement <2 x i64> %1311, i64 0, i32 1
  %1321 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %10, i64 0, i64 14
  store <2 x i64> %1320, <2 x i64>* %1321, align 16
  %1322 = shufflevector <2 x i64> %1311, <2 x i64> <i64 undef, i64 0>, <2 x i32> <i32 1, i32 3>
  %1323 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %10, i64 0, i64 15
  store <2 x i64> %1322, <2 x i64>* %1323, align 16
  %1324 = icmp eq void (<2 x i64>*, <2 x i64>*, i8)* %1179, @iidentity16_ssse3
  br i1 %1324, label %1325, label %1373

1325:                                             ; preds = %1194
  %1326 = bitcast <2 x i64> %1249 to <8 x i16>
  %1327 = bitcast <2 x i64> %1248 to <8 x i16>
  %1328 = shufflevector <8 x i16> %1327, <8 x i16> <i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef>, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1329 = shufflevector <8 x i16> %1327, <8 x i16> <i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1>, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1330 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1328, <8 x i16> <i16 11586, i16 6144, i16 11586, i16 6144, i16 11586, i16 6144, i16 11586, i16 6144>) #9
  %1331 = ashr <4 x i32> %1330, <i32 13, i32 13, i32 13, i32 13>
  %1332 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1329, <8 x i16> <i16 11586, i16 6144, i16 11586, i16 6144, i16 11586, i16 6144, i16 11586, i16 6144>) #9
  %1333 = ashr <4 x i32> %1332, <i32 13, i32 13, i32 13, i32 13>
  %1334 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1331, <4 x i32> %1333) #9
  store <8 x i16> %1334, <8 x i16>* %1204, align 16
  %1335 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %10, i64 0, i64 1
  %1336 = bitcast <2 x i64>* %1335 to <8 x i16>*
  %1337 = shufflevector <8 x i16> %1326, <8 x i16> <i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef>, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1338 = shufflevector <8 x i16> %1326, <8 x i16> <i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1>, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1339 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1337, <8 x i16> <i16 11586, i16 6144, i16 11586, i16 6144, i16 11586, i16 6144, i16 11586, i16 6144>) #9
  %1340 = ashr <4 x i32> %1339, <i32 13, i32 13, i32 13, i32 13>
  %1341 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1338, <8 x i16> <i16 11586, i16 6144, i16 11586, i16 6144, i16 11586, i16 6144, i16 11586, i16 6144>) #9
  %1342 = ashr <4 x i32> %1341, <i32 13, i32 13, i32 13, i32 13>
  %1343 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1340, <4 x i32> %1342) #9
  store <8 x i16> %1343, <8 x i16>* %1336, align 16
  br label %1344

1344:                                             ; preds = %1344, %1325
  %1345 = phi i64 [ 2, %1325 ], [ %1371, %1344 ]
  %1346 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %10, i64 0, i64 %1345
  %1347 = bitcast <2 x i64>* %1346 to <8 x i16>*
  %1348 = load <8 x i16>, <8 x i16>* %1347, align 16
  %1349 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %10, i64 0, i64 %1345
  %1350 = bitcast <2 x i64>* %1349 to <8 x i16>*
  %1351 = shufflevector <8 x i16> %1348, <8 x i16> <i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef>, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1352 = shufflevector <8 x i16> %1348, <8 x i16> <i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1>, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1353 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1351, <8 x i16> <i16 11586, i16 6144, i16 11586, i16 6144, i16 11586, i16 6144, i16 11586, i16 6144>) #9
  %1354 = ashr <4 x i32> %1353, <i32 13, i32 13, i32 13, i32 13>
  %1355 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1352, <8 x i16> <i16 11586, i16 6144, i16 11586, i16 6144, i16 11586, i16 6144, i16 11586, i16 6144>) #9
  %1356 = ashr <4 x i32> %1355, <i32 13, i32 13, i32 13, i32 13>
  %1357 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1354, <4 x i32> %1356) #9
  store <8 x i16> %1357, <8 x i16>* %1350, align 16
  %1358 = or i64 %1345, 1
  %1359 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %10, i64 0, i64 %1358
  %1360 = bitcast <2 x i64>* %1359 to <8 x i16>*
  %1361 = load <8 x i16>, <8 x i16>* %1360, align 16
  %1362 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %10, i64 0, i64 %1358
  %1363 = bitcast <2 x i64>* %1362 to <8 x i16>*
  %1364 = shufflevector <8 x i16> %1361, <8 x i16> <i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef>, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1365 = shufflevector <8 x i16> %1361, <8 x i16> <i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1>, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1366 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1364, <8 x i16> <i16 11586, i16 6144, i16 11586, i16 6144, i16 11586, i16 6144, i16 11586, i16 6144>) #9
  %1367 = ashr <4 x i32> %1366, <i32 13, i32 13, i32 13, i32 13>
  %1368 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1365, <8 x i16> <i16 11586, i16 6144, i16 11586, i16 6144, i16 11586, i16 6144, i16 11586, i16 6144>) #9
  %1369 = ashr <4 x i32> %1368, <i32 13, i32 13, i32 13, i32 13>
  %1370 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1367, <4 x i32> %1369) #9
  store <8 x i16> %1370, <8 x i16>* %1363, align 16
  %1371 = add nuw nsw i64 %1345, 2
  %1372 = icmp eq i64 %1371, 16
  br i1 %1372, label %1466, label %1344

1373:                                             ; preds = %1194
  call void %1179(<2 x i64>* nonnull %1197, <2 x i64>* nonnull %1197, i8 signext %1172) #9
  %1374 = load i8, i8* %1171, align 1
  %1375 = sext i8 %1374 to i32
  %1376 = icmp slt i8 %1374, 0
  br i1 %1376, label %1377, label %1423

1377:                                             ; preds = %1373
  %1378 = add nsw i32 %1375, 15
  %1379 = shl i32 1, %1378
  %1380 = trunc i32 %1379 to i16
  %1381 = insertelement <8 x i16> undef, i16 %1380, i32 0
  %1382 = shufflevector <8 x i16> %1381, <8 x i16> undef, <8 x i32> zeroinitializer
  %1383 = load <8 x i16>, <8 x i16>* %1204, align 16
  %1384 = call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %1383, <8 x i16> %1382) #9
  store <8 x i16> %1384, <8 x i16>* %1204, align 16
  %1385 = load <8 x i16>, <8 x i16>* %1213, align 16
  %1386 = call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %1385, <8 x i16> %1382) #9
  store <8 x i16> %1386, <8 x i16>* %1213, align 16
  %1387 = load <8 x i16>, <8 x i16>* %1222, align 16
  %1388 = call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %1387, <8 x i16> %1382) #9
  store <8 x i16> %1388, <8 x i16>* %1222, align 16
  %1389 = load <8 x i16>, <8 x i16>* %1231, align 16
  %1390 = call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %1389, <8 x i16> %1382) #9
  store <8 x i16> %1390, <8 x i16>* %1231, align 16
  %1391 = bitcast <2 x i64>* %1253 to <8 x i16>*
  %1392 = load <8 x i16>, <8 x i16>* %1391, align 16
  %1393 = call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %1392, <8 x i16> %1382) #9
  store <8 x i16> %1393, <8 x i16>* %1391, align 16
  %1394 = bitcast <2 x i64>* %1255 to <8 x i16>*
  %1395 = load <8 x i16>, <8 x i16>* %1394, align 16
  %1396 = call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %1395, <8 x i16> %1382) #9
  store <8 x i16> %1396, <8 x i16>* %1394, align 16
  %1397 = bitcast <2 x i64>* %1257 to <8 x i16>*
  %1398 = load <8 x i16>, <8 x i16>* %1397, align 16
  %1399 = call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %1398, <8 x i16> %1382) #9
  store <8 x i16> %1399, <8 x i16>* %1397, align 16
  %1400 = bitcast <2 x i64>* %1259 to <8 x i16>*
  %1401 = load <8 x i16>, <8 x i16>* %1400, align 16
  %1402 = call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %1401, <8 x i16> %1382) #9
  store <8 x i16> %1402, <8 x i16>* %1400, align 16
  %1403 = load <8 x i16>, <8 x i16>* %1268, align 16
  %1404 = call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %1403, <8 x i16> %1382) #9
  store <8 x i16> %1404, <8 x i16>* %1268, align 16
  %1405 = load <8 x i16>, <8 x i16>* %1277, align 16
  %1406 = call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %1405, <8 x i16> %1382) #9
  store <8 x i16> %1406, <8 x i16>* %1277, align 16
  %1407 = load <8 x i16>, <8 x i16>* %1286, align 16
  %1408 = call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %1407, <8 x i16> %1382) #9
  store <8 x i16> %1408, <8 x i16>* %1286, align 16
  %1409 = load <8 x i16>, <8 x i16>* %1295, align 16
  %1410 = call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %1409, <8 x i16> %1382) #9
  store <8 x i16> %1410, <8 x i16>* %1295, align 16
  %1411 = bitcast <2 x i64>* %1317 to <8 x i16>*
  %1412 = load <8 x i16>, <8 x i16>* %1411, align 16
  %1413 = call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %1412, <8 x i16> %1382) #9
  store <8 x i16> %1413, <8 x i16>* %1411, align 16
  %1414 = bitcast <2 x i64>* %1319 to <8 x i16>*
  %1415 = load <8 x i16>, <8 x i16>* %1414, align 16
  %1416 = call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %1415, <8 x i16> %1382) #9
  store <8 x i16> %1416, <8 x i16>* %1414, align 16
  %1417 = bitcast <2 x i64>* %1321 to <8 x i16>*
  %1418 = load <8 x i16>, <8 x i16>* %1417, align 16
  %1419 = call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %1418, <8 x i16> %1382) #9
  store <8 x i16> %1419, <8 x i16>* %1417, align 16
  %1420 = bitcast <2 x i64>* %1323 to <8 x i16>*
  %1421 = load <8 x i16>, <8 x i16>* %1420, align 16
  %1422 = call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %1421, <8 x i16> %1382) #9
  store <8 x i16> %1422, <8 x i16>* %1420, align 16
  br label %1466

1423:                                             ; preds = %1373
  %1424 = icmp eq i8 %1374, 0
  br i1 %1424, label %1466, label %1425

1425:                                             ; preds = %1423
  %1426 = load <8 x i16>, <8 x i16>* %1204, align 16
  %1427 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %1426, i32 %1375) #9
  store <8 x i16> %1427, <8 x i16>* %1204, align 16
  %1428 = load <8 x i16>, <8 x i16>* %1213, align 16
  %1429 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %1428, i32 %1375) #9
  store <8 x i16> %1429, <8 x i16>* %1213, align 16
  %1430 = load <8 x i16>, <8 x i16>* %1222, align 16
  %1431 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %1430, i32 %1375) #9
  store <8 x i16> %1431, <8 x i16>* %1222, align 16
  %1432 = load <8 x i16>, <8 x i16>* %1231, align 16
  %1433 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %1432, i32 %1375) #9
  store <8 x i16> %1433, <8 x i16>* %1231, align 16
  %1434 = bitcast <2 x i64>* %1253 to <8 x i16>*
  %1435 = load <8 x i16>, <8 x i16>* %1434, align 16
  %1436 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %1435, i32 %1375) #9
  store <8 x i16> %1436, <8 x i16>* %1434, align 16
  %1437 = bitcast <2 x i64>* %1255 to <8 x i16>*
  %1438 = load <8 x i16>, <8 x i16>* %1437, align 16
  %1439 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %1438, i32 %1375) #9
  store <8 x i16> %1439, <8 x i16>* %1437, align 16
  %1440 = bitcast <2 x i64>* %1257 to <8 x i16>*
  %1441 = load <8 x i16>, <8 x i16>* %1440, align 16
  %1442 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %1441, i32 %1375) #9
  store <8 x i16> %1442, <8 x i16>* %1440, align 16
  %1443 = bitcast <2 x i64>* %1259 to <8 x i16>*
  %1444 = load <8 x i16>, <8 x i16>* %1443, align 16
  %1445 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %1444, i32 %1375) #9
  store <8 x i16> %1445, <8 x i16>* %1443, align 16
  %1446 = load <8 x i16>, <8 x i16>* %1268, align 16
  %1447 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %1446, i32 %1375) #9
  store <8 x i16> %1447, <8 x i16>* %1268, align 16
  %1448 = load <8 x i16>, <8 x i16>* %1277, align 16
  %1449 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %1448, i32 %1375) #9
  store <8 x i16> %1449, <8 x i16>* %1277, align 16
  %1450 = load <8 x i16>, <8 x i16>* %1286, align 16
  %1451 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %1450, i32 %1375) #9
  store <8 x i16> %1451, <8 x i16>* %1286, align 16
  %1452 = load <8 x i16>, <8 x i16>* %1295, align 16
  %1453 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %1452, i32 %1375) #9
  store <8 x i16> %1453, <8 x i16>* %1295, align 16
  %1454 = bitcast <2 x i64>* %1317 to <8 x i16>*
  %1455 = load <8 x i16>, <8 x i16>* %1454, align 16
  %1456 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %1455, i32 %1375) #9
  store <8 x i16> %1456, <8 x i16>* %1454, align 16
  %1457 = bitcast <2 x i64>* %1319 to <8 x i16>*
  %1458 = load <8 x i16>, <8 x i16>* %1457, align 16
  %1459 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %1458, i32 %1375) #9
  store <8 x i16> %1459, <8 x i16>* %1457, align 16
  %1460 = bitcast <2 x i64>* %1321 to <8 x i16>*
  %1461 = load <8 x i16>, <8 x i16>* %1460, align 16
  %1462 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %1461, i32 %1375) #9
  store <8 x i16> %1462, <8 x i16>* %1460, align 16
  %1463 = bitcast <2 x i64>* %1323 to <8 x i16>*
  %1464 = load <8 x i16>, <8 x i16>* %1463, align 16
  %1465 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %1464, i32 %1375) #9
  store <8 x i16> %1465, <8 x i16>* %1463, align 16
  br label %1466

1466:                                             ; preds = %1344, %1425, %1423, %1377
  %1467 = icmp eq i32 %1196, 0
  %1468 = load <8 x i16>, <8 x i16>* %1204, align 16
  %1469 = load <8 x i16>, <8 x i16>* %1213, align 16
  br i1 %1467, label %1533, label %1470

1470:                                             ; preds = %1466
  %1471 = load <8 x i16>, <8 x i16>* %1222, align 16
  %1472 = load <8 x i16>, <8 x i16>* %1231, align 16
  %1473 = bitcast <2 x i64>* %1253 to <8 x i16>*
  %1474 = load <8 x i16>, <8 x i16>* %1473, align 16
  %1475 = bitcast <2 x i64>* %1255 to <8 x i16>*
  %1476 = load <8 x i16>, <8 x i16>* %1475, align 16
  %1477 = bitcast <2 x i64>* %1257 to <8 x i16>*
  %1478 = load <8 x i16>, <8 x i16>* %1477, align 16
  %1479 = bitcast <2 x i64>* %1259 to <8 x i16>*
  %1480 = load <8 x i16>, <8 x i16>* %1479, align 16
  %1481 = load <8 x i16>, <8 x i16>* %1268, align 16
  %1482 = load <8 x i16>, <8 x i16>* %1277, align 16
  %1483 = load <8 x i16>, <8 x i16>* %1286, align 16
  %1484 = load <8 x i16>, <8 x i16>* %1295, align 16
  %1485 = bitcast <2 x i64>* %1317 to <8 x i16>*
  %1486 = load <8 x i16>, <8 x i16>* %1485, align 16
  %1487 = bitcast <2 x i64>* %1319 to <8 x i16>*
  %1488 = load <8 x i16>, <8 x i16>* %1487, align 16
  %1489 = bitcast <2 x i64>* %1321 to <8 x i16>*
  %1490 = load <8 x i16>, <8 x i16>* %1489, align 16
  %1491 = bitcast <2 x i64>* %1323 to <8 x i16>*
  %1492 = load <8 x i16>, <8 x i16>* %1491, align 16
  %1493 = shufflevector <8 x i16> %1492, <8 x i16> %1490, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1494 = shufflevector <8 x i16> %1488, <8 x i16> %1486, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1495 = shufflevector <8 x i16> %1484, <8 x i16> %1483, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1496 = shufflevector <8 x i16> %1482, <8 x i16> %1481, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1497 = bitcast <8 x i16> %1493 to <4 x i32>
  %1498 = bitcast <8 x i16> %1494 to <4 x i32>
  %1499 = shufflevector <4 x i32> %1497, <4 x i32> %1498, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1500 = bitcast <4 x i32> %1499 to <2 x i64>
  %1501 = bitcast <8 x i16> %1495 to <4 x i32>
  %1502 = bitcast <8 x i16> %1496 to <4 x i32>
  %1503 = shufflevector <4 x i32> %1501, <4 x i32> %1502, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1504 = bitcast <4 x i32> %1503 to <2 x i64>
  %1505 = shufflevector <4 x i32> %1497, <4 x i32> %1498, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1506 = bitcast <4 x i32> %1505 to <2 x i64>
  %1507 = shufflevector <4 x i32> %1501, <4 x i32> %1502, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1508 = bitcast <4 x i32> %1507 to <2 x i64>
  %1509 = shufflevector <2 x i64> %1500, <2 x i64> %1504, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %1509, <2 x i64>* %1197, align 16
  %1510 = shufflevector <2 x i64> %1500, <2 x i64> %1504, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %1510, <2 x i64>* %1212, align 16
  %1511 = shufflevector <2 x i64> %1506, <2 x i64> %1508, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %1511, <2 x i64>* %1221, align 16
  %1512 = shufflevector <2 x i64> %1506, <2 x i64> %1508, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %1512, <2 x i64>* %1230, align 16
  %1513 = shufflevector <8 x i16> %1480, <8 x i16> %1478, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1514 = shufflevector <8 x i16> %1476, <8 x i16> %1474, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1515 = shufflevector <8 x i16> %1472, <8 x i16> %1471, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1516 = shufflevector <8 x i16> %1469, <8 x i16> %1468, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1517 = bitcast <8 x i16> %1513 to <4 x i32>
  %1518 = bitcast <8 x i16> %1514 to <4 x i32>
  %1519 = shufflevector <4 x i32> %1517, <4 x i32> %1518, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1520 = bitcast <4 x i32> %1519 to <2 x i64>
  %1521 = bitcast <8 x i16> %1515 to <4 x i32>
  %1522 = bitcast <8 x i16> %1516 to <4 x i32>
  %1523 = shufflevector <4 x i32> %1521, <4 x i32> %1522, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1524 = bitcast <4 x i32> %1523 to <2 x i64>
  %1525 = shufflevector <4 x i32> %1517, <4 x i32> %1518, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1526 = bitcast <4 x i32> %1525 to <2 x i64>
  %1527 = shufflevector <4 x i32> %1521, <4 x i32> %1522, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1528 = bitcast <4 x i32> %1527 to <2 x i64>
  %1529 = shufflevector <2 x i64> %1520, <2 x i64> %1524, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %1529, <2 x i64>* %1261, align 16
  %1530 = shufflevector <2 x i64> %1520, <2 x i64> %1524, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %1530, <2 x i64>* %1276, align 16
  %1531 = shufflevector <2 x i64> %1526, <2 x i64> %1528, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %1531, <2 x i64>* %1285, align 16
  %1532 = shufflevector <2 x i64> %1526, <2 x i64> %1528, <2 x i32> <i32 1, i32 3>
  br label %1596

1533:                                             ; preds = %1466
  %1534 = shufflevector <8 x i16> %1468, <8 x i16> %1469, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1535 = load <8 x i16>, <8 x i16>* %1222, align 16
  %1536 = load <8 x i16>, <8 x i16>* %1231, align 16
  %1537 = shufflevector <8 x i16> %1535, <8 x i16> %1536, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1538 = bitcast <2 x i64>* %1253 to <8 x i16>*
  %1539 = load <8 x i16>, <8 x i16>* %1538, align 16
  %1540 = bitcast <2 x i64>* %1255 to <8 x i16>*
  %1541 = load <8 x i16>, <8 x i16>* %1540, align 16
  %1542 = shufflevector <8 x i16> %1539, <8 x i16> %1541, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1543 = bitcast <2 x i64>* %1257 to <8 x i16>*
  %1544 = load <8 x i16>, <8 x i16>* %1543, align 16
  %1545 = bitcast <2 x i64>* %1259 to <8 x i16>*
  %1546 = load <8 x i16>, <8 x i16>* %1545, align 16
  %1547 = shufflevector <8 x i16> %1544, <8 x i16> %1546, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1548 = bitcast <8 x i16> %1534 to <4 x i32>
  %1549 = bitcast <8 x i16> %1537 to <4 x i32>
  %1550 = shufflevector <4 x i32> %1548, <4 x i32> %1549, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1551 = bitcast <4 x i32> %1550 to <2 x i64>
  %1552 = bitcast <8 x i16> %1542 to <4 x i32>
  %1553 = bitcast <8 x i16> %1547 to <4 x i32>
  %1554 = shufflevector <4 x i32> %1552, <4 x i32> %1553, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1555 = bitcast <4 x i32> %1554 to <2 x i64>
  %1556 = shufflevector <4 x i32> %1548, <4 x i32> %1549, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1557 = bitcast <4 x i32> %1556 to <2 x i64>
  %1558 = shufflevector <4 x i32> %1552, <4 x i32> %1553, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1559 = bitcast <4 x i32> %1558 to <2 x i64>
  %1560 = shufflevector <2 x i64> %1551, <2 x i64> %1555, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %1560, <2 x i64>* %1197, align 16
  %1561 = shufflevector <2 x i64> %1551, <2 x i64> %1555, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %1561, <2 x i64>* %1212, align 16
  %1562 = shufflevector <2 x i64> %1557, <2 x i64> %1559, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %1562, <2 x i64>* %1221, align 16
  %1563 = shufflevector <2 x i64> %1557, <2 x i64> %1559, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %1563, <2 x i64>* %1230, align 16
  %1564 = load <8 x i16>, <8 x i16>* %1268, align 16
  %1565 = load <8 x i16>, <8 x i16>* %1277, align 16
  %1566 = shufflevector <8 x i16> %1564, <8 x i16> %1565, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1567 = load <8 x i16>, <8 x i16>* %1286, align 16
  %1568 = load <8 x i16>, <8 x i16>* %1295, align 16
  %1569 = shufflevector <8 x i16> %1567, <8 x i16> %1568, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1570 = bitcast <2 x i64>* %1317 to <8 x i16>*
  %1571 = load <8 x i16>, <8 x i16>* %1570, align 16
  %1572 = bitcast <2 x i64>* %1319 to <8 x i16>*
  %1573 = load <8 x i16>, <8 x i16>* %1572, align 16
  %1574 = shufflevector <8 x i16> %1571, <8 x i16> %1573, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1575 = bitcast <2 x i64>* %1321 to <8 x i16>*
  %1576 = load <8 x i16>, <8 x i16>* %1575, align 16
  %1577 = bitcast <2 x i64>* %1323 to <8 x i16>*
  %1578 = load <8 x i16>, <8 x i16>* %1577, align 16
  %1579 = shufflevector <8 x i16> %1576, <8 x i16> %1578, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1580 = bitcast <8 x i16> %1566 to <4 x i32>
  %1581 = bitcast <8 x i16> %1569 to <4 x i32>
  %1582 = shufflevector <4 x i32> %1580, <4 x i32> %1581, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1583 = bitcast <4 x i32> %1582 to <2 x i64>
  %1584 = bitcast <8 x i16> %1574 to <4 x i32>
  %1585 = bitcast <8 x i16> %1579 to <4 x i32>
  %1586 = shufflevector <4 x i32> %1584, <4 x i32> %1585, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1587 = bitcast <4 x i32> %1586 to <2 x i64>
  %1588 = shufflevector <4 x i32> %1580, <4 x i32> %1581, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1589 = bitcast <4 x i32> %1588 to <2 x i64>
  %1590 = shufflevector <4 x i32> %1584, <4 x i32> %1585, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1591 = bitcast <4 x i32> %1590 to <2 x i64>
  %1592 = shufflevector <2 x i64> %1583, <2 x i64> %1587, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %1592, <2 x i64>* %1261, align 16
  %1593 = shufflevector <2 x i64> %1583, <2 x i64> %1587, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %1593, <2 x i64>* %1276, align 16
  %1594 = shufflevector <2 x i64> %1589, <2 x i64> %1591, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %1594, <2 x i64>* %1285, align 16
  %1595 = shufflevector <2 x i64> %1589, <2 x i64> %1591, <2 x i32> <i32 1, i32 3>
  br label %1596

1596:                                             ; preds = %1533, %1470
  %1597 = phi <2 x i64> [ %1532, %1470 ], [ %1595, %1533 ]
  store <2 x i64> %1597, <2 x i64>* %1294, align 16
  %1598 = getelementptr inbounds i8, i8* %1171, i64 1
  call void %1184(<2 x i64>* nonnull %1197, <2 x i64>* nonnull %1197, i8 signext %1173) #9
  %1599 = load i8, i8* %1598, align 1
  %1600 = sext i8 %1599 to i32
  %1601 = icmp slt i8 %1599, 0
  br i1 %1601, label %1602, label %1616

1602:                                             ; preds = %1596
  %1603 = add nsw i32 %1600, 15
  %1604 = shl i32 1, %1603
  %1605 = trunc i32 %1604 to i16
  %1606 = insertelement <8 x i16> undef, i16 %1605, i32 0
  %1607 = shufflevector <8 x i16> %1606, <8 x i16> undef, <8 x i32> zeroinitializer
  %1608 = load <8 x i16>, <8 x i16>* %1204, align 16
  %1609 = call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %1608, <8 x i16> %1607) #9
  store <8 x i16> %1609, <8 x i16>* %1204, align 16
  %1610 = load <8 x i16>, <8 x i16>* %1213, align 16
  %1611 = call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %1610, <8 x i16> %1607) #9
  store <8 x i16> %1611, <8 x i16>* %1213, align 16
  %1612 = load <8 x i16>, <8 x i16>* %1222, align 16
  %1613 = call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %1612, <8 x i16> %1607) #9
  store <8 x i16> %1613, <8 x i16>* %1222, align 16
  %1614 = load <8 x i16>, <8 x i16>* %1231, align 16
  %1615 = call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %1614, <8 x i16> %1607) #9
  br label %1627

1616:                                             ; preds = %1596
  %1617 = icmp eq i8 %1599, 0
  br i1 %1617, label %1629, label %1618

1618:                                             ; preds = %1616
  %1619 = load <8 x i16>, <8 x i16>* %1204, align 16
  %1620 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %1619, i32 %1600) #9
  store <8 x i16> %1620, <8 x i16>* %1204, align 16
  %1621 = load <8 x i16>, <8 x i16>* %1213, align 16
  %1622 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %1621, i32 %1600) #9
  store <8 x i16> %1622, <8 x i16>* %1213, align 16
  %1623 = load <8 x i16>, <8 x i16>* %1222, align 16
  %1624 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %1623, i32 %1600) #9
  store <8 x i16> %1624, <8 x i16>* %1222, align 16
  %1625 = load <8 x i16>, <8 x i16>* %1231, align 16
  %1626 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %1625, i32 %1600) #9
  br label %1627

1627:                                             ; preds = %1602, %1618
  %1628 = phi <8 x i16> [ %1626, %1618 ], [ %1615, %1602 ]
  store <8 x i16> %1628, <8 x i16>* %1231, align 16
  br label %1629

1629:                                             ; preds = %1627, %1616
  call void %1184(<2 x i64>* %1261, <2 x i64>* %1261, i8 signext %1173) #9
  %1630 = load i8, i8* %1598, align 1
  %1631 = sext i8 %1630 to i32
  %1632 = icmp slt i8 %1630, 0
  br i1 %1632, label %1644, label %1633

1633:                                             ; preds = %1629
  %1634 = icmp eq i8 %1630, 0
  br i1 %1634, label %1660, label %1635

1635:                                             ; preds = %1633
  %1636 = load <8 x i16>, <8 x i16>* %1268, align 16
  %1637 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %1636, i32 %1631) #9
  store <8 x i16> %1637, <8 x i16>* %1268, align 16
  %1638 = load <8 x i16>, <8 x i16>* %1277, align 16
  %1639 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %1638, i32 %1631) #9
  store <8 x i16> %1639, <8 x i16>* %1277, align 16
  %1640 = load <8 x i16>, <8 x i16>* %1286, align 16
  %1641 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %1640, i32 %1631) #9
  store <8 x i16> %1641, <8 x i16>* %1286, align 16
  %1642 = load <8 x i16>, <8 x i16>* %1295, align 16
  %1643 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %1642, i32 %1631) #9
  br label %1658

1644:                                             ; preds = %1629
  %1645 = add nsw i32 %1631, 15
  %1646 = shl i32 1, %1645
  %1647 = trunc i32 %1646 to i16
  %1648 = insertelement <8 x i16> undef, i16 %1647, i32 0
  %1649 = shufflevector <8 x i16> %1648, <8 x i16> undef, <8 x i32> zeroinitializer
  %1650 = load <8 x i16>, <8 x i16>* %1268, align 16
  %1651 = call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %1650, <8 x i16> %1649) #9
  store <8 x i16> %1651, <8 x i16>* %1268, align 16
  %1652 = load <8 x i16>, <8 x i16>* %1277, align 16
  %1653 = call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %1652, <8 x i16> %1649) #9
  store <8 x i16> %1653, <8 x i16>* %1277, align 16
  %1654 = load <8 x i16>, <8 x i16>* %1286, align 16
  %1655 = call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %1654, <8 x i16> %1649) #9
  store <8 x i16> %1655, <8 x i16>* %1286, align 16
  %1656 = load <8 x i16>, <8 x i16>* %1295, align 16
  %1657 = call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %1656, <8 x i16> %1649) #9
  br label %1658

1658:                                             ; preds = %1644, %1635
  %1659 = phi <8 x i16> [ %1643, %1635 ], [ %1657, %1644 ]
  store <8 x i16> %1659, <8 x i16>* %1295, align 16
  br label %1660

1660:                                             ; preds = %1658, %1633
  %1661 = icmp ne i32 %1195, 0
  %1662 = select i1 %1661, i64 3, i64 0
  %1663 = sext i32 %2 to i64
  %1664 = bitcast i8* %1 to i64*
  %1665 = load i64, i64* %1664, align 1
  %1666 = insertelement <2 x i64> undef, i64 %1665, i32 0
  %1667 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %10, i64 0, i64 %1662
  %1668 = bitcast <2 x i64>* %1667 to <8 x i16>*
  %1669 = load <8 x i16>, <8 x i16>* %1668, align 16
  %1670 = bitcast <2 x i64> %1666 to <16 x i8>
  %1671 = shufflevector <16 x i8> %1670, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %1672 = bitcast <16 x i8> %1671 to <8 x i16>
  %1673 = call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1669, <8 x i16> %1672) #9
  %1674 = call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %1673, <8 x i16> undef) #9
  %1675 = bitcast <16 x i8> %1674 to <2 x i64>
  %1676 = extractelement <2 x i64> %1675, i32 0
  store i64 %1676, i64* %1664, align 1
  %1677 = select i1 %1661, i64 2, i64 1
  %1678 = getelementptr inbounds i8, i8* %1, i64 %1663
  %1679 = bitcast i8* %1678 to i64*
  %1680 = load i64, i64* %1679, align 1
  %1681 = insertelement <2 x i64> undef, i64 %1680, i32 0
  %1682 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %10, i64 0, i64 %1677
  %1683 = bitcast <2 x i64>* %1682 to <8 x i16>*
  %1684 = load <8 x i16>, <8 x i16>* %1683, align 16
  %1685 = bitcast <2 x i64> %1681 to <16 x i8>
  %1686 = shufflevector <16 x i8> %1685, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %1687 = bitcast <16 x i8> %1686 to <8 x i16>
  %1688 = call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1684, <8 x i16> %1687) #9
  %1689 = call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %1688, <8 x i16> undef) #9
  %1690 = bitcast <16 x i8> %1689 to <2 x i64>
  %1691 = extractelement <2 x i64> %1690, i32 0
  store i64 %1691, i64* %1679, align 1
  %1692 = select i1 %1661, i64 1, i64 2
  %1693 = shl nsw i64 %1663, 1
  %1694 = getelementptr inbounds i8, i8* %1, i64 %1693
  %1695 = bitcast i8* %1694 to i64*
  %1696 = load i64, i64* %1695, align 1
  %1697 = insertelement <2 x i64> undef, i64 %1696, i32 0
  %1698 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %10, i64 0, i64 %1692
  %1699 = bitcast <2 x i64>* %1698 to <8 x i16>*
  %1700 = load <8 x i16>, <8 x i16>* %1699, align 16
  %1701 = bitcast <2 x i64> %1697 to <16 x i8>
  %1702 = shufflevector <16 x i8> %1701, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %1703 = bitcast <16 x i8> %1702 to <8 x i16>
  %1704 = call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1700, <8 x i16> %1703) #9
  %1705 = call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %1704, <8 x i16> undef) #9
  %1706 = bitcast <16 x i8> %1705 to <2 x i64>
  %1707 = extractelement <2 x i64> %1706, i32 0
  store i64 %1707, i64* %1695, align 1
  %1708 = select i1 %1661, i64 0, i64 3
  %1709 = mul nsw i64 %1663, 3
  %1710 = getelementptr inbounds i8, i8* %1, i64 %1709
  %1711 = bitcast i8* %1710 to i64*
  %1712 = load i64, i64* %1711, align 1
  %1713 = insertelement <2 x i64> undef, i64 %1712, i32 0
  %1714 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %10, i64 0, i64 %1708
  %1715 = bitcast <2 x i64>* %1714 to <8 x i16>*
  %1716 = load <8 x i16>, <8 x i16>* %1715, align 16
  %1717 = bitcast <2 x i64> %1713 to <16 x i8>
  %1718 = shufflevector <16 x i8> %1717, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %1719 = bitcast <16 x i8> %1718 to <8 x i16>
  %1720 = call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1716, <8 x i16> %1719) #9
  %1721 = call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %1720, <8 x i16> undef) #9
  %1722 = bitcast <16 x i8> %1721 to <2 x i64>
  %1723 = extractelement <2 x i64> %1722, i32 0
  store i64 %1723, i64* %1711, align 1
  %1724 = getelementptr inbounds i8, i8* %1, i64 8
  %1725 = bitcast i8* %1724 to i64*
  %1726 = load i64, i64* %1725, align 1
  %1727 = insertelement <2 x i64> undef, i64 %1726, i32 0
  %1728 = getelementptr inbounds <2 x i64>, <2 x i64>* %1261, i64 %1662
  %1729 = bitcast <2 x i64>* %1728 to <8 x i16>*
  %1730 = load <8 x i16>, <8 x i16>* %1729, align 16
  %1731 = bitcast <2 x i64> %1727 to <16 x i8>
  %1732 = shufflevector <16 x i8> %1731, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %1733 = bitcast <16 x i8> %1732 to <8 x i16>
  %1734 = call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1730, <8 x i16> %1733) #9
  %1735 = call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %1734, <8 x i16> undef) #9
  %1736 = bitcast <16 x i8> %1735 to <2 x i64>
  %1737 = extractelement <2 x i64> %1736, i32 0
  store i64 %1737, i64* %1725, align 1
  %1738 = getelementptr inbounds i8, i8* %1724, i64 %1663
  %1739 = bitcast i8* %1738 to i64*
  %1740 = load i64, i64* %1739, align 1
  %1741 = insertelement <2 x i64> undef, i64 %1740, i32 0
  %1742 = getelementptr inbounds <2 x i64>, <2 x i64>* %1261, i64 %1677
  %1743 = bitcast <2 x i64>* %1742 to <8 x i16>*
  %1744 = load <8 x i16>, <8 x i16>* %1743, align 16
  %1745 = bitcast <2 x i64> %1741 to <16 x i8>
  %1746 = shufflevector <16 x i8> %1745, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %1747 = bitcast <16 x i8> %1746 to <8 x i16>
  %1748 = call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1744, <8 x i16> %1747) #9
  %1749 = call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %1748, <8 x i16> undef) #9
  %1750 = bitcast <16 x i8> %1749 to <2 x i64>
  %1751 = extractelement <2 x i64> %1750, i32 0
  store i64 %1751, i64* %1739, align 1
  %1752 = getelementptr inbounds i8, i8* %1724, i64 %1693
  %1753 = bitcast i8* %1752 to i64*
  %1754 = load i64, i64* %1753, align 1
  %1755 = insertelement <2 x i64> undef, i64 %1754, i32 0
  %1756 = getelementptr inbounds <2 x i64>, <2 x i64>* %1261, i64 %1692
  %1757 = bitcast <2 x i64>* %1756 to <8 x i16>*
  %1758 = load <8 x i16>, <8 x i16>* %1757, align 16
  %1759 = bitcast <2 x i64> %1755 to <16 x i8>
  %1760 = shufflevector <16 x i8> %1759, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %1761 = bitcast <16 x i8> %1760 to <8 x i16>
  %1762 = call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1758, <8 x i16> %1761) #9
  %1763 = call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %1762, <8 x i16> undef) #9
  %1764 = bitcast <16 x i8> %1763 to <2 x i64>
  %1765 = extractelement <2 x i64> %1764, i32 0
  store i64 %1765, i64* %1753, align 1
  %1766 = getelementptr inbounds i8, i8* %1724, i64 %1709
  %1767 = bitcast i8* %1766 to i64*
  %1768 = load i64, i64* %1767, align 1
  %1769 = insertelement <2 x i64> undef, i64 %1768, i32 0
  %1770 = getelementptr inbounds <2 x i64>, <2 x i64>* %1261, i64 %1708
  %1771 = bitcast <2 x i64>* %1770 to <8 x i16>*
  %1772 = load <8 x i16>, <8 x i16>* %1771, align 16
  %1773 = bitcast <2 x i64> %1769 to <16 x i8>
  %1774 = shufflevector <16 x i8> %1773, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %1775 = bitcast <16 x i8> %1774 to <8 x i16>
  %1776 = call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1772, <8 x i16> %1775) #9
  %1777 = call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %1776, <8 x i16> undef) #9
  %1778 = bitcast <16 x i8> %1777 to <2 x i64>
  %1779 = extractelement <2 x i64> %1778, i32 0
  store i64 %1779, i64* %1767, align 1
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %1170) #9
  br label %2784

1780:                                             ; preds = %6
  switch i8 %3, label %2783 [
    i8 0, label %1781
    i8 9, label %1782
    i8 10, label %1984
    i8 12, label %1984
    i8 14, label %1984
    i8 11, label %2214
    i8 13, label %2214
    i8 15, label %2214
  ]

1781:                                             ; preds = %1780
  tail call fastcc void @lowbd_inv_txfm2d_add_no_identity_ssse3(i32* %0, i8* %1, i32 %2, i8 zeroext 0, i8 zeroext %4, i32 %5) #9
  br label %2784

1782:                                             ; preds = %1780
  %1783 = zext i8 %4 to i64
  %1784 = getelementptr inbounds [19 x i8*], [19 x i8*]* @av1_inv_txfm_shift_ls, i64 0, i64 %1783
  %1785 = load i8*, i8** %1784, align 8
  %1786 = getelementptr inbounds [19 x i32], [19 x i32]* @tx_size_wide_log2, i64 0, i64 %1783
  %1787 = load i32, i32* %1786, align 4
  %1788 = add nsw i32 %1787, -2
  %1789 = getelementptr inbounds [19 x i32], [19 x i32]* @tx_size_high_log2, i64 0, i64 %1783
  %1790 = load i32, i32* %1789, align 4
  %1791 = add nsw i32 %1790, -2
  %1792 = getelementptr inbounds [19 x i32], [19 x i32]* @tx_size_wide, i64 0, i64 %1783
  %1793 = load i32, i32* %1792, align 4
  %1794 = getelementptr inbounds [19 x i32], [19 x i32]* @tx_size_high, i64 0, i64 %1783
  %1795 = load i32, i32* %1794, align 4
  %1796 = icmp slt i32 %1793, 32
  %1797 = select i1 %1796, i32 %1793, i32 32
  %1798 = icmp slt i32 %1795, 32
  %1799 = select i1 %1798, i32 %1795, i32 32
  %1800 = icmp eq i32 %1793, %1795
  br i1 %1800, label %1816, label %1801

1801:                                             ; preds = %1782
  %1802 = icmp sgt i32 %1793, %1795
  br i1 %1802, label %1803, label %1809

1803:                                             ; preds = %1801
  %1804 = shl nsw i32 %1795, 1
  %1805 = icmp eq i32 %1804, %1793
  br i1 %1805, label %1816, label %1806

1806:                                             ; preds = %1803
  %1807 = shl nsw i32 %1795, 2
  %1808 = icmp eq i32 %1807, %1793
  br i1 %1808, label %1816, label %1815

1809:                                             ; preds = %1801
  %1810 = shl nsw i32 %1793, 1
  %1811 = icmp eq i32 %1810, %1795
  br i1 %1811, label %1816, label %1812

1812:                                             ; preds = %1809
  %1813 = shl nsw i32 %1793, 2
  %1814 = icmp eq i32 %1813, %1795
  br i1 %1814, label %1816, label %1815

1815:                                             ; preds = %1812, %1806
  br label %1816

1816:                                             ; preds = %1815, %1812, %1809, %1806, %1803, %1782
  %1817 = phi i32 [ 0, %1815 ], [ 0, %1782 ], [ 1, %1803 ], [ 2, %1806 ], [ -1, %1809 ], [ -2, %1812 ]
  %1818 = bitcast [32 x <2 x i64>]* %9 to i8*
  call void @llvm.lifetime.start.p0i8(i64 512, i8* nonnull %1818) #9
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %1818, i8 -86, i64 512, i1 false) #9
  %1819 = icmp sgt i32 %1797, 7
  br i1 %1819, label %1820, label %1983

1820:                                             ; preds = %1816
  %1821 = lshr i32 %1797, 3
  %1822 = sext i32 %1788 to i64
  %1823 = getelementptr inbounds [5 x i32], [5 x i32]* @NewSqrt2list, i64 0, i64 %1822
  %1824 = load i32, i32* %1823, align 4
  %1825 = trunc i32 %1824 to i16
  %1826 = insertelement <8 x i16> undef, i16 %1825, i32 0
  %1827 = shufflevector <8 x i16> %1826, <8 x i16> undef, <8 x i32> <i32 0, i32 0, i32 0, i32 0, i32 undef, i32 undef, i32 undef, i32 undef>
  %1828 = sext i32 %1797 to i64
  %1829 = zext i32 %1799 to i64
  %1830 = getelementptr inbounds i8, i8* %1785, i64 1
  %1831 = sext i32 %1791 to i64
  %1832 = getelementptr inbounds [5 x i32], [5 x i32]* @NewSqrt2list, i64 0, i64 %1831
  %1833 = load i32, i32* %1832, align 4
  %1834 = trunc i32 %1833 to i16
  %1835 = insertelement <8 x i16> undef, i16 %1834, i32 0
  %1836 = shufflevector <8 x i16> %1835, <8 x i16> undef, <8 x i32> <i32 0, i32 0, i32 0, i32 0, i32 undef, i32 undef, i32 undef, i32 undef>
  %1837 = shufflevector <8 x i16> %1836, <8 x i16> <i16 2048, i16 2048, i16 2048, i16 2048, i16 undef, i16 undef, i16 undef, i16 undef>, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1838 = sext i32 %2 to i64
  %1839 = zext i32 %1821 to i64
  %1840 = and i64 %1829, 1
  %1841 = icmp eq i32 %1799, 1
  %1842 = sub nsw i64 %1829, %1840
  %1843 = icmp eq i64 %1840, 0
  br label %1844

1844:                                             ; preds = %1980, %1820
  %1845 = phi i64 [ 0, %1820 ], [ %1981, %1980 ]
  %1846 = shl nsw i64 %1845, 3
  %1847 = getelementptr inbounds i32, i32* %0, i64 %1846
  %1848 = load i8, i8* %1785, align 1
  %1849 = sext i8 %1848 to i32
  %1850 = sub nsw i32 12, %1849
  %1851 = sub nsw i32 11, %1849
  %1852 = shl i32 1, %1851
  %1853 = trunc i32 %1852 to i16
  %1854 = add i16 %1853, 2048
  %1855 = insertelement <8 x i16> undef, i16 %1854, i32 0
  %1856 = shufflevector <8 x i16> %1855, <8 x i16> undef, <8 x i32> <i32 0, i32 0, i32 0, i32 0, i32 undef, i32 undef, i32 undef, i32 undef>
  %1857 = shufflevector <8 x i16> %1827, <8 x i16> %1856, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  switch i32 %1817, label %1859 [
    i32 -1, label %1858
    i32 1, label %1858
  ]

1858:                                             ; preds = %1844, %1844
  br label %1900

1859:                                             ; preds = %1844
  br i1 %1841, label %1922, label %1860

1860:                                             ; preds = %1859, %1860
  %1861 = phi i64 [ %1897, %1860 ], [ 0, %1859 ]
  %1862 = phi i32* [ %1887, %1860 ], [ %1847, %1859 ]
  %1863 = phi i64 [ %1898, %1860 ], [ %1842, %1859 ]
  %1864 = bitcast i32* %1862 to <4 x i32>*
  %1865 = load <4 x i32>, <4 x i32>* %1864, align 16
  %1866 = getelementptr inbounds i32, i32* %1862, i64 4
  %1867 = bitcast i32* %1866 to <4 x i32>*
  %1868 = load <4 x i32>, <4 x i32>* %1867, align 16
  %1869 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1865, <4 x i32> %1868) #9
  %1870 = getelementptr inbounds i32, i32* %1862, i64 %1828
  %1871 = shufflevector <8 x i16> %1869, <8 x i16> <i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef>, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1872 = shufflevector <8 x i16> %1869, <8 x i16> <i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1>, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1873 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1871, <8 x i16> %1857) #9
  %1874 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1872, <8 x i16> %1857) #9
  %1875 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1873, i32 %1850) #9
  %1876 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1874, i32 %1850) #9
  %1877 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1875, <4 x i32> %1876) #9
  %1878 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %9, i64 0, i64 %1861
  %1879 = bitcast <2 x i64>* %1878 to <8 x i16>*
  store <8 x i16> %1877, <8 x i16>* %1879, align 16
  %1880 = or i64 %1861, 1
  %1881 = bitcast i32* %1870 to <4 x i32>*
  %1882 = load <4 x i32>, <4 x i32>* %1881, align 16
  %1883 = getelementptr inbounds i32, i32* %1870, i64 4
  %1884 = bitcast i32* %1883 to <4 x i32>*
  %1885 = load <4 x i32>, <4 x i32>* %1884, align 16
  %1886 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1882, <4 x i32> %1885) #9
  %1887 = getelementptr inbounds i32, i32* %1870, i64 %1828
  %1888 = shufflevector <8 x i16> %1886, <8 x i16> <i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef>, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1889 = shufflevector <8 x i16> %1886, <8 x i16> <i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1>, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1890 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1888, <8 x i16> %1857) #9
  %1891 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1889, <8 x i16> %1857) #9
  %1892 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1890, i32 %1850) #9
  %1893 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1891, i32 %1850) #9
  %1894 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1892, <4 x i32> %1893) #9
  %1895 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %9, i64 0, i64 %1880
  %1896 = bitcast <2 x i64>* %1895 to <8 x i16>*
  store <8 x i16> %1894, <8 x i16>* %1896, align 16
  %1897 = add nuw nsw i64 %1861, 2
  %1898 = add i64 %1863, -2
  %1899 = icmp eq i64 %1898, 0
  br i1 %1899, label %1922, label %1860

1900:                                             ; preds = %1858, %1900
  %1901 = phi i64 [ %1920, %1900 ], [ 0, %1858 ]
  %1902 = phi i32* [ %1910, %1900 ], [ %1847, %1858 ]
  %1903 = bitcast i32* %1902 to <4 x i32>*
  %1904 = load <4 x i32>, <4 x i32>* %1903, align 16
  %1905 = getelementptr inbounds i32, i32* %1902, i64 4
  %1906 = bitcast i32* %1905 to <4 x i32>*
  %1907 = load <4 x i32>, <4 x i32>* %1906, align 16
  %1908 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1904, <4 x i32> %1907) #9
  %1909 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %1908, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #9
  %1910 = getelementptr inbounds i32, i32* %1902, i64 %1828
  %1911 = shufflevector <8 x i16> %1909, <8 x i16> <i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef>, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1912 = shufflevector <8 x i16> %1909, <8 x i16> <i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1>, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1913 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1911, <8 x i16> %1857) #9
  %1914 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1912, <8 x i16> %1857) #9
  %1915 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1913, i32 %1850) #9
  %1916 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1914, i32 %1850) #9
  %1917 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1915, <4 x i32> %1916) #9
  %1918 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %9, i64 0, i64 %1901
  %1919 = bitcast <2 x i64>* %1918 to <8 x i16>*
  store <8 x i16> %1917, <8 x i16>* %1919, align 16
  %1920 = add nuw nsw i64 %1901, 1
  %1921 = icmp eq i64 %1920, %1829
  br i1 %1921, label %1941, label %1900

1922:                                             ; preds = %1860, %1859
  %1923 = phi i64 [ 0, %1859 ], [ %1897, %1860 ]
  %1924 = phi i32* [ %1847, %1859 ], [ %1887, %1860 ]
  br i1 %1843, label %1941, label %1925

1925:                                             ; preds = %1922
  %1926 = bitcast i32* %1924 to <4 x i32>*
  %1927 = load <4 x i32>, <4 x i32>* %1926, align 16
  %1928 = getelementptr inbounds i32, i32* %1924, i64 4
  %1929 = bitcast i32* %1928 to <4 x i32>*
  %1930 = load <4 x i32>, <4 x i32>* %1929, align 16
  %1931 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1927, <4 x i32> %1930) #9
  %1932 = shufflevector <8 x i16> %1931, <8 x i16> <i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef>, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1933 = shufflevector <8 x i16> %1931, <8 x i16> <i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1>, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1934 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1932, <8 x i16> %1857) #9
  %1935 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1933, <8 x i16> %1857) #9
  %1936 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1934, i32 %1850) #9
  %1937 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1935, i32 %1850) #9
  %1938 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1936, <4 x i32> %1937) #9
  %1939 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %9, i64 0, i64 %1923
  %1940 = bitcast <2 x i64>* %1939 to <8 x i16>*
  store <8 x i16> %1938, <8 x i16>* %1940, align 16
  br label %1941

1941:                                             ; preds = %1900, %1925, %1922
  %1942 = load i8, i8* %1830, align 1
  %1943 = sext i8 %1942 to i32
  %1944 = sub nsw i32 0, %1943
  %1945 = xor i32 %1943, -1
  %1946 = shl i32 1, %1945
  %1947 = insertelement <4 x i32> undef, i32 %1946, i32 0
  %1948 = shufflevector <4 x i32> %1947, <4 x i32> undef, <4 x i32> zeroinitializer
  %1949 = getelementptr inbounds i8, i8* %1, i64 %1846
  br label %1950

1950:                                             ; preds = %1950, %1941
  %1951 = phi i64 [ 0, %1941 ], [ %1978, %1950 ]
  %1952 = phi i8* [ %1949, %1941 ], [ %1977, %1950 ]
  %1953 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %9, i64 0, i64 %1951
  %1954 = bitcast <2 x i64>* %1953 to <8 x i16>*
  %1955 = load <8 x i16>, <8 x i16>* %1954, align 16
  %1956 = shufflevector <8 x i16> %1955, <8 x i16> <i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef>, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1957 = shufflevector <8 x i16> %1955, <8 x i16> <i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1>, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1958 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1956, <8 x i16> %1837) #9
  %1959 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1957, <8 x i16> %1837) #9
  %1960 = ashr <4 x i32> %1958, <i32 12, i32 12, i32 12, i32 12>
  %1961 = ashr <4 x i32> %1959, <i32 12, i32 12, i32 12, i32 12>
  %1962 = add <4 x i32> %1960, %1948
  %1963 = add <4 x i32> %1961, %1948
  %1964 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1962, i32 %1944) #9
  %1965 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1963, i32 %1944) #9
  %1966 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1964, <4 x i32> %1965) #9
  %1967 = bitcast i8* %1952 to i64*
  %1968 = load i64, i64* %1967, align 1
  %1969 = insertelement <2 x i64> undef, i64 %1968, i32 0
  %1970 = bitcast <2 x i64> %1969 to <16 x i8>
  %1971 = shufflevector <16 x i8> %1970, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %1972 = bitcast <16 x i8> %1971 to <8 x i16>
  %1973 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1966, <8 x i16> %1972) #9
  %1974 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %1973, <8 x i16> undef) #9
  %1975 = bitcast <16 x i8> %1974 to <2 x i64>
  %1976 = extractelement <2 x i64> %1975, i32 0
  store i64 %1976, i64* %1967, align 1
  %1977 = getelementptr inbounds i8, i8* %1952, i64 %1838
  %1978 = add nuw nsw i64 %1951, 1
  %1979 = icmp eq i64 %1978, %1829
  br i1 %1979, label %1980, label %1950

1980:                                             ; preds = %1950
  %1981 = add nuw nsw i64 %1845, 1
  %1982 = icmp ult i64 %1981, %1839
  br i1 %1982, label %1844, label %1983

1983:                                             ; preds = %1980, %1816
  call void @llvm.lifetime.end.p0i8(i64 512, i8* nonnull %1818) #9
  br label %2784

1984:                                             ; preds = %1780, %1780, %1780
  %1985 = zext i8 %4 to i64
  %1986 = getelementptr inbounds [19 x i8*], [19 x i8*]* @av1_inv_txfm_shift_ls, i64 0, i64 %1985
  %1987 = load i8*, i8** %1986, align 8
  %1988 = add nsw i32 %5, -1
  %1989 = getelementptr inbounds [19 x i32], [19 x i32]* @tx_size_wide, i64 0, i64 %1985
  %1990 = load i32, i32* %1989, align 4
  %1991 = icmp slt i32 %1990, 32
  %1992 = select i1 %1991, i32 %1990, i32 32
  %1993 = icmp sgt i32 %1992, %5
  br i1 %1993, label %1996, label %1994

1994:                                             ; preds = %1984
  %1995 = add nsw i32 %1992, -1
  br label %2000

1996:                                             ; preds = %1984
  %1997 = sext i32 %1988 to i64
  %1998 = getelementptr inbounds [32 x i32], [32 x i32]* @eob_fill, i64 0, i64 %1997
  %1999 = load i32, i32* %1998, align 4
  br label %2000

2000:                                             ; preds = %1996, %1994
  %2001 = phi i32 [ %1995, %1994 ], [ %1999, %1996 ]
  %2002 = sdiv i32 %1988, %1992
  %2003 = sext i32 %2002 to i64
  %2004 = getelementptr inbounds [32 x i32], [32 x i32]* @eob_fill, i64 0, i64 %2003
  %2005 = load i32, i32* %2004, align 4
  %2006 = getelementptr inbounds [19 x i32], [19 x i32]* @tx_size_wide_log2, i64 0, i64 %1985
  %2007 = load i32, i32* %2006, align 4
  %2008 = add nsw i32 %2007, -2
  %2009 = getelementptr inbounds [19 x i32], [19 x i32]* @tx_size_high_log2, i64 0, i64 %1985
  %2010 = load i32, i32* %2009, align 4
  %2011 = add nsw i32 %2010, -2
  %2012 = sext i32 %2008 to i64
  %2013 = sext i32 %2011 to i64
  %2014 = getelementptr inbounds [5 x [5 x i8]], [5 x [5 x i8]]* @av1_inv_cos_bit_col, i64 0, i64 %2012, i64 %2013
  %2015 = load i8, i8* %2014, align 1
  %2016 = getelementptr inbounds [19 x i32], [19 x i32]* @tx_size_high, i64 0, i64 %1985
  %2017 = load i32, i32* %2016, align 4
  %2018 = add nsw i32 %2001, 8
  %2019 = ashr i32 %2018, 3
  %2020 = icmp eq i32 %1990, %2017
  br i1 %2020, label %2036, label %2021

2021:                                             ; preds = %2000
  %2022 = icmp sgt i32 %1990, %2017
  br i1 %2022, label %2023, label %2029

2023:                                             ; preds = %2021
  %2024 = shl nsw i32 %2017, 1
  %2025 = icmp eq i32 %2024, %1990
  br i1 %2025, label %2036, label %2026

2026:                                             ; preds = %2023
  %2027 = shl nsw i32 %2017, 2
  %2028 = icmp eq i32 %2027, %1990
  br i1 %2028, label %2036, label %2035

2029:                                             ; preds = %2021
  %2030 = shl nsw i32 %1990, 1
  %2031 = icmp eq i32 %2030, %2017
  br i1 %2031, label %2036, label %2032

2032:                                             ; preds = %2029
  %2033 = shl nsw i32 %1990, 2
  %2034 = icmp eq i32 %2033, %2017
  br i1 %2034, label %2036, label %2035

2035:                                             ; preds = %2032, %2026
  br label %2036

2036:                                             ; preds = %2035, %2032, %2029, %2026, %2023, %2000
  %2037 = phi i32 [ 0, %2035 ], [ 0, %2000 ], [ 1, %2023 ], [ 2, %2026 ], [ -1, %2029 ], [ -2, %2032 ]
  %2038 = sext i32 %2005 to i64
  %2039 = getelementptr inbounds [32 x i32], [32 x i32]* @lowbd_txfm_all_1d_zeros_idx, i64 0, i64 %2038
  %2040 = load i32, i32* %2039, align 4
  %2041 = zext i8 %3 to i64
  %2042 = getelementptr inbounds [16 x i8], [16 x i8]* @vitx_1d_tab, i64 0, i64 %2041
  %2043 = load i8, i8* %2042, align 1
  %2044 = zext i8 %2043 to i64
  %2045 = sext i32 %2040 to i64
  %2046 = getelementptr inbounds [5 x [3 x [4 x void (<2 x i64>*, <2 x i64>*, i8)*]]], [5 x [3 x [4 x void (<2 x i64>*, <2 x i64>*, i8)*]]]* @lowbd_txfm_all_1d_zeros_w8_arr, i64 0, i64 %2013, i64 %2044, i64 %2045
  %2047 = load void (<2 x i64>*, <2 x i64>*, i8)*, void (<2 x i64>*, <2 x i64>*, i8)** %2046, align 8
  %2048 = add nsw i8 %3, -4
  %2049 = lshr i8 %2048, 1
  %2050 = shl i8 %2048, 7
  %2051 = or i8 %2049, %2050
  %2052 = icmp ult i8 %2051, 6
  br i1 %2052, label %2053, label %2057

2053:                                             ; preds = %2036
  %2054 = sext i8 %2051 to i64
  %2055 = getelementptr inbounds [6 x i32], [6 x i32]* @switch.table.av1_lowbd_inv_txfm2d_add_ssse3.2, i64 0, i64 %2054
  %2056 = load i32, i32* %2055, align 4
  br label %2057

2057:                                             ; preds = %2053, %2036
  %2058 = phi i32 [ 0, %2036 ], [ %2056, %2053 ]
  %2059 = icmp sgt i32 %2018, 7
  br i1 %2059, label %2060, label %2784

2060:                                             ; preds = %2057
  %2061 = bitcast [64 x <2 x i64>]* %8 to i8*
  %2062 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %8, i64 0, i64 0
  %2063 = add nsw i32 %2005, 1
  %2064 = getelementptr inbounds [5 x i32], [5 x i32]* @NewSqrt2list, i64 0, i64 %2012
  %2065 = load i32, i32* %2064, align 4
  %2066 = trunc i32 %2065 to i16
  %2067 = insertelement <8 x i16> undef, i16 %2066, i32 0
  %2068 = shufflevector <8 x i16> %2067, <8 x i16> undef, <8 x i32> <i32 0, i32 0, i32 0, i32 0, i32 undef, i32 undef, i32 undef, i32 undef>
  %2069 = sext i32 %1992 to i64
  %2070 = zext i32 %2063 to i64
  %2071 = getelementptr inbounds i8, i8* %1987, i64 1
  %2072 = icmp ne i32 %2058, 0
  %2073 = add nsw i32 %2017, -1
  %2074 = select i1 %2072, i32 %2073, i32 0
  %2075 = select i1 %2072, i64 -1, i64 1
  %2076 = sext i32 %2 to i64
  %2077 = sext i32 %2074 to i64
  %2078 = sext i32 %2019 to i64
  %2079 = and i64 %2070, 1
  %2080 = icmp eq i32 %2002, 0
  %2081 = sub nsw i64 %2070, %2079
  %2082 = icmp eq i64 %2079, 0
  br label %2083

2083:                                             ; preds = %2189, %2060
  %2084 = phi i64 [ 0, %2060 ], [ %2190, %2189 ]
  call void @llvm.lifetime.start.p0i8(i64 1024, i8* nonnull %2061) #9
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %2061, i8 -86, i64 1024, i1 false) #9
  %2085 = shl nsw i64 %2084, 3
  %2086 = getelementptr inbounds i32, i32* %0, i64 %2085
  %2087 = load i8, i8* %1987, align 1
  %2088 = sext i8 %2087 to i32
  %2089 = sub nsw i32 12, %2088
  %2090 = sub nsw i32 11, %2088
  %2091 = shl i32 1, %2090
  %2092 = trunc i32 %2091 to i16
  %2093 = add i16 %2092, 2048
  %2094 = insertelement <8 x i16> undef, i16 %2093, i32 0
  %2095 = shufflevector <8 x i16> %2094, <8 x i16> undef, <8 x i32> <i32 0, i32 0, i32 0, i32 0, i32 undef, i32 undef, i32 undef, i32 undef>
  %2096 = shufflevector <8 x i16> %2068, <8 x i16> %2095, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  switch i32 %2037, label %2098 [
    i32 -1, label %2097
    i32 1, label %2097
  ]

2097:                                             ; preds = %2083, %2083
  br label %2139

2098:                                             ; preds = %2083
  br i1 %2080, label %2161, label %2099

2099:                                             ; preds = %2098, %2099
  %2100 = phi i64 [ %2136, %2099 ], [ 0, %2098 ]
  %2101 = phi i32* [ %2126, %2099 ], [ %2086, %2098 ]
  %2102 = phi i64 [ %2137, %2099 ], [ %2081, %2098 ]
  %2103 = bitcast i32* %2101 to <4 x i32>*
  %2104 = load <4 x i32>, <4 x i32>* %2103, align 16
  %2105 = getelementptr inbounds i32, i32* %2101, i64 4
  %2106 = bitcast i32* %2105 to <4 x i32>*
  %2107 = load <4 x i32>, <4 x i32>* %2106, align 16
  %2108 = call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %2104, <4 x i32> %2107) #9
  %2109 = getelementptr inbounds i32, i32* %2101, i64 %2069
  %2110 = shufflevector <8 x i16> %2108, <8 x i16> <i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef>, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %2111 = shufflevector <8 x i16> %2108, <8 x i16> <i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1>, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %2112 = call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2110, <8 x i16> %2096) #9
  %2113 = call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2111, <8 x i16> %2096) #9
  %2114 = call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %2112, i32 %2089) #9
  %2115 = call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %2113, i32 %2089) #9
  %2116 = call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %2114, <4 x i32> %2115) #9
  %2117 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %8, i64 0, i64 %2100
  %2118 = bitcast <2 x i64>* %2117 to <8 x i16>*
  store <8 x i16> %2116, <8 x i16>* %2118, align 16
  %2119 = or i64 %2100, 1
  %2120 = bitcast i32* %2109 to <4 x i32>*
  %2121 = load <4 x i32>, <4 x i32>* %2120, align 16
  %2122 = getelementptr inbounds i32, i32* %2109, i64 4
  %2123 = bitcast i32* %2122 to <4 x i32>*
  %2124 = load <4 x i32>, <4 x i32>* %2123, align 16
  %2125 = call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %2121, <4 x i32> %2124) #9
  %2126 = getelementptr inbounds i32, i32* %2109, i64 %2069
  %2127 = shufflevector <8 x i16> %2125, <8 x i16> <i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef>, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %2128 = shufflevector <8 x i16> %2125, <8 x i16> <i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1>, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %2129 = call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2127, <8 x i16> %2096) #9
  %2130 = call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2128, <8 x i16> %2096) #9
  %2131 = call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %2129, i32 %2089) #9
  %2132 = call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %2130, i32 %2089) #9
  %2133 = call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %2131, <4 x i32> %2132) #9
  %2134 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %8, i64 0, i64 %2119
  %2135 = bitcast <2 x i64>* %2134 to <8 x i16>*
  store <8 x i16> %2133, <8 x i16>* %2135, align 16
  %2136 = add nuw nsw i64 %2100, 2
  %2137 = add i64 %2102, -2
  %2138 = icmp eq i64 %2137, 0
  br i1 %2138, label %2161, label %2099

2139:                                             ; preds = %2097, %2139
  %2140 = phi i64 [ %2159, %2139 ], [ 0, %2097 ]
  %2141 = phi i32* [ %2149, %2139 ], [ %2086, %2097 ]
  %2142 = bitcast i32* %2141 to <4 x i32>*
  %2143 = load <4 x i32>, <4 x i32>* %2142, align 16
  %2144 = getelementptr inbounds i32, i32* %2141, i64 4
  %2145 = bitcast i32* %2144 to <4 x i32>*
  %2146 = load <4 x i32>, <4 x i32>* %2145, align 16
  %2147 = call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %2143, <4 x i32> %2146) #9
  %2148 = call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %2147, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #9
  %2149 = getelementptr inbounds i32, i32* %2141, i64 %2069
  %2150 = shufflevector <8 x i16> %2148, <8 x i16> <i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef>, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %2151 = shufflevector <8 x i16> %2148, <8 x i16> <i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1>, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %2152 = call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2150, <8 x i16> %2096) #9
  %2153 = call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2151, <8 x i16> %2096) #9
  %2154 = call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %2152, i32 %2089) #9
  %2155 = call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %2153, i32 %2089) #9
  %2156 = call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %2154, <4 x i32> %2155) #9
  %2157 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %8, i64 0, i64 %2140
  %2158 = bitcast <2 x i64>* %2157 to <8 x i16>*
  store <8 x i16> %2156, <8 x i16>* %2158, align 16
  %2159 = add nuw nsw i64 %2140, 1
  %2160 = icmp eq i64 %2159, %2070
  br i1 %2160, label %2180, label %2139

2161:                                             ; preds = %2099, %2098
  %2162 = phi i64 [ 0, %2098 ], [ %2136, %2099 ]
  %2163 = phi i32* [ %2086, %2098 ], [ %2126, %2099 ]
  br i1 %2082, label %2180, label %2164

2164:                                             ; preds = %2161
  %2165 = bitcast i32* %2163 to <4 x i32>*
  %2166 = load <4 x i32>, <4 x i32>* %2165, align 16
  %2167 = getelementptr inbounds i32, i32* %2163, i64 4
  %2168 = bitcast i32* %2167 to <4 x i32>*
  %2169 = load <4 x i32>, <4 x i32>* %2168, align 16
  %2170 = call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %2166, <4 x i32> %2169) #9
  %2171 = shufflevector <8 x i16> %2170, <8 x i16> <i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef>, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %2172 = shufflevector <8 x i16> %2170, <8 x i16> <i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1>, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %2173 = call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2171, <8 x i16> %2096) #9
  %2174 = call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2172, <8 x i16> %2096) #9
  %2175 = call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %2173, i32 %2089) #9
  %2176 = call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %2174, i32 %2089) #9
  %2177 = call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %2175, <4 x i32> %2176) #9
  %2178 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %8, i64 0, i64 %2162
  %2179 = bitcast <2 x i64>* %2178 to <8 x i16>*
  store <8 x i16> %2177, <8 x i16>* %2179, align 16
  br label %2180

2180:                                             ; preds = %2139, %2164, %2161
  call void %2047(<2 x i64>* nonnull %2062, <2 x i64>* nonnull %2062, i8 signext %2015) #9
  %2181 = load i8, i8* %2071, align 1
  %2182 = sext i8 %2181 to i32
  %2183 = add nsw i32 %2182, 15
  %2184 = shl i32 1, %2183
  %2185 = trunc i32 %2184 to i16
  %2186 = insertelement <8 x i16> undef, i16 %2185, i32 0
  %2187 = shufflevector <8 x i16> %2186, <8 x i16> undef, <8 x i32> zeroinitializer
  %2188 = getelementptr inbounds i8, i8* %1, i64 %2085
  br label %2192

2189:                                             ; preds = %2192
  call void @llvm.lifetime.end.p0i8(i64 1024, i8* nonnull %2061) #9
  %2190 = add nuw nsw i64 %2084, 1
  %2191 = icmp slt i64 %2190, %2078
  br i1 %2191, label %2083, label %2784

2192:                                             ; preds = %2192, %2180
  %2193 = phi i64 [ %2077, %2180 ], [ %2212, %2192 ]
  %2194 = phi i32 [ 0, %2180 ], [ %2211, %2192 ]
  %2195 = phi i8* [ %2188, %2180 ], [ %2210, %2192 ]
  %2196 = bitcast i8* %2195 to i64*
  %2197 = load i64, i64* %2196, align 1
  %2198 = insertelement <2 x i64> undef, i64 %2197, i32 0
  %2199 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %8, i64 0, i64 %2193
  %2200 = bitcast <2 x i64>* %2199 to <8 x i16>*
  %2201 = load <8 x i16>, <8 x i16>* %2200, align 16
  %2202 = call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %2201, <8 x i16> %2187) #9
  %2203 = bitcast <2 x i64> %2198 to <16 x i8>
  %2204 = shufflevector <16 x i8> %2203, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %2205 = bitcast <16 x i8> %2204 to <8 x i16>
  %2206 = call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %2202, <8 x i16> %2205) #9
  %2207 = call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %2206, <8 x i16> undef) #9
  %2208 = bitcast <16 x i8> %2207 to <2 x i64>
  %2209 = extractelement <2 x i64> %2208, i32 0
  store i64 %2209, i64* %2196, align 1
  %2210 = getelementptr inbounds i8, i8* %2195, i64 %2076
  %2211 = add nuw nsw i32 %2194, 1
  %2212 = add i64 %2193, %2075
  %2213 = icmp slt i32 %2211, %2017
  br i1 %2213, label %2192, label %2189

2214:                                             ; preds = %1780, %1780, %1780
  %2215 = bitcast [64 x <2 x i64>]* %8 to i8*
  call void @llvm.lifetime.start.p0i8(i64 1024, i8* nonnull %2215) #9
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %2215, i8 -86, i64 1024, i1 false) #9
  %2216 = add nsw i32 %5, -1
  %2217 = zext i8 %4 to i64
  %2218 = getelementptr inbounds [19 x i32], [19 x i32]* @tx_size_high, i64 0, i64 %2217
  %2219 = load i32, i32* %2218, align 4
  %2220 = icmp slt i32 %2219, 32
  %2221 = select i1 %2220, i32 %2219, i32 32
  %2222 = sdiv i32 %2216, %2221
  %2223 = icmp sgt i32 %2221, %5
  br i1 %2223, label %2226, label %2224

2224:                                             ; preds = %2214
  %2225 = add nsw i32 %2221, -1
  br label %2230

2226:                                             ; preds = %2214
  %2227 = sext i32 %2216 to i64
  %2228 = getelementptr inbounds [32 x i32], [32 x i32]* @eob_fill, i64 0, i64 %2227
  %2229 = load i32, i32* %2228, align 4
  br label %2230

2230:                                             ; preds = %2226, %2224
  %2231 = phi i32 [ %2225, %2224 ], [ %2229, %2226 ]
  %2232 = getelementptr inbounds [19 x i8*], [19 x i8*]* @av1_inv_txfm_shift_ls, i64 0, i64 %2217
  %2233 = load i8*, i8** %2232, align 8
  %2234 = getelementptr inbounds [19 x i32], [19 x i32]* @tx_size_wide_log2, i64 0, i64 %2217
  %2235 = load i32, i32* %2234, align 4
  %2236 = add nsw i32 %2235, -2
  %2237 = getelementptr inbounds [19 x i32], [19 x i32]* @tx_size_high_log2, i64 0, i64 %2217
  %2238 = load i32, i32* %2237, align 4
  %2239 = add nsw i32 %2238, -2
  %2240 = sext i32 %2236 to i64
  %2241 = sext i32 %2239 to i64
  %2242 = getelementptr inbounds [5 x [5 x i8]], [5 x [5 x i8]]* @av1_inv_cos_bit_row, i64 0, i64 %2240, i64 %2241
  %2243 = load i8, i8* %2242, align 1
  %2244 = getelementptr inbounds [19 x i32], [19 x i32]* @tx_size_wide, i64 0, i64 %2217
  %2245 = load i32, i32* %2244, align 4
  %2246 = ashr i32 %2245, 3
  %2247 = add nsw i32 %2231, 8
  %2248 = ashr i32 %2247, 3
  %2249 = icmp slt i32 %2245, 32
  %2250 = select i1 %2249, i32 %2245, i32 32
  %2251 = icmp eq i32 %2245, %2219
  br i1 %2251, label %2267, label %2252

2252:                                             ; preds = %2230
  %2253 = icmp sgt i32 %2245, %2219
  br i1 %2253, label %2254, label %2260

2254:                                             ; preds = %2252
  %2255 = shl nsw i32 %2219, 1
  %2256 = icmp eq i32 %2255, %2245
  br i1 %2256, label %2267, label %2257

2257:                                             ; preds = %2254
  %2258 = shl nsw i32 %2219, 2
  %2259 = icmp eq i32 %2258, %2245
  br i1 %2259, label %2267, label %2266

2260:                                             ; preds = %2252
  %2261 = shl nsw i32 %2245, 1
  %2262 = icmp eq i32 %2261, %2219
  br i1 %2262, label %2267, label %2263

2263:                                             ; preds = %2260
  %2264 = shl nsw i32 %2245, 2
  %2265 = icmp eq i32 %2264, %2219
  br i1 %2265, label %2267, label %2266

2266:                                             ; preds = %2263, %2257
  br label %2267

2267:                                             ; preds = %2266, %2263, %2260, %2257, %2254, %2230
  %2268 = phi i32 [ 0, %2266 ], [ 0, %2230 ], [ 1, %2254 ], [ 2, %2257 ], [ -1, %2260 ], [ -2, %2263 ]
  %2269 = sext i32 %2222 to i64
  %2270 = getelementptr inbounds [32 x i32], [32 x i32]* @lowbd_txfm_all_1d_zeros_idx, i64 0, i64 %2269
  %2271 = load i32, i32* %2270, align 4
  %2272 = zext i8 %3 to i64
  %2273 = getelementptr inbounds [16 x i8], [16 x i8]* @hitx_1d_tab, i64 0, i64 %2272
  %2274 = load i8, i8* %2273, align 1
  %2275 = zext i8 %2274 to i64
  %2276 = sext i32 %2271 to i64
  %2277 = getelementptr inbounds [5 x [3 x [4 x void (<2 x i64>*, <2 x i64>*, i8)*]]], [5 x [3 x [4 x void (<2 x i64>*, <2 x i64>*, i8)*]]]* @lowbd_txfm_all_1d_zeros_w8_arr, i64 0, i64 %2240, i64 %2275, i64 %2276
  %2278 = load void (<2 x i64>*, <2 x i64>*, i8)*, void (<2 x i64>*, <2 x i64>*, i8)** %2277, align 8
  %2279 = icmp sgt i32 %2247, 7
  br i1 %2279, label %2280, label %2782

2280:                                             ; preds = %2267
  %2281 = icmp eq i8 %3, 15
  %2282 = bitcast [64 x <2 x i64>]* %7 to i8*
  %2283 = shl i32 %2250, 3
  %2284 = icmp slt i32 %2246, 4
  %2285 = select i1 %2284, i32 %2246, i32 4
  %2286 = lshr i64 516062, %2217
  %2287 = and i64 %2286, 1
  %2288 = icmp eq i64 %2287, 0
  %2289 = sext i32 %2250 to i64
  %2290 = zext i32 %2250 to i64
  %2291 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %7, i64 0, i64 0
  %2292 = zext i32 %2245 to i64
  %2293 = shl i32 %2, 3
  %2294 = getelementptr inbounds i8, i8* %2233, i64 1
  %2295 = getelementptr inbounds [5 x i32], [5 x i32]* @NewSqrt2list, i64 0, i64 %2241
  %2296 = sext i32 %2 to i64
  %2297 = sext i32 %2285 to i64
  %2298 = sext i32 %2246 to i64
  %2299 = sext i32 %2248 to i64
  %2300 = shl nsw i64 %2289, 1
  %2301 = mul nsw i64 %2289, 3
  %2302 = shl nsw i64 %2289, 2
  %2303 = mul nsw i64 %2289, 5
  %2304 = mul nsw i64 %2289, 6
  %2305 = mul nsw i64 %2289, 7
  %2306 = add nsw i64 %2290, -1
  %2307 = add nsw i64 %2292, -1
  %2308 = and i64 %2290, 3
  %2309 = icmp ult i64 %2306, 3
  %2310 = sub nsw i64 %2290, %2308
  %2311 = icmp eq i64 %2308, 0
  %2312 = and i64 %2292, 3
  %2313 = icmp ult i64 %2307, 3
  %2314 = sub nsw i64 %2292, %2312
  %2315 = icmp eq i64 %2312, 0
  %2316 = and i64 %2292, 3
  %2317 = icmp ult i64 %2307, 3
  %2318 = sub nsw i64 %2292, %2316
  %2319 = icmp eq i64 %2316, 0
  br label %2320

2320:                                             ; preds = %2734, %2280
  %2321 = phi i64 [ 0, %2280 ], [ %2735, %2734 ]
  call void @llvm.lifetime.start.p0i8(i64 1024, i8* nonnull %2282) #9
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %2282, i8 -86, i64 1024, i1 false) #9
  %2322 = trunc i64 %2321 to i32
  %2323 = mul i32 %2283, %2322
  %2324 = sext i32 %2323 to i64
  %2325 = getelementptr inbounds i32, i32* %0, i64 %2324
  br i1 %2288, label %2326, label %2328

2326:                                             ; preds = %2328, %2320
  switch i32 %2268, label %2475 [
    i32 -1, label %2327
    i32 1, label %2327
  ]

2327:                                             ; preds = %2326, %2326
  br i1 %2309, label %2463, label %2438

2328:                                             ; preds = %2320, %2328
  %2329 = phi i64 [ %2436, %2328 ], [ 0, %2320 ]
  %2330 = shl i64 %2329, 3
  %2331 = and i64 %2330, 4294967288
  %2332 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %7, i64 0, i64 %2331
  %2333 = getelementptr inbounds i32, i32* %2325, i64 %2331
  %2334 = bitcast i32* %2333 to <4 x i32>*
  %2335 = load <4 x i32>, <4 x i32>* %2334, align 16
  %2336 = getelementptr inbounds i32, i32* %2333, i64 4
  %2337 = bitcast i32* %2336 to <4 x i32>*
  %2338 = load <4 x i32>, <4 x i32>* %2337, align 16
  %2339 = call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %2335, <4 x i32> %2338) #9
  %2340 = getelementptr inbounds i32, i32* %2333, i64 %2289
  %2341 = bitcast i32* %2340 to <4 x i32>*
  %2342 = load <4 x i32>, <4 x i32>* %2341, align 16
  %2343 = getelementptr inbounds i32, i32* %2340, i64 4
  %2344 = bitcast i32* %2343 to <4 x i32>*
  %2345 = load <4 x i32>, <4 x i32>* %2344, align 16
  %2346 = call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %2342, <4 x i32> %2345) #9
  %2347 = getelementptr inbounds <2 x i64>, <2 x i64>* %2332, i64 1
  %2348 = getelementptr inbounds i32, i32* %2333, i64 %2300
  %2349 = bitcast i32* %2348 to <4 x i32>*
  %2350 = load <4 x i32>, <4 x i32>* %2349, align 16
  %2351 = getelementptr inbounds i32, i32* %2348, i64 4
  %2352 = bitcast i32* %2351 to <4 x i32>*
  %2353 = load <4 x i32>, <4 x i32>* %2352, align 16
  %2354 = call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %2350, <4 x i32> %2353) #9
  %2355 = getelementptr inbounds <2 x i64>, <2 x i64>* %2332, i64 2
  %2356 = getelementptr inbounds i32, i32* %2333, i64 %2301
  %2357 = bitcast i32* %2356 to <4 x i32>*
  %2358 = load <4 x i32>, <4 x i32>* %2357, align 16
  %2359 = getelementptr inbounds i32, i32* %2356, i64 4
  %2360 = bitcast i32* %2359 to <4 x i32>*
  %2361 = load <4 x i32>, <4 x i32>* %2360, align 16
  %2362 = call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %2358, <4 x i32> %2361) #9
  %2363 = getelementptr inbounds <2 x i64>, <2 x i64>* %2332, i64 3
  %2364 = getelementptr inbounds i32, i32* %2333, i64 %2302
  %2365 = bitcast i32* %2364 to <4 x i32>*
  %2366 = load <4 x i32>, <4 x i32>* %2365, align 16
  %2367 = getelementptr inbounds i32, i32* %2364, i64 4
  %2368 = bitcast i32* %2367 to <4 x i32>*
  %2369 = load <4 x i32>, <4 x i32>* %2368, align 16
  %2370 = call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %2366, <4 x i32> %2369) #9
  %2371 = getelementptr inbounds <2 x i64>, <2 x i64>* %2332, i64 4
  %2372 = getelementptr inbounds i32, i32* %2333, i64 %2303
  %2373 = bitcast i32* %2372 to <4 x i32>*
  %2374 = load <4 x i32>, <4 x i32>* %2373, align 16
  %2375 = getelementptr inbounds i32, i32* %2372, i64 4
  %2376 = bitcast i32* %2375 to <4 x i32>*
  %2377 = load <4 x i32>, <4 x i32>* %2376, align 16
  %2378 = call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %2374, <4 x i32> %2377) #9
  %2379 = getelementptr inbounds <2 x i64>, <2 x i64>* %2332, i64 5
  %2380 = getelementptr inbounds i32, i32* %2333, i64 %2304
  %2381 = bitcast i32* %2380 to <4 x i32>*
  %2382 = load <4 x i32>, <4 x i32>* %2381, align 16
  %2383 = getelementptr inbounds i32, i32* %2380, i64 4
  %2384 = bitcast i32* %2383 to <4 x i32>*
  %2385 = load <4 x i32>, <4 x i32>* %2384, align 16
  %2386 = call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %2382, <4 x i32> %2385) #9
  %2387 = getelementptr inbounds <2 x i64>, <2 x i64>* %2332, i64 6
  %2388 = getelementptr inbounds i32, i32* %2333, i64 %2305
  %2389 = bitcast i32* %2388 to <4 x i32>*
  %2390 = load <4 x i32>, <4 x i32>* %2389, align 16
  %2391 = getelementptr inbounds i32, i32* %2388, i64 4
  %2392 = bitcast i32* %2391 to <4 x i32>*
  %2393 = load <4 x i32>, <4 x i32>* %2392, align 16
  %2394 = call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %2390, <4 x i32> %2393) #9
  %2395 = getelementptr inbounds <2 x i64>, <2 x i64>* %2332, i64 7
  %2396 = shufflevector <8 x i16> %2339, <8 x i16> %2346, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %2397 = shufflevector <8 x i16> %2354, <8 x i16> %2362, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %2398 = shufflevector <8 x i16> %2370, <8 x i16> %2378, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %2399 = shufflevector <8 x i16> %2386, <8 x i16> %2394, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %2400 = shufflevector <8 x i16> %2339, <8 x i16> %2346, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %2401 = shufflevector <8 x i16> %2354, <8 x i16> %2362, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %2402 = shufflevector <8 x i16> %2370, <8 x i16> %2378, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %2403 = shufflevector <8 x i16> %2386, <8 x i16> %2394, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %2404 = bitcast <8 x i16> %2396 to <4 x i32>
  %2405 = bitcast <8 x i16> %2397 to <4 x i32>
  %2406 = shufflevector <4 x i32> %2404, <4 x i32> %2405, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %2407 = bitcast <4 x i32> %2406 to <2 x i64>
  %2408 = bitcast <8 x i16> %2398 to <4 x i32>
  %2409 = bitcast <8 x i16> %2399 to <4 x i32>
  %2410 = shufflevector <4 x i32> %2408, <4 x i32> %2409, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %2411 = bitcast <4 x i32> %2410 to <2 x i64>
  %2412 = bitcast <8 x i16> %2400 to <4 x i32>
  %2413 = bitcast <8 x i16> %2401 to <4 x i32>
  %2414 = shufflevector <4 x i32> %2412, <4 x i32> %2413, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %2415 = bitcast <4 x i32> %2414 to <2 x i64>
  %2416 = bitcast <8 x i16> %2402 to <4 x i32>
  %2417 = bitcast <8 x i16> %2403 to <4 x i32>
  %2418 = shufflevector <4 x i32> %2416, <4 x i32> %2417, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %2419 = bitcast <4 x i32> %2418 to <2 x i64>
  %2420 = shufflevector <4 x i32> %2404, <4 x i32> %2405, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %2421 = bitcast <4 x i32> %2420 to <2 x i64>
  %2422 = shufflevector <4 x i32> %2408, <4 x i32> %2409, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %2423 = bitcast <4 x i32> %2422 to <2 x i64>
  %2424 = shufflevector <4 x i32> %2412, <4 x i32> %2413, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %2425 = bitcast <4 x i32> %2424 to <2 x i64>
  %2426 = shufflevector <4 x i32> %2416, <4 x i32> %2417, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %2427 = bitcast <4 x i32> %2426 to <2 x i64>
  %2428 = shufflevector <2 x i64> %2407, <2 x i64> %2411, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %2428, <2 x i64>* %2332, align 16
  %2429 = shufflevector <2 x i64> %2407, <2 x i64> %2411, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %2429, <2 x i64>* %2347, align 16
  %2430 = shufflevector <2 x i64> %2421, <2 x i64> %2423, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %2430, <2 x i64>* %2355, align 16
  %2431 = shufflevector <2 x i64> %2421, <2 x i64> %2423, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %2431, <2 x i64>* %2363, align 16
  %2432 = shufflevector <2 x i64> %2415, <2 x i64> %2419, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %2432, <2 x i64>* %2371, align 16
  %2433 = shufflevector <2 x i64> %2415, <2 x i64> %2419, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %2433, <2 x i64>* %2379, align 16
  %2434 = shufflevector <2 x i64> %2425, <2 x i64> %2427, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %2434, <2 x i64>* %2387, align 16
  %2435 = shufflevector <2 x i64> %2425, <2 x i64> %2427, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %2435, <2 x i64>* %2395, align 16
  %2436 = add nuw nsw i64 %2329, 1
  %2437 = icmp slt i64 %2436, %2297
  br i1 %2437, label %2328, label %2326

2438:                                             ; preds = %2327, %2438
  %2439 = phi i64 [ %2460, %2438 ], [ 0, %2327 ]
  %2440 = phi i64 [ %2461, %2438 ], [ %2310, %2327 ]
  %2441 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %7, i64 0, i64 %2439
  %2442 = bitcast <2 x i64>* %2441 to <8 x i16>*
  %2443 = load <8 x i16>, <8 x i16>* %2442, align 16
  %2444 = call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %2443, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #9
  store <8 x i16> %2444, <8 x i16>* %2442, align 16
  %2445 = or i64 %2439, 1
  %2446 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %7, i64 0, i64 %2445
  %2447 = bitcast <2 x i64>* %2446 to <8 x i16>*
  %2448 = load <8 x i16>, <8 x i16>* %2447, align 16
  %2449 = call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %2448, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #9
  store <8 x i16> %2449, <8 x i16>* %2447, align 16
  %2450 = or i64 %2439, 2
  %2451 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %7, i64 0, i64 %2450
  %2452 = bitcast <2 x i64>* %2451 to <8 x i16>*
  %2453 = load <8 x i16>, <8 x i16>* %2452, align 16
  %2454 = call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %2453, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #9
  store <8 x i16> %2454, <8 x i16>* %2452, align 16
  %2455 = or i64 %2439, 3
  %2456 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %7, i64 0, i64 %2455
  %2457 = bitcast <2 x i64>* %2456 to <8 x i16>*
  %2458 = load <8 x i16>, <8 x i16>* %2457, align 16
  %2459 = call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %2458, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #9
  store <8 x i16> %2459, <8 x i16>* %2457, align 16
  %2460 = add nuw nsw i64 %2439, 4
  %2461 = add i64 %2440, -4
  %2462 = icmp eq i64 %2461, 0
  br i1 %2462, label %2463, label %2438

2463:                                             ; preds = %2438, %2327
  %2464 = phi i64 [ 0, %2327 ], [ %2460, %2438 ]
  br i1 %2311, label %2475, label %2465

2465:                                             ; preds = %2463, %2465
  %2466 = phi i64 [ %2472, %2465 ], [ %2464, %2463 ]
  %2467 = phi i64 [ %2473, %2465 ], [ %2308, %2463 ]
  %2468 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %7, i64 0, i64 %2466
  %2469 = bitcast <2 x i64>* %2468 to <8 x i16>*
  %2470 = load <8 x i16>, <8 x i16>* %2469, align 16
  %2471 = call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %2470, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #9
  store <8 x i16> %2471, <8 x i16>* %2469, align 16
  %2472 = add nuw nsw i64 %2466, 1
  %2473 = add i64 %2467, -1
  %2474 = icmp eq i64 %2473, 0
  br i1 %2474, label %2475, label %2465, !llvm.loop !2

2475:                                             ; preds = %2463, %2465, %2326
  call void %2278(<2 x i64>* nonnull %2291, <2 x i64>* nonnull %2291, i8 signext %2243) #9
  %2476 = load i8, i8* %2233, align 1
  %2477 = sext i8 %2476 to i32
  %2478 = icmp slt i8 %2476, 0
  br i1 %2478, label %2479, label %2510

2479:                                             ; preds = %2475
  %2480 = add nsw i32 %2477, 15
  %2481 = shl i32 1, %2480
  %2482 = trunc i32 %2481 to i16
  %2483 = insertelement <8 x i16> undef, i16 %2482, i32 0
  %2484 = shufflevector <8 x i16> %2483, <8 x i16> undef, <8 x i32> zeroinitializer
  br i1 %2317, label %2538, label %2485

2485:                                             ; preds = %2479, %2485
  %2486 = phi i64 [ %2507, %2485 ], [ 0, %2479 ]
  %2487 = phi i64 [ %2508, %2485 ], [ %2318, %2479 ]
  %2488 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %7, i64 0, i64 %2486
  %2489 = bitcast <2 x i64>* %2488 to <8 x i16>*
  %2490 = load <8 x i16>, <8 x i16>* %2489, align 16
  %2491 = call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %2490, <8 x i16> %2484) #9
  store <8 x i16> %2491, <8 x i16>* %2489, align 16
  %2492 = or i64 %2486, 1
  %2493 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %7, i64 0, i64 %2492
  %2494 = bitcast <2 x i64>* %2493 to <8 x i16>*
  %2495 = load <8 x i16>, <8 x i16>* %2494, align 16
  %2496 = call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %2495, <8 x i16> %2484) #9
  store <8 x i16> %2496, <8 x i16>* %2494, align 16
  %2497 = or i64 %2486, 2
  %2498 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %7, i64 0, i64 %2497
  %2499 = bitcast <2 x i64>* %2498 to <8 x i16>*
  %2500 = load <8 x i16>, <8 x i16>* %2499, align 16
  %2501 = call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %2500, <8 x i16> %2484) #9
  store <8 x i16> %2501, <8 x i16>* %2499, align 16
  %2502 = or i64 %2486, 3
  %2503 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %7, i64 0, i64 %2502
  %2504 = bitcast <2 x i64>* %2503 to <8 x i16>*
  %2505 = load <8 x i16>, <8 x i16>* %2504, align 16
  %2506 = call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %2505, <8 x i16> %2484) #9
  store <8 x i16> %2506, <8 x i16>* %2504, align 16
  %2507 = add nuw nsw i64 %2486, 4
  %2508 = add i64 %2487, -4
  %2509 = icmp eq i64 %2508, 0
  br i1 %2509, label %2538, label %2485

2510:                                             ; preds = %2475
  %2511 = icmp eq i8 %2476, 0
  br i1 %2511, label %2562, label %2512

2512:                                             ; preds = %2510
  br i1 %2313, label %2550, label %2513

2513:                                             ; preds = %2512, %2513
  %2514 = phi i64 [ %2535, %2513 ], [ 0, %2512 ]
  %2515 = phi i64 [ %2536, %2513 ], [ %2314, %2512 ]
  %2516 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %7, i64 0, i64 %2514
  %2517 = bitcast <2 x i64>* %2516 to <8 x i16>*
  %2518 = load <8 x i16>, <8 x i16>* %2517, align 16
  %2519 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %2518, i32 %2477) #9
  store <8 x i16> %2519, <8 x i16>* %2517, align 16
  %2520 = or i64 %2514, 1
  %2521 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %7, i64 0, i64 %2520
  %2522 = bitcast <2 x i64>* %2521 to <8 x i16>*
  %2523 = load <8 x i16>, <8 x i16>* %2522, align 16
  %2524 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %2523, i32 %2477) #9
  store <8 x i16> %2524, <8 x i16>* %2522, align 16
  %2525 = or i64 %2514, 2
  %2526 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %7, i64 0, i64 %2525
  %2527 = bitcast <2 x i64>* %2526 to <8 x i16>*
  %2528 = load <8 x i16>, <8 x i16>* %2527, align 16
  %2529 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %2528, i32 %2477) #9
  store <8 x i16> %2529, <8 x i16>* %2527, align 16
  %2530 = or i64 %2514, 3
  %2531 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %7, i64 0, i64 %2530
  %2532 = bitcast <2 x i64>* %2531 to <8 x i16>*
  %2533 = load <8 x i16>, <8 x i16>* %2532, align 16
  %2534 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %2533, i32 %2477) #9
  store <8 x i16> %2534, <8 x i16>* %2532, align 16
  %2535 = add nuw nsw i64 %2514, 4
  %2536 = add i64 %2515, -4
  %2537 = icmp eq i64 %2536, 0
  br i1 %2537, label %2550, label %2513

2538:                                             ; preds = %2485, %2479
  %2539 = phi i64 [ 0, %2479 ], [ %2507, %2485 ]
  br i1 %2319, label %2562, label %2540

2540:                                             ; preds = %2538, %2540
  %2541 = phi i64 [ %2547, %2540 ], [ %2539, %2538 ]
  %2542 = phi i64 [ %2548, %2540 ], [ %2316, %2538 ]
  %2543 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %7, i64 0, i64 %2541
  %2544 = bitcast <2 x i64>* %2543 to <8 x i16>*
  %2545 = load <8 x i16>, <8 x i16>* %2544, align 16
  %2546 = call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %2545, <8 x i16> %2484) #9
  store <8 x i16> %2546, <8 x i16>* %2544, align 16
  %2547 = add nuw nsw i64 %2541, 1
  %2548 = add i64 %2542, -1
  %2549 = icmp eq i64 %2548, 0
  br i1 %2549, label %2562, label %2540, !llvm.loop !4

2550:                                             ; preds = %2513, %2512
  %2551 = phi i64 [ 0, %2512 ], [ %2535, %2513 ]
  br i1 %2315, label %2562, label %2552

2552:                                             ; preds = %2550, %2552
  %2553 = phi i64 [ %2559, %2552 ], [ %2551, %2550 ]
  %2554 = phi i64 [ %2560, %2552 ], [ %2312, %2550 ]
  %2555 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %7, i64 0, i64 %2553
  %2556 = bitcast <2 x i64>* %2555 to <8 x i16>*
  %2557 = load <8 x i16>, <8 x i16>* %2556, align 16
  %2558 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %2557, i32 %2477) #9
  store <8 x i16> %2558, <8 x i16>* %2556, align 16
  %2559 = add nuw nsw i64 %2553, 1
  %2560 = add i64 %2554, -1
  %2561 = icmp eq i64 %2560, 0
  br i1 %2561, label %2562, label %2552, !llvm.loop !5

2562:                                             ; preds = %2550, %2552, %2538, %2540, %2510
  br i1 %2281, label %2563, label %2564

2563:                                             ; preds = %2562
  br i1 %2288, label %2734, label %2565

2564:                                             ; preds = %2562
  br i1 %2288, label %2734, label %2647

2565:                                             ; preds = %2563, %2565
  %2566 = phi i64 [ %2645, %2565 ], [ 0, %2563 ]
  %2567 = shl nsw i64 %2566, 3
  %2568 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %7, i64 0, i64 %2567
  %2569 = bitcast <2 x i64>* %2568 to <8 x i16>*
  %2570 = load <8 x i16>, <8 x i16>* %2569, align 16
  %2571 = getelementptr inbounds <2 x i64>, <2 x i64>* %2568, i64 1
  %2572 = bitcast <2 x i64>* %2571 to <8 x i16>*
  %2573 = load <8 x i16>, <8 x i16>* %2572, align 16
  %2574 = getelementptr inbounds <2 x i64>, <2 x i64>* %2568, i64 2
  %2575 = bitcast <2 x i64>* %2574 to <8 x i16>*
  %2576 = load <8 x i16>, <8 x i16>* %2575, align 16
  %2577 = getelementptr inbounds <2 x i64>, <2 x i64>* %2568, i64 3
  %2578 = bitcast <2 x i64>* %2577 to <8 x i16>*
  %2579 = load <8 x i16>, <8 x i16>* %2578, align 16
  %2580 = getelementptr inbounds <2 x i64>, <2 x i64>* %2568, i64 4
  %2581 = bitcast <2 x i64>* %2580 to <8 x i16>*
  %2582 = load <8 x i16>, <8 x i16>* %2581, align 16
  %2583 = getelementptr inbounds <2 x i64>, <2 x i64>* %2568, i64 5
  %2584 = bitcast <2 x i64>* %2583 to <8 x i16>*
  %2585 = load <8 x i16>, <8 x i16>* %2584, align 16
  %2586 = getelementptr inbounds <2 x i64>, <2 x i64>* %2568, i64 6
  %2587 = bitcast <2 x i64>* %2586 to <8 x i16>*
  %2588 = load <8 x i16>, <8 x i16>* %2587, align 16
  %2589 = getelementptr inbounds <2 x i64>, <2 x i64>* %2568, i64 7
  %2590 = bitcast <2 x i64>* %2589 to <8 x i16>*
  %2591 = load <8 x i16>, <8 x i16>* %2590, align 16
  %2592 = trunc i64 %2566 to i32
  %2593 = xor i32 %2592, -1
  %2594 = add i32 %2246, %2593
  %2595 = shl nsw i32 %2594, 3
  %2596 = sext i32 %2595 to i64
  %2597 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %8, i64 0, i64 %2596
  %2598 = shufflevector <8 x i16> %2591, <8 x i16> %2588, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %2599 = shufflevector <8 x i16> %2585, <8 x i16> %2582, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %2600 = shufflevector <8 x i16> %2579, <8 x i16> %2576, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %2601 = shufflevector <8 x i16> %2573, <8 x i16> %2570, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %2602 = shufflevector <8 x i16> %2591, <8 x i16> %2588, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %2603 = shufflevector <8 x i16> %2585, <8 x i16> %2582, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %2604 = shufflevector <8 x i16> %2579, <8 x i16> %2576, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %2605 = shufflevector <8 x i16> %2573, <8 x i16> %2570, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %2606 = bitcast <8 x i16> %2598 to <4 x i32>
  %2607 = bitcast <8 x i16> %2599 to <4 x i32>
  %2608 = shufflevector <4 x i32> %2606, <4 x i32> %2607, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %2609 = bitcast <4 x i32> %2608 to <2 x i64>
  %2610 = bitcast <8 x i16> %2600 to <4 x i32>
  %2611 = bitcast <8 x i16> %2601 to <4 x i32>
  %2612 = shufflevector <4 x i32> %2610, <4 x i32> %2611, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %2613 = bitcast <4 x i32> %2612 to <2 x i64>
  %2614 = bitcast <8 x i16> %2602 to <4 x i32>
  %2615 = bitcast <8 x i16> %2603 to <4 x i32>
  %2616 = shufflevector <4 x i32> %2614, <4 x i32> %2615, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %2617 = bitcast <4 x i32> %2616 to <2 x i64>
  %2618 = bitcast <8 x i16> %2604 to <4 x i32>
  %2619 = bitcast <8 x i16> %2605 to <4 x i32>
  %2620 = shufflevector <4 x i32> %2618, <4 x i32> %2619, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %2621 = bitcast <4 x i32> %2620 to <2 x i64>
  %2622 = shufflevector <4 x i32> %2606, <4 x i32> %2607, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %2623 = bitcast <4 x i32> %2622 to <2 x i64>
  %2624 = shufflevector <4 x i32> %2610, <4 x i32> %2611, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %2625 = bitcast <4 x i32> %2624 to <2 x i64>
  %2626 = shufflevector <4 x i32> %2614, <4 x i32> %2615, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %2627 = bitcast <4 x i32> %2626 to <2 x i64>
  %2628 = shufflevector <4 x i32> %2618, <4 x i32> %2619, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %2629 = bitcast <4 x i32> %2628 to <2 x i64>
  %2630 = shufflevector <2 x i64> %2609, <2 x i64> %2613, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %2630, <2 x i64>* %2597, align 16
  %2631 = shufflevector <2 x i64> %2609, <2 x i64> %2613, <2 x i32> <i32 1, i32 3>
  %2632 = getelementptr inbounds <2 x i64>, <2 x i64>* %2597, i64 1
  store <2 x i64> %2631, <2 x i64>* %2632, align 16
  %2633 = shufflevector <2 x i64> %2623, <2 x i64> %2625, <2 x i32> <i32 0, i32 2>
  %2634 = getelementptr inbounds <2 x i64>, <2 x i64>* %2597, i64 2
  store <2 x i64> %2633, <2 x i64>* %2634, align 16
  %2635 = shufflevector <2 x i64> %2623, <2 x i64> %2625, <2 x i32> <i32 1, i32 3>
  %2636 = getelementptr inbounds <2 x i64>, <2 x i64>* %2597, i64 3
  store <2 x i64> %2635, <2 x i64>* %2636, align 16
  %2637 = shufflevector <2 x i64> %2617, <2 x i64> %2621, <2 x i32> <i32 0, i32 2>
  %2638 = getelementptr inbounds <2 x i64>, <2 x i64>* %2597, i64 4
  store <2 x i64> %2637, <2 x i64>* %2638, align 16
  %2639 = shufflevector <2 x i64> %2617, <2 x i64> %2621, <2 x i32> <i32 1, i32 3>
  %2640 = getelementptr inbounds <2 x i64>, <2 x i64>* %2597, i64 5
  store <2 x i64> %2639, <2 x i64>* %2640, align 16
  %2641 = shufflevector <2 x i64> %2627, <2 x i64> %2629, <2 x i32> <i32 0, i32 2>
  %2642 = getelementptr inbounds <2 x i64>, <2 x i64>* %2597, i64 6
  store <2 x i64> %2641, <2 x i64>* %2642, align 16
  %2643 = shufflevector <2 x i64> %2627, <2 x i64> %2629, <2 x i32> <i32 1, i32 3>
  %2644 = getelementptr inbounds <2 x i64>, <2 x i64>* %2597, i64 7
  store <2 x i64> %2643, <2 x i64>* %2644, align 16
  %2645 = add nuw nsw i64 %2566, 1
  %2646 = icmp slt i64 %2645, %2298
  br i1 %2646, label %2565, label %2724

2647:                                             ; preds = %2564, %2647
  %2648 = phi i64 [ %2722, %2647 ], [ 0, %2564 ]
  %2649 = shl nsw i64 %2648, 3
  %2650 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %7, i64 0, i64 %2649
  %2651 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %8, i64 0, i64 %2649
  %2652 = bitcast <2 x i64>* %2650 to <8 x i16>*
  %2653 = load <8 x i16>, <8 x i16>* %2652, align 16
  %2654 = getelementptr inbounds <2 x i64>, <2 x i64>* %2650, i64 1
  %2655 = bitcast <2 x i64>* %2654 to <8 x i16>*
  %2656 = load <8 x i16>, <8 x i16>* %2655, align 16
  %2657 = shufflevector <8 x i16> %2653, <8 x i16> %2656, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %2658 = getelementptr inbounds <2 x i64>, <2 x i64>* %2650, i64 2
  %2659 = bitcast <2 x i64>* %2658 to <8 x i16>*
  %2660 = load <8 x i16>, <8 x i16>* %2659, align 16
  %2661 = getelementptr inbounds <2 x i64>, <2 x i64>* %2650, i64 3
  %2662 = bitcast <2 x i64>* %2661 to <8 x i16>*
  %2663 = load <8 x i16>, <8 x i16>* %2662, align 16
  %2664 = shufflevector <8 x i16> %2660, <8 x i16> %2663, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %2665 = getelementptr inbounds <2 x i64>, <2 x i64>* %2650, i64 4
  %2666 = bitcast <2 x i64>* %2665 to <8 x i16>*
  %2667 = load <8 x i16>, <8 x i16>* %2666, align 16
  %2668 = getelementptr inbounds <2 x i64>, <2 x i64>* %2650, i64 5
  %2669 = bitcast <2 x i64>* %2668 to <8 x i16>*
  %2670 = load <8 x i16>, <8 x i16>* %2669, align 16
  %2671 = shufflevector <8 x i16> %2667, <8 x i16> %2670, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %2672 = getelementptr inbounds <2 x i64>, <2 x i64>* %2650, i64 6
  %2673 = bitcast <2 x i64>* %2672 to <8 x i16>*
  %2674 = load <8 x i16>, <8 x i16>* %2673, align 16
  %2675 = getelementptr inbounds <2 x i64>, <2 x i64>* %2650, i64 7
  %2676 = bitcast <2 x i64>* %2675 to <8 x i16>*
  %2677 = load <8 x i16>, <8 x i16>* %2676, align 16
  %2678 = shufflevector <8 x i16> %2674, <8 x i16> %2677, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %2679 = shufflevector <8 x i16> %2653, <8 x i16> %2656, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %2680 = shufflevector <8 x i16> %2660, <8 x i16> %2663, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %2681 = shufflevector <8 x i16> %2667, <8 x i16> %2670, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %2682 = shufflevector <8 x i16> %2674, <8 x i16> %2677, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %2683 = bitcast <8 x i16> %2657 to <4 x i32>
  %2684 = bitcast <8 x i16> %2664 to <4 x i32>
  %2685 = shufflevector <4 x i32> %2683, <4 x i32> %2684, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %2686 = bitcast <4 x i32> %2685 to <2 x i64>
  %2687 = bitcast <8 x i16> %2671 to <4 x i32>
  %2688 = bitcast <8 x i16> %2678 to <4 x i32>
  %2689 = shufflevector <4 x i32> %2687, <4 x i32> %2688, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %2690 = bitcast <4 x i32> %2689 to <2 x i64>
  %2691 = bitcast <8 x i16> %2679 to <4 x i32>
  %2692 = bitcast <8 x i16> %2680 to <4 x i32>
  %2693 = shufflevector <4 x i32> %2691, <4 x i32> %2692, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %2694 = bitcast <4 x i32> %2693 to <2 x i64>
  %2695 = bitcast <8 x i16> %2681 to <4 x i32>
  %2696 = bitcast <8 x i16> %2682 to <4 x i32>
  %2697 = shufflevector <4 x i32> %2695, <4 x i32> %2696, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %2698 = bitcast <4 x i32> %2697 to <2 x i64>
  %2699 = shufflevector <4 x i32> %2683, <4 x i32> %2684, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %2700 = bitcast <4 x i32> %2699 to <2 x i64>
  %2701 = shufflevector <4 x i32> %2687, <4 x i32> %2688, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %2702 = bitcast <4 x i32> %2701 to <2 x i64>
  %2703 = shufflevector <4 x i32> %2691, <4 x i32> %2692, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %2704 = bitcast <4 x i32> %2703 to <2 x i64>
  %2705 = shufflevector <4 x i32> %2695, <4 x i32> %2696, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %2706 = bitcast <4 x i32> %2705 to <2 x i64>
  %2707 = shufflevector <2 x i64> %2686, <2 x i64> %2690, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %2707, <2 x i64>* %2651, align 16
  %2708 = shufflevector <2 x i64> %2686, <2 x i64> %2690, <2 x i32> <i32 1, i32 3>
  %2709 = getelementptr inbounds <2 x i64>, <2 x i64>* %2651, i64 1
  store <2 x i64> %2708, <2 x i64>* %2709, align 16
  %2710 = shufflevector <2 x i64> %2700, <2 x i64> %2702, <2 x i32> <i32 0, i32 2>
  %2711 = getelementptr inbounds <2 x i64>, <2 x i64>* %2651, i64 2
  store <2 x i64> %2710, <2 x i64>* %2711, align 16
  %2712 = shufflevector <2 x i64> %2700, <2 x i64> %2702, <2 x i32> <i32 1, i32 3>
  %2713 = getelementptr inbounds <2 x i64>, <2 x i64>* %2651, i64 3
  store <2 x i64> %2712, <2 x i64>* %2713, align 16
  %2714 = shufflevector <2 x i64> %2694, <2 x i64> %2698, <2 x i32> <i32 0, i32 2>
  %2715 = getelementptr inbounds <2 x i64>, <2 x i64>* %2651, i64 4
  store <2 x i64> %2714, <2 x i64>* %2715, align 16
  %2716 = shufflevector <2 x i64> %2694, <2 x i64> %2698, <2 x i32> <i32 1, i32 3>
  %2717 = getelementptr inbounds <2 x i64>, <2 x i64>* %2651, i64 5
  store <2 x i64> %2716, <2 x i64>* %2717, align 16
  %2718 = shufflevector <2 x i64> %2704, <2 x i64> %2706, <2 x i32> <i32 0, i32 2>
  %2719 = getelementptr inbounds <2 x i64>, <2 x i64>* %2651, i64 6
  store <2 x i64> %2718, <2 x i64>* %2719, align 16
  %2720 = shufflevector <2 x i64> %2704, <2 x i64> %2706, <2 x i32> <i32 1, i32 3>
  %2721 = getelementptr inbounds <2 x i64>, <2 x i64>* %2651, i64 7
  store <2 x i64> %2720, <2 x i64>* %2721, align 16
  %2722 = add nuw nsw i64 %2648, 1
  %2723 = icmp slt i64 %2722, %2298
  br i1 %2723, label %2647, label %2724

2724:                                             ; preds = %2647, %2565
  br i1 %2288, label %2734, label %2725

2725:                                             ; preds = %2724
  %2726 = mul i32 %2293, %2322
  %2727 = sext i32 %2726 to i64
  %2728 = getelementptr inbounds i8, i8* %1, i64 %2727
  %2729 = load i32, i32* %2295, align 4
  %2730 = trunc i32 %2729 to i16
  %2731 = insertelement <8 x i16> undef, i16 %2730, i32 0
  %2732 = shufflevector <8 x i16> %2731, <8 x i16> undef, <8 x i32> <i32 0, i32 0, i32 0, i32 0, i32 undef, i32 undef, i32 undef, i32 undef>
  %2733 = shufflevector <8 x i16> %2732, <8 x i16> <i16 2048, i16 2048, i16 2048, i16 2048, i16 undef, i16 undef, i16 undef, i16 undef>, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  br label %2737

2734:                                             ; preds = %2779, %2724, %2564, %2563
  call void @llvm.lifetime.end.p0i8(i64 1024, i8* nonnull %2282) #9
  %2735 = add nuw nsw i64 %2321, 1
  %2736 = icmp slt i64 %2735, %2299
  br i1 %2736, label %2320, label %2782

2737:                                             ; preds = %2779, %2725
  %2738 = phi i64 [ 0, %2725 ], [ %2780, %2779 ]
  %2739 = shl nsw i64 %2738, 3
  %2740 = getelementptr inbounds i8, i8* %2728, i64 %2739
  %2741 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %8, i64 0, i64 %2739
  %2742 = load i8, i8* %2294, align 1
  %2743 = sext i8 %2742 to i32
  %2744 = sub nsw i32 0, %2743
  %2745 = xor i32 %2743, -1
  %2746 = shl i32 1, %2745
  %2747 = insertelement <4 x i32> undef, i32 %2746, i32 0
  %2748 = shufflevector <4 x i32> %2747, <4 x i32> undef, <4 x i32> zeroinitializer
  br label %2749

2749:                                             ; preds = %2749, %2737
  %2750 = phi i64 [ 0, %2737 ], [ %2777, %2749 ]
  %2751 = phi i8* [ %2740, %2737 ], [ %2776, %2749 ]
  %2752 = getelementptr inbounds <2 x i64>, <2 x i64>* %2741, i64 %2750
  %2753 = bitcast <2 x i64>* %2752 to <8 x i16>*
  %2754 = load <8 x i16>, <8 x i16>* %2753, align 16
  %2755 = shufflevector <8 x i16> %2754, <8 x i16> <i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef>, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %2756 = shufflevector <8 x i16> %2754, <8 x i16> <i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1>, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %2757 = call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2755, <8 x i16> %2733) #9
  %2758 = call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2756, <8 x i16> %2733) #9
  %2759 = ashr <4 x i32> %2757, <i32 12, i32 12, i32 12, i32 12>
  %2760 = ashr <4 x i32> %2758, <i32 12, i32 12, i32 12, i32 12>
  %2761 = add <4 x i32> %2759, %2748
  %2762 = add <4 x i32> %2760, %2748
  %2763 = call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %2761, i32 %2744) #9
  %2764 = call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %2762, i32 %2744) #9
  %2765 = call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %2763, <4 x i32> %2764) #9
  %2766 = bitcast i8* %2751 to i64*
  %2767 = load i64, i64* %2766, align 1
  %2768 = insertelement <2 x i64> undef, i64 %2767, i32 0
  %2769 = bitcast <2 x i64> %2768 to <16 x i8>
  %2770 = shufflevector <16 x i8> %2769, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %2771 = bitcast <16 x i8> %2770 to <8 x i16>
  %2772 = call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %2765, <8 x i16> %2771) #9
  %2773 = call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %2772, <8 x i16> undef) #9
  %2774 = bitcast <16 x i8> %2773 to <2 x i64>
  %2775 = extractelement <2 x i64> %2774, i32 0
  store i64 %2775, i64* %2766, align 1
  %2776 = getelementptr inbounds i8, i8* %2751, i64 %2296
  %2777 = add nuw nsw i64 %2750, 1
  %2778 = icmp eq i64 %2777, 8
  br i1 %2778, label %2779, label %2749

2779:                                             ; preds = %2749
  %2780 = add nuw nsw i64 %2738, 1
  %2781 = icmp slt i64 %2780, %2298
  br i1 %2781, label %2737, label %2734

2782:                                             ; preds = %2734, %2267
  call void @llvm.lifetime.end.p0i8(i64 1024, i8* nonnull %2215) #9
  br label %2784

2783:                                             ; preds = %1780
  tail call fastcc void @lowbd_inv_txfm2d_add_no_identity_ssse3(i32* %0, i8* %1, i32 %2, i8 zeroext %3, i8 zeroext %4, i32 %5) #9
  br label %2784

2784:                                             ; preds = %2189, %2783, %2782, %2057, %1983, %1781, %1660, %1168, %710, %388, %131
  ret void
}

; Function Attrs: nounwind ssp uwtable
define hidden void @av1_inv_txfm_add_ssse3(i32*, i8*, i32, %struct.txfm_param*) local_unnamed_addr #1 {
  %5 = getelementptr inbounds %struct.txfm_param, %struct.txfm_param* %3, i64 0, i32 2
  %6 = load i32, i32* %5, align 4
  %7 = icmp eq i32 %6, 0
  br i1 %7, label %8, label %15

8:                                                ; preds = %4
  %9 = getelementptr inbounds %struct.txfm_param, %struct.txfm_param* %3, i64 0, i32 0
  %10 = load i8, i8* %9, align 4
  %11 = getelementptr inbounds %struct.txfm_param, %struct.txfm_param* %3, i64 0, i32 1
  %12 = load i8, i8* %11, align 1
  %13 = getelementptr inbounds %struct.txfm_param, %struct.txfm_param* %3, i64 0, i32 6
  %14 = load i32, i32* %13, align 4
  tail call void @av1_lowbd_inv_txfm2d_add_ssse3(i32* %0, i8* %1, i32 %2, i8 zeroext %10, i8 zeroext %12, i32 %14)
  br label %16

15:                                               ; preds = %4
  tail call void @av1_inv_txfm_add_c(i32* %0, i8* %1, i32 %2, %struct.txfm_param* %3) #9
  br label %16

16:                                               ; preds = %15, %8
  ret void
}

; Function Attrs: argmemonly nounwind
declare void @llvm.lifetime.start.p0i8(i64 immarg, i8* nocapture) #2

; Function Attrs: argmemonly nounwind
declare void @llvm.lifetime.end.p0i8(i64 immarg, i8* nocapture) #2

declare void @av1_inv_txfm_add_c(i32*, i8*, i32, %struct.txfm_param*) local_unnamed_addr #3

; Function Attrs: argmemonly nounwind
declare void @llvm.memset.p0i8.i64(i8* nocapture writeonly, i8, i64, i1 immarg) #2

; Function Attrs: nofree nounwind ssp uwtable
define internal void @idct4_w4_sse2(<2 x i64>* nocapture readonly, <2 x i64>* nocapture, i8 signext) #4 {
  %4 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 32), align 16
  %5 = and i32 %4, 65535
  %6 = shl i32 %4, 16
  %7 = or i32 %5, %6
  %8 = insertelement <4 x i32> undef, i32 %7, i32 0
  %9 = shufflevector <4 x i32> %8, <4 x i32> undef, <4 x i32> zeroinitializer
  %10 = sub i32 0, %6
  %11 = or i32 %5, %10
  %12 = insertelement <4 x i32> undef, i32 %11, i32 0
  %13 = shufflevector <4 x i32> %12, <4 x i32> undef, <4 x i32> zeroinitializer
  %14 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 48), align 16
  %15 = and i32 %14, 65535
  %16 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 16), align 16
  %17 = shl i32 %16, 16
  %18 = sub i32 0, %17
  %19 = or i32 %15, %18
  %20 = insertelement <4 x i32> undef, i32 %19, i32 0
  %21 = shufflevector <4 x i32> %20, <4 x i32> undef, <4 x i32> zeroinitializer
  %22 = and i32 %16, 65535
  %23 = shl i32 %14, 16
  %24 = or i32 %22, %23
  %25 = insertelement <4 x i32> undef, i32 %24, i32 0
  %26 = shufflevector <4 x i32> %25, <4 x i32> undef, <4 x i32> zeroinitializer
  %27 = bitcast <2 x i64>* %0 to <8 x i16>*
  %28 = load <8 x i16>, <8 x i16>* %27, align 16
  %29 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 2
  %30 = bitcast <2 x i64>* %29 to <8 x i16>*
  %31 = load <8 x i16>, <8 x i16>* %30, align 16
  %32 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 1
  %33 = bitcast <2 x i64>* %32 to <8 x i16>*
  %34 = load <8 x i16>, <8 x i16>* %33, align 16
  %35 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 3
  %36 = bitcast <2 x i64>* %35 to <8 x i16>*
  %37 = load <8 x i16>, <8 x i16>* %36, align 16
  %38 = shufflevector <8 x i16> %28, <8 x i16> %31, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %39 = bitcast <4 x i32> %9 to <8 x i16>
  %40 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %38, <8 x i16> %39) #9
  %41 = bitcast <4 x i32> %13 to <8 x i16>
  %42 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %38, <8 x i16> %41) #9
  %43 = add <4 x i32> %40, <i32 2048, i32 2048, i32 2048, i32 2048>
  %44 = add <4 x i32> %42, <i32 2048, i32 2048, i32 2048, i32 2048>
  %45 = sext i8 %2 to i32
  %46 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %43, i32 %45) #9
  %47 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %44, i32 %45) #9
  %48 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %46, <4 x i32> %46) #9
  %49 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %47, <4 x i32> %47) #9
  %50 = shufflevector <8 x i16> %34, <8 x i16> %37, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %51 = bitcast <4 x i32> %21 to <8 x i16>
  %52 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %50, <8 x i16> %51) #9
  %53 = bitcast <4 x i32> %26 to <8 x i16>
  %54 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %50, <8 x i16> %53) #9
  %55 = add <4 x i32> %52, <i32 2048, i32 2048, i32 2048, i32 2048>
  %56 = add <4 x i32> %54, <i32 2048, i32 2048, i32 2048, i32 2048>
  %57 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %55, i32 %45) #9
  %58 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %56, i32 %45) #9
  %59 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %57, <4 x i32> %57) #9
  %60 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %58, <4 x i32> %58) #9
  %61 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %48, <8 x i16> %60) #9
  %62 = bitcast <2 x i64>* %1 to <8 x i16>*
  store <8 x i16> %61, <8 x i16>* %62, align 16
  %63 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %48, <8 x i16> %60) #9
  %64 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 3
  %65 = bitcast <2 x i64>* %64 to <8 x i16>*
  store <8 x i16> %63, <8 x i16>* %65, align 16
  %66 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %49, <8 x i16> %59) #9
  %67 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 1
  %68 = bitcast <2 x i64>* %67 to <8 x i16>*
  store <8 x i16> %66, <8 x i16>* %68, align 16
  %69 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %49, <8 x i16> %59) #9
  %70 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 2
  %71 = bitcast <2 x i64>* %70 to <8 x i16>*
  store <8 x i16> %69, <8 x i16>* %71, align 16
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal void @iadst4_w4_sse2(<2 x i64>* nocapture readonly, <2 x i64>* nocapture, i8 signext) #0 {
  %4 = load i32, i32* getelementptr inbounds ([7 x [5 x i32]], [7 x [5 x i32]]* @av1_sinpi_arr_data, i64 0, i64 2, i64 1), align 4
  %5 = and i32 %4, 65535
  %6 = load i32, i32* getelementptr inbounds ([7 x [5 x i32]], [7 x [5 x i32]]* @av1_sinpi_arr_data, i64 0, i64 2, i64 4), align 8
  %7 = shl i32 %6, 16
  %8 = or i32 %7, %5
  %9 = insertelement <4 x i32> undef, i32 %8, i32 0
  %10 = shufflevector <4 x i32> %9, <4 x i32> undef, <4 x i32> zeroinitializer
  %11 = load i32, i32* getelementptr inbounds ([7 x [5 x i32]], [7 x [5 x i32]]* @av1_sinpi_arr_data, i64 0, i64 2, i64 2), align 8
  %12 = and i32 %11, 65535
  %13 = shl i32 %4, 16
  %14 = sub i32 0, %13
  %15 = or i32 %12, %14
  %16 = insertelement <4 x i32> undef, i32 %15, i32 0
  %17 = shufflevector <4 x i32> %16, <4 x i32> undef, <4 x i32> zeroinitializer
  %18 = load i32, i32* getelementptr inbounds ([7 x [5 x i32]], [7 x [5 x i32]]* @av1_sinpi_arr_data, i64 0, i64 2, i64 3), align 4
  %19 = and i32 %18, 65535
  %20 = shl i32 %11, 16
  %21 = or i32 %19, %20
  %22 = insertelement <4 x i32> undef, i32 %21, i32 0
  %23 = shufflevector <4 x i32> %22, <4 x i32> undef, <4 x i32> zeroinitializer
  %24 = sub i32 0, %7
  %25 = or i32 %19, %24
  %26 = insertelement <4 x i32> undef, i32 %25, i32 0
  %27 = shufflevector <4 x i32> %26, <4 x i32> undef, <4 x i32> zeroinitializer
  %28 = shl i32 %18, 16
  %29 = sub i32 0, %28
  %30 = or i32 %19, %29
  %31 = insertelement <4 x i32> undef, i32 %30, i32 0
  %32 = shufflevector <4 x i32> %31, <4 x i32> undef, <4 x i32> zeroinitializer
  %33 = insertelement <4 x i32> undef, i32 %28, i32 0
  %34 = shufflevector <4 x i32> %33, <4 x i32> undef, <4 x i32> zeroinitializer
  %35 = and i32 %6, 65535
  %36 = or i32 %20, %35
  %37 = insertelement <4 x i32> undef, i32 %36, i32 0
  %38 = shufflevector <4 x i32> %37, <4 x i32> undef, <4 x i32> zeroinitializer
  %39 = sub i32 0, %18
  %40 = and i32 %39, 65535
  %41 = or i32 %40, %14
  %42 = insertelement <4 x i32> undef, i32 %41, i32 0
  %43 = shufflevector <4 x i32> %42, <4 x i32> undef, <4 x i32> zeroinitializer
  %44 = bitcast <2 x i64>* %0 to <8 x i16>*
  %45 = load <8 x i16>, <8 x i16>* %44, align 16
  %46 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 1
  %47 = bitcast <2 x i64>* %46 to <8 x i16>*
  %48 = load <8 x i16>, <8 x i16>* %47, align 16
  %49 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 2
  %50 = bitcast <2 x i64>* %49 to <8 x i16>*
  %51 = load <8 x i16>, <8 x i16>* %50, align 16
  %52 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 3
  %53 = bitcast <2 x i64>* %52 to <8 x i16>*
  %54 = load <8 x i16>, <8 x i16>* %53, align 16
  %55 = shufflevector <8 x i16> %45, <8 x i16> %51, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %56 = shufflevector <8 x i16> %48, <8 x i16> %54, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %57 = bitcast <4 x i32> %10 to <8 x i16>
  %58 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %55, <8 x i16> %57) #9
  %59 = bitcast <4 x i32> %17 to <8 x i16>
  %60 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %55, <8 x i16> %59) #9
  %61 = bitcast <4 x i32> %23 to <8 x i16>
  %62 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %56, <8 x i16> %61) #9
  %63 = bitcast <4 x i32> %27 to <8 x i16>
  %64 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %56, <8 x i16> %63) #9
  %65 = bitcast <4 x i32> %32 to <8 x i16>
  %66 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %55, <8 x i16> %65) #9
  %67 = bitcast <4 x i32> %34 to <8 x i16>
  %68 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %56, <8 x i16> %67) #9
  %69 = bitcast <4 x i32> %38 to <8 x i16>
  %70 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %55, <8 x i16> %69) #9
  %71 = bitcast <4 x i32> %43 to <8 x i16>
  %72 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %56, <8 x i16> %71) #9
  %73 = add <4 x i32> %62, %58
  %74 = add <4 x i32> %64, %60
  %75 = add <4 x i32> %68, %66
  %76 = add <4 x i32> %72, %70
  %77 = add <4 x i32> %73, <i32 2048, i32 2048, i32 2048, i32 2048>
  %78 = ashr <4 x i32> %77, <i32 12, i32 12, i32 12, i32 12>
  %79 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %78, <4 x i32> %78) #9
  %80 = bitcast <2 x i64>* %1 to <8 x i16>*
  store <8 x i16> %79, <8 x i16>* %80, align 16
  %81 = add <4 x i32> %74, <i32 2048, i32 2048, i32 2048, i32 2048>
  %82 = ashr <4 x i32> %81, <i32 12, i32 12, i32 12, i32 12>
  %83 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %82, <4 x i32> %82) #9
  %84 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 1
  %85 = bitcast <2 x i64>* %84 to <8 x i16>*
  store <8 x i16> %83, <8 x i16>* %85, align 16
  %86 = add <4 x i32> %75, <i32 2048, i32 2048, i32 2048, i32 2048>
  %87 = ashr <4 x i32> %86, <i32 12, i32 12, i32 12, i32 12>
  %88 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %87, <4 x i32> %87) #9
  %89 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 2
  %90 = bitcast <2 x i64>* %89 to <8 x i16>*
  store <8 x i16> %88, <8 x i16>* %90, align 16
  %91 = add <4 x i32> %76, <i32 2048, i32 2048, i32 2048, i32 2048>
  %92 = ashr <4 x i32> %91, <i32 12, i32 12, i32 12, i32 12>
  %93 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %92, <4 x i32> %92) #9
  %94 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 3
  %95 = bitcast <2 x i64>* %94 to <8 x i16>*
  store <8 x i16> %93, <8 x i16>* %95, align 16
  ret void
}

; Function Attrs: nofree nounwind ssp uwtable
define internal void @iidentity4_ssse3(<2 x i64>* nocapture readonly, <2 x i64>* nocapture, i8 signext) #4 {
  %4 = bitcast <2 x i64>* %0 to <8 x i16>*
  %5 = load <8 x i16>, <8 x i16>* %4, align 16
  %6 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %5, <8 x i16> <i16 13576, i16 13576, i16 13576, i16 13576, i16 13576, i16 13576, i16 13576, i16 13576>) #9
  %7 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %6, <8 x i16> %5) #9
  %8 = bitcast <2 x i64>* %1 to <8 x i16>*
  store <8 x i16> %7, <8 x i16>* %8, align 16
  %9 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 1
  %10 = bitcast <2 x i64>* %9 to <8 x i16>*
  %11 = load <8 x i16>, <8 x i16>* %10, align 16
  %12 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %11, <8 x i16> <i16 13576, i16 13576, i16 13576, i16 13576, i16 13576, i16 13576, i16 13576, i16 13576>) #9
  %13 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %12, <8 x i16> %11) #9
  %14 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 1
  %15 = bitcast <2 x i64>* %14 to <8 x i16>*
  store <8 x i16> %13, <8 x i16>* %15, align 16
  %16 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 2
  %17 = bitcast <2 x i64>* %16 to <8 x i16>*
  %18 = load <8 x i16>, <8 x i16>* %17, align 16
  %19 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %18, <8 x i16> <i16 13576, i16 13576, i16 13576, i16 13576, i16 13576, i16 13576, i16 13576, i16 13576>) #9
  %20 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %19, <8 x i16> %18) #9
  %21 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 2
  %22 = bitcast <2 x i64>* %21 to <8 x i16>*
  store <8 x i16> %20, <8 x i16>* %22, align 16
  %23 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 3
  %24 = bitcast <2 x i64>* %23 to <8 x i16>*
  %25 = load <8 x i16>, <8 x i16>* %24, align 16
  %26 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %25, <8 x i16> <i16 13576, i16 13576, i16 13576, i16 13576, i16 13576, i16 13576, i16 13576, i16 13576>) #9
  %27 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %26, <8 x i16> %25) #9
  %28 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 3
  %29 = bitcast <2 x i64>* %28 to <8 x i16>*
  store <8 x i16> %27, <8 x i16>* %29, align 16
  ret void
}

; Function Attrs: nofree nounwind ssp uwtable
define internal void @idct8_w4_sse2(<2 x i64>* nocapture readonly, <2 x i64>* nocapture, i8 signext) #4 {
  %4 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 56), align 16
  %5 = and i32 %4, 65535
  %6 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 8), align 16
  %7 = shl i32 %6, 16
  %8 = sub i32 0, %7
  %9 = or i32 %5, %8
  %10 = insertelement <4 x i32> undef, i32 %9, i32 0
  %11 = shufflevector <4 x i32> %10, <4 x i32> undef, <4 x i32> zeroinitializer
  %12 = and i32 %6, 65535
  %13 = shl i32 %4, 16
  %14 = or i32 %12, %13
  %15 = insertelement <4 x i32> undef, i32 %14, i32 0
  %16 = shufflevector <4 x i32> %15, <4 x i32> undef, <4 x i32> zeroinitializer
  %17 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 24), align 16
  %18 = and i32 %17, 65535
  %19 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 40), align 16
  %20 = shl i32 %19, 16
  %21 = sub i32 0, %20
  %22 = or i32 %18, %21
  %23 = insertelement <4 x i32> undef, i32 %22, i32 0
  %24 = shufflevector <4 x i32> %23, <4 x i32> undef, <4 x i32> zeroinitializer
  %25 = and i32 %19, 65535
  %26 = shl i32 %17, 16
  %27 = or i32 %25, %26
  %28 = insertelement <4 x i32> undef, i32 %27, i32 0
  %29 = shufflevector <4 x i32> %28, <4 x i32> undef, <4 x i32> zeroinitializer
  %30 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 32), align 16
  %31 = and i32 %30, 65535
  %32 = shl i32 %30, 16
  %33 = or i32 %31, %32
  %34 = insertelement <4 x i32> undef, i32 %33, i32 0
  %35 = shufflevector <4 x i32> %34, <4 x i32> undef, <4 x i32> zeroinitializer
  %36 = sub i32 0, %32
  %37 = or i32 %31, %36
  %38 = insertelement <4 x i32> undef, i32 %37, i32 0
  %39 = shufflevector <4 x i32> %38, <4 x i32> undef, <4 x i32> zeroinitializer
  %40 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 48), align 16
  %41 = and i32 %40, 65535
  %42 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 16), align 16
  %43 = shl i32 %42, 16
  %44 = sub i32 0, %43
  %45 = or i32 %41, %44
  %46 = insertelement <4 x i32> undef, i32 %45, i32 0
  %47 = shufflevector <4 x i32> %46, <4 x i32> undef, <4 x i32> zeroinitializer
  %48 = and i32 %42, 65535
  %49 = shl i32 %40, 16
  %50 = or i32 %48, %49
  %51 = insertelement <4 x i32> undef, i32 %50, i32 0
  %52 = shufflevector <4 x i32> %51, <4 x i32> undef, <4 x i32> zeroinitializer
  %53 = sub i32 0, %30
  %54 = and i32 %53, 65535
  %55 = or i32 %54, %32
  %56 = insertelement <4 x i32> undef, i32 %55, i32 0
  %57 = shufflevector <4 x i32> %56, <4 x i32> undef, <4 x i32> zeroinitializer
  %58 = bitcast <2 x i64>* %0 to <8 x i16>*
  %59 = load <8 x i16>, <8 x i16>* %58, align 16
  %60 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 4
  %61 = bitcast <2 x i64>* %60 to <8 x i16>*
  %62 = load <8 x i16>, <8 x i16>* %61, align 16
  %63 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 2
  %64 = bitcast <2 x i64>* %63 to <8 x i16>*
  %65 = load <8 x i16>, <8 x i16>* %64, align 16
  %66 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 6
  %67 = bitcast <2 x i64>* %66 to <8 x i16>*
  %68 = load <8 x i16>, <8 x i16>* %67, align 16
  %69 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 1
  %70 = bitcast <2 x i64>* %69 to <8 x i16>*
  %71 = load <8 x i16>, <8 x i16>* %70, align 16
  %72 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 5
  %73 = bitcast <2 x i64>* %72 to <8 x i16>*
  %74 = load <8 x i16>, <8 x i16>* %73, align 16
  %75 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 3
  %76 = bitcast <2 x i64>* %75 to <8 x i16>*
  %77 = load <8 x i16>, <8 x i16>* %76, align 16
  %78 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 7
  %79 = bitcast <2 x i64>* %78 to <8 x i16>*
  %80 = load <8 x i16>, <8 x i16>* %79, align 16
  %81 = shufflevector <8 x i16> %71, <8 x i16> %80, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %82 = bitcast <4 x i32> %11 to <8 x i16>
  %83 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %81, <8 x i16> %82) #9
  %84 = bitcast <4 x i32> %16 to <8 x i16>
  %85 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %81, <8 x i16> %84) #9
  %86 = add <4 x i32> %83, <i32 2048, i32 2048, i32 2048, i32 2048>
  %87 = add <4 x i32> %85, <i32 2048, i32 2048, i32 2048, i32 2048>
  %88 = sext i8 %2 to i32
  %89 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %86, i32 %88) #9
  %90 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %87, i32 %88) #9
  %91 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %89, <4 x i32> %89) #9
  %92 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %90, <4 x i32> %90) #9
  %93 = shufflevector <8 x i16> %74, <8 x i16> %77, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %94 = bitcast <4 x i32> %24 to <8 x i16>
  %95 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %93, <8 x i16> %94) #9
  %96 = bitcast <4 x i32> %29 to <8 x i16>
  %97 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %93, <8 x i16> %96) #9
  %98 = add <4 x i32> %95, <i32 2048, i32 2048, i32 2048, i32 2048>
  %99 = add <4 x i32> %97, <i32 2048, i32 2048, i32 2048, i32 2048>
  %100 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %98, i32 %88) #9
  %101 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %99, i32 %88) #9
  %102 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %100, <4 x i32> %100) #9
  %103 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %101, <4 x i32> %101) #9
  %104 = shufflevector <8 x i16> %59, <8 x i16> %62, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %105 = bitcast <4 x i32> %35 to <8 x i16>
  %106 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %104, <8 x i16> %105) #9
  %107 = bitcast <4 x i32> %39 to <8 x i16>
  %108 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %104, <8 x i16> %107) #9
  %109 = add <4 x i32> %106, <i32 2048, i32 2048, i32 2048, i32 2048>
  %110 = add <4 x i32> %108, <i32 2048, i32 2048, i32 2048, i32 2048>
  %111 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %109, i32 %88) #9
  %112 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %110, i32 %88) #9
  %113 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %111, <4 x i32> %111) #9
  %114 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %112, <4 x i32> %112) #9
  %115 = shufflevector <8 x i16> %65, <8 x i16> %68, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %116 = bitcast <4 x i32> %47 to <8 x i16>
  %117 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %115, <8 x i16> %116) #9
  %118 = bitcast <4 x i32> %52 to <8 x i16>
  %119 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %115, <8 x i16> %118) #9
  %120 = add <4 x i32> %117, <i32 2048, i32 2048, i32 2048, i32 2048>
  %121 = add <4 x i32> %119, <i32 2048, i32 2048, i32 2048, i32 2048>
  %122 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %120, i32 %88) #9
  %123 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %121, i32 %88) #9
  %124 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %122, <4 x i32> %122) #9
  %125 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %123, <4 x i32> %123) #9
  %126 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %91, <8 x i16> %102) #9
  %127 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %91, <8 x i16> %102) #9
  %128 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %92, <8 x i16> %103) #9
  %129 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %92, <8 x i16> %103) #9
  %130 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %113, <8 x i16> %125) #9
  %131 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %113, <8 x i16> %125) #9
  %132 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %114, <8 x i16> %124) #9
  %133 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %114, <8 x i16> %124) #9
  %134 = shufflevector <8 x i16> %127, <8 x i16> %128, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %135 = bitcast <4 x i32> %57 to <8 x i16>
  %136 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %134, <8 x i16> %135) #9
  %137 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %134, <8 x i16> %105) #9
  %138 = add <4 x i32> %136, <i32 2048, i32 2048, i32 2048, i32 2048>
  %139 = add <4 x i32> %137, <i32 2048, i32 2048, i32 2048, i32 2048>
  %140 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %138, i32 %88) #9
  %141 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %139, i32 %88) #9
  %142 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %140, <4 x i32> %140) #9
  %143 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %141, <4 x i32> %141) #9
  %144 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %130, <8 x i16> %129) #9
  %145 = bitcast <2 x i64>* %1 to <8 x i16>*
  store <8 x i16> %144, <8 x i16>* %145, align 16
  %146 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %130, <8 x i16> %129) #9
  %147 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 7
  %148 = bitcast <2 x i64>* %147 to <8 x i16>*
  store <8 x i16> %146, <8 x i16>* %148, align 16
  %149 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %132, <8 x i16> %143) #9
  %150 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 1
  %151 = bitcast <2 x i64>* %150 to <8 x i16>*
  store <8 x i16> %149, <8 x i16>* %151, align 16
  %152 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %132, <8 x i16> %143) #9
  %153 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 6
  %154 = bitcast <2 x i64>* %153 to <8 x i16>*
  store <8 x i16> %152, <8 x i16>* %154, align 16
  %155 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %133, <8 x i16> %142) #9
  %156 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 2
  %157 = bitcast <2 x i64>* %156 to <8 x i16>*
  store <8 x i16> %155, <8 x i16>* %157, align 16
  %158 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %133, <8 x i16> %142) #9
  %159 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 5
  %160 = bitcast <2 x i64>* %159 to <8 x i16>*
  store <8 x i16> %158, <8 x i16>* %160, align 16
  %161 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %131, <8 x i16> %126) #9
  %162 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 3
  %163 = bitcast <2 x i64>* %162 to <8 x i16>*
  store <8 x i16> %161, <8 x i16>* %163, align 16
  %164 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %131, <8 x i16> %126) #9
  %165 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 4
  %166 = bitcast <2 x i64>* %165 to <8 x i16>*
  store <8 x i16> %164, <8 x i16>* %166, align 16
  ret void
}

; Function Attrs: nofree nounwind ssp uwtable
define internal void @iadst8_w4_sse2(<2 x i64>* nocapture readonly, <2 x i64>* nocapture, i8 signext) #4 {
  %4 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 4), align 16
  %5 = and i32 %4, 65535
  %6 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 60), align 16
  %7 = shl i32 %6, 16
  %8 = or i32 %7, %5
  %9 = insertelement <4 x i32> undef, i32 %8, i32 0
  %10 = shufflevector <4 x i32> %9, <4 x i32> undef, <4 x i32> zeroinitializer
  %11 = and i32 %6, 65535
  %12 = shl i32 %4, 16
  %13 = sub i32 0, %12
  %14 = or i32 %11, %13
  %15 = insertelement <4 x i32> undef, i32 %14, i32 0
  %16 = shufflevector <4 x i32> %15, <4 x i32> undef, <4 x i32> zeroinitializer
  %17 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 20), align 16
  %18 = and i32 %17, 65535
  %19 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 44), align 16
  %20 = shl i32 %19, 16
  %21 = or i32 %20, %18
  %22 = insertelement <4 x i32> undef, i32 %21, i32 0
  %23 = shufflevector <4 x i32> %22, <4 x i32> undef, <4 x i32> zeroinitializer
  %24 = and i32 %19, 65535
  %25 = shl i32 %17, 16
  %26 = sub i32 0, %25
  %27 = or i32 %24, %26
  %28 = insertelement <4 x i32> undef, i32 %27, i32 0
  %29 = shufflevector <4 x i32> %28, <4 x i32> undef, <4 x i32> zeroinitializer
  %30 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 36), align 16
  %31 = and i32 %30, 65535
  %32 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 28), align 16
  %33 = shl i32 %32, 16
  %34 = or i32 %33, %31
  %35 = insertelement <4 x i32> undef, i32 %34, i32 0
  %36 = shufflevector <4 x i32> %35, <4 x i32> undef, <4 x i32> zeroinitializer
  %37 = and i32 %32, 65535
  %38 = shl i32 %30, 16
  %39 = sub i32 0, %38
  %40 = or i32 %37, %39
  %41 = insertelement <4 x i32> undef, i32 %40, i32 0
  %42 = shufflevector <4 x i32> %41, <4 x i32> undef, <4 x i32> zeroinitializer
  %43 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 52), align 16
  %44 = and i32 %43, 65535
  %45 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 12), align 16
  %46 = shl i32 %45, 16
  %47 = or i32 %46, %44
  %48 = insertelement <4 x i32> undef, i32 %47, i32 0
  %49 = shufflevector <4 x i32> %48, <4 x i32> undef, <4 x i32> zeroinitializer
  %50 = and i32 %45, 65535
  %51 = shl i32 %43, 16
  %52 = sub i32 0, %51
  %53 = or i32 %50, %52
  %54 = insertelement <4 x i32> undef, i32 %53, i32 0
  %55 = shufflevector <4 x i32> %54, <4 x i32> undef, <4 x i32> zeroinitializer
  %56 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 16), align 16
  %57 = and i32 %56, 65535
  %58 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 48), align 16
  %59 = shl i32 %58, 16
  %60 = or i32 %59, %57
  %61 = insertelement <4 x i32> undef, i32 %60, i32 0
  %62 = shufflevector <4 x i32> %61, <4 x i32> undef, <4 x i32> zeroinitializer
  %63 = and i32 %58, 65535
  %64 = shl i32 %56, 16
  %65 = sub i32 0, %64
  %66 = or i32 %63, %65
  %67 = insertelement <4 x i32> undef, i32 %66, i32 0
  %68 = shufflevector <4 x i32> %67, <4 x i32> undef, <4 x i32> zeroinitializer
  %69 = sub i32 0, %58
  %70 = and i32 %69, 65535
  %71 = or i32 %70, %64
  %72 = insertelement <4 x i32> undef, i32 %71, i32 0
  %73 = shufflevector <4 x i32> %72, <4 x i32> undef, <4 x i32> zeroinitializer
  %74 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 32), align 16
  %75 = and i32 %74, 65535
  %76 = shl i32 %74, 16
  %77 = or i32 %75, %76
  %78 = insertelement <4 x i32> undef, i32 %77, i32 0
  %79 = shufflevector <4 x i32> %78, <4 x i32> undef, <4 x i32> zeroinitializer
  %80 = sub i32 0, %76
  %81 = or i32 %75, %80
  %82 = insertelement <4 x i32> undef, i32 %81, i32 0
  %83 = shufflevector <4 x i32> %82, <4 x i32> undef, <4 x i32> zeroinitializer
  %84 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 7
  %85 = bitcast <2 x i64>* %84 to <8 x i16>*
  %86 = load <8 x i16>, <8 x i16>* %85, align 16
  %87 = bitcast <2 x i64>* %0 to <8 x i16>*
  %88 = load <8 x i16>, <8 x i16>* %87, align 16
  %89 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 5
  %90 = bitcast <2 x i64>* %89 to <8 x i16>*
  %91 = load <8 x i16>, <8 x i16>* %90, align 16
  %92 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 2
  %93 = bitcast <2 x i64>* %92 to <8 x i16>*
  %94 = load <8 x i16>, <8 x i16>* %93, align 16
  %95 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 3
  %96 = bitcast <2 x i64>* %95 to <8 x i16>*
  %97 = load <8 x i16>, <8 x i16>* %96, align 16
  %98 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 4
  %99 = bitcast <2 x i64>* %98 to <8 x i16>*
  %100 = load <8 x i16>, <8 x i16>* %99, align 16
  %101 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 1
  %102 = bitcast <2 x i64>* %101 to <8 x i16>*
  %103 = load <8 x i16>, <8 x i16>* %102, align 16
  %104 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 6
  %105 = bitcast <2 x i64>* %104 to <8 x i16>*
  %106 = load <8 x i16>, <8 x i16>* %105, align 16
  %107 = shufflevector <8 x i16> %86, <8 x i16> %88, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %108 = bitcast <4 x i32> %10 to <8 x i16>
  %109 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %107, <8 x i16> %108) #9
  %110 = bitcast <4 x i32> %16 to <8 x i16>
  %111 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %107, <8 x i16> %110) #9
  %112 = add <4 x i32> %109, <i32 2048, i32 2048, i32 2048, i32 2048>
  %113 = add <4 x i32> %111, <i32 2048, i32 2048, i32 2048, i32 2048>
  %114 = sext i8 %2 to i32
  %115 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %112, i32 %114) #9
  %116 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %113, i32 %114) #9
  %117 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %115, <4 x i32> %115) #9
  %118 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %116, <4 x i32> %116) #9
  %119 = shufflevector <8 x i16> %91, <8 x i16> %94, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %120 = bitcast <4 x i32> %23 to <8 x i16>
  %121 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %119, <8 x i16> %120) #9
  %122 = bitcast <4 x i32> %29 to <8 x i16>
  %123 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %119, <8 x i16> %122) #9
  %124 = add <4 x i32> %121, <i32 2048, i32 2048, i32 2048, i32 2048>
  %125 = add <4 x i32> %123, <i32 2048, i32 2048, i32 2048, i32 2048>
  %126 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %124, i32 %114) #9
  %127 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %125, i32 %114) #9
  %128 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %126, <4 x i32> %126) #9
  %129 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %127, <4 x i32> %127) #9
  %130 = shufflevector <8 x i16> %97, <8 x i16> %100, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %131 = bitcast <4 x i32> %36 to <8 x i16>
  %132 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %130, <8 x i16> %131) #9
  %133 = bitcast <4 x i32> %42 to <8 x i16>
  %134 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %130, <8 x i16> %133) #9
  %135 = add <4 x i32> %132, <i32 2048, i32 2048, i32 2048, i32 2048>
  %136 = add <4 x i32> %134, <i32 2048, i32 2048, i32 2048, i32 2048>
  %137 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %135, i32 %114) #9
  %138 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %136, i32 %114) #9
  %139 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %137, <4 x i32> %137) #9
  %140 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %138, <4 x i32> %138) #9
  %141 = shufflevector <8 x i16> %103, <8 x i16> %106, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %142 = bitcast <4 x i32> %49 to <8 x i16>
  %143 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %141, <8 x i16> %142) #9
  %144 = bitcast <4 x i32> %55 to <8 x i16>
  %145 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %141, <8 x i16> %144) #9
  %146 = add <4 x i32> %143, <i32 2048, i32 2048, i32 2048, i32 2048>
  %147 = add <4 x i32> %145, <i32 2048, i32 2048, i32 2048, i32 2048>
  %148 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %146, i32 %114) #9
  %149 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %147, i32 %114) #9
  %150 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %148, <4 x i32> %148) #9
  %151 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %149, <4 x i32> %149) #9
  %152 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %117, <8 x i16> %139) #9
  %153 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %117, <8 x i16> %139) #9
  %154 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %118, <8 x i16> %140) #9
  %155 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %118, <8 x i16> %140) #9
  %156 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %128, <8 x i16> %150) #9
  %157 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %128, <8 x i16> %150) #9
  %158 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %129, <8 x i16> %151) #9
  %159 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %129, <8 x i16> %151) #9
  %160 = shufflevector <8 x i16> %153, <8 x i16> %155, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %161 = bitcast <4 x i32> %62 to <8 x i16>
  %162 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %160, <8 x i16> %161) #9
  %163 = bitcast <4 x i32> %68 to <8 x i16>
  %164 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %160, <8 x i16> %163) #9
  %165 = add <4 x i32> %162, <i32 2048, i32 2048, i32 2048, i32 2048>
  %166 = add <4 x i32> %164, <i32 2048, i32 2048, i32 2048, i32 2048>
  %167 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %165, i32 %114) #9
  %168 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %166, i32 %114) #9
  %169 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %167, <4 x i32> %167) #9
  %170 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %168, <4 x i32> %168) #9
  %171 = shufflevector <8 x i16> %157, <8 x i16> %159, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %172 = bitcast <4 x i32> %73 to <8 x i16>
  %173 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %171, <8 x i16> %172) #9
  %174 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %171, <8 x i16> %161) #9
  %175 = add <4 x i32> %173, <i32 2048, i32 2048, i32 2048, i32 2048>
  %176 = add <4 x i32> %174, <i32 2048, i32 2048, i32 2048, i32 2048>
  %177 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %175, i32 %114) #9
  %178 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %176, i32 %114) #9
  %179 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %177, <4 x i32> %177) #9
  %180 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %178, <4 x i32> %178) #9
  %181 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %152, <8 x i16> %156) #9
  %182 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %152, <8 x i16> %156) #9
  %183 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %154, <8 x i16> %158) #9
  %184 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %154, <8 x i16> %158) #9
  %185 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %169, <8 x i16> %179) #9
  %186 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %169, <8 x i16> %179) #9
  %187 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %170, <8 x i16> %180) #9
  %188 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %170, <8 x i16> %180) #9
  %189 = shufflevector <8 x i16> %182, <8 x i16> %184, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %190 = bitcast <4 x i32> %79 to <8 x i16>
  %191 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %189, <8 x i16> %190) #9
  %192 = bitcast <4 x i32> %83 to <8 x i16>
  %193 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %189, <8 x i16> %192) #9
  %194 = add <4 x i32> %191, <i32 2048, i32 2048, i32 2048, i32 2048>
  %195 = add <4 x i32> %193, <i32 2048, i32 2048, i32 2048, i32 2048>
  %196 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %194, i32 %114) #9
  %197 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %195, i32 %114) #9
  %198 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %196, <4 x i32> %196) #9
  %199 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %197, <4 x i32> %197) #9
  %200 = shufflevector <8 x i16> %186, <8 x i16> %188, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %201 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %200, <8 x i16> %190) #9
  %202 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %200, <8 x i16> %192) #9
  %203 = add <4 x i32> %201, <i32 2048, i32 2048, i32 2048, i32 2048>
  %204 = add <4 x i32> %202, <i32 2048, i32 2048, i32 2048, i32 2048>
  %205 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %203, i32 %114) #9
  %206 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %204, i32 %114) #9
  %207 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %205, <4 x i32> %205) #9
  %208 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %206, <4 x i32> %206) #9
  %209 = bitcast <2 x i64>* %1 to <8 x i16>*
  store <8 x i16> %181, <8 x i16>* %209, align 16
  %210 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %185) #9
  %211 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 1
  %212 = bitcast <2 x i64>* %211 to <8 x i16>*
  store <8 x i16> %210, <8 x i16>* %212, align 16
  %213 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 2
  %214 = bitcast <2 x i64>* %213 to <8 x i16>*
  store <8 x i16> %207, <8 x i16>* %214, align 16
  %215 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %198) #9
  %216 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 3
  %217 = bitcast <2 x i64>* %216 to <8 x i16>*
  store <8 x i16> %215, <8 x i16>* %217, align 16
  %218 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 4
  %219 = bitcast <2 x i64>* %218 to <8 x i16>*
  store <8 x i16> %199, <8 x i16>* %219, align 16
  %220 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %208) #9
  %221 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 5
  %222 = bitcast <2 x i64>* %221 to <8 x i16>*
  store <8 x i16> %220, <8 x i16>* %222, align 16
  %223 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 6
  %224 = bitcast <2 x i64>* %223 to <8 x i16>*
  store <8 x i16> %187, <8 x i16>* %224, align 16
  %225 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %183) #9
  %226 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 7
  %227 = bitcast <2 x i64>* %226 to <8 x i16>*
  store <8 x i16> %225, <8 x i16>* %227, align 16
  ret void
}

; Function Attrs: nofree nounwind ssp uwtable
define internal void @iidentity8_sse2(<2 x i64>* nocapture readonly, <2 x i64>* nocapture, i8 signext) #4 {
  %4 = bitcast <2 x i64>* %0 to <8 x i16>*
  %5 = load <8 x i16>, <8 x i16>* %4, align 16
  %6 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %5, <8 x i16> %5) #9
  %7 = bitcast <2 x i64>* %1 to <8 x i16>*
  store <8 x i16> %6, <8 x i16>* %7, align 16
  %8 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 1
  %9 = bitcast <2 x i64>* %8 to <8 x i16>*
  %10 = load <8 x i16>, <8 x i16>* %9, align 16
  %11 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %10, <8 x i16> %10) #9
  %12 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 1
  %13 = bitcast <2 x i64>* %12 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %13, align 16
  %14 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 2
  %15 = bitcast <2 x i64>* %14 to <8 x i16>*
  %16 = load <8 x i16>, <8 x i16>* %15, align 16
  %17 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %16, <8 x i16> %16) #9
  %18 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 2
  %19 = bitcast <2 x i64>* %18 to <8 x i16>*
  store <8 x i16> %17, <8 x i16>* %19, align 16
  %20 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 3
  %21 = bitcast <2 x i64>* %20 to <8 x i16>*
  %22 = load <8 x i16>, <8 x i16>* %21, align 16
  %23 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %22, <8 x i16> %22) #9
  %24 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 3
  %25 = bitcast <2 x i64>* %24 to <8 x i16>*
  store <8 x i16> %23, <8 x i16>* %25, align 16
  %26 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 4
  %27 = bitcast <2 x i64>* %26 to <8 x i16>*
  %28 = load <8 x i16>, <8 x i16>* %27, align 16
  %29 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %28, <8 x i16> %28) #9
  %30 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 4
  %31 = bitcast <2 x i64>* %30 to <8 x i16>*
  store <8 x i16> %29, <8 x i16>* %31, align 16
  %32 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 5
  %33 = bitcast <2 x i64>* %32 to <8 x i16>*
  %34 = load <8 x i16>, <8 x i16>* %33, align 16
  %35 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %34, <8 x i16> %34) #9
  %36 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 5
  %37 = bitcast <2 x i64>* %36 to <8 x i16>*
  store <8 x i16> %35, <8 x i16>* %37, align 16
  %38 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 6
  %39 = bitcast <2 x i64>* %38 to <8 x i16>*
  %40 = load <8 x i16>, <8 x i16>* %39, align 16
  %41 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %40, <8 x i16> %40) #9
  %42 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 6
  %43 = bitcast <2 x i64>* %42 to <8 x i16>*
  store <8 x i16> %41, <8 x i16>* %43, align 16
  %44 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 7
  %45 = bitcast <2 x i64>* %44 to <8 x i16>*
  %46 = load <8 x i16>, <8 x i16>* %45, align 16
  %47 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %46, <8 x i16> %46) #9
  %48 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 7
  %49 = bitcast <2 x i64>* %48 to <8 x i16>*
  store <8 x i16> %47, <8 x i16>* %49, align 16
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal void @idct16_w4_sse2(<2 x i64>* nocapture readonly, <2 x i64>* nocapture, i8 signext) #0 {
  %4 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 60), align 16
  %5 = and i32 %4, 65535
  %6 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 4), align 16
  %7 = shl i32 %6, 16
  %8 = sub i32 0, %7
  %9 = or i32 %5, %8
  %10 = insertelement <4 x i32> undef, i32 %9, i32 0
  %11 = shufflevector <4 x i32> %10, <4 x i32> undef, <4 x i32> zeroinitializer
  %12 = and i32 %6, 65535
  %13 = shl i32 %4, 16
  %14 = or i32 %12, %13
  %15 = insertelement <4 x i32> undef, i32 %14, i32 0
  %16 = shufflevector <4 x i32> %15, <4 x i32> undef, <4 x i32> zeroinitializer
  %17 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 28), align 16
  %18 = and i32 %17, 65535
  %19 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 36), align 16
  %20 = shl i32 %19, 16
  %21 = sub i32 0, %20
  %22 = or i32 %18, %21
  %23 = insertelement <4 x i32> undef, i32 %22, i32 0
  %24 = shufflevector <4 x i32> %23, <4 x i32> undef, <4 x i32> zeroinitializer
  %25 = and i32 %19, 65535
  %26 = shl i32 %17, 16
  %27 = or i32 %25, %26
  %28 = insertelement <4 x i32> undef, i32 %27, i32 0
  %29 = shufflevector <4 x i32> %28, <4 x i32> undef, <4 x i32> zeroinitializer
  %30 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 44), align 16
  %31 = and i32 %30, 65535
  %32 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 20), align 16
  %33 = shl i32 %32, 16
  %34 = sub i32 0, %33
  %35 = or i32 %31, %34
  %36 = insertelement <4 x i32> undef, i32 %35, i32 0
  %37 = shufflevector <4 x i32> %36, <4 x i32> undef, <4 x i32> zeroinitializer
  %38 = and i32 %32, 65535
  %39 = shl i32 %30, 16
  %40 = or i32 %38, %39
  %41 = insertelement <4 x i32> undef, i32 %40, i32 0
  %42 = shufflevector <4 x i32> %41, <4 x i32> undef, <4 x i32> zeroinitializer
  %43 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 12), align 16
  %44 = and i32 %43, 65535
  %45 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 52), align 16
  %46 = shl i32 %45, 16
  %47 = sub i32 0, %46
  %48 = or i32 %44, %47
  %49 = insertelement <4 x i32> undef, i32 %48, i32 0
  %50 = shufflevector <4 x i32> %49, <4 x i32> undef, <4 x i32> zeroinitializer
  %51 = and i32 %45, 65535
  %52 = shl i32 %43, 16
  %53 = or i32 %51, %52
  %54 = insertelement <4 x i32> undef, i32 %53, i32 0
  %55 = shufflevector <4 x i32> %54, <4 x i32> undef, <4 x i32> zeroinitializer
  %56 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 56), align 16
  %57 = and i32 %56, 65535
  %58 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 8), align 16
  %59 = shl i32 %58, 16
  %60 = sub i32 0, %59
  %61 = or i32 %57, %60
  %62 = insertelement <4 x i32> undef, i32 %61, i32 0
  %63 = shufflevector <4 x i32> %62, <4 x i32> undef, <4 x i32> zeroinitializer
  %64 = and i32 %58, 65535
  %65 = shl i32 %56, 16
  %66 = or i32 %64, %65
  %67 = insertelement <4 x i32> undef, i32 %66, i32 0
  %68 = shufflevector <4 x i32> %67, <4 x i32> undef, <4 x i32> zeroinitializer
  %69 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 24), align 16
  %70 = and i32 %69, 65535
  %71 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 40), align 16
  %72 = shl i32 %71, 16
  %73 = sub i32 0, %72
  %74 = or i32 %70, %73
  %75 = insertelement <4 x i32> undef, i32 %74, i32 0
  %76 = shufflevector <4 x i32> %75, <4 x i32> undef, <4 x i32> zeroinitializer
  %77 = and i32 %71, 65535
  %78 = shl i32 %69, 16
  %79 = or i32 %77, %78
  %80 = insertelement <4 x i32> undef, i32 %79, i32 0
  %81 = shufflevector <4 x i32> %80, <4 x i32> undef, <4 x i32> zeroinitializer
  %82 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 32), align 16
  %83 = and i32 %82, 65535
  %84 = shl i32 %82, 16
  %85 = or i32 %83, %84
  %86 = insertelement <4 x i32> undef, i32 %85, i32 0
  %87 = shufflevector <4 x i32> %86, <4 x i32> undef, <4 x i32> zeroinitializer
  %88 = sub i32 0, %84
  %89 = or i32 %83, %88
  %90 = insertelement <4 x i32> undef, i32 %89, i32 0
  %91 = shufflevector <4 x i32> %90, <4 x i32> undef, <4 x i32> zeroinitializer
  %92 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 48), align 16
  %93 = and i32 %92, 65535
  %94 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 16), align 16
  %95 = shl i32 %94, 16
  %96 = sub i32 0, %95
  %97 = or i32 %93, %96
  %98 = insertelement <4 x i32> undef, i32 %97, i32 0
  %99 = shufflevector <4 x i32> %98, <4 x i32> undef, <4 x i32> zeroinitializer
  %100 = and i32 %94, 65535
  %101 = shl i32 %92, 16
  %102 = or i32 %100, %101
  %103 = insertelement <4 x i32> undef, i32 %102, i32 0
  %104 = shufflevector <4 x i32> %103, <4 x i32> undef, <4 x i32> zeroinitializer
  %105 = sub i32 0, %94
  %106 = and i32 %105, 65535
  %107 = or i32 %106, %101
  %108 = insertelement <4 x i32> undef, i32 %107, i32 0
  %109 = shufflevector <4 x i32> %108, <4 x i32> undef, <4 x i32> zeroinitializer
  %110 = or i32 %95, %93
  %111 = insertelement <4 x i32> undef, i32 %110, i32 0
  %112 = shufflevector <4 x i32> %111, <4 x i32> undef, <4 x i32> zeroinitializer
  %113 = sub i32 0, %92
  %114 = and i32 %113, 65535
  %115 = or i32 %114, %96
  %116 = insertelement <4 x i32> undef, i32 %115, i32 0
  %117 = shufflevector <4 x i32> %116, <4 x i32> undef, <4 x i32> zeroinitializer
  %118 = sub i32 0, %82
  %119 = and i32 %118, 65535
  %120 = or i32 %119, %84
  %121 = insertelement <4 x i32> undef, i32 %120, i32 0
  %122 = shufflevector <4 x i32> %121, <4 x i32> undef, <4 x i32> zeroinitializer
  %123 = bitcast <2 x i64>* %0 to <8 x i16>*
  %124 = load <8 x i16>, <8 x i16>* %123, align 16
  %125 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 8
  %126 = bitcast <2 x i64>* %125 to <8 x i16>*
  %127 = load <8 x i16>, <8 x i16>* %126, align 16
  %128 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 4
  %129 = bitcast <2 x i64>* %128 to <8 x i16>*
  %130 = load <8 x i16>, <8 x i16>* %129, align 16
  %131 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 12
  %132 = bitcast <2 x i64>* %131 to <8 x i16>*
  %133 = load <8 x i16>, <8 x i16>* %132, align 16
  %134 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 2
  %135 = bitcast <2 x i64>* %134 to <8 x i16>*
  %136 = load <8 x i16>, <8 x i16>* %135, align 16
  %137 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 10
  %138 = bitcast <2 x i64>* %137 to <8 x i16>*
  %139 = load <8 x i16>, <8 x i16>* %138, align 16
  %140 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 6
  %141 = bitcast <2 x i64>* %140 to <8 x i16>*
  %142 = load <8 x i16>, <8 x i16>* %141, align 16
  %143 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 14
  %144 = bitcast <2 x i64>* %143 to <8 x i16>*
  %145 = load <8 x i16>, <8 x i16>* %144, align 16
  %146 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 1
  %147 = bitcast <2 x i64>* %146 to <8 x i16>*
  %148 = load <8 x i16>, <8 x i16>* %147, align 16
  %149 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 9
  %150 = bitcast <2 x i64>* %149 to <8 x i16>*
  %151 = load <8 x i16>, <8 x i16>* %150, align 16
  %152 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 5
  %153 = bitcast <2 x i64>* %152 to <8 x i16>*
  %154 = load <8 x i16>, <8 x i16>* %153, align 16
  %155 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 13
  %156 = bitcast <2 x i64>* %155 to <8 x i16>*
  %157 = load <8 x i16>, <8 x i16>* %156, align 16
  %158 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 3
  %159 = bitcast <2 x i64>* %158 to <8 x i16>*
  %160 = load <8 x i16>, <8 x i16>* %159, align 16
  %161 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 11
  %162 = bitcast <2 x i64>* %161 to <8 x i16>*
  %163 = load <8 x i16>, <8 x i16>* %162, align 16
  %164 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 7
  %165 = bitcast <2 x i64>* %164 to <8 x i16>*
  %166 = load <8 x i16>, <8 x i16>* %165, align 16
  %167 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 15
  %168 = bitcast <2 x i64>* %167 to <8 x i16>*
  %169 = load <8 x i16>, <8 x i16>* %168, align 16
  %170 = shufflevector <8 x i16> %148, <8 x i16> %169, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %171 = bitcast <4 x i32> %11 to <8 x i16>
  %172 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %170, <8 x i16> %171) #9
  %173 = bitcast <4 x i32> %16 to <8 x i16>
  %174 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %170, <8 x i16> %173) #9
  %175 = add <4 x i32> %172, <i32 2048, i32 2048, i32 2048, i32 2048>
  %176 = add <4 x i32> %174, <i32 2048, i32 2048, i32 2048, i32 2048>
  %177 = sext i8 %2 to i32
  %178 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %175, i32 %177) #9
  %179 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %176, i32 %177) #9
  %180 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %178, <4 x i32> %178) #9
  %181 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %179, <4 x i32> %179) #9
  %182 = shufflevector <8 x i16> %151, <8 x i16> %166, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %183 = bitcast <4 x i32> %24 to <8 x i16>
  %184 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %182, <8 x i16> %183) #9
  %185 = bitcast <4 x i32> %29 to <8 x i16>
  %186 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %182, <8 x i16> %185) #9
  %187 = add <4 x i32> %184, <i32 2048, i32 2048, i32 2048, i32 2048>
  %188 = add <4 x i32> %186, <i32 2048, i32 2048, i32 2048, i32 2048>
  %189 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %187, i32 %177) #9
  %190 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %188, i32 %177) #9
  %191 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %189, <4 x i32> %189) #9
  %192 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %190, <4 x i32> %190) #9
  %193 = shufflevector <8 x i16> %154, <8 x i16> %163, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %194 = bitcast <4 x i32> %37 to <8 x i16>
  %195 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %193, <8 x i16> %194) #9
  %196 = bitcast <4 x i32> %42 to <8 x i16>
  %197 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %193, <8 x i16> %196) #9
  %198 = add <4 x i32> %195, <i32 2048, i32 2048, i32 2048, i32 2048>
  %199 = add <4 x i32> %197, <i32 2048, i32 2048, i32 2048, i32 2048>
  %200 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %198, i32 %177) #9
  %201 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %199, i32 %177) #9
  %202 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %200, <4 x i32> %200) #9
  %203 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %201, <4 x i32> %201) #9
  %204 = shufflevector <8 x i16> %157, <8 x i16> %160, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %205 = bitcast <4 x i32> %50 to <8 x i16>
  %206 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %204, <8 x i16> %205) #9
  %207 = bitcast <4 x i32> %55 to <8 x i16>
  %208 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %204, <8 x i16> %207) #9
  %209 = add <4 x i32> %206, <i32 2048, i32 2048, i32 2048, i32 2048>
  %210 = add <4 x i32> %208, <i32 2048, i32 2048, i32 2048, i32 2048>
  %211 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %209, i32 %177) #9
  %212 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %210, i32 %177) #9
  %213 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %211, <4 x i32> %211) #9
  %214 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %212, <4 x i32> %212) #9
  %215 = shufflevector <8 x i16> %136, <8 x i16> %145, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %216 = bitcast <4 x i32> %63 to <8 x i16>
  %217 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %215, <8 x i16> %216) #9
  %218 = bitcast <4 x i32> %68 to <8 x i16>
  %219 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %215, <8 x i16> %218) #9
  %220 = add <4 x i32> %217, <i32 2048, i32 2048, i32 2048, i32 2048>
  %221 = add <4 x i32> %219, <i32 2048, i32 2048, i32 2048, i32 2048>
  %222 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %220, i32 %177) #9
  %223 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %221, i32 %177) #9
  %224 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %222, <4 x i32> %222) #9
  %225 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %223, <4 x i32> %223) #9
  %226 = shufflevector <8 x i16> %139, <8 x i16> %142, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %227 = bitcast <4 x i32> %76 to <8 x i16>
  %228 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %226, <8 x i16> %227) #9
  %229 = bitcast <4 x i32> %81 to <8 x i16>
  %230 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %226, <8 x i16> %229) #9
  %231 = add <4 x i32> %228, <i32 2048, i32 2048, i32 2048, i32 2048>
  %232 = add <4 x i32> %230, <i32 2048, i32 2048, i32 2048, i32 2048>
  %233 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %231, i32 %177) #9
  %234 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %232, i32 %177) #9
  %235 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %233, <4 x i32> %233) #9
  %236 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %234, <4 x i32> %234) #9
  %237 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %180, <8 x i16> %191) #9
  %238 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %180, <8 x i16> %191) #9
  %239 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %213, <8 x i16> %202) #9
  %240 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %213, <8 x i16> %202) #9
  %241 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %214, <8 x i16> %203) #9
  %242 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %214, <8 x i16> %203) #9
  %243 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %181, <8 x i16> %192) #9
  %244 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %181, <8 x i16> %192) #9
  %245 = shufflevector <8 x i16> %124, <8 x i16> %127, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %246 = bitcast <4 x i32> %87 to <8 x i16>
  %247 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %245, <8 x i16> %246) #9
  %248 = bitcast <4 x i32> %91 to <8 x i16>
  %249 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %245, <8 x i16> %248) #9
  %250 = add <4 x i32> %247, <i32 2048, i32 2048, i32 2048, i32 2048>
  %251 = add <4 x i32> %249, <i32 2048, i32 2048, i32 2048, i32 2048>
  %252 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %250, i32 %177) #9
  %253 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %251, i32 %177) #9
  %254 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %252, <4 x i32> %252) #9
  %255 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %253, <4 x i32> %253) #9
  %256 = shufflevector <8 x i16> %130, <8 x i16> %133, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %257 = bitcast <4 x i32> %99 to <8 x i16>
  %258 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %256, <8 x i16> %257) #9
  %259 = bitcast <4 x i32> %104 to <8 x i16>
  %260 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %256, <8 x i16> %259) #9
  %261 = add <4 x i32> %258, <i32 2048, i32 2048, i32 2048, i32 2048>
  %262 = add <4 x i32> %260, <i32 2048, i32 2048, i32 2048, i32 2048>
  %263 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %261, i32 %177) #9
  %264 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %262, i32 %177) #9
  %265 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %263, <4 x i32> %263) #9
  %266 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %264, <4 x i32> %264) #9
  %267 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %224, <8 x i16> %235) #9
  %268 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %224, <8 x i16> %235) #9
  %269 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %225, <8 x i16> %236) #9
  %270 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %225, <8 x i16> %236) #9
  %271 = shufflevector <8 x i16> %238, <8 x i16> %243, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %272 = bitcast <4 x i32> %109 to <8 x i16>
  %273 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %271, <8 x i16> %272) #9
  %274 = bitcast <4 x i32> %112 to <8 x i16>
  %275 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %271, <8 x i16> %274) #9
  %276 = add <4 x i32> %273, <i32 2048, i32 2048, i32 2048, i32 2048>
  %277 = add <4 x i32> %275, <i32 2048, i32 2048, i32 2048, i32 2048>
  %278 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %276, i32 %177) #9
  %279 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %277, i32 %177) #9
  %280 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %278, <4 x i32> %278) #9
  %281 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %279, <4 x i32> %279) #9
  %282 = shufflevector <8 x i16> %239, <8 x i16> %242, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %283 = bitcast <4 x i32> %117 to <8 x i16>
  %284 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %282, <8 x i16> %283) #9
  %285 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %282, <8 x i16> %272) #9
  %286 = add <4 x i32> %284, <i32 2048, i32 2048, i32 2048, i32 2048>
  %287 = add <4 x i32> %285, <i32 2048, i32 2048, i32 2048, i32 2048>
  %288 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %286, i32 %177) #9
  %289 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %287, i32 %177) #9
  %290 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %288, <4 x i32> %288) #9
  %291 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %289, <4 x i32> %289) #9
  %292 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %254, <8 x i16> %266) #9
  %293 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %254, <8 x i16> %266) #9
  %294 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %255, <8 x i16> %265) #9
  %295 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %255, <8 x i16> %265) #9
  %296 = shufflevector <8 x i16> %268, <8 x i16> %269, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %297 = bitcast <4 x i32> %122 to <8 x i16>
  %298 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %296, <8 x i16> %297) #9
  %299 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %296, <8 x i16> %246) #9
  %300 = add <4 x i32> %298, <i32 2048, i32 2048, i32 2048, i32 2048>
  %301 = add <4 x i32> %299, <i32 2048, i32 2048, i32 2048, i32 2048>
  %302 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %300, i32 %177) #9
  %303 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %301, i32 %177) #9
  %304 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %302, <4 x i32> %302) #9
  %305 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %303, <4 x i32> %303) #9
  %306 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %237, <8 x i16> %240) #9
  %307 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %237, <8 x i16> %240) #9
  %308 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %280, <8 x i16> %290) #9
  %309 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %280, <8 x i16> %290) #9
  %310 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %244, <8 x i16> %241) #9
  %311 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %244, <8 x i16> %241) #9
  %312 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %281, <8 x i16> %291) #9
  %313 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %281, <8 x i16> %291) #9
  %314 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %292, <8 x i16> %270) #9
  %315 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %292, <8 x i16> %270) #9
  %316 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %294, <8 x i16> %305) #9
  %317 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %294, <8 x i16> %305) #9
  %318 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %295, <8 x i16> %304) #9
  %319 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %295, <8 x i16> %304) #9
  %320 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %293, <8 x i16> %267) #9
  %321 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %293, <8 x i16> %267) #9
  %322 = shufflevector <8 x i16> %309, <8 x i16> %312, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %323 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %322, <8 x i16> %297) #9
  %324 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %322, <8 x i16> %246) #9
  %325 = add <4 x i32> %323, <i32 2048, i32 2048, i32 2048, i32 2048>
  %326 = add <4 x i32> %324, <i32 2048, i32 2048, i32 2048, i32 2048>
  %327 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %325, i32 %177) #9
  %328 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %326, i32 %177) #9
  %329 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %327, <4 x i32> %327) #9
  %330 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %328, <4 x i32> %328) #9
  %331 = shufflevector <8 x i16> %307, <8 x i16> %310, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %332 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %331, <8 x i16> %297) #9
  %333 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %331, <8 x i16> %246) #9
  %334 = add <4 x i32> %332, <i32 2048, i32 2048, i32 2048, i32 2048>
  %335 = add <4 x i32> %333, <i32 2048, i32 2048, i32 2048, i32 2048>
  %336 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %334, i32 %177) #9
  %337 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %335, i32 %177) #9
  %338 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %336, <4 x i32> %336) #9
  %339 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %337, <4 x i32> %337) #9
  %340 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %314, <8 x i16> %311) #9
  %341 = bitcast <2 x i64>* %1 to <8 x i16>*
  store <8 x i16> %340, <8 x i16>* %341, align 16
  %342 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %314, <8 x i16> %311) #9
  %343 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 15
  %344 = bitcast <2 x i64>* %343 to <8 x i16>*
  store <8 x i16> %342, <8 x i16>* %344, align 16
  %345 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %316, <8 x i16> %313) #9
  %346 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 1
  %347 = bitcast <2 x i64>* %346 to <8 x i16>*
  store <8 x i16> %345, <8 x i16>* %347, align 16
  %348 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %316, <8 x i16> %313) #9
  %349 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 14
  %350 = bitcast <2 x i64>* %349 to <8 x i16>*
  store <8 x i16> %348, <8 x i16>* %350, align 16
  %351 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %318, <8 x i16> %330) #9
  %352 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 2
  %353 = bitcast <2 x i64>* %352 to <8 x i16>*
  store <8 x i16> %351, <8 x i16>* %353, align 16
  %354 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %318, <8 x i16> %330) #9
  %355 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 13
  %356 = bitcast <2 x i64>* %355 to <8 x i16>*
  store <8 x i16> %354, <8 x i16>* %356, align 16
  %357 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %320, <8 x i16> %339) #9
  %358 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 3
  %359 = bitcast <2 x i64>* %358 to <8 x i16>*
  store <8 x i16> %357, <8 x i16>* %359, align 16
  %360 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %320, <8 x i16> %339) #9
  %361 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 12
  %362 = bitcast <2 x i64>* %361 to <8 x i16>*
  store <8 x i16> %360, <8 x i16>* %362, align 16
  %363 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %321, <8 x i16> %338) #9
  %364 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 4
  %365 = bitcast <2 x i64>* %364 to <8 x i16>*
  store <8 x i16> %363, <8 x i16>* %365, align 16
  %366 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %321, <8 x i16> %338) #9
  %367 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 11
  %368 = bitcast <2 x i64>* %367 to <8 x i16>*
  store <8 x i16> %366, <8 x i16>* %368, align 16
  %369 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %319, <8 x i16> %329) #9
  %370 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 5
  %371 = bitcast <2 x i64>* %370 to <8 x i16>*
  store <8 x i16> %369, <8 x i16>* %371, align 16
  %372 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %319, <8 x i16> %329) #9
  %373 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 10
  %374 = bitcast <2 x i64>* %373 to <8 x i16>*
  store <8 x i16> %372, <8 x i16>* %374, align 16
  %375 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %317, <8 x i16> %308) #9
  %376 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 6
  %377 = bitcast <2 x i64>* %376 to <8 x i16>*
  store <8 x i16> %375, <8 x i16>* %377, align 16
  %378 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %317, <8 x i16> %308) #9
  %379 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 9
  %380 = bitcast <2 x i64>* %379 to <8 x i16>*
  store <8 x i16> %378, <8 x i16>* %380, align 16
  %381 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %315, <8 x i16> %306) #9
  %382 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 7
  %383 = bitcast <2 x i64>* %382 to <8 x i16>*
  store <8 x i16> %381, <8 x i16>* %383, align 16
  %384 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %315, <8 x i16> %306) #9
  %385 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 8
  %386 = bitcast <2 x i64>* %385 to <8 x i16>*
  store <8 x i16> %384, <8 x i16>* %386, align 16
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal void @iadst16_w4_sse2(<2 x i64>* nocapture readonly, <2 x i64>* nocapture, i8 signext) #0 {
  %4 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 2), align 8
  %5 = and i32 %4, 65535
  %6 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 62), align 8
  %7 = shl i32 %6, 16
  %8 = or i32 %7, %5
  %9 = insertelement <4 x i32> undef, i32 %8, i32 0
  %10 = shufflevector <4 x i32> %9, <4 x i32> undef, <4 x i32> zeroinitializer
  %11 = and i32 %6, 65535
  %12 = shl i32 %4, 16
  %13 = sub i32 0, %12
  %14 = or i32 %11, %13
  %15 = insertelement <4 x i32> undef, i32 %14, i32 0
  %16 = shufflevector <4 x i32> %15, <4 x i32> undef, <4 x i32> zeroinitializer
  %17 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 10), align 8
  %18 = and i32 %17, 65535
  %19 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 54), align 8
  %20 = shl i32 %19, 16
  %21 = or i32 %20, %18
  %22 = insertelement <4 x i32> undef, i32 %21, i32 0
  %23 = shufflevector <4 x i32> %22, <4 x i32> undef, <4 x i32> zeroinitializer
  %24 = and i32 %19, 65535
  %25 = shl i32 %17, 16
  %26 = sub i32 0, %25
  %27 = or i32 %24, %26
  %28 = insertelement <4 x i32> undef, i32 %27, i32 0
  %29 = shufflevector <4 x i32> %28, <4 x i32> undef, <4 x i32> zeroinitializer
  %30 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 18), align 8
  %31 = and i32 %30, 65535
  %32 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 46), align 8
  %33 = shl i32 %32, 16
  %34 = or i32 %33, %31
  %35 = insertelement <4 x i32> undef, i32 %34, i32 0
  %36 = shufflevector <4 x i32> %35, <4 x i32> undef, <4 x i32> zeroinitializer
  %37 = and i32 %32, 65535
  %38 = shl i32 %30, 16
  %39 = sub i32 0, %38
  %40 = or i32 %37, %39
  %41 = insertelement <4 x i32> undef, i32 %40, i32 0
  %42 = shufflevector <4 x i32> %41, <4 x i32> undef, <4 x i32> zeroinitializer
  %43 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 26), align 8
  %44 = and i32 %43, 65535
  %45 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 38), align 8
  %46 = shl i32 %45, 16
  %47 = or i32 %46, %44
  %48 = insertelement <4 x i32> undef, i32 %47, i32 0
  %49 = shufflevector <4 x i32> %48, <4 x i32> undef, <4 x i32> zeroinitializer
  %50 = and i32 %45, 65535
  %51 = shl i32 %43, 16
  %52 = sub i32 0, %51
  %53 = or i32 %50, %52
  %54 = insertelement <4 x i32> undef, i32 %53, i32 0
  %55 = shufflevector <4 x i32> %54, <4 x i32> undef, <4 x i32> zeroinitializer
  %56 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 34), align 8
  %57 = and i32 %56, 65535
  %58 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 30), align 8
  %59 = shl i32 %58, 16
  %60 = or i32 %59, %57
  %61 = insertelement <4 x i32> undef, i32 %60, i32 0
  %62 = shufflevector <4 x i32> %61, <4 x i32> undef, <4 x i32> zeroinitializer
  %63 = and i32 %58, 65535
  %64 = shl i32 %56, 16
  %65 = sub i32 0, %64
  %66 = or i32 %63, %65
  %67 = insertelement <4 x i32> undef, i32 %66, i32 0
  %68 = shufflevector <4 x i32> %67, <4 x i32> undef, <4 x i32> zeroinitializer
  %69 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 42), align 8
  %70 = and i32 %69, 65535
  %71 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 22), align 8
  %72 = shl i32 %71, 16
  %73 = or i32 %72, %70
  %74 = insertelement <4 x i32> undef, i32 %73, i32 0
  %75 = shufflevector <4 x i32> %74, <4 x i32> undef, <4 x i32> zeroinitializer
  %76 = and i32 %71, 65535
  %77 = shl i32 %69, 16
  %78 = sub i32 0, %77
  %79 = or i32 %76, %78
  %80 = insertelement <4 x i32> undef, i32 %79, i32 0
  %81 = shufflevector <4 x i32> %80, <4 x i32> undef, <4 x i32> zeroinitializer
  %82 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 50), align 8
  %83 = and i32 %82, 65535
  %84 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 14), align 8
  %85 = shl i32 %84, 16
  %86 = or i32 %85, %83
  %87 = insertelement <4 x i32> undef, i32 %86, i32 0
  %88 = shufflevector <4 x i32> %87, <4 x i32> undef, <4 x i32> zeroinitializer
  %89 = and i32 %84, 65535
  %90 = shl i32 %82, 16
  %91 = sub i32 0, %90
  %92 = or i32 %89, %91
  %93 = insertelement <4 x i32> undef, i32 %92, i32 0
  %94 = shufflevector <4 x i32> %93, <4 x i32> undef, <4 x i32> zeroinitializer
  %95 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 58), align 8
  %96 = and i32 %95, 65535
  %97 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 6), align 8
  %98 = shl i32 %97, 16
  %99 = or i32 %98, %96
  %100 = insertelement <4 x i32> undef, i32 %99, i32 0
  %101 = shufflevector <4 x i32> %100, <4 x i32> undef, <4 x i32> zeroinitializer
  %102 = and i32 %97, 65535
  %103 = shl i32 %95, 16
  %104 = sub i32 0, %103
  %105 = or i32 %102, %104
  %106 = insertelement <4 x i32> undef, i32 %105, i32 0
  %107 = shufflevector <4 x i32> %106, <4 x i32> undef, <4 x i32> zeroinitializer
  %108 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 8), align 16
  %109 = and i32 %108, 65535
  %110 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 56), align 16
  %111 = shl i32 %110, 16
  %112 = or i32 %111, %109
  %113 = insertelement <4 x i32> undef, i32 %112, i32 0
  %114 = shufflevector <4 x i32> %113, <4 x i32> undef, <4 x i32> zeroinitializer
  %115 = and i32 %110, 65535
  %116 = shl i32 %108, 16
  %117 = sub i32 0, %116
  %118 = or i32 %115, %117
  %119 = insertelement <4 x i32> undef, i32 %118, i32 0
  %120 = shufflevector <4 x i32> %119, <4 x i32> undef, <4 x i32> zeroinitializer
  %121 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 40), align 16
  %122 = and i32 %121, 65535
  %123 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 24), align 16
  %124 = shl i32 %123, 16
  %125 = or i32 %124, %122
  %126 = insertelement <4 x i32> undef, i32 %125, i32 0
  %127 = shufflevector <4 x i32> %126, <4 x i32> undef, <4 x i32> zeroinitializer
  %128 = and i32 %123, 65535
  %129 = shl i32 %121, 16
  %130 = sub i32 0, %129
  %131 = or i32 %128, %130
  %132 = insertelement <4 x i32> undef, i32 %131, i32 0
  %133 = shufflevector <4 x i32> %132, <4 x i32> undef, <4 x i32> zeroinitializer
  %134 = sub i32 0, %110
  %135 = and i32 %134, 65535
  %136 = or i32 %135, %116
  %137 = insertelement <4 x i32> undef, i32 %136, i32 0
  %138 = shufflevector <4 x i32> %137, <4 x i32> undef, <4 x i32> zeroinitializer
  %139 = sub i32 0, %123
  %140 = and i32 %139, 65535
  %141 = or i32 %140, %129
  %142 = insertelement <4 x i32> undef, i32 %141, i32 0
  %143 = shufflevector <4 x i32> %142, <4 x i32> undef, <4 x i32> zeroinitializer
  %144 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 16), align 16
  %145 = and i32 %144, 65535
  %146 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 48), align 16
  %147 = shl i32 %146, 16
  %148 = or i32 %147, %145
  %149 = insertelement <4 x i32> undef, i32 %148, i32 0
  %150 = shufflevector <4 x i32> %149, <4 x i32> undef, <4 x i32> zeroinitializer
  %151 = and i32 %146, 65535
  %152 = shl i32 %144, 16
  %153 = sub i32 0, %152
  %154 = or i32 %151, %153
  %155 = insertelement <4 x i32> undef, i32 %154, i32 0
  %156 = shufflevector <4 x i32> %155, <4 x i32> undef, <4 x i32> zeroinitializer
  %157 = sub i32 0, %146
  %158 = and i32 %157, 65535
  %159 = or i32 %158, %152
  %160 = insertelement <4 x i32> undef, i32 %159, i32 0
  %161 = shufflevector <4 x i32> %160, <4 x i32> undef, <4 x i32> zeroinitializer
  %162 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 32), align 16
  %163 = and i32 %162, 65535
  %164 = shl i32 %162, 16
  %165 = or i32 %163, %164
  %166 = insertelement <4 x i32> undef, i32 %165, i32 0
  %167 = shufflevector <4 x i32> %166, <4 x i32> undef, <4 x i32> zeroinitializer
  %168 = sub i32 0, %164
  %169 = or i32 %163, %168
  %170 = insertelement <4 x i32> undef, i32 %169, i32 0
  %171 = shufflevector <4 x i32> %170, <4 x i32> undef, <4 x i32> zeroinitializer
  %172 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 15
  %173 = bitcast <2 x i64>* %172 to <8 x i16>*
  %174 = load <8 x i16>, <8 x i16>* %173, align 16
  %175 = bitcast <2 x i64>* %0 to <8 x i16>*
  %176 = load <8 x i16>, <8 x i16>* %175, align 16
  %177 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 13
  %178 = bitcast <2 x i64>* %177 to <8 x i16>*
  %179 = load <8 x i16>, <8 x i16>* %178, align 16
  %180 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 2
  %181 = bitcast <2 x i64>* %180 to <8 x i16>*
  %182 = load <8 x i16>, <8 x i16>* %181, align 16
  %183 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 11
  %184 = bitcast <2 x i64>* %183 to <8 x i16>*
  %185 = load <8 x i16>, <8 x i16>* %184, align 16
  %186 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 4
  %187 = bitcast <2 x i64>* %186 to <8 x i16>*
  %188 = load <8 x i16>, <8 x i16>* %187, align 16
  %189 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 9
  %190 = bitcast <2 x i64>* %189 to <8 x i16>*
  %191 = load <8 x i16>, <8 x i16>* %190, align 16
  %192 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 6
  %193 = bitcast <2 x i64>* %192 to <8 x i16>*
  %194 = load <8 x i16>, <8 x i16>* %193, align 16
  %195 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 7
  %196 = bitcast <2 x i64>* %195 to <8 x i16>*
  %197 = load <8 x i16>, <8 x i16>* %196, align 16
  %198 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 8
  %199 = bitcast <2 x i64>* %198 to <8 x i16>*
  %200 = load <8 x i16>, <8 x i16>* %199, align 16
  %201 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 5
  %202 = bitcast <2 x i64>* %201 to <8 x i16>*
  %203 = load <8 x i16>, <8 x i16>* %202, align 16
  %204 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 10
  %205 = bitcast <2 x i64>* %204 to <8 x i16>*
  %206 = load <8 x i16>, <8 x i16>* %205, align 16
  %207 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 3
  %208 = bitcast <2 x i64>* %207 to <8 x i16>*
  %209 = load <8 x i16>, <8 x i16>* %208, align 16
  %210 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 12
  %211 = bitcast <2 x i64>* %210 to <8 x i16>*
  %212 = load <8 x i16>, <8 x i16>* %211, align 16
  %213 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 1
  %214 = bitcast <2 x i64>* %213 to <8 x i16>*
  %215 = load <8 x i16>, <8 x i16>* %214, align 16
  %216 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 14
  %217 = bitcast <2 x i64>* %216 to <8 x i16>*
  %218 = load <8 x i16>, <8 x i16>* %217, align 16
  %219 = shufflevector <8 x i16> %174, <8 x i16> %176, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %220 = bitcast <4 x i32> %10 to <8 x i16>
  %221 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %219, <8 x i16> %220) #9
  %222 = bitcast <4 x i32> %16 to <8 x i16>
  %223 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %219, <8 x i16> %222) #9
  %224 = add <4 x i32> %221, <i32 2048, i32 2048, i32 2048, i32 2048>
  %225 = add <4 x i32> %223, <i32 2048, i32 2048, i32 2048, i32 2048>
  %226 = sext i8 %2 to i32
  %227 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %224, i32 %226) #9
  %228 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %225, i32 %226) #9
  %229 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %227, <4 x i32> %227) #9
  %230 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %228, <4 x i32> %228) #9
  %231 = shufflevector <8 x i16> %179, <8 x i16> %182, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %232 = bitcast <4 x i32> %23 to <8 x i16>
  %233 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %231, <8 x i16> %232) #9
  %234 = bitcast <4 x i32> %29 to <8 x i16>
  %235 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %231, <8 x i16> %234) #9
  %236 = add <4 x i32> %233, <i32 2048, i32 2048, i32 2048, i32 2048>
  %237 = add <4 x i32> %235, <i32 2048, i32 2048, i32 2048, i32 2048>
  %238 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %236, i32 %226) #9
  %239 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %237, i32 %226) #9
  %240 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %238, <4 x i32> %238) #9
  %241 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %239, <4 x i32> %239) #9
  %242 = shufflevector <8 x i16> %185, <8 x i16> %188, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %243 = bitcast <4 x i32> %36 to <8 x i16>
  %244 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %242, <8 x i16> %243) #9
  %245 = bitcast <4 x i32> %42 to <8 x i16>
  %246 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %242, <8 x i16> %245) #9
  %247 = add <4 x i32> %244, <i32 2048, i32 2048, i32 2048, i32 2048>
  %248 = add <4 x i32> %246, <i32 2048, i32 2048, i32 2048, i32 2048>
  %249 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %247, i32 %226) #9
  %250 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %248, i32 %226) #9
  %251 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %249, <4 x i32> %249) #9
  %252 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %250, <4 x i32> %250) #9
  %253 = shufflevector <8 x i16> %191, <8 x i16> %194, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %254 = bitcast <4 x i32> %49 to <8 x i16>
  %255 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %253, <8 x i16> %254) #9
  %256 = bitcast <4 x i32> %55 to <8 x i16>
  %257 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %253, <8 x i16> %256) #9
  %258 = add <4 x i32> %255, <i32 2048, i32 2048, i32 2048, i32 2048>
  %259 = add <4 x i32> %257, <i32 2048, i32 2048, i32 2048, i32 2048>
  %260 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %258, i32 %226) #9
  %261 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %259, i32 %226) #9
  %262 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %260, <4 x i32> %260) #9
  %263 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %261, <4 x i32> %261) #9
  %264 = shufflevector <8 x i16> %197, <8 x i16> %200, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %265 = bitcast <4 x i32> %62 to <8 x i16>
  %266 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %264, <8 x i16> %265) #9
  %267 = bitcast <4 x i32> %68 to <8 x i16>
  %268 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %264, <8 x i16> %267) #9
  %269 = add <4 x i32> %266, <i32 2048, i32 2048, i32 2048, i32 2048>
  %270 = add <4 x i32> %268, <i32 2048, i32 2048, i32 2048, i32 2048>
  %271 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %269, i32 %226) #9
  %272 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %270, i32 %226) #9
  %273 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %271, <4 x i32> %271) #9
  %274 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %272, <4 x i32> %272) #9
  %275 = shufflevector <8 x i16> %203, <8 x i16> %206, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %276 = bitcast <4 x i32> %75 to <8 x i16>
  %277 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %275, <8 x i16> %276) #9
  %278 = bitcast <4 x i32> %81 to <8 x i16>
  %279 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %275, <8 x i16> %278) #9
  %280 = add <4 x i32> %277, <i32 2048, i32 2048, i32 2048, i32 2048>
  %281 = add <4 x i32> %279, <i32 2048, i32 2048, i32 2048, i32 2048>
  %282 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %280, i32 %226) #9
  %283 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %281, i32 %226) #9
  %284 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %282, <4 x i32> %282) #9
  %285 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %283, <4 x i32> %283) #9
  %286 = shufflevector <8 x i16> %209, <8 x i16> %212, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %287 = bitcast <4 x i32> %88 to <8 x i16>
  %288 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %286, <8 x i16> %287) #9
  %289 = bitcast <4 x i32> %94 to <8 x i16>
  %290 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %286, <8 x i16> %289) #9
  %291 = add <4 x i32> %288, <i32 2048, i32 2048, i32 2048, i32 2048>
  %292 = add <4 x i32> %290, <i32 2048, i32 2048, i32 2048, i32 2048>
  %293 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %291, i32 %226) #9
  %294 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %292, i32 %226) #9
  %295 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %293, <4 x i32> %293) #9
  %296 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %294, <4 x i32> %294) #9
  %297 = shufflevector <8 x i16> %215, <8 x i16> %218, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %298 = bitcast <4 x i32> %101 to <8 x i16>
  %299 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %297, <8 x i16> %298) #9
  %300 = bitcast <4 x i32> %107 to <8 x i16>
  %301 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %297, <8 x i16> %300) #9
  %302 = add <4 x i32> %299, <i32 2048, i32 2048, i32 2048, i32 2048>
  %303 = add <4 x i32> %301, <i32 2048, i32 2048, i32 2048, i32 2048>
  %304 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %302, i32 %226) #9
  %305 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %303, i32 %226) #9
  %306 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %304, <4 x i32> %304) #9
  %307 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %305, <4 x i32> %305) #9
  %308 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %229, <8 x i16> %273) #9
  %309 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %229, <8 x i16> %273) #9
  %310 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %230, <8 x i16> %274) #9
  %311 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %230, <8 x i16> %274) #9
  %312 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %240, <8 x i16> %284) #9
  %313 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %240, <8 x i16> %284) #9
  %314 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %241, <8 x i16> %285) #9
  %315 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %241, <8 x i16> %285) #9
  %316 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %251, <8 x i16> %295) #9
  %317 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %251, <8 x i16> %295) #9
  %318 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %252, <8 x i16> %296) #9
  %319 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %252, <8 x i16> %296) #9
  %320 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %262, <8 x i16> %306) #9
  %321 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %262, <8 x i16> %306) #9
  %322 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %263, <8 x i16> %307) #9
  %323 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %263, <8 x i16> %307) #9
  %324 = shufflevector <8 x i16> %309, <8 x i16> %311, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %325 = bitcast <4 x i32> %114 to <8 x i16>
  %326 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %324, <8 x i16> %325) #9
  %327 = bitcast <4 x i32> %120 to <8 x i16>
  %328 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %324, <8 x i16> %327) #9
  %329 = add <4 x i32> %326, <i32 2048, i32 2048, i32 2048, i32 2048>
  %330 = add <4 x i32> %328, <i32 2048, i32 2048, i32 2048, i32 2048>
  %331 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %329, i32 %226) #9
  %332 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %330, i32 %226) #9
  %333 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %331, <4 x i32> %331) #9
  %334 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %332, <4 x i32> %332) #9
  %335 = shufflevector <8 x i16> %313, <8 x i16> %315, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %336 = bitcast <4 x i32> %127 to <8 x i16>
  %337 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %335, <8 x i16> %336) #9
  %338 = bitcast <4 x i32> %133 to <8 x i16>
  %339 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %335, <8 x i16> %338) #9
  %340 = add <4 x i32> %337, <i32 2048, i32 2048, i32 2048, i32 2048>
  %341 = add <4 x i32> %339, <i32 2048, i32 2048, i32 2048, i32 2048>
  %342 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %340, i32 %226) #9
  %343 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %341, i32 %226) #9
  %344 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %342, <4 x i32> %342) #9
  %345 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %343, <4 x i32> %343) #9
  %346 = shufflevector <8 x i16> %317, <8 x i16> %319, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %347 = bitcast <4 x i32> %138 to <8 x i16>
  %348 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %346, <8 x i16> %347) #9
  %349 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %346, <8 x i16> %325) #9
  %350 = add <4 x i32> %348, <i32 2048, i32 2048, i32 2048, i32 2048>
  %351 = add <4 x i32> %349, <i32 2048, i32 2048, i32 2048, i32 2048>
  %352 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %350, i32 %226) #9
  %353 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %351, i32 %226) #9
  %354 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %352, <4 x i32> %352) #9
  %355 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %353, <4 x i32> %353) #9
  %356 = shufflevector <8 x i16> %321, <8 x i16> %323, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %357 = bitcast <4 x i32> %143 to <8 x i16>
  %358 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %356, <8 x i16> %357) #9
  %359 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %356, <8 x i16> %336) #9
  %360 = add <4 x i32> %358, <i32 2048, i32 2048, i32 2048, i32 2048>
  %361 = add <4 x i32> %359, <i32 2048, i32 2048, i32 2048, i32 2048>
  %362 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %360, i32 %226) #9
  %363 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %361, i32 %226) #9
  %364 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %362, <4 x i32> %362) #9
  %365 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %363, <4 x i32> %363) #9
  %366 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %308, <8 x i16> %316) #9
  %367 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %308, <8 x i16> %316) #9
  %368 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %310, <8 x i16> %318) #9
  %369 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %310, <8 x i16> %318) #9
  %370 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %312, <8 x i16> %320) #9
  %371 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %312, <8 x i16> %320) #9
  %372 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %314, <8 x i16> %322) #9
  %373 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %314, <8 x i16> %322) #9
  %374 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %333, <8 x i16> %354) #9
  %375 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %333, <8 x i16> %354) #9
  %376 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %334, <8 x i16> %355) #9
  %377 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %334, <8 x i16> %355) #9
  %378 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %344, <8 x i16> %364) #9
  %379 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %344, <8 x i16> %364) #9
  %380 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %345, <8 x i16> %365) #9
  %381 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %345, <8 x i16> %365) #9
  %382 = shufflevector <8 x i16> %367, <8 x i16> %369, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %383 = bitcast <4 x i32> %150 to <8 x i16>
  %384 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %382, <8 x i16> %383) #9
  %385 = bitcast <4 x i32> %156 to <8 x i16>
  %386 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %382, <8 x i16> %385) #9
  %387 = add <4 x i32> %384, <i32 2048, i32 2048, i32 2048, i32 2048>
  %388 = add <4 x i32> %386, <i32 2048, i32 2048, i32 2048, i32 2048>
  %389 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %387, i32 %226) #9
  %390 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %388, i32 %226) #9
  %391 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %389, <4 x i32> %389) #9
  %392 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %390, <4 x i32> %390) #9
  %393 = shufflevector <8 x i16> %371, <8 x i16> %373, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %394 = bitcast <4 x i32> %161 to <8 x i16>
  %395 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %393, <8 x i16> %394) #9
  %396 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %393, <8 x i16> %383) #9
  %397 = add <4 x i32> %395, <i32 2048, i32 2048, i32 2048, i32 2048>
  %398 = add <4 x i32> %396, <i32 2048, i32 2048, i32 2048, i32 2048>
  %399 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %397, i32 %226) #9
  %400 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %398, i32 %226) #9
  %401 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %399, <4 x i32> %399) #9
  %402 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %400, <4 x i32> %400) #9
  %403 = shufflevector <8 x i16> %375, <8 x i16> %377, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %404 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %403, <8 x i16> %383) #9
  %405 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %403, <8 x i16> %385) #9
  %406 = add <4 x i32> %404, <i32 2048, i32 2048, i32 2048, i32 2048>
  %407 = add <4 x i32> %405, <i32 2048, i32 2048, i32 2048, i32 2048>
  %408 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %406, i32 %226) #9
  %409 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %407, i32 %226) #9
  %410 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %408, <4 x i32> %408) #9
  %411 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %409, <4 x i32> %409) #9
  %412 = shufflevector <8 x i16> %379, <8 x i16> %381, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %413 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %412, <8 x i16> %394) #9
  %414 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %412, <8 x i16> %383) #9
  %415 = add <4 x i32> %413, <i32 2048, i32 2048, i32 2048, i32 2048>
  %416 = add <4 x i32> %414, <i32 2048, i32 2048, i32 2048, i32 2048>
  %417 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %415, i32 %226) #9
  %418 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %416, i32 %226) #9
  %419 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %417, <4 x i32> %417) #9
  %420 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %418, <4 x i32> %418) #9
  %421 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %366, <8 x i16> %370) #9
  %422 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %366, <8 x i16> %370) #9
  %423 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %368, <8 x i16> %372) #9
  %424 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %368, <8 x i16> %372) #9
  %425 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %391, <8 x i16> %401) #9
  %426 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %391, <8 x i16> %401) #9
  %427 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %392, <8 x i16> %402) #9
  %428 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %392, <8 x i16> %402) #9
  %429 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %374, <8 x i16> %378) #9
  %430 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %374, <8 x i16> %378) #9
  %431 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %376, <8 x i16> %380) #9
  %432 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %376, <8 x i16> %380) #9
  %433 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %410, <8 x i16> %419) #9
  %434 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %410, <8 x i16> %419) #9
  %435 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %411, <8 x i16> %420) #9
  %436 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %411, <8 x i16> %420) #9
  %437 = shufflevector <8 x i16> %422, <8 x i16> %424, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %438 = bitcast <4 x i32> %167 to <8 x i16>
  %439 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %437, <8 x i16> %438) #9
  %440 = bitcast <4 x i32> %171 to <8 x i16>
  %441 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %437, <8 x i16> %440) #9
  %442 = add <4 x i32> %439, <i32 2048, i32 2048, i32 2048, i32 2048>
  %443 = add <4 x i32> %441, <i32 2048, i32 2048, i32 2048, i32 2048>
  %444 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %442, i32 %226) #9
  %445 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %443, i32 %226) #9
  %446 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %444, <4 x i32> %444) #9
  %447 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %445, <4 x i32> %445) #9
  %448 = shufflevector <8 x i16> %426, <8 x i16> %428, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %449 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %448, <8 x i16> %438) #9
  %450 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %448, <8 x i16> %440) #9
  %451 = add <4 x i32> %449, <i32 2048, i32 2048, i32 2048, i32 2048>
  %452 = add <4 x i32> %450, <i32 2048, i32 2048, i32 2048, i32 2048>
  %453 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %451, i32 %226) #9
  %454 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %452, i32 %226) #9
  %455 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %453, <4 x i32> %453) #9
  %456 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %454, <4 x i32> %454) #9
  %457 = shufflevector <8 x i16> %430, <8 x i16> %432, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %458 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %457, <8 x i16> %438) #9
  %459 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %457, <8 x i16> %440) #9
  %460 = add <4 x i32> %458, <i32 2048, i32 2048, i32 2048, i32 2048>
  %461 = add <4 x i32> %459, <i32 2048, i32 2048, i32 2048, i32 2048>
  %462 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %460, i32 %226) #9
  %463 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %461, i32 %226) #9
  %464 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %462, <4 x i32> %462) #9
  %465 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %463, <4 x i32> %463) #9
  %466 = shufflevector <8 x i16> %434, <8 x i16> %436, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %467 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %466, <8 x i16> %438) #9
  %468 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %466, <8 x i16> %440) #9
  %469 = add <4 x i32> %467, <i32 2048, i32 2048, i32 2048, i32 2048>
  %470 = add <4 x i32> %468, <i32 2048, i32 2048, i32 2048, i32 2048>
  %471 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %469, i32 %226) #9
  %472 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %470, i32 %226) #9
  %473 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %471, <4 x i32> %471) #9
  %474 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %472, <4 x i32> %472) #9
  %475 = bitcast <2 x i64>* %1 to <8 x i16>*
  store <8 x i16> %421, <8 x i16>* %475, align 16
  %476 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %429) #9
  %477 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 1
  %478 = bitcast <2 x i64>* %477 to <8 x i16>*
  store <8 x i16> %476, <8 x i16>* %478, align 16
  %479 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 2
  %480 = bitcast <2 x i64>* %479 to <8 x i16>*
  store <8 x i16> %433, <8 x i16>* %480, align 16
  %481 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %425) #9
  %482 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 3
  %483 = bitcast <2 x i64>* %482 to <8 x i16>*
  store <8 x i16> %481, <8 x i16>* %483, align 16
  %484 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 4
  %485 = bitcast <2 x i64>* %484 to <8 x i16>*
  store <8 x i16> %455, <8 x i16>* %485, align 16
  %486 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %473) #9
  %487 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 5
  %488 = bitcast <2 x i64>* %487 to <8 x i16>*
  store <8 x i16> %486, <8 x i16>* %488, align 16
  %489 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 6
  %490 = bitcast <2 x i64>* %489 to <8 x i16>*
  store <8 x i16> %464, <8 x i16>* %490, align 16
  %491 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %446) #9
  %492 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 7
  %493 = bitcast <2 x i64>* %492 to <8 x i16>*
  store <8 x i16> %491, <8 x i16>* %493, align 16
  %494 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 8
  %495 = bitcast <2 x i64>* %494 to <8 x i16>*
  store <8 x i16> %447, <8 x i16>* %495, align 16
  %496 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %465) #9
  %497 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 9
  %498 = bitcast <2 x i64>* %497 to <8 x i16>*
  store <8 x i16> %496, <8 x i16>* %498, align 16
  %499 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 10
  %500 = bitcast <2 x i64>* %499 to <8 x i16>*
  store <8 x i16> %474, <8 x i16>* %500, align 16
  %501 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %456) #9
  %502 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 11
  %503 = bitcast <2 x i64>* %502 to <8 x i16>*
  store <8 x i16> %501, <8 x i16>* %503, align 16
  %504 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 12
  %505 = bitcast <2 x i64>* %504 to <8 x i16>*
  store <8 x i16> %427, <8 x i16>* %505, align 16
  %506 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %435) #9
  %507 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 13
  %508 = bitcast <2 x i64>* %507 to <8 x i16>*
  store <8 x i16> %506, <8 x i16>* %508, align 16
  %509 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 14
  %510 = bitcast <2 x i64>* %509 to <8 x i16>*
  store <8 x i16> %431, <8 x i16>* %510, align 16
  %511 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %423) #9
  %512 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 15
  %513 = bitcast <2 x i64>* %512 to <8 x i16>*
  store <8 x i16> %511, <8 x i16>* %513, align 16
  ret void
}

; Function Attrs: nofree nounwind ssp uwtable
define internal void @iidentity16_ssse3(<2 x i64>* nocapture readonly, <2 x i64>* nocapture, i8 signext) #4 {
  %4 = bitcast <2 x i64>* %0 to <8 x i16>*
  %5 = load <8 x i16>, <8 x i16>* %4, align 16
  %6 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %5, <8 x i16> <i16 27152, i16 27152, i16 27152, i16 27152, i16 27152, i16 27152, i16 27152, i16 27152>) #9
  %7 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %5, <8 x i16> %5) #9
  %8 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %6, <8 x i16> %7) #9
  %9 = bitcast <2 x i64>* %1 to <8 x i16>*
  store <8 x i16> %8, <8 x i16>* %9, align 16
  %10 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 1
  %11 = bitcast <2 x i64>* %10 to <8 x i16>*
  %12 = load <8 x i16>, <8 x i16>* %11, align 16
  %13 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %12, <8 x i16> <i16 27152, i16 27152, i16 27152, i16 27152, i16 27152, i16 27152, i16 27152, i16 27152>) #9
  %14 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %12, <8 x i16> %12) #9
  %15 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %13, <8 x i16> %14) #9
  %16 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 1
  %17 = bitcast <2 x i64>* %16 to <8 x i16>*
  store <8 x i16> %15, <8 x i16>* %17, align 16
  %18 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 2
  %19 = bitcast <2 x i64>* %18 to <8 x i16>*
  %20 = load <8 x i16>, <8 x i16>* %19, align 16
  %21 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %20, <8 x i16> <i16 27152, i16 27152, i16 27152, i16 27152, i16 27152, i16 27152, i16 27152, i16 27152>) #9
  %22 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %20, <8 x i16> %20) #9
  %23 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %21, <8 x i16> %22) #9
  %24 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 2
  %25 = bitcast <2 x i64>* %24 to <8 x i16>*
  store <8 x i16> %23, <8 x i16>* %25, align 16
  %26 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 3
  %27 = bitcast <2 x i64>* %26 to <8 x i16>*
  %28 = load <8 x i16>, <8 x i16>* %27, align 16
  %29 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %28, <8 x i16> <i16 27152, i16 27152, i16 27152, i16 27152, i16 27152, i16 27152, i16 27152, i16 27152>) #9
  %30 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %28, <8 x i16> %28) #9
  %31 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %29, <8 x i16> %30) #9
  %32 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 3
  %33 = bitcast <2 x i64>* %32 to <8 x i16>*
  store <8 x i16> %31, <8 x i16>* %33, align 16
  %34 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 4
  %35 = bitcast <2 x i64>* %34 to <8 x i16>*
  %36 = load <8 x i16>, <8 x i16>* %35, align 16
  %37 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %36, <8 x i16> <i16 27152, i16 27152, i16 27152, i16 27152, i16 27152, i16 27152, i16 27152, i16 27152>) #9
  %38 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %36, <8 x i16> %36) #9
  %39 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %37, <8 x i16> %38) #9
  %40 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 4
  %41 = bitcast <2 x i64>* %40 to <8 x i16>*
  store <8 x i16> %39, <8 x i16>* %41, align 16
  %42 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 5
  %43 = bitcast <2 x i64>* %42 to <8 x i16>*
  %44 = load <8 x i16>, <8 x i16>* %43, align 16
  %45 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %44, <8 x i16> <i16 27152, i16 27152, i16 27152, i16 27152, i16 27152, i16 27152, i16 27152, i16 27152>) #9
  %46 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %44, <8 x i16> %44) #9
  %47 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %45, <8 x i16> %46) #9
  %48 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 5
  %49 = bitcast <2 x i64>* %48 to <8 x i16>*
  store <8 x i16> %47, <8 x i16>* %49, align 16
  %50 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 6
  %51 = bitcast <2 x i64>* %50 to <8 x i16>*
  %52 = load <8 x i16>, <8 x i16>* %51, align 16
  %53 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %52, <8 x i16> <i16 27152, i16 27152, i16 27152, i16 27152, i16 27152, i16 27152, i16 27152, i16 27152>) #9
  %54 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %52, <8 x i16> %52) #9
  %55 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %53, <8 x i16> %54) #9
  %56 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 6
  %57 = bitcast <2 x i64>* %56 to <8 x i16>*
  store <8 x i16> %55, <8 x i16>* %57, align 16
  %58 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 7
  %59 = bitcast <2 x i64>* %58 to <8 x i16>*
  %60 = load <8 x i16>, <8 x i16>* %59, align 16
  %61 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %60, <8 x i16> <i16 27152, i16 27152, i16 27152, i16 27152, i16 27152, i16 27152, i16 27152, i16 27152>) #9
  %62 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %60, <8 x i16> %60) #9
  %63 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %61, <8 x i16> %62) #9
  %64 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 7
  %65 = bitcast <2 x i64>* %64 to <8 x i16>*
  store <8 x i16> %63, <8 x i16>* %65, align 16
  %66 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 8
  %67 = bitcast <2 x i64>* %66 to <8 x i16>*
  %68 = load <8 x i16>, <8 x i16>* %67, align 16
  %69 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %68, <8 x i16> <i16 27152, i16 27152, i16 27152, i16 27152, i16 27152, i16 27152, i16 27152, i16 27152>) #9
  %70 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %68, <8 x i16> %68) #9
  %71 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %69, <8 x i16> %70) #9
  %72 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 8
  %73 = bitcast <2 x i64>* %72 to <8 x i16>*
  store <8 x i16> %71, <8 x i16>* %73, align 16
  %74 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 9
  %75 = bitcast <2 x i64>* %74 to <8 x i16>*
  %76 = load <8 x i16>, <8 x i16>* %75, align 16
  %77 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %76, <8 x i16> <i16 27152, i16 27152, i16 27152, i16 27152, i16 27152, i16 27152, i16 27152, i16 27152>) #9
  %78 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %76, <8 x i16> %76) #9
  %79 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %77, <8 x i16> %78) #9
  %80 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 9
  %81 = bitcast <2 x i64>* %80 to <8 x i16>*
  store <8 x i16> %79, <8 x i16>* %81, align 16
  %82 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 10
  %83 = bitcast <2 x i64>* %82 to <8 x i16>*
  %84 = load <8 x i16>, <8 x i16>* %83, align 16
  %85 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %84, <8 x i16> <i16 27152, i16 27152, i16 27152, i16 27152, i16 27152, i16 27152, i16 27152, i16 27152>) #9
  %86 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %84, <8 x i16> %84) #9
  %87 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %85, <8 x i16> %86) #9
  %88 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 10
  %89 = bitcast <2 x i64>* %88 to <8 x i16>*
  store <8 x i16> %87, <8 x i16>* %89, align 16
  %90 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 11
  %91 = bitcast <2 x i64>* %90 to <8 x i16>*
  %92 = load <8 x i16>, <8 x i16>* %91, align 16
  %93 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %92, <8 x i16> <i16 27152, i16 27152, i16 27152, i16 27152, i16 27152, i16 27152, i16 27152, i16 27152>) #9
  %94 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %92, <8 x i16> %92) #9
  %95 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %93, <8 x i16> %94) #9
  %96 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 11
  %97 = bitcast <2 x i64>* %96 to <8 x i16>*
  store <8 x i16> %95, <8 x i16>* %97, align 16
  %98 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 12
  %99 = bitcast <2 x i64>* %98 to <8 x i16>*
  %100 = load <8 x i16>, <8 x i16>* %99, align 16
  %101 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %100, <8 x i16> <i16 27152, i16 27152, i16 27152, i16 27152, i16 27152, i16 27152, i16 27152, i16 27152>) #9
  %102 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %100, <8 x i16> %100) #9
  %103 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %101, <8 x i16> %102) #9
  %104 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 12
  %105 = bitcast <2 x i64>* %104 to <8 x i16>*
  store <8 x i16> %103, <8 x i16>* %105, align 16
  %106 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 13
  %107 = bitcast <2 x i64>* %106 to <8 x i16>*
  %108 = load <8 x i16>, <8 x i16>* %107, align 16
  %109 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %108, <8 x i16> <i16 27152, i16 27152, i16 27152, i16 27152, i16 27152, i16 27152, i16 27152, i16 27152>) #9
  %110 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %108, <8 x i16> %108) #9
  %111 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %109, <8 x i16> %110) #9
  %112 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 13
  %113 = bitcast <2 x i64>* %112 to <8 x i16>*
  store <8 x i16> %111, <8 x i16>* %113, align 16
  %114 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 14
  %115 = bitcast <2 x i64>* %114 to <8 x i16>*
  %116 = load <8 x i16>, <8 x i16>* %115, align 16
  %117 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %116, <8 x i16> <i16 27152, i16 27152, i16 27152, i16 27152, i16 27152, i16 27152, i16 27152, i16 27152>) #9
  %118 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %116, <8 x i16> %116) #9
  %119 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %117, <8 x i16> %118) #9
  %120 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 14
  %121 = bitcast <2 x i64>* %120 to <8 x i16>*
  store <8 x i16> %119, <8 x i16>* %121, align 16
  %122 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 15
  %123 = bitcast <2 x i64>* %122 to <8 x i16>*
  %124 = load <8 x i16>, <8 x i16>* %123, align 16
  %125 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %124, <8 x i16> <i16 27152, i16 27152, i16 27152, i16 27152, i16 27152, i16 27152, i16 27152, i16 27152>) #9
  %126 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %124, <8 x i16> %124) #9
  %127 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %125, <8 x i16> %126) #9
  %128 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 15
  %129 = bitcast <2 x i64>* %128 to <8 x i16>*
  store <8 x i16> %127, <8 x i16>* %129, align 16
  ret void
}

; Function Attrs: nounwind readnone
declare <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16>, <8 x i16>) #5

; Function Attrs: nounwind readnone
declare <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32>, i32) #5

; Function Attrs: nounwind readnone
declare <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32>, <4 x i32>) #5

; Function Attrs: nounwind readnone speculatable
declare <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16>, <8 x i16>) #6

; Function Attrs: nounwind readnone speculatable
declare <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16>, <8 x i16>) #6

; Function Attrs: nounwind readnone
declare <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16>, <8 x i16>) #5

; Function Attrs: nounwind readnone
declare <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16>, i32) #5

; Function Attrs: nounwind readnone
declare <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16>, <8 x i16>) #5

; Function Attrs: nofree nounwind ssp uwtable
define internal void @idct4_sse2(<2 x i64>* nocapture readonly, <2 x i64>* nocapture, i8 signext) #4 {
  %4 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 32), align 16
  %5 = and i32 %4, 65535
  %6 = shl i32 %4, 16
  %7 = or i32 %5, %6
  %8 = insertelement <4 x i32> undef, i32 %7, i32 0
  %9 = shufflevector <4 x i32> %8, <4 x i32> undef, <4 x i32> zeroinitializer
  %10 = sub i32 0, %6
  %11 = or i32 %5, %10
  %12 = insertelement <4 x i32> undef, i32 %11, i32 0
  %13 = shufflevector <4 x i32> %12, <4 x i32> undef, <4 x i32> zeroinitializer
  %14 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 48), align 16
  %15 = and i32 %14, 65535
  %16 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 16), align 16
  %17 = shl i32 %16, 16
  %18 = sub i32 0, %17
  %19 = or i32 %15, %18
  %20 = insertelement <4 x i32> undef, i32 %19, i32 0
  %21 = shufflevector <4 x i32> %20, <4 x i32> undef, <4 x i32> zeroinitializer
  %22 = and i32 %16, 65535
  %23 = shl i32 %14, 16
  %24 = or i32 %22, %23
  %25 = insertelement <4 x i32> undef, i32 %24, i32 0
  %26 = shufflevector <4 x i32> %25, <4 x i32> undef, <4 x i32> zeroinitializer
  %27 = bitcast <2 x i64>* %0 to <8 x i16>*
  %28 = load <8 x i16>, <8 x i16>* %27, align 16
  %29 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 2
  %30 = bitcast <2 x i64>* %29 to <8 x i16>*
  %31 = load <8 x i16>, <8 x i16>* %30, align 16
  %32 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 1
  %33 = bitcast <2 x i64>* %32 to <8 x i16>*
  %34 = load <8 x i16>, <8 x i16>* %33, align 16
  %35 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 3
  %36 = bitcast <2 x i64>* %35 to <8 x i16>*
  %37 = load <8 x i16>, <8 x i16>* %36, align 16
  %38 = shufflevector <8 x i16> %28, <8 x i16> %31, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %39 = shufflevector <8 x i16> %28, <8 x i16> %31, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %40 = bitcast <4 x i32> %9 to <8 x i16>
  %41 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %38, <8 x i16> %40) #9
  %42 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %39, <8 x i16> %40) #9
  %43 = bitcast <4 x i32> %13 to <8 x i16>
  %44 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %38, <8 x i16> %43) #9
  %45 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %39, <8 x i16> %43) #9
  %46 = add <4 x i32> %41, <i32 2048, i32 2048, i32 2048, i32 2048>
  %47 = add <4 x i32> %42, <i32 2048, i32 2048, i32 2048, i32 2048>
  %48 = add <4 x i32> %44, <i32 2048, i32 2048, i32 2048, i32 2048>
  %49 = add <4 x i32> %45, <i32 2048, i32 2048, i32 2048, i32 2048>
  %50 = sext i8 %2 to i32
  %51 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %46, i32 %50) #9
  %52 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %47, i32 %50) #9
  %53 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %48, i32 %50) #9
  %54 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %49, i32 %50) #9
  %55 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %51, <4 x i32> %52) #9
  %56 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %53, <4 x i32> %54) #9
  %57 = shufflevector <8 x i16> %34, <8 x i16> %37, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %58 = shufflevector <8 x i16> %34, <8 x i16> %37, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %59 = bitcast <4 x i32> %21 to <8 x i16>
  %60 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %57, <8 x i16> %59) #9
  %61 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %58, <8 x i16> %59) #9
  %62 = bitcast <4 x i32> %26 to <8 x i16>
  %63 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %57, <8 x i16> %62) #9
  %64 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %58, <8 x i16> %62) #9
  %65 = add <4 x i32> %60, <i32 2048, i32 2048, i32 2048, i32 2048>
  %66 = add <4 x i32> %61, <i32 2048, i32 2048, i32 2048, i32 2048>
  %67 = add <4 x i32> %63, <i32 2048, i32 2048, i32 2048, i32 2048>
  %68 = add <4 x i32> %64, <i32 2048, i32 2048, i32 2048, i32 2048>
  %69 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %65, i32 %50) #9
  %70 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %66, i32 %50) #9
  %71 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %67, i32 %50) #9
  %72 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %68, i32 %50) #9
  %73 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %69, <4 x i32> %70) #9
  %74 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %71, <4 x i32> %72) #9
  %75 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %55, <8 x i16> %74) #9
  %76 = bitcast <2 x i64>* %1 to <8 x i16>*
  store <8 x i16> %75, <8 x i16>* %76, align 16
  %77 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %55, <8 x i16> %74) #9
  %78 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 3
  %79 = bitcast <2 x i64>* %78 to <8 x i16>*
  store <8 x i16> %77, <8 x i16>* %79, align 16
  %80 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %56, <8 x i16> %73) #9
  %81 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 1
  %82 = bitcast <2 x i64>* %81 to <8 x i16>*
  store <8 x i16> %80, <8 x i16>* %82, align 16
  %83 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %56, <8 x i16> %73) #9
  %84 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 2
  %85 = bitcast <2 x i64>* %84 to <8 x i16>*
  store <8 x i16> %83, <8 x i16>* %85, align 16
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal void @iadst4_sse2(<2 x i64>* nocapture readonly, <2 x i64>* nocapture, i8 signext) #0 {
  %4 = load i32, i32* getelementptr inbounds ([7 x [5 x i32]], [7 x [5 x i32]]* @av1_sinpi_arr_data, i64 0, i64 2, i64 1), align 4
  %5 = and i32 %4, 65535
  %6 = load i32, i32* getelementptr inbounds ([7 x [5 x i32]], [7 x [5 x i32]]* @av1_sinpi_arr_data, i64 0, i64 2, i64 4), align 8
  %7 = shl i32 %6, 16
  %8 = or i32 %7, %5
  %9 = insertelement <4 x i32> undef, i32 %8, i32 0
  %10 = shufflevector <4 x i32> %9, <4 x i32> undef, <4 x i32> zeroinitializer
  %11 = load i32, i32* getelementptr inbounds ([7 x [5 x i32]], [7 x [5 x i32]]* @av1_sinpi_arr_data, i64 0, i64 2, i64 2), align 8
  %12 = and i32 %11, 65535
  %13 = shl i32 %4, 16
  %14 = sub i32 0, %13
  %15 = or i32 %12, %14
  %16 = insertelement <4 x i32> undef, i32 %15, i32 0
  %17 = shufflevector <4 x i32> %16, <4 x i32> undef, <4 x i32> zeroinitializer
  %18 = load i32, i32* getelementptr inbounds ([7 x [5 x i32]], [7 x [5 x i32]]* @av1_sinpi_arr_data, i64 0, i64 2, i64 3), align 4
  %19 = and i32 %18, 65535
  %20 = shl i32 %11, 16
  %21 = or i32 %19, %20
  %22 = insertelement <4 x i32> undef, i32 %21, i32 0
  %23 = shufflevector <4 x i32> %22, <4 x i32> undef, <4 x i32> zeroinitializer
  %24 = sub i32 0, %7
  %25 = or i32 %19, %24
  %26 = insertelement <4 x i32> undef, i32 %25, i32 0
  %27 = shufflevector <4 x i32> %26, <4 x i32> undef, <4 x i32> zeroinitializer
  %28 = shl i32 %18, 16
  %29 = sub i32 0, %28
  %30 = or i32 %19, %29
  %31 = insertelement <4 x i32> undef, i32 %30, i32 0
  %32 = shufflevector <4 x i32> %31, <4 x i32> undef, <4 x i32> zeroinitializer
  %33 = insertelement <4 x i32> undef, i32 %28, i32 0
  %34 = shufflevector <4 x i32> %33, <4 x i32> undef, <4 x i32> zeroinitializer
  %35 = and i32 %6, 65535
  %36 = or i32 %20, %35
  %37 = insertelement <4 x i32> undef, i32 %36, i32 0
  %38 = shufflevector <4 x i32> %37, <4 x i32> undef, <4 x i32> zeroinitializer
  %39 = sub i32 0, %18
  %40 = and i32 %39, 65535
  %41 = or i32 %40, %14
  %42 = insertelement <4 x i32> undef, i32 %41, i32 0
  %43 = shufflevector <4 x i32> %42, <4 x i32> undef, <4 x i32> zeroinitializer
  %44 = bitcast <2 x i64>* %0 to <8 x i16>*
  %45 = load <8 x i16>, <8 x i16>* %44, align 16
  %46 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 1
  %47 = bitcast <2 x i64>* %46 to <8 x i16>*
  %48 = load <8 x i16>, <8 x i16>* %47, align 16
  %49 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 2
  %50 = bitcast <2 x i64>* %49 to <8 x i16>*
  %51 = load <8 x i16>, <8 x i16>* %50, align 16
  %52 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 3
  %53 = bitcast <2 x i64>* %52 to <8 x i16>*
  %54 = load <8 x i16>, <8 x i16>* %53, align 16
  %55 = shufflevector <8 x i16> %45, <8 x i16> %51, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %56 = shufflevector <8 x i16> %45, <8 x i16> %51, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %57 = shufflevector <8 x i16> %48, <8 x i16> %54, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %58 = shufflevector <8 x i16> %48, <8 x i16> %54, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %59 = bitcast <4 x i32> %10 to <8 x i16>
  %60 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %55, <8 x i16> %59) #9
  %61 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %56, <8 x i16> %59) #9
  %62 = bitcast <4 x i32> %17 to <8 x i16>
  %63 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %55, <8 x i16> %62) #9
  %64 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %56, <8 x i16> %62) #9
  %65 = bitcast <4 x i32> %23 to <8 x i16>
  %66 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %57, <8 x i16> %65) #9
  %67 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %58, <8 x i16> %65) #9
  %68 = bitcast <4 x i32> %27 to <8 x i16>
  %69 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %57, <8 x i16> %68) #9
  %70 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %58, <8 x i16> %68) #9
  %71 = bitcast <4 x i32> %32 to <8 x i16>
  %72 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %55, <8 x i16> %71) #9
  %73 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %56, <8 x i16> %71) #9
  %74 = bitcast <4 x i32> %34 to <8 x i16>
  %75 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %57, <8 x i16> %74) #9
  %76 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %58, <8 x i16> %74) #9
  %77 = bitcast <4 x i32> %38 to <8 x i16>
  %78 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %55, <8 x i16> %77) #9
  %79 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %56, <8 x i16> %77) #9
  %80 = bitcast <4 x i32> %43 to <8 x i16>
  %81 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %57, <8 x i16> %80) #9
  %82 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %58, <8 x i16> %80) #9
  %83 = add <4 x i32> %66, %60
  %84 = add <4 x i32> %67, %61
  %85 = add <4 x i32> %69, %63
  %86 = add <4 x i32> %70, %64
  %87 = add <4 x i32> %75, %72
  %88 = add <4 x i32> %76, %73
  %89 = add <4 x i32> %81, %78
  %90 = add <4 x i32> %82, %79
  %91 = add <4 x i32> %83, <i32 2048, i32 2048, i32 2048, i32 2048>
  %92 = add <4 x i32> %84, <i32 2048, i32 2048, i32 2048, i32 2048>
  %93 = ashr <4 x i32> %91, <i32 12, i32 12, i32 12, i32 12>
  %94 = ashr <4 x i32> %92, <i32 12, i32 12, i32 12, i32 12>
  %95 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %93, <4 x i32> %94) #9
  %96 = bitcast <2 x i64>* %1 to <8 x i16>*
  store <8 x i16> %95, <8 x i16>* %96, align 16
  %97 = add <4 x i32> %85, <i32 2048, i32 2048, i32 2048, i32 2048>
  %98 = add <4 x i32> %86, <i32 2048, i32 2048, i32 2048, i32 2048>
  %99 = ashr <4 x i32> %97, <i32 12, i32 12, i32 12, i32 12>
  %100 = ashr <4 x i32> %98, <i32 12, i32 12, i32 12, i32 12>
  %101 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %99, <4 x i32> %100) #9
  %102 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 1
  %103 = bitcast <2 x i64>* %102 to <8 x i16>*
  store <8 x i16> %101, <8 x i16>* %103, align 16
  %104 = add <4 x i32> %87, <i32 2048, i32 2048, i32 2048, i32 2048>
  %105 = add <4 x i32> %88, <i32 2048, i32 2048, i32 2048, i32 2048>
  %106 = ashr <4 x i32> %104, <i32 12, i32 12, i32 12, i32 12>
  %107 = ashr <4 x i32> %105, <i32 12, i32 12, i32 12, i32 12>
  %108 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %106, <4 x i32> %107) #9
  %109 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 2
  %110 = bitcast <2 x i64>* %109 to <8 x i16>*
  store <8 x i16> %108, <8 x i16>* %110, align 16
  %111 = add <4 x i32> %89, <i32 2048, i32 2048, i32 2048, i32 2048>
  %112 = add <4 x i32> %90, <i32 2048, i32 2048, i32 2048, i32 2048>
  %113 = ashr <4 x i32> %111, <i32 12, i32 12, i32 12, i32 12>
  %114 = ashr <4 x i32> %112, <i32 12, i32 12, i32 12, i32 12>
  %115 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %113, <4 x i32> %114) #9
  %116 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 3
  %117 = bitcast <2 x i64>* %116 to <8 x i16>*
  store <8 x i16> %115, <8 x i16>* %117, align 16
  ret void
}

; Function Attrs: nofree nounwind ssp uwtable
define internal void @idct8_sse2(<2 x i64>* nocapture readonly, <2 x i64>* nocapture, i8 signext) #4 {
  %4 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 56), align 16
  %5 = and i32 %4, 65535
  %6 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 8), align 16
  %7 = shl i32 %6, 16
  %8 = sub i32 0, %7
  %9 = or i32 %5, %8
  %10 = insertelement <4 x i32> undef, i32 %9, i32 0
  %11 = shufflevector <4 x i32> %10, <4 x i32> undef, <4 x i32> zeroinitializer
  %12 = and i32 %6, 65535
  %13 = shl i32 %4, 16
  %14 = or i32 %12, %13
  %15 = insertelement <4 x i32> undef, i32 %14, i32 0
  %16 = shufflevector <4 x i32> %15, <4 x i32> undef, <4 x i32> zeroinitializer
  %17 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 24), align 16
  %18 = and i32 %17, 65535
  %19 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 40), align 16
  %20 = shl i32 %19, 16
  %21 = sub i32 0, %20
  %22 = or i32 %18, %21
  %23 = insertelement <4 x i32> undef, i32 %22, i32 0
  %24 = shufflevector <4 x i32> %23, <4 x i32> undef, <4 x i32> zeroinitializer
  %25 = and i32 %19, 65535
  %26 = shl i32 %17, 16
  %27 = or i32 %25, %26
  %28 = insertelement <4 x i32> undef, i32 %27, i32 0
  %29 = shufflevector <4 x i32> %28, <4 x i32> undef, <4 x i32> zeroinitializer
  %30 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 32), align 16
  %31 = and i32 %30, 65535
  %32 = shl i32 %30, 16
  %33 = or i32 %31, %32
  %34 = insertelement <4 x i32> undef, i32 %33, i32 0
  %35 = shufflevector <4 x i32> %34, <4 x i32> undef, <4 x i32> zeroinitializer
  %36 = sub i32 0, %32
  %37 = or i32 %31, %36
  %38 = insertelement <4 x i32> undef, i32 %37, i32 0
  %39 = shufflevector <4 x i32> %38, <4 x i32> undef, <4 x i32> zeroinitializer
  %40 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 48), align 16
  %41 = and i32 %40, 65535
  %42 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 16), align 16
  %43 = shl i32 %42, 16
  %44 = sub i32 0, %43
  %45 = or i32 %41, %44
  %46 = insertelement <4 x i32> undef, i32 %45, i32 0
  %47 = shufflevector <4 x i32> %46, <4 x i32> undef, <4 x i32> zeroinitializer
  %48 = and i32 %42, 65535
  %49 = shl i32 %40, 16
  %50 = or i32 %48, %49
  %51 = insertelement <4 x i32> undef, i32 %50, i32 0
  %52 = shufflevector <4 x i32> %51, <4 x i32> undef, <4 x i32> zeroinitializer
  %53 = sub i32 0, %30
  %54 = and i32 %53, 65535
  %55 = or i32 %54, %32
  %56 = insertelement <4 x i32> undef, i32 %55, i32 0
  %57 = shufflevector <4 x i32> %56, <4 x i32> undef, <4 x i32> zeroinitializer
  %58 = bitcast <2 x i64>* %0 to <8 x i16>*
  %59 = load <8 x i16>, <8 x i16>* %58, align 16
  %60 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 4
  %61 = bitcast <2 x i64>* %60 to <8 x i16>*
  %62 = load <8 x i16>, <8 x i16>* %61, align 16
  %63 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 2
  %64 = bitcast <2 x i64>* %63 to <8 x i16>*
  %65 = load <8 x i16>, <8 x i16>* %64, align 16
  %66 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 6
  %67 = bitcast <2 x i64>* %66 to <8 x i16>*
  %68 = load <8 x i16>, <8 x i16>* %67, align 16
  %69 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 1
  %70 = bitcast <2 x i64>* %69 to <8 x i16>*
  %71 = load <8 x i16>, <8 x i16>* %70, align 16
  %72 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 5
  %73 = bitcast <2 x i64>* %72 to <8 x i16>*
  %74 = load <8 x i16>, <8 x i16>* %73, align 16
  %75 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 3
  %76 = bitcast <2 x i64>* %75 to <8 x i16>*
  %77 = load <8 x i16>, <8 x i16>* %76, align 16
  %78 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 7
  %79 = bitcast <2 x i64>* %78 to <8 x i16>*
  %80 = load <8 x i16>, <8 x i16>* %79, align 16
  %81 = shufflevector <8 x i16> %71, <8 x i16> %80, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %82 = shufflevector <8 x i16> %71, <8 x i16> %80, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %83 = bitcast <4 x i32> %11 to <8 x i16>
  %84 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %81, <8 x i16> %83) #9
  %85 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %82, <8 x i16> %83) #9
  %86 = bitcast <4 x i32> %16 to <8 x i16>
  %87 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %81, <8 x i16> %86) #9
  %88 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %82, <8 x i16> %86) #9
  %89 = add <4 x i32> %84, <i32 2048, i32 2048, i32 2048, i32 2048>
  %90 = add <4 x i32> %85, <i32 2048, i32 2048, i32 2048, i32 2048>
  %91 = add <4 x i32> %87, <i32 2048, i32 2048, i32 2048, i32 2048>
  %92 = add <4 x i32> %88, <i32 2048, i32 2048, i32 2048, i32 2048>
  %93 = sext i8 %2 to i32
  %94 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %89, i32 %93) #9
  %95 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %90, i32 %93) #9
  %96 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %91, i32 %93) #9
  %97 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %92, i32 %93) #9
  %98 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %94, <4 x i32> %95) #9
  %99 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %96, <4 x i32> %97) #9
  %100 = shufflevector <8 x i16> %74, <8 x i16> %77, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %101 = shufflevector <8 x i16> %74, <8 x i16> %77, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %102 = bitcast <4 x i32> %24 to <8 x i16>
  %103 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %100, <8 x i16> %102) #9
  %104 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %101, <8 x i16> %102) #9
  %105 = bitcast <4 x i32> %29 to <8 x i16>
  %106 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %100, <8 x i16> %105) #9
  %107 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %101, <8 x i16> %105) #9
  %108 = add <4 x i32> %103, <i32 2048, i32 2048, i32 2048, i32 2048>
  %109 = add <4 x i32> %104, <i32 2048, i32 2048, i32 2048, i32 2048>
  %110 = add <4 x i32> %106, <i32 2048, i32 2048, i32 2048, i32 2048>
  %111 = add <4 x i32> %107, <i32 2048, i32 2048, i32 2048, i32 2048>
  %112 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %108, i32 %93) #9
  %113 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %109, i32 %93) #9
  %114 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %110, i32 %93) #9
  %115 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %111, i32 %93) #9
  %116 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %112, <4 x i32> %113) #9
  %117 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %114, <4 x i32> %115) #9
  %118 = shufflevector <8 x i16> %59, <8 x i16> %62, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %119 = shufflevector <8 x i16> %59, <8 x i16> %62, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %120 = bitcast <4 x i32> %35 to <8 x i16>
  %121 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %118, <8 x i16> %120) #9
  %122 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %119, <8 x i16> %120) #9
  %123 = bitcast <4 x i32> %39 to <8 x i16>
  %124 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %118, <8 x i16> %123) #9
  %125 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %119, <8 x i16> %123) #9
  %126 = add <4 x i32> %121, <i32 2048, i32 2048, i32 2048, i32 2048>
  %127 = add <4 x i32> %122, <i32 2048, i32 2048, i32 2048, i32 2048>
  %128 = add <4 x i32> %124, <i32 2048, i32 2048, i32 2048, i32 2048>
  %129 = add <4 x i32> %125, <i32 2048, i32 2048, i32 2048, i32 2048>
  %130 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %126, i32 %93) #9
  %131 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %127, i32 %93) #9
  %132 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %128, i32 %93) #9
  %133 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %129, i32 %93) #9
  %134 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %130, <4 x i32> %131) #9
  %135 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %132, <4 x i32> %133) #9
  %136 = shufflevector <8 x i16> %65, <8 x i16> %68, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %137 = shufflevector <8 x i16> %65, <8 x i16> %68, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %138 = bitcast <4 x i32> %47 to <8 x i16>
  %139 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %136, <8 x i16> %138) #9
  %140 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %137, <8 x i16> %138) #9
  %141 = bitcast <4 x i32> %52 to <8 x i16>
  %142 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %136, <8 x i16> %141) #9
  %143 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %137, <8 x i16> %141) #9
  %144 = add <4 x i32> %139, <i32 2048, i32 2048, i32 2048, i32 2048>
  %145 = add <4 x i32> %140, <i32 2048, i32 2048, i32 2048, i32 2048>
  %146 = add <4 x i32> %142, <i32 2048, i32 2048, i32 2048, i32 2048>
  %147 = add <4 x i32> %143, <i32 2048, i32 2048, i32 2048, i32 2048>
  %148 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %144, i32 %93) #9
  %149 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %145, i32 %93) #9
  %150 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %146, i32 %93) #9
  %151 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %147, i32 %93) #9
  %152 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %148, <4 x i32> %149) #9
  %153 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %150, <4 x i32> %151) #9
  %154 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %98, <8 x i16> %116) #9
  %155 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %98, <8 x i16> %116) #9
  %156 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %99, <8 x i16> %117) #9
  %157 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %99, <8 x i16> %117) #9
  %158 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %134, <8 x i16> %153) #9
  %159 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %134, <8 x i16> %153) #9
  %160 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %135, <8 x i16> %152) #9
  %161 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %135, <8 x i16> %152) #9
  %162 = shufflevector <8 x i16> %155, <8 x i16> %156, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %163 = shufflevector <8 x i16> %155, <8 x i16> %156, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %164 = bitcast <4 x i32> %57 to <8 x i16>
  %165 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %162, <8 x i16> %164) #9
  %166 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %163, <8 x i16> %164) #9
  %167 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %162, <8 x i16> %120) #9
  %168 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %163, <8 x i16> %120) #9
  %169 = add <4 x i32> %165, <i32 2048, i32 2048, i32 2048, i32 2048>
  %170 = add <4 x i32> %166, <i32 2048, i32 2048, i32 2048, i32 2048>
  %171 = add <4 x i32> %167, <i32 2048, i32 2048, i32 2048, i32 2048>
  %172 = add <4 x i32> %168, <i32 2048, i32 2048, i32 2048, i32 2048>
  %173 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %169, i32 %93) #9
  %174 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %170, i32 %93) #9
  %175 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %171, i32 %93) #9
  %176 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %172, i32 %93) #9
  %177 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %173, <4 x i32> %174) #9
  %178 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %175, <4 x i32> %176) #9
  %179 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %158, <8 x i16> %157) #9
  %180 = bitcast <2 x i64>* %1 to <8 x i16>*
  store <8 x i16> %179, <8 x i16>* %180, align 16
  %181 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %158, <8 x i16> %157) #9
  %182 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 7
  %183 = bitcast <2 x i64>* %182 to <8 x i16>*
  store <8 x i16> %181, <8 x i16>* %183, align 16
  %184 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %160, <8 x i16> %178) #9
  %185 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 1
  %186 = bitcast <2 x i64>* %185 to <8 x i16>*
  store <8 x i16> %184, <8 x i16>* %186, align 16
  %187 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %160, <8 x i16> %178) #9
  %188 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 6
  %189 = bitcast <2 x i64>* %188 to <8 x i16>*
  store <8 x i16> %187, <8 x i16>* %189, align 16
  %190 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %161, <8 x i16> %177) #9
  %191 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 2
  %192 = bitcast <2 x i64>* %191 to <8 x i16>*
  store <8 x i16> %190, <8 x i16>* %192, align 16
  %193 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %161, <8 x i16> %177) #9
  %194 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 5
  %195 = bitcast <2 x i64>* %194 to <8 x i16>*
  store <8 x i16> %193, <8 x i16>* %195, align 16
  %196 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %159, <8 x i16> %154) #9
  %197 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 3
  %198 = bitcast <2 x i64>* %197 to <8 x i16>*
  store <8 x i16> %196, <8 x i16>* %198, align 16
  %199 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %159, <8 x i16> %154) #9
  %200 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 4
  %201 = bitcast <2 x i64>* %200 to <8 x i16>*
  store <8 x i16> %199, <8 x i16>* %201, align 16
  ret void
}

; Function Attrs: nofree nounwind ssp uwtable
define internal void @iadst8_sse2(<2 x i64>* nocapture readonly, <2 x i64>* nocapture, i8 signext) #4 {
  %4 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 4), align 16
  %5 = and i32 %4, 65535
  %6 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 60), align 16
  %7 = shl i32 %6, 16
  %8 = or i32 %7, %5
  %9 = insertelement <4 x i32> undef, i32 %8, i32 0
  %10 = shufflevector <4 x i32> %9, <4 x i32> undef, <4 x i32> zeroinitializer
  %11 = and i32 %6, 65535
  %12 = shl i32 %4, 16
  %13 = sub i32 0, %12
  %14 = or i32 %11, %13
  %15 = insertelement <4 x i32> undef, i32 %14, i32 0
  %16 = shufflevector <4 x i32> %15, <4 x i32> undef, <4 x i32> zeroinitializer
  %17 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 20), align 16
  %18 = and i32 %17, 65535
  %19 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 44), align 16
  %20 = shl i32 %19, 16
  %21 = or i32 %20, %18
  %22 = insertelement <4 x i32> undef, i32 %21, i32 0
  %23 = shufflevector <4 x i32> %22, <4 x i32> undef, <4 x i32> zeroinitializer
  %24 = and i32 %19, 65535
  %25 = shl i32 %17, 16
  %26 = sub i32 0, %25
  %27 = or i32 %24, %26
  %28 = insertelement <4 x i32> undef, i32 %27, i32 0
  %29 = shufflevector <4 x i32> %28, <4 x i32> undef, <4 x i32> zeroinitializer
  %30 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 36), align 16
  %31 = and i32 %30, 65535
  %32 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 28), align 16
  %33 = shl i32 %32, 16
  %34 = or i32 %33, %31
  %35 = insertelement <4 x i32> undef, i32 %34, i32 0
  %36 = shufflevector <4 x i32> %35, <4 x i32> undef, <4 x i32> zeroinitializer
  %37 = and i32 %32, 65535
  %38 = shl i32 %30, 16
  %39 = sub i32 0, %38
  %40 = or i32 %37, %39
  %41 = insertelement <4 x i32> undef, i32 %40, i32 0
  %42 = shufflevector <4 x i32> %41, <4 x i32> undef, <4 x i32> zeroinitializer
  %43 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 52), align 16
  %44 = and i32 %43, 65535
  %45 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 12), align 16
  %46 = shl i32 %45, 16
  %47 = or i32 %46, %44
  %48 = insertelement <4 x i32> undef, i32 %47, i32 0
  %49 = shufflevector <4 x i32> %48, <4 x i32> undef, <4 x i32> zeroinitializer
  %50 = and i32 %45, 65535
  %51 = shl i32 %43, 16
  %52 = sub i32 0, %51
  %53 = or i32 %50, %52
  %54 = insertelement <4 x i32> undef, i32 %53, i32 0
  %55 = shufflevector <4 x i32> %54, <4 x i32> undef, <4 x i32> zeroinitializer
  %56 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 16), align 16
  %57 = and i32 %56, 65535
  %58 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 48), align 16
  %59 = shl i32 %58, 16
  %60 = or i32 %59, %57
  %61 = insertelement <4 x i32> undef, i32 %60, i32 0
  %62 = shufflevector <4 x i32> %61, <4 x i32> undef, <4 x i32> zeroinitializer
  %63 = and i32 %58, 65535
  %64 = shl i32 %56, 16
  %65 = sub i32 0, %64
  %66 = or i32 %63, %65
  %67 = insertelement <4 x i32> undef, i32 %66, i32 0
  %68 = shufflevector <4 x i32> %67, <4 x i32> undef, <4 x i32> zeroinitializer
  %69 = sub i32 0, %58
  %70 = and i32 %69, 65535
  %71 = or i32 %70, %64
  %72 = insertelement <4 x i32> undef, i32 %71, i32 0
  %73 = shufflevector <4 x i32> %72, <4 x i32> undef, <4 x i32> zeroinitializer
  %74 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 32), align 16
  %75 = and i32 %74, 65535
  %76 = shl i32 %74, 16
  %77 = or i32 %75, %76
  %78 = insertelement <4 x i32> undef, i32 %77, i32 0
  %79 = shufflevector <4 x i32> %78, <4 x i32> undef, <4 x i32> zeroinitializer
  %80 = sub i32 0, %76
  %81 = or i32 %75, %80
  %82 = insertelement <4 x i32> undef, i32 %81, i32 0
  %83 = shufflevector <4 x i32> %82, <4 x i32> undef, <4 x i32> zeroinitializer
  %84 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 7
  %85 = bitcast <2 x i64>* %84 to <8 x i16>*
  %86 = load <8 x i16>, <8 x i16>* %85, align 16
  %87 = bitcast <2 x i64>* %0 to <8 x i16>*
  %88 = load <8 x i16>, <8 x i16>* %87, align 16
  %89 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 5
  %90 = bitcast <2 x i64>* %89 to <8 x i16>*
  %91 = load <8 x i16>, <8 x i16>* %90, align 16
  %92 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 2
  %93 = bitcast <2 x i64>* %92 to <8 x i16>*
  %94 = load <8 x i16>, <8 x i16>* %93, align 16
  %95 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 3
  %96 = bitcast <2 x i64>* %95 to <8 x i16>*
  %97 = load <8 x i16>, <8 x i16>* %96, align 16
  %98 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 4
  %99 = bitcast <2 x i64>* %98 to <8 x i16>*
  %100 = load <8 x i16>, <8 x i16>* %99, align 16
  %101 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 1
  %102 = bitcast <2 x i64>* %101 to <8 x i16>*
  %103 = load <8 x i16>, <8 x i16>* %102, align 16
  %104 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 6
  %105 = bitcast <2 x i64>* %104 to <8 x i16>*
  %106 = load <8 x i16>, <8 x i16>* %105, align 16
  %107 = shufflevector <8 x i16> %86, <8 x i16> %88, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %108 = shufflevector <8 x i16> %86, <8 x i16> %88, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %109 = bitcast <4 x i32> %10 to <8 x i16>
  %110 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %107, <8 x i16> %109) #9
  %111 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %108, <8 x i16> %109) #9
  %112 = bitcast <4 x i32> %16 to <8 x i16>
  %113 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %107, <8 x i16> %112) #9
  %114 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %108, <8 x i16> %112) #9
  %115 = add <4 x i32> %110, <i32 2048, i32 2048, i32 2048, i32 2048>
  %116 = add <4 x i32> %111, <i32 2048, i32 2048, i32 2048, i32 2048>
  %117 = add <4 x i32> %113, <i32 2048, i32 2048, i32 2048, i32 2048>
  %118 = add <4 x i32> %114, <i32 2048, i32 2048, i32 2048, i32 2048>
  %119 = sext i8 %2 to i32
  %120 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %115, i32 %119) #9
  %121 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %116, i32 %119) #9
  %122 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %117, i32 %119) #9
  %123 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %118, i32 %119) #9
  %124 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %120, <4 x i32> %121) #9
  %125 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %122, <4 x i32> %123) #9
  %126 = shufflevector <8 x i16> %91, <8 x i16> %94, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %127 = shufflevector <8 x i16> %91, <8 x i16> %94, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %128 = bitcast <4 x i32> %23 to <8 x i16>
  %129 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %126, <8 x i16> %128) #9
  %130 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %127, <8 x i16> %128) #9
  %131 = bitcast <4 x i32> %29 to <8 x i16>
  %132 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %126, <8 x i16> %131) #9
  %133 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %127, <8 x i16> %131) #9
  %134 = add <4 x i32> %129, <i32 2048, i32 2048, i32 2048, i32 2048>
  %135 = add <4 x i32> %130, <i32 2048, i32 2048, i32 2048, i32 2048>
  %136 = add <4 x i32> %132, <i32 2048, i32 2048, i32 2048, i32 2048>
  %137 = add <4 x i32> %133, <i32 2048, i32 2048, i32 2048, i32 2048>
  %138 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %134, i32 %119) #9
  %139 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %135, i32 %119) #9
  %140 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %136, i32 %119) #9
  %141 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %137, i32 %119) #9
  %142 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %138, <4 x i32> %139) #9
  %143 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %140, <4 x i32> %141) #9
  %144 = shufflevector <8 x i16> %97, <8 x i16> %100, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %145 = shufflevector <8 x i16> %97, <8 x i16> %100, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %146 = bitcast <4 x i32> %36 to <8 x i16>
  %147 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %144, <8 x i16> %146) #9
  %148 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %145, <8 x i16> %146) #9
  %149 = bitcast <4 x i32> %42 to <8 x i16>
  %150 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %144, <8 x i16> %149) #9
  %151 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %145, <8 x i16> %149) #9
  %152 = add <4 x i32> %147, <i32 2048, i32 2048, i32 2048, i32 2048>
  %153 = add <4 x i32> %148, <i32 2048, i32 2048, i32 2048, i32 2048>
  %154 = add <4 x i32> %150, <i32 2048, i32 2048, i32 2048, i32 2048>
  %155 = add <4 x i32> %151, <i32 2048, i32 2048, i32 2048, i32 2048>
  %156 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %152, i32 %119) #9
  %157 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %153, i32 %119) #9
  %158 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %154, i32 %119) #9
  %159 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %155, i32 %119) #9
  %160 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %156, <4 x i32> %157) #9
  %161 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %158, <4 x i32> %159) #9
  %162 = shufflevector <8 x i16> %103, <8 x i16> %106, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %163 = shufflevector <8 x i16> %103, <8 x i16> %106, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %164 = bitcast <4 x i32> %49 to <8 x i16>
  %165 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %162, <8 x i16> %164) #9
  %166 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %163, <8 x i16> %164) #9
  %167 = bitcast <4 x i32> %55 to <8 x i16>
  %168 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %162, <8 x i16> %167) #9
  %169 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %163, <8 x i16> %167) #9
  %170 = add <4 x i32> %165, <i32 2048, i32 2048, i32 2048, i32 2048>
  %171 = add <4 x i32> %166, <i32 2048, i32 2048, i32 2048, i32 2048>
  %172 = add <4 x i32> %168, <i32 2048, i32 2048, i32 2048, i32 2048>
  %173 = add <4 x i32> %169, <i32 2048, i32 2048, i32 2048, i32 2048>
  %174 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %170, i32 %119) #9
  %175 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %171, i32 %119) #9
  %176 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %172, i32 %119) #9
  %177 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %173, i32 %119) #9
  %178 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %174, <4 x i32> %175) #9
  %179 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %176, <4 x i32> %177) #9
  %180 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %124, <8 x i16> %160) #9
  %181 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %124, <8 x i16> %160) #9
  %182 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %125, <8 x i16> %161) #9
  %183 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %125, <8 x i16> %161) #9
  %184 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %142, <8 x i16> %178) #9
  %185 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %142, <8 x i16> %178) #9
  %186 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %143, <8 x i16> %179) #9
  %187 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %143, <8 x i16> %179) #9
  %188 = shufflevector <8 x i16> %181, <8 x i16> %183, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %189 = shufflevector <8 x i16> %181, <8 x i16> %183, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %190 = bitcast <4 x i32> %62 to <8 x i16>
  %191 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %188, <8 x i16> %190) #9
  %192 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %189, <8 x i16> %190) #9
  %193 = bitcast <4 x i32> %68 to <8 x i16>
  %194 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %188, <8 x i16> %193) #9
  %195 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %189, <8 x i16> %193) #9
  %196 = add <4 x i32> %191, <i32 2048, i32 2048, i32 2048, i32 2048>
  %197 = add <4 x i32> %192, <i32 2048, i32 2048, i32 2048, i32 2048>
  %198 = add <4 x i32> %194, <i32 2048, i32 2048, i32 2048, i32 2048>
  %199 = add <4 x i32> %195, <i32 2048, i32 2048, i32 2048, i32 2048>
  %200 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %196, i32 %119) #9
  %201 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %197, i32 %119) #9
  %202 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %198, i32 %119) #9
  %203 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %199, i32 %119) #9
  %204 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %200, <4 x i32> %201) #9
  %205 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %202, <4 x i32> %203) #9
  %206 = shufflevector <8 x i16> %185, <8 x i16> %187, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %207 = shufflevector <8 x i16> %185, <8 x i16> %187, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %208 = bitcast <4 x i32> %73 to <8 x i16>
  %209 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %206, <8 x i16> %208) #9
  %210 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %207, <8 x i16> %208) #9
  %211 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %206, <8 x i16> %190) #9
  %212 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %207, <8 x i16> %190) #9
  %213 = add <4 x i32> %209, <i32 2048, i32 2048, i32 2048, i32 2048>
  %214 = add <4 x i32> %210, <i32 2048, i32 2048, i32 2048, i32 2048>
  %215 = add <4 x i32> %211, <i32 2048, i32 2048, i32 2048, i32 2048>
  %216 = add <4 x i32> %212, <i32 2048, i32 2048, i32 2048, i32 2048>
  %217 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %213, i32 %119) #9
  %218 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %214, i32 %119) #9
  %219 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %215, i32 %119) #9
  %220 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %216, i32 %119) #9
  %221 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %217, <4 x i32> %218) #9
  %222 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %219, <4 x i32> %220) #9
  %223 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %180, <8 x i16> %184) #9
  %224 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %180, <8 x i16> %184) #9
  %225 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %182, <8 x i16> %186) #9
  %226 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %182, <8 x i16> %186) #9
  %227 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %204, <8 x i16> %221) #9
  %228 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %204, <8 x i16> %221) #9
  %229 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %205, <8 x i16> %222) #9
  %230 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %205, <8 x i16> %222) #9
  %231 = shufflevector <8 x i16> %224, <8 x i16> %226, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %232 = shufflevector <8 x i16> %224, <8 x i16> %226, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %233 = bitcast <4 x i32> %79 to <8 x i16>
  %234 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %231, <8 x i16> %233) #9
  %235 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %232, <8 x i16> %233) #9
  %236 = bitcast <4 x i32> %83 to <8 x i16>
  %237 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %231, <8 x i16> %236) #9
  %238 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %232, <8 x i16> %236) #9
  %239 = add <4 x i32> %234, <i32 2048, i32 2048, i32 2048, i32 2048>
  %240 = add <4 x i32> %235, <i32 2048, i32 2048, i32 2048, i32 2048>
  %241 = add <4 x i32> %237, <i32 2048, i32 2048, i32 2048, i32 2048>
  %242 = add <4 x i32> %238, <i32 2048, i32 2048, i32 2048, i32 2048>
  %243 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %239, i32 %119) #9
  %244 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %240, i32 %119) #9
  %245 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %241, i32 %119) #9
  %246 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %242, i32 %119) #9
  %247 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %243, <4 x i32> %244) #9
  %248 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %245, <4 x i32> %246) #9
  %249 = shufflevector <8 x i16> %228, <8 x i16> %230, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %250 = shufflevector <8 x i16> %228, <8 x i16> %230, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %251 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %249, <8 x i16> %233) #9
  %252 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %250, <8 x i16> %233) #9
  %253 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %249, <8 x i16> %236) #9
  %254 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %250, <8 x i16> %236) #9
  %255 = add <4 x i32> %251, <i32 2048, i32 2048, i32 2048, i32 2048>
  %256 = add <4 x i32> %252, <i32 2048, i32 2048, i32 2048, i32 2048>
  %257 = add <4 x i32> %253, <i32 2048, i32 2048, i32 2048, i32 2048>
  %258 = add <4 x i32> %254, <i32 2048, i32 2048, i32 2048, i32 2048>
  %259 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %255, i32 %119) #9
  %260 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %256, i32 %119) #9
  %261 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %257, i32 %119) #9
  %262 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %258, i32 %119) #9
  %263 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %259, <4 x i32> %260) #9
  %264 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %261, <4 x i32> %262) #9
  %265 = bitcast <2 x i64>* %1 to <8 x i16>*
  store <8 x i16> %223, <8 x i16>* %265, align 16
  %266 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %227) #9
  %267 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 1
  %268 = bitcast <2 x i64>* %267 to <8 x i16>*
  store <8 x i16> %266, <8 x i16>* %268, align 16
  %269 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 2
  %270 = bitcast <2 x i64>* %269 to <8 x i16>*
  store <8 x i16> %263, <8 x i16>* %270, align 16
  %271 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %247) #9
  %272 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 3
  %273 = bitcast <2 x i64>* %272 to <8 x i16>*
  store <8 x i16> %271, <8 x i16>* %273, align 16
  %274 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 4
  %275 = bitcast <2 x i64>* %274 to <8 x i16>*
  store <8 x i16> %248, <8 x i16>* %275, align 16
  %276 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %264) #9
  %277 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 5
  %278 = bitcast <2 x i64>* %277 to <8 x i16>*
  store <8 x i16> %276, <8 x i16>* %278, align 16
  %279 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 6
  %280 = bitcast <2 x i64>* %279 to <8 x i16>*
  store <8 x i16> %229, <8 x i16>* %280, align 16
  %281 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %225) #9
  %282 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 7
  %283 = bitcast <2 x i64>* %282 to <8 x i16>*
  store <8 x i16> %281, <8 x i16>* %283, align 16
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal void @idct16_sse2(<2 x i64>* nocapture readonly, <2 x i64>* nocapture, i8 signext) #0 {
  %4 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 60), align 16
  %5 = and i32 %4, 65535
  %6 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 4), align 16
  %7 = shl i32 %6, 16
  %8 = sub i32 0, %7
  %9 = or i32 %5, %8
  %10 = insertelement <4 x i32> undef, i32 %9, i32 0
  %11 = shufflevector <4 x i32> %10, <4 x i32> undef, <4 x i32> zeroinitializer
  %12 = and i32 %6, 65535
  %13 = shl i32 %4, 16
  %14 = or i32 %12, %13
  %15 = insertelement <4 x i32> undef, i32 %14, i32 0
  %16 = shufflevector <4 x i32> %15, <4 x i32> undef, <4 x i32> zeroinitializer
  %17 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 28), align 16
  %18 = and i32 %17, 65535
  %19 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 36), align 16
  %20 = shl i32 %19, 16
  %21 = sub i32 0, %20
  %22 = or i32 %18, %21
  %23 = insertelement <4 x i32> undef, i32 %22, i32 0
  %24 = shufflevector <4 x i32> %23, <4 x i32> undef, <4 x i32> zeroinitializer
  %25 = and i32 %19, 65535
  %26 = shl i32 %17, 16
  %27 = or i32 %25, %26
  %28 = insertelement <4 x i32> undef, i32 %27, i32 0
  %29 = shufflevector <4 x i32> %28, <4 x i32> undef, <4 x i32> zeroinitializer
  %30 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 44), align 16
  %31 = and i32 %30, 65535
  %32 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 20), align 16
  %33 = shl i32 %32, 16
  %34 = sub i32 0, %33
  %35 = or i32 %31, %34
  %36 = insertelement <4 x i32> undef, i32 %35, i32 0
  %37 = shufflevector <4 x i32> %36, <4 x i32> undef, <4 x i32> zeroinitializer
  %38 = and i32 %32, 65535
  %39 = shl i32 %30, 16
  %40 = or i32 %38, %39
  %41 = insertelement <4 x i32> undef, i32 %40, i32 0
  %42 = shufflevector <4 x i32> %41, <4 x i32> undef, <4 x i32> zeroinitializer
  %43 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 12), align 16
  %44 = and i32 %43, 65535
  %45 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 52), align 16
  %46 = shl i32 %45, 16
  %47 = sub i32 0, %46
  %48 = or i32 %44, %47
  %49 = insertelement <4 x i32> undef, i32 %48, i32 0
  %50 = shufflevector <4 x i32> %49, <4 x i32> undef, <4 x i32> zeroinitializer
  %51 = and i32 %45, 65535
  %52 = shl i32 %43, 16
  %53 = or i32 %51, %52
  %54 = insertelement <4 x i32> undef, i32 %53, i32 0
  %55 = shufflevector <4 x i32> %54, <4 x i32> undef, <4 x i32> zeroinitializer
  %56 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 56), align 16
  %57 = and i32 %56, 65535
  %58 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 8), align 16
  %59 = shl i32 %58, 16
  %60 = sub i32 0, %59
  %61 = or i32 %57, %60
  %62 = insertelement <4 x i32> undef, i32 %61, i32 0
  %63 = shufflevector <4 x i32> %62, <4 x i32> undef, <4 x i32> zeroinitializer
  %64 = and i32 %58, 65535
  %65 = shl i32 %56, 16
  %66 = or i32 %64, %65
  %67 = insertelement <4 x i32> undef, i32 %66, i32 0
  %68 = shufflevector <4 x i32> %67, <4 x i32> undef, <4 x i32> zeroinitializer
  %69 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 24), align 16
  %70 = and i32 %69, 65535
  %71 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 40), align 16
  %72 = shl i32 %71, 16
  %73 = sub i32 0, %72
  %74 = or i32 %70, %73
  %75 = insertelement <4 x i32> undef, i32 %74, i32 0
  %76 = shufflevector <4 x i32> %75, <4 x i32> undef, <4 x i32> zeroinitializer
  %77 = and i32 %71, 65535
  %78 = shl i32 %69, 16
  %79 = or i32 %77, %78
  %80 = insertelement <4 x i32> undef, i32 %79, i32 0
  %81 = shufflevector <4 x i32> %80, <4 x i32> undef, <4 x i32> zeroinitializer
  %82 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 32), align 16
  %83 = and i32 %82, 65535
  %84 = shl i32 %82, 16
  %85 = or i32 %83, %84
  %86 = insertelement <4 x i32> undef, i32 %85, i32 0
  %87 = shufflevector <4 x i32> %86, <4 x i32> undef, <4 x i32> zeroinitializer
  %88 = sub i32 0, %84
  %89 = or i32 %83, %88
  %90 = insertelement <4 x i32> undef, i32 %89, i32 0
  %91 = shufflevector <4 x i32> %90, <4 x i32> undef, <4 x i32> zeroinitializer
  %92 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 48), align 16
  %93 = and i32 %92, 65535
  %94 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 16), align 16
  %95 = shl i32 %94, 16
  %96 = sub i32 0, %95
  %97 = or i32 %93, %96
  %98 = insertelement <4 x i32> undef, i32 %97, i32 0
  %99 = shufflevector <4 x i32> %98, <4 x i32> undef, <4 x i32> zeroinitializer
  %100 = and i32 %94, 65535
  %101 = shl i32 %92, 16
  %102 = or i32 %100, %101
  %103 = insertelement <4 x i32> undef, i32 %102, i32 0
  %104 = shufflevector <4 x i32> %103, <4 x i32> undef, <4 x i32> zeroinitializer
  %105 = sub i32 0, %94
  %106 = and i32 %105, 65535
  %107 = or i32 %106, %101
  %108 = insertelement <4 x i32> undef, i32 %107, i32 0
  %109 = shufflevector <4 x i32> %108, <4 x i32> undef, <4 x i32> zeroinitializer
  %110 = or i32 %95, %93
  %111 = insertelement <4 x i32> undef, i32 %110, i32 0
  %112 = shufflevector <4 x i32> %111, <4 x i32> undef, <4 x i32> zeroinitializer
  %113 = sub i32 0, %92
  %114 = and i32 %113, 65535
  %115 = or i32 %114, %96
  %116 = insertelement <4 x i32> undef, i32 %115, i32 0
  %117 = shufflevector <4 x i32> %116, <4 x i32> undef, <4 x i32> zeroinitializer
  %118 = bitcast <2 x i64>* %0 to <8 x i16>*
  %119 = load <8 x i16>, <8 x i16>* %118, align 16
  %120 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 8
  %121 = bitcast <2 x i64>* %120 to <8 x i16>*
  %122 = load <8 x i16>, <8 x i16>* %121, align 16
  %123 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 4
  %124 = bitcast <2 x i64>* %123 to <8 x i16>*
  %125 = load <8 x i16>, <8 x i16>* %124, align 16
  %126 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 12
  %127 = bitcast <2 x i64>* %126 to <8 x i16>*
  %128 = load <8 x i16>, <8 x i16>* %127, align 16
  %129 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 2
  %130 = bitcast <2 x i64>* %129 to <8 x i16>*
  %131 = load <8 x i16>, <8 x i16>* %130, align 16
  %132 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 10
  %133 = bitcast <2 x i64>* %132 to <8 x i16>*
  %134 = load <8 x i16>, <8 x i16>* %133, align 16
  %135 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 6
  %136 = bitcast <2 x i64>* %135 to <8 x i16>*
  %137 = load <8 x i16>, <8 x i16>* %136, align 16
  %138 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 14
  %139 = bitcast <2 x i64>* %138 to <8 x i16>*
  %140 = load <8 x i16>, <8 x i16>* %139, align 16
  %141 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 1
  %142 = bitcast <2 x i64>* %141 to <8 x i16>*
  %143 = load <8 x i16>, <8 x i16>* %142, align 16
  %144 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 9
  %145 = bitcast <2 x i64>* %144 to <8 x i16>*
  %146 = load <8 x i16>, <8 x i16>* %145, align 16
  %147 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 5
  %148 = bitcast <2 x i64>* %147 to <8 x i16>*
  %149 = load <8 x i16>, <8 x i16>* %148, align 16
  %150 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 13
  %151 = bitcast <2 x i64>* %150 to <8 x i16>*
  %152 = load <8 x i16>, <8 x i16>* %151, align 16
  %153 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 3
  %154 = bitcast <2 x i64>* %153 to <8 x i16>*
  %155 = load <8 x i16>, <8 x i16>* %154, align 16
  %156 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 11
  %157 = bitcast <2 x i64>* %156 to <8 x i16>*
  %158 = load <8 x i16>, <8 x i16>* %157, align 16
  %159 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 7
  %160 = bitcast <2 x i64>* %159 to <8 x i16>*
  %161 = load <8 x i16>, <8 x i16>* %160, align 16
  %162 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 15
  %163 = bitcast <2 x i64>* %162 to <8 x i16>*
  %164 = load <8 x i16>, <8 x i16>* %163, align 16
  %165 = shufflevector <8 x i16> %143, <8 x i16> %164, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %166 = shufflevector <8 x i16> %143, <8 x i16> %164, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %167 = bitcast <4 x i32> %11 to <8 x i16>
  %168 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %165, <8 x i16> %167) #9
  %169 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %166, <8 x i16> %167) #9
  %170 = bitcast <4 x i32> %16 to <8 x i16>
  %171 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %165, <8 x i16> %170) #9
  %172 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %166, <8 x i16> %170) #9
  %173 = add <4 x i32> %168, <i32 2048, i32 2048, i32 2048, i32 2048>
  %174 = add <4 x i32> %169, <i32 2048, i32 2048, i32 2048, i32 2048>
  %175 = add <4 x i32> %171, <i32 2048, i32 2048, i32 2048, i32 2048>
  %176 = add <4 x i32> %172, <i32 2048, i32 2048, i32 2048, i32 2048>
  %177 = sext i8 %2 to i32
  %178 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %173, i32 %177) #9
  %179 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %174, i32 %177) #9
  %180 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %175, i32 %177) #9
  %181 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %176, i32 %177) #9
  %182 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %178, <4 x i32> %179) #9
  %183 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %180, <4 x i32> %181) #9
  %184 = shufflevector <8 x i16> %146, <8 x i16> %161, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %185 = shufflevector <8 x i16> %146, <8 x i16> %161, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %186 = bitcast <4 x i32> %24 to <8 x i16>
  %187 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %184, <8 x i16> %186) #9
  %188 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %185, <8 x i16> %186) #9
  %189 = bitcast <4 x i32> %29 to <8 x i16>
  %190 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %184, <8 x i16> %189) #9
  %191 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %185, <8 x i16> %189) #9
  %192 = add <4 x i32> %187, <i32 2048, i32 2048, i32 2048, i32 2048>
  %193 = add <4 x i32> %188, <i32 2048, i32 2048, i32 2048, i32 2048>
  %194 = add <4 x i32> %190, <i32 2048, i32 2048, i32 2048, i32 2048>
  %195 = add <4 x i32> %191, <i32 2048, i32 2048, i32 2048, i32 2048>
  %196 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %192, i32 %177) #9
  %197 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %193, i32 %177) #9
  %198 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %194, i32 %177) #9
  %199 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %195, i32 %177) #9
  %200 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %196, <4 x i32> %197) #9
  %201 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %198, <4 x i32> %199) #9
  %202 = shufflevector <8 x i16> %149, <8 x i16> %158, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %203 = shufflevector <8 x i16> %149, <8 x i16> %158, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %204 = bitcast <4 x i32> %37 to <8 x i16>
  %205 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %202, <8 x i16> %204) #9
  %206 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %203, <8 x i16> %204) #9
  %207 = bitcast <4 x i32> %42 to <8 x i16>
  %208 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %202, <8 x i16> %207) #9
  %209 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %203, <8 x i16> %207) #9
  %210 = add <4 x i32> %205, <i32 2048, i32 2048, i32 2048, i32 2048>
  %211 = add <4 x i32> %206, <i32 2048, i32 2048, i32 2048, i32 2048>
  %212 = add <4 x i32> %208, <i32 2048, i32 2048, i32 2048, i32 2048>
  %213 = add <4 x i32> %209, <i32 2048, i32 2048, i32 2048, i32 2048>
  %214 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %210, i32 %177) #9
  %215 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %211, i32 %177) #9
  %216 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %212, i32 %177) #9
  %217 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %213, i32 %177) #9
  %218 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %214, <4 x i32> %215) #9
  %219 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %216, <4 x i32> %217) #9
  %220 = shufflevector <8 x i16> %152, <8 x i16> %155, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %221 = shufflevector <8 x i16> %152, <8 x i16> %155, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %222 = bitcast <4 x i32> %50 to <8 x i16>
  %223 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %220, <8 x i16> %222) #9
  %224 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %221, <8 x i16> %222) #9
  %225 = bitcast <4 x i32> %55 to <8 x i16>
  %226 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %220, <8 x i16> %225) #9
  %227 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %221, <8 x i16> %225) #9
  %228 = add <4 x i32> %223, <i32 2048, i32 2048, i32 2048, i32 2048>
  %229 = add <4 x i32> %224, <i32 2048, i32 2048, i32 2048, i32 2048>
  %230 = add <4 x i32> %226, <i32 2048, i32 2048, i32 2048, i32 2048>
  %231 = add <4 x i32> %227, <i32 2048, i32 2048, i32 2048, i32 2048>
  %232 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %228, i32 %177) #9
  %233 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %229, i32 %177) #9
  %234 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %230, i32 %177) #9
  %235 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %231, i32 %177) #9
  %236 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %232, <4 x i32> %233) #9
  %237 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %234, <4 x i32> %235) #9
  %238 = shufflevector <8 x i16> %131, <8 x i16> %140, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %239 = shufflevector <8 x i16> %131, <8 x i16> %140, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %240 = bitcast <4 x i32> %63 to <8 x i16>
  %241 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %238, <8 x i16> %240) #9
  %242 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %239, <8 x i16> %240) #9
  %243 = bitcast <4 x i32> %68 to <8 x i16>
  %244 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %238, <8 x i16> %243) #9
  %245 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %239, <8 x i16> %243) #9
  %246 = add <4 x i32> %241, <i32 2048, i32 2048, i32 2048, i32 2048>
  %247 = add <4 x i32> %242, <i32 2048, i32 2048, i32 2048, i32 2048>
  %248 = add <4 x i32> %244, <i32 2048, i32 2048, i32 2048, i32 2048>
  %249 = add <4 x i32> %245, <i32 2048, i32 2048, i32 2048, i32 2048>
  %250 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %246, i32 %177) #9
  %251 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %247, i32 %177) #9
  %252 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %248, i32 %177) #9
  %253 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %249, i32 %177) #9
  %254 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %250, <4 x i32> %251) #9
  %255 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %252, <4 x i32> %253) #9
  %256 = shufflevector <8 x i16> %134, <8 x i16> %137, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %257 = shufflevector <8 x i16> %134, <8 x i16> %137, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %258 = bitcast <4 x i32> %76 to <8 x i16>
  %259 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %256, <8 x i16> %258) #9
  %260 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %257, <8 x i16> %258) #9
  %261 = bitcast <4 x i32> %81 to <8 x i16>
  %262 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %256, <8 x i16> %261) #9
  %263 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %257, <8 x i16> %261) #9
  %264 = add <4 x i32> %259, <i32 2048, i32 2048, i32 2048, i32 2048>
  %265 = add <4 x i32> %260, <i32 2048, i32 2048, i32 2048, i32 2048>
  %266 = add <4 x i32> %262, <i32 2048, i32 2048, i32 2048, i32 2048>
  %267 = add <4 x i32> %263, <i32 2048, i32 2048, i32 2048, i32 2048>
  %268 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %264, i32 %177) #9
  %269 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %265, i32 %177) #9
  %270 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %266, i32 %177) #9
  %271 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %267, i32 %177) #9
  %272 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %268, <4 x i32> %269) #9
  %273 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %270, <4 x i32> %271) #9
  %274 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %182, <8 x i16> %200) #9
  %275 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %182, <8 x i16> %200) #9
  %276 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %236, <8 x i16> %218) #9
  %277 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %236, <8 x i16> %218) #9
  %278 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %237, <8 x i16> %219) #9
  %279 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %237, <8 x i16> %219) #9
  %280 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %183, <8 x i16> %201) #9
  %281 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %183, <8 x i16> %201) #9
  %282 = shufflevector <8 x i16> %119, <8 x i16> %122, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %283 = shufflevector <8 x i16> %119, <8 x i16> %122, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %284 = bitcast <4 x i32> %87 to <8 x i16>
  %285 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %282, <8 x i16> %284) #9
  %286 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %283, <8 x i16> %284) #9
  %287 = bitcast <4 x i32> %91 to <8 x i16>
  %288 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %282, <8 x i16> %287) #9
  %289 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %283, <8 x i16> %287) #9
  %290 = add <4 x i32> %285, <i32 2048, i32 2048, i32 2048, i32 2048>
  %291 = add <4 x i32> %286, <i32 2048, i32 2048, i32 2048, i32 2048>
  %292 = add <4 x i32> %288, <i32 2048, i32 2048, i32 2048, i32 2048>
  %293 = add <4 x i32> %289, <i32 2048, i32 2048, i32 2048, i32 2048>
  %294 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %290, i32 %177) #9
  %295 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %291, i32 %177) #9
  %296 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %292, i32 %177) #9
  %297 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %293, i32 %177) #9
  %298 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %294, <4 x i32> %295) #9
  %299 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %296, <4 x i32> %297) #9
  %300 = shufflevector <8 x i16> %125, <8 x i16> %128, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %301 = shufflevector <8 x i16> %125, <8 x i16> %128, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %302 = bitcast <4 x i32> %99 to <8 x i16>
  %303 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %300, <8 x i16> %302) #9
  %304 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %301, <8 x i16> %302) #9
  %305 = bitcast <4 x i32> %104 to <8 x i16>
  %306 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %300, <8 x i16> %305) #9
  %307 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %301, <8 x i16> %305) #9
  %308 = add <4 x i32> %303, <i32 2048, i32 2048, i32 2048, i32 2048>
  %309 = add <4 x i32> %304, <i32 2048, i32 2048, i32 2048, i32 2048>
  %310 = add <4 x i32> %306, <i32 2048, i32 2048, i32 2048, i32 2048>
  %311 = add <4 x i32> %307, <i32 2048, i32 2048, i32 2048, i32 2048>
  %312 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %308, i32 %177) #9
  %313 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %309, i32 %177) #9
  %314 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %310, i32 %177) #9
  %315 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %311, i32 %177) #9
  %316 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %312, <4 x i32> %313) #9
  %317 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %314, <4 x i32> %315) #9
  %318 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %254, <8 x i16> %272) #9
  %319 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %254, <8 x i16> %272) #9
  %320 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %255, <8 x i16> %273) #9
  %321 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %255, <8 x i16> %273) #9
  %322 = shufflevector <8 x i16> %275, <8 x i16> %280, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %323 = shufflevector <8 x i16> %275, <8 x i16> %280, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %324 = bitcast <4 x i32> %109 to <8 x i16>
  %325 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %322, <8 x i16> %324) #9
  %326 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %323, <8 x i16> %324) #9
  %327 = bitcast <4 x i32> %112 to <8 x i16>
  %328 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %322, <8 x i16> %327) #9
  %329 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %323, <8 x i16> %327) #9
  %330 = add <4 x i32> %325, <i32 2048, i32 2048, i32 2048, i32 2048>
  %331 = add <4 x i32> %326, <i32 2048, i32 2048, i32 2048, i32 2048>
  %332 = add <4 x i32> %328, <i32 2048, i32 2048, i32 2048, i32 2048>
  %333 = add <4 x i32> %329, <i32 2048, i32 2048, i32 2048, i32 2048>
  %334 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %330, i32 %177) #9
  %335 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %331, i32 %177) #9
  %336 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %332, i32 %177) #9
  %337 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %333, i32 %177) #9
  %338 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %334, <4 x i32> %335) #9
  %339 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %336, <4 x i32> %337) #9
  %340 = shufflevector <8 x i16> %276, <8 x i16> %279, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %341 = shufflevector <8 x i16> %276, <8 x i16> %279, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %342 = bitcast <4 x i32> %117 to <8 x i16>
  %343 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %340, <8 x i16> %342) #9
  %344 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %341, <8 x i16> %342) #9
  %345 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %340, <8 x i16> %324) #9
  %346 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %341, <8 x i16> %324) #9
  %347 = add <4 x i32> %343, <i32 2048, i32 2048, i32 2048, i32 2048>
  %348 = add <4 x i32> %344, <i32 2048, i32 2048, i32 2048, i32 2048>
  %349 = add <4 x i32> %345, <i32 2048, i32 2048, i32 2048, i32 2048>
  %350 = add <4 x i32> %346, <i32 2048, i32 2048, i32 2048, i32 2048>
  %351 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %347, i32 %177) #9
  %352 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %348, i32 %177) #9
  %353 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %349, i32 %177) #9
  %354 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %350, i32 %177) #9
  %355 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %351, <4 x i32> %352) #9
  %356 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %353, <4 x i32> %354) #9
  %357 = sub i32 0, %82
  %358 = and i32 %357, 65535
  %359 = or i32 %358, %84
  %360 = insertelement <4 x i32> undef, i32 %359, i32 0
  %361 = shufflevector <4 x i32> %360, <4 x i32> undef, <4 x i32> zeroinitializer
  %362 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %298, <8 x i16> %317) #9
  %363 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %298, <8 x i16> %317) #9
  %364 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %299, <8 x i16> %316) #9
  %365 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %299, <8 x i16> %316) #9
  %366 = shufflevector <8 x i16> %319, <8 x i16> %320, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %367 = shufflevector <8 x i16> %319, <8 x i16> %320, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %368 = bitcast <4 x i32> %361 to <8 x i16>
  %369 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %366, <8 x i16> %368) #9
  %370 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %367, <8 x i16> %368) #9
  %371 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %366, <8 x i16> %284) #9
  %372 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %367, <8 x i16> %284) #9
  %373 = add <4 x i32> %369, <i32 2048, i32 2048, i32 2048, i32 2048>
  %374 = add <4 x i32> %370, <i32 2048, i32 2048, i32 2048, i32 2048>
  %375 = add <4 x i32> %371, <i32 2048, i32 2048, i32 2048, i32 2048>
  %376 = add <4 x i32> %372, <i32 2048, i32 2048, i32 2048, i32 2048>
  %377 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %373, i32 %177) #9
  %378 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %374, i32 %177) #9
  %379 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %375, i32 %177) #9
  %380 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %376, i32 %177) #9
  %381 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %377, <4 x i32> %378) #9
  %382 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %379, <4 x i32> %380) #9
  %383 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %274, <8 x i16> %277) #9
  %384 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %274, <8 x i16> %277) #9
  %385 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %338, <8 x i16> %355) #9
  %386 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %338, <8 x i16> %355) #9
  %387 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %281, <8 x i16> %278) #9
  %388 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %281, <8 x i16> %278) #9
  %389 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %339, <8 x i16> %356) #9
  %390 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %339, <8 x i16> %356) #9
  %391 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %362, <8 x i16> %321) #9
  %392 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %362, <8 x i16> %321) #9
  %393 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %364, <8 x i16> %382) #9
  %394 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %364, <8 x i16> %382) #9
  %395 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %365, <8 x i16> %381) #9
  %396 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %365, <8 x i16> %381) #9
  %397 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %363, <8 x i16> %318) #9
  %398 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %363, <8 x i16> %318) #9
  %399 = shufflevector <8 x i16> %386, <8 x i16> %389, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %400 = shufflevector <8 x i16> %386, <8 x i16> %389, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %401 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %399, <8 x i16> %368) #9
  %402 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %400, <8 x i16> %368) #9
  %403 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %399, <8 x i16> %284) #9
  %404 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %400, <8 x i16> %284) #9
  %405 = add <4 x i32> %401, <i32 2048, i32 2048, i32 2048, i32 2048>
  %406 = add <4 x i32> %402, <i32 2048, i32 2048, i32 2048, i32 2048>
  %407 = add <4 x i32> %403, <i32 2048, i32 2048, i32 2048, i32 2048>
  %408 = add <4 x i32> %404, <i32 2048, i32 2048, i32 2048, i32 2048>
  %409 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %405, i32 %177) #9
  %410 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %406, i32 %177) #9
  %411 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %407, i32 %177) #9
  %412 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %408, i32 %177) #9
  %413 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %409, <4 x i32> %410) #9
  %414 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %411, <4 x i32> %412) #9
  %415 = shufflevector <8 x i16> %384, <8 x i16> %387, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %416 = shufflevector <8 x i16> %384, <8 x i16> %387, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %417 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %415, <8 x i16> %368) #9
  %418 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %416, <8 x i16> %368) #9
  %419 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %415, <8 x i16> %284) #9
  %420 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %416, <8 x i16> %284) #9
  %421 = add <4 x i32> %417, <i32 2048, i32 2048, i32 2048, i32 2048>
  %422 = add <4 x i32> %418, <i32 2048, i32 2048, i32 2048, i32 2048>
  %423 = add <4 x i32> %419, <i32 2048, i32 2048, i32 2048, i32 2048>
  %424 = add <4 x i32> %420, <i32 2048, i32 2048, i32 2048, i32 2048>
  %425 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %421, i32 %177) #9
  %426 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %422, i32 %177) #9
  %427 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %423, i32 %177) #9
  %428 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %424, i32 %177) #9
  %429 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %425, <4 x i32> %426) #9
  %430 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %427, <4 x i32> %428) #9
  %431 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %391, <8 x i16> %388) #9
  %432 = bitcast <2 x i64>* %1 to <8 x i16>*
  store <8 x i16> %431, <8 x i16>* %432, align 16
  %433 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %391, <8 x i16> %388) #9
  %434 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 15
  %435 = bitcast <2 x i64>* %434 to <8 x i16>*
  store <8 x i16> %433, <8 x i16>* %435, align 16
  %436 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %393, <8 x i16> %390) #9
  %437 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 1
  %438 = bitcast <2 x i64>* %437 to <8 x i16>*
  store <8 x i16> %436, <8 x i16>* %438, align 16
  %439 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %393, <8 x i16> %390) #9
  %440 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 14
  %441 = bitcast <2 x i64>* %440 to <8 x i16>*
  store <8 x i16> %439, <8 x i16>* %441, align 16
  %442 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %395, <8 x i16> %414) #9
  %443 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 2
  %444 = bitcast <2 x i64>* %443 to <8 x i16>*
  store <8 x i16> %442, <8 x i16>* %444, align 16
  %445 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %395, <8 x i16> %414) #9
  %446 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 13
  %447 = bitcast <2 x i64>* %446 to <8 x i16>*
  store <8 x i16> %445, <8 x i16>* %447, align 16
  %448 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %397, <8 x i16> %430) #9
  %449 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 3
  %450 = bitcast <2 x i64>* %449 to <8 x i16>*
  store <8 x i16> %448, <8 x i16>* %450, align 16
  %451 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %397, <8 x i16> %430) #9
  %452 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 12
  %453 = bitcast <2 x i64>* %452 to <8 x i16>*
  store <8 x i16> %451, <8 x i16>* %453, align 16
  %454 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %398, <8 x i16> %429) #9
  %455 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 4
  %456 = bitcast <2 x i64>* %455 to <8 x i16>*
  store <8 x i16> %454, <8 x i16>* %456, align 16
  %457 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %398, <8 x i16> %429) #9
  %458 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 11
  %459 = bitcast <2 x i64>* %458 to <8 x i16>*
  store <8 x i16> %457, <8 x i16>* %459, align 16
  %460 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %396, <8 x i16> %413) #9
  %461 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 5
  %462 = bitcast <2 x i64>* %461 to <8 x i16>*
  store <8 x i16> %460, <8 x i16>* %462, align 16
  %463 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %396, <8 x i16> %413) #9
  %464 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 10
  %465 = bitcast <2 x i64>* %464 to <8 x i16>*
  store <8 x i16> %463, <8 x i16>* %465, align 16
  %466 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %394, <8 x i16> %385) #9
  %467 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 6
  %468 = bitcast <2 x i64>* %467 to <8 x i16>*
  store <8 x i16> %466, <8 x i16>* %468, align 16
  %469 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %394, <8 x i16> %385) #9
  %470 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 9
  %471 = bitcast <2 x i64>* %470 to <8 x i16>*
  store <8 x i16> %469, <8 x i16>* %471, align 16
  %472 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %392, <8 x i16> %383) #9
  %473 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 7
  %474 = bitcast <2 x i64>* %473 to <8 x i16>*
  store <8 x i16> %472, <8 x i16>* %474, align 16
  %475 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %392, <8 x i16> %383) #9
  %476 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 8
  %477 = bitcast <2 x i64>* %476 to <8 x i16>*
  store <8 x i16> %475, <8 x i16>* %477, align 16
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal void @iadst16_sse2(<2 x i64>* nocapture readonly, <2 x i64>* nocapture, i8 signext) #0 {
  %4 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 2), align 8
  %5 = and i32 %4, 65535
  %6 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 62), align 8
  %7 = shl i32 %6, 16
  %8 = or i32 %7, %5
  %9 = insertelement <4 x i32> undef, i32 %8, i32 0
  %10 = shufflevector <4 x i32> %9, <4 x i32> undef, <4 x i32> zeroinitializer
  %11 = and i32 %6, 65535
  %12 = shl i32 %4, 16
  %13 = sub i32 0, %12
  %14 = or i32 %11, %13
  %15 = insertelement <4 x i32> undef, i32 %14, i32 0
  %16 = shufflevector <4 x i32> %15, <4 x i32> undef, <4 x i32> zeroinitializer
  %17 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 10), align 8
  %18 = and i32 %17, 65535
  %19 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 54), align 8
  %20 = shl i32 %19, 16
  %21 = or i32 %20, %18
  %22 = insertelement <4 x i32> undef, i32 %21, i32 0
  %23 = shufflevector <4 x i32> %22, <4 x i32> undef, <4 x i32> zeroinitializer
  %24 = and i32 %19, 65535
  %25 = shl i32 %17, 16
  %26 = sub i32 0, %25
  %27 = or i32 %24, %26
  %28 = insertelement <4 x i32> undef, i32 %27, i32 0
  %29 = shufflevector <4 x i32> %28, <4 x i32> undef, <4 x i32> zeroinitializer
  %30 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 18), align 8
  %31 = and i32 %30, 65535
  %32 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 46), align 8
  %33 = shl i32 %32, 16
  %34 = or i32 %33, %31
  %35 = insertelement <4 x i32> undef, i32 %34, i32 0
  %36 = shufflevector <4 x i32> %35, <4 x i32> undef, <4 x i32> zeroinitializer
  %37 = and i32 %32, 65535
  %38 = shl i32 %30, 16
  %39 = sub i32 0, %38
  %40 = or i32 %37, %39
  %41 = insertelement <4 x i32> undef, i32 %40, i32 0
  %42 = shufflevector <4 x i32> %41, <4 x i32> undef, <4 x i32> zeroinitializer
  %43 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 26), align 8
  %44 = and i32 %43, 65535
  %45 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 38), align 8
  %46 = shl i32 %45, 16
  %47 = or i32 %46, %44
  %48 = insertelement <4 x i32> undef, i32 %47, i32 0
  %49 = shufflevector <4 x i32> %48, <4 x i32> undef, <4 x i32> zeroinitializer
  %50 = and i32 %45, 65535
  %51 = shl i32 %43, 16
  %52 = sub i32 0, %51
  %53 = or i32 %50, %52
  %54 = insertelement <4 x i32> undef, i32 %53, i32 0
  %55 = shufflevector <4 x i32> %54, <4 x i32> undef, <4 x i32> zeroinitializer
  %56 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 34), align 8
  %57 = and i32 %56, 65535
  %58 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 30), align 8
  %59 = shl i32 %58, 16
  %60 = or i32 %59, %57
  %61 = insertelement <4 x i32> undef, i32 %60, i32 0
  %62 = shufflevector <4 x i32> %61, <4 x i32> undef, <4 x i32> zeroinitializer
  %63 = and i32 %58, 65535
  %64 = shl i32 %56, 16
  %65 = sub i32 0, %64
  %66 = or i32 %63, %65
  %67 = insertelement <4 x i32> undef, i32 %66, i32 0
  %68 = shufflevector <4 x i32> %67, <4 x i32> undef, <4 x i32> zeroinitializer
  %69 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 42), align 8
  %70 = and i32 %69, 65535
  %71 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 22), align 8
  %72 = shl i32 %71, 16
  %73 = or i32 %72, %70
  %74 = insertelement <4 x i32> undef, i32 %73, i32 0
  %75 = shufflevector <4 x i32> %74, <4 x i32> undef, <4 x i32> zeroinitializer
  %76 = and i32 %71, 65535
  %77 = shl i32 %69, 16
  %78 = sub i32 0, %77
  %79 = or i32 %76, %78
  %80 = insertelement <4 x i32> undef, i32 %79, i32 0
  %81 = shufflevector <4 x i32> %80, <4 x i32> undef, <4 x i32> zeroinitializer
  %82 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 50), align 8
  %83 = and i32 %82, 65535
  %84 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 14), align 8
  %85 = shl i32 %84, 16
  %86 = or i32 %85, %83
  %87 = insertelement <4 x i32> undef, i32 %86, i32 0
  %88 = shufflevector <4 x i32> %87, <4 x i32> undef, <4 x i32> zeroinitializer
  %89 = and i32 %84, 65535
  %90 = shl i32 %82, 16
  %91 = sub i32 0, %90
  %92 = or i32 %89, %91
  %93 = insertelement <4 x i32> undef, i32 %92, i32 0
  %94 = shufflevector <4 x i32> %93, <4 x i32> undef, <4 x i32> zeroinitializer
  %95 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 58), align 8
  %96 = and i32 %95, 65535
  %97 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 6), align 8
  %98 = shl i32 %97, 16
  %99 = or i32 %98, %96
  %100 = insertelement <4 x i32> undef, i32 %99, i32 0
  %101 = shufflevector <4 x i32> %100, <4 x i32> undef, <4 x i32> zeroinitializer
  %102 = and i32 %97, 65535
  %103 = shl i32 %95, 16
  %104 = sub i32 0, %103
  %105 = or i32 %102, %104
  %106 = insertelement <4 x i32> undef, i32 %105, i32 0
  %107 = shufflevector <4 x i32> %106, <4 x i32> undef, <4 x i32> zeroinitializer
  %108 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 15
  %109 = bitcast <2 x i64>* %108 to <8 x i16>*
  %110 = load <8 x i16>, <8 x i16>* %109, align 16
  %111 = bitcast <2 x i64>* %0 to <8 x i16>*
  %112 = load <8 x i16>, <8 x i16>* %111, align 16
  %113 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 13
  %114 = bitcast <2 x i64>* %113 to <8 x i16>*
  %115 = load <8 x i16>, <8 x i16>* %114, align 16
  %116 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 2
  %117 = bitcast <2 x i64>* %116 to <8 x i16>*
  %118 = load <8 x i16>, <8 x i16>* %117, align 16
  %119 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 11
  %120 = bitcast <2 x i64>* %119 to <8 x i16>*
  %121 = load <8 x i16>, <8 x i16>* %120, align 16
  %122 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 4
  %123 = bitcast <2 x i64>* %122 to <8 x i16>*
  %124 = load <8 x i16>, <8 x i16>* %123, align 16
  %125 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 9
  %126 = bitcast <2 x i64>* %125 to <8 x i16>*
  %127 = load <8 x i16>, <8 x i16>* %126, align 16
  %128 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 6
  %129 = bitcast <2 x i64>* %128 to <8 x i16>*
  %130 = load <8 x i16>, <8 x i16>* %129, align 16
  %131 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 7
  %132 = bitcast <2 x i64>* %131 to <8 x i16>*
  %133 = load <8 x i16>, <8 x i16>* %132, align 16
  %134 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 8
  %135 = bitcast <2 x i64>* %134 to <8 x i16>*
  %136 = load <8 x i16>, <8 x i16>* %135, align 16
  %137 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 5
  %138 = bitcast <2 x i64>* %137 to <8 x i16>*
  %139 = load <8 x i16>, <8 x i16>* %138, align 16
  %140 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 10
  %141 = bitcast <2 x i64>* %140 to <8 x i16>*
  %142 = load <8 x i16>, <8 x i16>* %141, align 16
  %143 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 3
  %144 = bitcast <2 x i64>* %143 to <8 x i16>*
  %145 = load <8 x i16>, <8 x i16>* %144, align 16
  %146 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 12
  %147 = bitcast <2 x i64>* %146 to <8 x i16>*
  %148 = load <8 x i16>, <8 x i16>* %147, align 16
  %149 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 1
  %150 = bitcast <2 x i64>* %149 to <8 x i16>*
  %151 = load <8 x i16>, <8 x i16>* %150, align 16
  %152 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 14
  %153 = bitcast <2 x i64>* %152 to <8 x i16>*
  %154 = load <8 x i16>, <8 x i16>* %153, align 16
  %155 = shufflevector <8 x i16> %110, <8 x i16> %112, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %156 = shufflevector <8 x i16> %110, <8 x i16> %112, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %157 = bitcast <4 x i32> %10 to <8 x i16>
  %158 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %155, <8 x i16> %157) #9
  %159 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %156, <8 x i16> %157) #9
  %160 = bitcast <4 x i32> %16 to <8 x i16>
  %161 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %155, <8 x i16> %160) #9
  %162 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %156, <8 x i16> %160) #9
  %163 = add <4 x i32> %158, <i32 2048, i32 2048, i32 2048, i32 2048>
  %164 = add <4 x i32> %159, <i32 2048, i32 2048, i32 2048, i32 2048>
  %165 = add <4 x i32> %161, <i32 2048, i32 2048, i32 2048, i32 2048>
  %166 = add <4 x i32> %162, <i32 2048, i32 2048, i32 2048, i32 2048>
  %167 = sext i8 %2 to i32
  %168 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %163, i32 %167) #9
  %169 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %164, i32 %167) #9
  %170 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %165, i32 %167) #9
  %171 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %166, i32 %167) #9
  %172 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %168, <4 x i32> %169) #9
  %173 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %170, <4 x i32> %171) #9
  %174 = shufflevector <8 x i16> %115, <8 x i16> %118, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %175 = shufflevector <8 x i16> %115, <8 x i16> %118, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %176 = bitcast <4 x i32> %23 to <8 x i16>
  %177 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %174, <8 x i16> %176) #9
  %178 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %175, <8 x i16> %176) #9
  %179 = bitcast <4 x i32> %29 to <8 x i16>
  %180 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %174, <8 x i16> %179) #9
  %181 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %175, <8 x i16> %179) #9
  %182 = add <4 x i32> %177, <i32 2048, i32 2048, i32 2048, i32 2048>
  %183 = add <4 x i32> %178, <i32 2048, i32 2048, i32 2048, i32 2048>
  %184 = add <4 x i32> %180, <i32 2048, i32 2048, i32 2048, i32 2048>
  %185 = add <4 x i32> %181, <i32 2048, i32 2048, i32 2048, i32 2048>
  %186 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %182, i32 %167) #9
  %187 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %183, i32 %167) #9
  %188 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %184, i32 %167) #9
  %189 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %185, i32 %167) #9
  %190 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %186, <4 x i32> %187) #9
  %191 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %188, <4 x i32> %189) #9
  %192 = shufflevector <8 x i16> %121, <8 x i16> %124, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %193 = shufflevector <8 x i16> %121, <8 x i16> %124, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %194 = bitcast <4 x i32> %36 to <8 x i16>
  %195 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %192, <8 x i16> %194) #9
  %196 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %193, <8 x i16> %194) #9
  %197 = bitcast <4 x i32> %42 to <8 x i16>
  %198 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %192, <8 x i16> %197) #9
  %199 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %193, <8 x i16> %197) #9
  %200 = add <4 x i32> %195, <i32 2048, i32 2048, i32 2048, i32 2048>
  %201 = add <4 x i32> %196, <i32 2048, i32 2048, i32 2048, i32 2048>
  %202 = add <4 x i32> %198, <i32 2048, i32 2048, i32 2048, i32 2048>
  %203 = add <4 x i32> %199, <i32 2048, i32 2048, i32 2048, i32 2048>
  %204 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %200, i32 %167) #9
  %205 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %201, i32 %167) #9
  %206 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %202, i32 %167) #9
  %207 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %203, i32 %167) #9
  %208 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %204, <4 x i32> %205) #9
  %209 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %206, <4 x i32> %207) #9
  %210 = shufflevector <8 x i16> %127, <8 x i16> %130, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %211 = shufflevector <8 x i16> %127, <8 x i16> %130, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %212 = bitcast <4 x i32> %49 to <8 x i16>
  %213 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %210, <8 x i16> %212) #9
  %214 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %211, <8 x i16> %212) #9
  %215 = bitcast <4 x i32> %55 to <8 x i16>
  %216 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %210, <8 x i16> %215) #9
  %217 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %211, <8 x i16> %215) #9
  %218 = add <4 x i32> %213, <i32 2048, i32 2048, i32 2048, i32 2048>
  %219 = add <4 x i32> %214, <i32 2048, i32 2048, i32 2048, i32 2048>
  %220 = add <4 x i32> %216, <i32 2048, i32 2048, i32 2048, i32 2048>
  %221 = add <4 x i32> %217, <i32 2048, i32 2048, i32 2048, i32 2048>
  %222 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %218, i32 %167) #9
  %223 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %219, i32 %167) #9
  %224 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %220, i32 %167) #9
  %225 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %221, i32 %167) #9
  %226 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %222, <4 x i32> %223) #9
  %227 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %224, <4 x i32> %225) #9
  %228 = shufflevector <8 x i16> %133, <8 x i16> %136, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %229 = shufflevector <8 x i16> %133, <8 x i16> %136, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %230 = bitcast <4 x i32> %62 to <8 x i16>
  %231 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %228, <8 x i16> %230) #9
  %232 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %229, <8 x i16> %230) #9
  %233 = bitcast <4 x i32> %68 to <8 x i16>
  %234 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %228, <8 x i16> %233) #9
  %235 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %229, <8 x i16> %233) #9
  %236 = add <4 x i32> %231, <i32 2048, i32 2048, i32 2048, i32 2048>
  %237 = add <4 x i32> %232, <i32 2048, i32 2048, i32 2048, i32 2048>
  %238 = add <4 x i32> %234, <i32 2048, i32 2048, i32 2048, i32 2048>
  %239 = add <4 x i32> %235, <i32 2048, i32 2048, i32 2048, i32 2048>
  %240 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %236, i32 %167) #9
  %241 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %237, i32 %167) #9
  %242 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %238, i32 %167) #9
  %243 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %239, i32 %167) #9
  %244 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %240, <4 x i32> %241) #9
  %245 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %242, <4 x i32> %243) #9
  %246 = shufflevector <8 x i16> %139, <8 x i16> %142, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %247 = shufflevector <8 x i16> %139, <8 x i16> %142, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %248 = bitcast <4 x i32> %75 to <8 x i16>
  %249 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %246, <8 x i16> %248) #9
  %250 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %247, <8 x i16> %248) #9
  %251 = bitcast <4 x i32> %81 to <8 x i16>
  %252 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %246, <8 x i16> %251) #9
  %253 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %247, <8 x i16> %251) #9
  %254 = add <4 x i32> %249, <i32 2048, i32 2048, i32 2048, i32 2048>
  %255 = add <4 x i32> %250, <i32 2048, i32 2048, i32 2048, i32 2048>
  %256 = add <4 x i32> %252, <i32 2048, i32 2048, i32 2048, i32 2048>
  %257 = add <4 x i32> %253, <i32 2048, i32 2048, i32 2048, i32 2048>
  %258 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %254, i32 %167) #9
  %259 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %255, i32 %167) #9
  %260 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %256, i32 %167) #9
  %261 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %257, i32 %167) #9
  %262 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %258, <4 x i32> %259) #9
  %263 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %260, <4 x i32> %261) #9
  %264 = shufflevector <8 x i16> %145, <8 x i16> %148, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %265 = shufflevector <8 x i16> %145, <8 x i16> %148, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %266 = bitcast <4 x i32> %88 to <8 x i16>
  %267 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %264, <8 x i16> %266) #9
  %268 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %265, <8 x i16> %266) #9
  %269 = bitcast <4 x i32> %94 to <8 x i16>
  %270 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %264, <8 x i16> %269) #9
  %271 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %265, <8 x i16> %269) #9
  %272 = add <4 x i32> %267, <i32 2048, i32 2048, i32 2048, i32 2048>
  %273 = add <4 x i32> %268, <i32 2048, i32 2048, i32 2048, i32 2048>
  %274 = add <4 x i32> %270, <i32 2048, i32 2048, i32 2048, i32 2048>
  %275 = add <4 x i32> %271, <i32 2048, i32 2048, i32 2048, i32 2048>
  %276 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %272, i32 %167) #9
  %277 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %273, i32 %167) #9
  %278 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %274, i32 %167) #9
  %279 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %275, i32 %167) #9
  %280 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %276, <4 x i32> %277) #9
  %281 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %278, <4 x i32> %279) #9
  %282 = shufflevector <8 x i16> %151, <8 x i16> %154, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %283 = shufflevector <8 x i16> %151, <8 x i16> %154, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %284 = bitcast <4 x i32> %101 to <8 x i16>
  %285 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %282, <8 x i16> %284) #9
  %286 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %283, <8 x i16> %284) #9
  %287 = bitcast <4 x i32> %107 to <8 x i16>
  %288 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %282, <8 x i16> %287) #9
  %289 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %283, <8 x i16> %287) #9
  %290 = add <4 x i32> %285, <i32 2048, i32 2048, i32 2048, i32 2048>
  %291 = add <4 x i32> %286, <i32 2048, i32 2048, i32 2048, i32 2048>
  %292 = add <4 x i32> %288, <i32 2048, i32 2048, i32 2048, i32 2048>
  %293 = add <4 x i32> %289, <i32 2048, i32 2048, i32 2048, i32 2048>
  %294 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %290, i32 %167) #9
  %295 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %291, i32 %167) #9
  %296 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %292, i32 %167) #9
  %297 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %293, i32 %167) #9
  %298 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %294, <4 x i32> %295) #9
  %299 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %296, <4 x i32> %297) #9
  %300 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %172, <8 x i16> %244) #9
  %301 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %172, <8 x i16> %244) #9
  %302 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %173, <8 x i16> %245) #9
  %303 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %173, <8 x i16> %245) #9
  %304 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %190, <8 x i16> %262) #9
  %305 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %190, <8 x i16> %262) #9
  %306 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %191, <8 x i16> %263) #9
  %307 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %191, <8 x i16> %263) #9
  %308 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %208, <8 x i16> %280) #9
  %309 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %208, <8 x i16> %280) #9
  %310 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %209, <8 x i16> %281) #9
  %311 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %209, <8 x i16> %281) #9
  %312 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %226, <8 x i16> %298) #9
  %313 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %226, <8 x i16> %298) #9
  %314 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %227, <8 x i16> %299) #9
  %315 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %227, <8 x i16> %299) #9
  %316 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 8), align 16
  %317 = and i32 %316, 65535
  %318 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 56), align 16
  %319 = shl i32 %318, 16
  %320 = or i32 %319, %317
  %321 = insertelement <4 x i32> undef, i32 %320, i32 0
  %322 = shufflevector <4 x i32> %321, <4 x i32> undef, <4 x i32> zeroinitializer
  %323 = and i32 %318, 65535
  %324 = shl i32 %316, 16
  %325 = sub i32 0, %324
  %326 = or i32 %323, %325
  %327 = insertelement <4 x i32> undef, i32 %326, i32 0
  %328 = shufflevector <4 x i32> %327, <4 x i32> undef, <4 x i32> zeroinitializer
  %329 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 40), align 16
  %330 = and i32 %329, 65535
  %331 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 24), align 16
  %332 = shl i32 %331, 16
  %333 = or i32 %332, %330
  %334 = insertelement <4 x i32> undef, i32 %333, i32 0
  %335 = shufflevector <4 x i32> %334, <4 x i32> undef, <4 x i32> zeroinitializer
  %336 = and i32 %331, 65535
  %337 = shl i32 %329, 16
  %338 = sub i32 0, %337
  %339 = or i32 %336, %338
  %340 = insertelement <4 x i32> undef, i32 %339, i32 0
  %341 = shufflevector <4 x i32> %340, <4 x i32> undef, <4 x i32> zeroinitializer
  %342 = sub i32 0, %318
  %343 = and i32 %342, 65535
  %344 = or i32 %343, %324
  %345 = insertelement <4 x i32> undef, i32 %344, i32 0
  %346 = shufflevector <4 x i32> %345, <4 x i32> undef, <4 x i32> zeroinitializer
  %347 = sub i32 0, %331
  %348 = and i32 %347, 65535
  %349 = or i32 %348, %337
  %350 = insertelement <4 x i32> undef, i32 %349, i32 0
  %351 = shufflevector <4 x i32> %350, <4 x i32> undef, <4 x i32> zeroinitializer
  %352 = shufflevector <8 x i16> %301, <8 x i16> %303, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %353 = shufflevector <8 x i16> %301, <8 x i16> %303, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %354 = bitcast <4 x i32> %322 to <8 x i16>
  %355 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %352, <8 x i16> %354) #9
  %356 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %353, <8 x i16> %354) #9
  %357 = bitcast <4 x i32> %328 to <8 x i16>
  %358 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %352, <8 x i16> %357) #9
  %359 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %353, <8 x i16> %357) #9
  %360 = add <4 x i32> %355, <i32 2048, i32 2048, i32 2048, i32 2048>
  %361 = add <4 x i32> %356, <i32 2048, i32 2048, i32 2048, i32 2048>
  %362 = add <4 x i32> %358, <i32 2048, i32 2048, i32 2048, i32 2048>
  %363 = add <4 x i32> %359, <i32 2048, i32 2048, i32 2048, i32 2048>
  %364 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %360, i32 %167) #9
  %365 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %361, i32 %167) #9
  %366 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %362, i32 %167) #9
  %367 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %363, i32 %167) #9
  %368 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %364, <4 x i32> %365) #9
  %369 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %366, <4 x i32> %367) #9
  %370 = shufflevector <8 x i16> %305, <8 x i16> %307, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %371 = shufflevector <8 x i16> %305, <8 x i16> %307, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %372 = bitcast <4 x i32> %335 to <8 x i16>
  %373 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %370, <8 x i16> %372) #9
  %374 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %371, <8 x i16> %372) #9
  %375 = bitcast <4 x i32> %341 to <8 x i16>
  %376 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %370, <8 x i16> %375) #9
  %377 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %371, <8 x i16> %375) #9
  %378 = add <4 x i32> %373, <i32 2048, i32 2048, i32 2048, i32 2048>
  %379 = add <4 x i32> %374, <i32 2048, i32 2048, i32 2048, i32 2048>
  %380 = add <4 x i32> %376, <i32 2048, i32 2048, i32 2048, i32 2048>
  %381 = add <4 x i32> %377, <i32 2048, i32 2048, i32 2048, i32 2048>
  %382 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %378, i32 %167) #9
  %383 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %379, i32 %167) #9
  %384 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %380, i32 %167) #9
  %385 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %381, i32 %167) #9
  %386 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %382, <4 x i32> %383) #9
  %387 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %384, <4 x i32> %385) #9
  %388 = shufflevector <8 x i16> %309, <8 x i16> %311, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %389 = shufflevector <8 x i16> %309, <8 x i16> %311, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %390 = bitcast <4 x i32> %346 to <8 x i16>
  %391 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %388, <8 x i16> %390) #9
  %392 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %389, <8 x i16> %390) #9
  %393 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %388, <8 x i16> %354) #9
  %394 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %389, <8 x i16> %354) #9
  %395 = add <4 x i32> %391, <i32 2048, i32 2048, i32 2048, i32 2048>
  %396 = add <4 x i32> %392, <i32 2048, i32 2048, i32 2048, i32 2048>
  %397 = add <4 x i32> %393, <i32 2048, i32 2048, i32 2048, i32 2048>
  %398 = add <4 x i32> %394, <i32 2048, i32 2048, i32 2048, i32 2048>
  %399 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %395, i32 %167) #9
  %400 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %396, i32 %167) #9
  %401 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %397, i32 %167) #9
  %402 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %398, i32 %167) #9
  %403 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %399, <4 x i32> %400) #9
  %404 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %401, <4 x i32> %402) #9
  %405 = shufflevector <8 x i16> %313, <8 x i16> %315, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %406 = shufflevector <8 x i16> %313, <8 x i16> %315, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %407 = bitcast <4 x i32> %351 to <8 x i16>
  %408 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %405, <8 x i16> %407) #9
  %409 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %406, <8 x i16> %407) #9
  %410 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %405, <8 x i16> %372) #9
  %411 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %406, <8 x i16> %372) #9
  %412 = add <4 x i32> %408, <i32 2048, i32 2048, i32 2048, i32 2048>
  %413 = add <4 x i32> %409, <i32 2048, i32 2048, i32 2048, i32 2048>
  %414 = add <4 x i32> %410, <i32 2048, i32 2048, i32 2048, i32 2048>
  %415 = add <4 x i32> %411, <i32 2048, i32 2048, i32 2048, i32 2048>
  %416 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %412, i32 %167) #9
  %417 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %413, i32 %167) #9
  %418 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %414, i32 %167) #9
  %419 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %415, i32 %167) #9
  %420 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %416, <4 x i32> %417) #9
  %421 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %418, <4 x i32> %419) #9
  %422 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %300, <8 x i16> %308) #9
  %423 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %300, <8 x i16> %308) #9
  %424 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %302, <8 x i16> %310) #9
  %425 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %302, <8 x i16> %310) #9
  %426 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %304, <8 x i16> %312) #9
  %427 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %304, <8 x i16> %312) #9
  %428 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %306, <8 x i16> %314) #9
  %429 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %306, <8 x i16> %314) #9
  %430 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %368, <8 x i16> %403) #9
  %431 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %368, <8 x i16> %403) #9
  %432 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %369, <8 x i16> %404) #9
  %433 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %369, <8 x i16> %404) #9
  %434 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %386, <8 x i16> %420) #9
  %435 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %386, <8 x i16> %420) #9
  %436 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %387, <8 x i16> %421) #9
  %437 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %387, <8 x i16> %421) #9
  %438 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 16), align 16
  %439 = and i32 %438, 65535
  %440 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 48), align 16
  %441 = shl i32 %440, 16
  %442 = or i32 %441, %439
  %443 = insertelement <4 x i32> undef, i32 %442, i32 0
  %444 = shufflevector <4 x i32> %443, <4 x i32> undef, <4 x i32> zeroinitializer
  %445 = and i32 %440, 65535
  %446 = shl i32 %438, 16
  %447 = sub i32 0, %446
  %448 = or i32 %445, %447
  %449 = insertelement <4 x i32> undef, i32 %448, i32 0
  %450 = shufflevector <4 x i32> %449, <4 x i32> undef, <4 x i32> zeroinitializer
  %451 = sub i32 0, %440
  %452 = and i32 %451, 65535
  %453 = or i32 %452, %446
  %454 = insertelement <4 x i32> undef, i32 %453, i32 0
  %455 = shufflevector <4 x i32> %454, <4 x i32> undef, <4 x i32> zeroinitializer
  %456 = shufflevector <8 x i16> %423, <8 x i16> %425, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %457 = shufflevector <8 x i16> %423, <8 x i16> %425, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %458 = bitcast <4 x i32> %444 to <8 x i16>
  %459 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %456, <8 x i16> %458) #9
  %460 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %457, <8 x i16> %458) #9
  %461 = bitcast <4 x i32> %450 to <8 x i16>
  %462 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %456, <8 x i16> %461) #9
  %463 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %457, <8 x i16> %461) #9
  %464 = add <4 x i32> %459, <i32 2048, i32 2048, i32 2048, i32 2048>
  %465 = add <4 x i32> %460, <i32 2048, i32 2048, i32 2048, i32 2048>
  %466 = add <4 x i32> %462, <i32 2048, i32 2048, i32 2048, i32 2048>
  %467 = add <4 x i32> %463, <i32 2048, i32 2048, i32 2048, i32 2048>
  %468 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %464, i32 %167) #9
  %469 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %465, i32 %167) #9
  %470 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %466, i32 %167) #9
  %471 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %467, i32 %167) #9
  %472 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %468, <4 x i32> %469) #9
  %473 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %470, <4 x i32> %471) #9
  %474 = shufflevector <8 x i16> %427, <8 x i16> %429, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %475 = shufflevector <8 x i16> %427, <8 x i16> %429, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %476 = bitcast <4 x i32> %455 to <8 x i16>
  %477 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %474, <8 x i16> %476) #9
  %478 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %475, <8 x i16> %476) #9
  %479 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %474, <8 x i16> %458) #9
  %480 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %475, <8 x i16> %458) #9
  %481 = add <4 x i32> %477, <i32 2048, i32 2048, i32 2048, i32 2048>
  %482 = add <4 x i32> %478, <i32 2048, i32 2048, i32 2048, i32 2048>
  %483 = add <4 x i32> %479, <i32 2048, i32 2048, i32 2048, i32 2048>
  %484 = add <4 x i32> %480, <i32 2048, i32 2048, i32 2048, i32 2048>
  %485 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %481, i32 %167) #9
  %486 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %482, i32 %167) #9
  %487 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %483, i32 %167) #9
  %488 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %484, i32 %167) #9
  %489 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %485, <4 x i32> %486) #9
  %490 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %487, <4 x i32> %488) #9
  %491 = shufflevector <8 x i16> %431, <8 x i16> %433, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %492 = shufflevector <8 x i16> %431, <8 x i16> %433, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %493 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %491, <8 x i16> %458) #9
  %494 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %492, <8 x i16> %458) #9
  %495 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %491, <8 x i16> %461) #9
  %496 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %492, <8 x i16> %461) #9
  %497 = add <4 x i32> %493, <i32 2048, i32 2048, i32 2048, i32 2048>
  %498 = add <4 x i32> %494, <i32 2048, i32 2048, i32 2048, i32 2048>
  %499 = add <4 x i32> %495, <i32 2048, i32 2048, i32 2048, i32 2048>
  %500 = add <4 x i32> %496, <i32 2048, i32 2048, i32 2048, i32 2048>
  %501 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %497, i32 %167) #9
  %502 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %498, i32 %167) #9
  %503 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %499, i32 %167) #9
  %504 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %500, i32 %167) #9
  %505 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %501, <4 x i32> %502) #9
  %506 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %503, <4 x i32> %504) #9
  %507 = shufflevector <8 x i16> %435, <8 x i16> %437, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %508 = shufflevector <8 x i16> %435, <8 x i16> %437, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %509 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %507, <8 x i16> %476) #9
  %510 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %508, <8 x i16> %476) #9
  %511 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %507, <8 x i16> %458) #9
  %512 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %508, <8 x i16> %458) #9
  %513 = add <4 x i32> %509, <i32 2048, i32 2048, i32 2048, i32 2048>
  %514 = add <4 x i32> %510, <i32 2048, i32 2048, i32 2048, i32 2048>
  %515 = add <4 x i32> %511, <i32 2048, i32 2048, i32 2048, i32 2048>
  %516 = add <4 x i32> %512, <i32 2048, i32 2048, i32 2048, i32 2048>
  %517 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %513, i32 %167) #9
  %518 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %514, i32 %167) #9
  %519 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %515, i32 %167) #9
  %520 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %516, i32 %167) #9
  %521 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %517, <4 x i32> %518) #9
  %522 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %519, <4 x i32> %520) #9
  %523 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %422, <8 x i16> %426) #9
  %524 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %422, <8 x i16> %426) #9
  %525 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %424, <8 x i16> %428) #9
  %526 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %424, <8 x i16> %428) #9
  %527 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %472, <8 x i16> %489) #9
  %528 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %472, <8 x i16> %489) #9
  %529 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %473, <8 x i16> %490) #9
  %530 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %473, <8 x i16> %490) #9
  %531 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %430, <8 x i16> %434) #9
  %532 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %430, <8 x i16> %434) #9
  %533 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %432, <8 x i16> %436) #9
  %534 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %432, <8 x i16> %436) #9
  %535 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %505, <8 x i16> %521) #9
  %536 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %505, <8 x i16> %521) #9
  %537 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %506, <8 x i16> %522) #9
  %538 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %506, <8 x i16> %522) #9
  %539 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 32), align 16
  %540 = and i32 %539, 65535
  %541 = shl i32 %539, 16
  %542 = or i32 %540, %541
  %543 = insertelement <4 x i32> undef, i32 %542, i32 0
  %544 = shufflevector <4 x i32> %543, <4 x i32> undef, <4 x i32> zeroinitializer
  %545 = sub i32 0, %541
  %546 = or i32 %540, %545
  %547 = insertelement <4 x i32> undef, i32 %546, i32 0
  %548 = shufflevector <4 x i32> %547, <4 x i32> undef, <4 x i32> zeroinitializer
  %549 = shufflevector <8 x i16> %524, <8 x i16> %526, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %550 = shufflevector <8 x i16> %524, <8 x i16> %526, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %551 = bitcast <4 x i32> %544 to <8 x i16>
  %552 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %549, <8 x i16> %551) #9
  %553 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %550, <8 x i16> %551) #9
  %554 = bitcast <4 x i32> %548 to <8 x i16>
  %555 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %549, <8 x i16> %554) #9
  %556 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %550, <8 x i16> %554) #9
  %557 = add <4 x i32> %552, <i32 2048, i32 2048, i32 2048, i32 2048>
  %558 = add <4 x i32> %553, <i32 2048, i32 2048, i32 2048, i32 2048>
  %559 = add <4 x i32> %555, <i32 2048, i32 2048, i32 2048, i32 2048>
  %560 = add <4 x i32> %556, <i32 2048, i32 2048, i32 2048, i32 2048>
  %561 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %557, i32 %167) #9
  %562 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %558, i32 %167) #9
  %563 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %559, i32 %167) #9
  %564 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %560, i32 %167) #9
  %565 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %561, <4 x i32> %562) #9
  %566 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %563, <4 x i32> %564) #9
  %567 = shufflevector <8 x i16> %528, <8 x i16> %530, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %568 = shufflevector <8 x i16> %528, <8 x i16> %530, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %569 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %567, <8 x i16> %551) #9
  %570 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %568, <8 x i16> %551) #9
  %571 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %567, <8 x i16> %554) #9
  %572 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %568, <8 x i16> %554) #9
  %573 = add <4 x i32> %569, <i32 2048, i32 2048, i32 2048, i32 2048>
  %574 = add <4 x i32> %570, <i32 2048, i32 2048, i32 2048, i32 2048>
  %575 = add <4 x i32> %571, <i32 2048, i32 2048, i32 2048, i32 2048>
  %576 = add <4 x i32> %572, <i32 2048, i32 2048, i32 2048, i32 2048>
  %577 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %573, i32 %167) #9
  %578 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %574, i32 %167) #9
  %579 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %575, i32 %167) #9
  %580 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %576, i32 %167) #9
  %581 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %577, <4 x i32> %578) #9
  %582 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %579, <4 x i32> %580) #9
  %583 = shufflevector <8 x i16> %532, <8 x i16> %534, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %584 = shufflevector <8 x i16> %532, <8 x i16> %534, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %585 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %583, <8 x i16> %551) #9
  %586 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %584, <8 x i16> %551) #9
  %587 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %583, <8 x i16> %554) #9
  %588 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %584, <8 x i16> %554) #9
  %589 = add <4 x i32> %585, <i32 2048, i32 2048, i32 2048, i32 2048>
  %590 = add <4 x i32> %586, <i32 2048, i32 2048, i32 2048, i32 2048>
  %591 = add <4 x i32> %587, <i32 2048, i32 2048, i32 2048, i32 2048>
  %592 = add <4 x i32> %588, <i32 2048, i32 2048, i32 2048, i32 2048>
  %593 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %589, i32 %167) #9
  %594 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %590, i32 %167) #9
  %595 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %591, i32 %167) #9
  %596 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %592, i32 %167) #9
  %597 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %593, <4 x i32> %594) #9
  %598 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %595, <4 x i32> %596) #9
  %599 = shufflevector <8 x i16> %536, <8 x i16> %538, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %600 = shufflevector <8 x i16> %536, <8 x i16> %538, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %601 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %599, <8 x i16> %551) #9
  %602 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %600, <8 x i16> %551) #9
  %603 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %599, <8 x i16> %554) #9
  %604 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %600, <8 x i16> %554) #9
  %605 = add <4 x i32> %601, <i32 2048, i32 2048, i32 2048, i32 2048>
  %606 = add <4 x i32> %602, <i32 2048, i32 2048, i32 2048, i32 2048>
  %607 = add <4 x i32> %603, <i32 2048, i32 2048, i32 2048, i32 2048>
  %608 = add <4 x i32> %604, <i32 2048, i32 2048, i32 2048, i32 2048>
  %609 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %605, i32 %167) #9
  %610 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %606, i32 %167) #9
  %611 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %607, i32 %167) #9
  %612 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %608, i32 %167) #9
  %613 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %609, <4 x i32> %610) #9
  %614 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %611, <4 x i32> %612) #9
  %615 = bitcast <2 x i64>* %1 to <8 x i16>*
  store <8 x i16> %523, <8 x i16>* %615, align 16
  %616 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %531) #9
  %617 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 1
  %618 = bitcast <2 x i64>* %617 to <8 x i16>*
  store <8 x i16> %616, <8 x i16>* %618, align 16
  %619 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 2
  %620 = bitcast <2 x i64>* %619 to <8 x i16>*
  store <8 x i16> %535, <8 x i16>* %620, align 16
  %621 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %527) #9
  %622 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 3
  %623 = bitcast <2 x i64>* %622 to <8 x i16>*
  store <8 x i16> %621, <8 x i16>* %623, align 16
  %624 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 4
  %625 = bitcast <2 x i64>* %624 to <8 x i16>*
  store <8 x i16> %581, <8 x i16>* %625, align 16
  %626 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %613) #9
  %627 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 5
  %628 = bitcast <2 x i64>* %627 to <8 x i16>*
  store <8 x i16> %626, <8 x i16>* %628, align 16
  %629 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 6
  %630 = bitcast <2 x i64>* %629 to <8 x i16>*
  store <8 x i16> %597, <8 x i16>* %630, align 16
  %631 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %565) #9
  %632 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 7
  %633 = bitcast <2 x i64>* %632 to <8 x i16>*
  store <8 x i16> %631, <8 x i16>* %633, align 16
  %634 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 8
  %635 = bitcast <2 x i64>* %634 to <8 x i16>*
  store <8 x i16> %566, <8 x i16>* %635, align 16
  %636 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %598) #9
  %637 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 9
  %638 = bitcast <2 x i64>* %637 to <8 x i16>*
  store <8 x i16> %636, <8 x i16>* %638, align 16
  %639 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 10
  %640 = bitcast <2 x i64>* %639 to <8 x i16>*
  store <8 x i16> %614, <8 x i16>* %640, align 16
  %641 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %582) #9
  %642 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 11
  %643 = bitcast <2 x i64>* %642 to <8 x i16>*
  store <8 x i16> %641, <8 x i16>* %643, align 16
  %644 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 12
  %645 = bitcast <2 x i64>* %644 to <8 x i16>*
  store <8 x i16> %529, <8 x i16>* %645, align 16
  %646 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %537) #9
  %647 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 13
  %648 = bitcast <2 x i64>* %647 to <8 x i16>*
  store <8 x i16> %646, <8 x i16>* %648, align 16
  %649 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 14
  %650 = bitcast <2 x i64>* %649 to <8 x i16>*
  store <8 x i16> %533, <8 x i16>* %650, align 16
  %651 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %525) #9
  %652 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 15
  %653 = bitcast <2 x i64>* %652 to <8 x i16>*
  store <8 x i16> %651, <8 x i16>* %653, align 16
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal void @idct32_sse2(<2 x i64>* readonly, <2 x i64>*, i8 signext) #0 {
  %4 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 62), align 8
  %5 = and i32 %4, 65535
  %6 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 2), align 8
  %7 = shl i32 %6, 16
  %8 = sub i32 0, %7
  %9 = or i32 %5, %8
  %10 = insertelement <4 x i32> undef, i32 %9, i32 0
  %11 = shufflevector <4 x i32> %10, <4 x i32> undef, <4 x i32> zeroinitializer
  %12 = and i32 %6, 65535
  %13 = shl i32 %4, 16
  %14 = or i32 %12, %13
  %15 = insertelement <4 x i32> undef, i32 %14, i32 0
  %16 = shufflevector <4 x i32> %15, <4 x i32> undef, <4 x i32> zeroinitializer
  %17 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 30), align 8
  %18 = and i32 %17, 65535
  %19 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 34), align 8
  %20 = shl i32 %19, 16
  %21 = sub i32 0, %20
  %22 = or i32 %18, %21
  %23 = insertelement <4 x i32> undef, i32 %22, i32 0
  %24 = shufflevector <4 x i32> %23, <4 x i32> undef, <4 x i32> zeroinitializer
  %25 = and i32 %19, 65535
  %26 = shl i32 %17, 16
  %27 = or i32 %25, %26
  %28 = insertelement <4 x i32> undef, i32 %27, i32 0
  %29 = shufflevector <4 x i32> %28, <4 x i32> undef, <4 x i32> zeroinitializer
  %30 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 46), align 8
  %31 = and i32 %30, 65535
  %32 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 18), align 8
  %33 = shl i32 %32, 16
  %34 = sub i32 0, %33
  %35 = or i32 %31, %34
  %36 = insertelement <4 x i32> undef, i32 %35, i32 0
  %37 = shufflevector <4 x i32> %36, <4 x i32> undef, <4 x i32> zeroinitializer
  %38 = and i32 %32, 65535
  %39 = shl i32 %30, 16
  %40 = or i32 %38, %39
  %41 = insertelement <4 x i32> undef, i32 %40, i32 0
  %42 = shufflevector <4 x i32> %41, <4 x i32> undef, <4 x i32> zeroinitializer
  %43 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 14), align 8
  %44 = and i32 %43, 65535
  %45 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 50), align 8
  %46 = shl i32 %45, 16
  %47 = sub i32 0, %46
  %48 = or i32 %44, %47
  %49 = insertelement <4 x i32> undef, i32 %48, i32 0
  %50 = shufflevector <4 x i32> %49, <4 x i32> undef, <4 x i32> zeroinitializer
  %51 = and i32 %45, 65535
  %52 = shl i32 %43, 16
  %53 = or i32 %51, %52
  %54 = insertelement <4 x i32> undef, i32 %53, i32 0
  %55 = shufflevector <4 x i32> %54, <4 x i32> undef, <4 x i32> zeroinitializer
  %56 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 54), align 8
  %57 = and i32 %56, 65535
  %58 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 10), align 8
  %59 = shl i32 %58, 16
  %60 = sub i32 0, %59
  %61 = or i32 %57, %60
  %62 = insertelement <4 x i32> undef, i32 %61, i32 0
  %63 = shufflevector <4 x i32> %62, <4 x i32> undef, <4 x i32> zeroinitializer
  %64 = and i32 %58, 65535
  %65 = shl i32 %56, 16
  %66 = or i32 %64, %65
  %67 = insertelement <4 x i32> undef, i32 %66, i32 0
  %68 = shufflevector <4 x i32> %67, <4 x i32> undef, <4 x i32> zeroinitializer
  %69 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 22), align 8
  %70 = and i32 %69, 65535
  %71 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 42), align 8
  %72 = shl i32 %71, 16
  %73 = sub i32 0, %72
  %74 = or i32 %70, %73
  %75 = insertelement <4 x i32> undef, i32 %74, i32 0
  %76 = shufflevector <4 x i32> %75, <4 x i32> undef, <4 x i32> zeroinitializer
  %77 = and i32 %71, 65535
  %78 = shl i32 %69, 16
  %79 = or i32 %77, %78
  %80 = insertelement <4 x i32> undef, i32 %79, i32 0
  %81 = shufflevector <4 x i32> %80, <4 x i32> undef, <4 x i32> zeroinitializer
  %82 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 38), align 8
  %83 = and i32 %82, 65535
  %84 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 26), align 8
  %85 = shl i32 %84, 16
  %86 = sub i32 0, %85
  %87 = or i32 %83, %86
  %88 = insertelement <4 x i32> undef, i32 %87, i32 0
  %89 = shufflevector <4 x i32> %88, <4 x i32> undef, <4 x i32> zeroinitializer
  %90 = and i32 %84, 65535
  %91 = shl i32 %82, 16
  %92 = or i32 %90, %91
  %93 = insertelement <4 x i32> undef, i32 %92, i32 0
  %94 = shufflevector <4 x i32> %93, <4 x i32> undef, <4 x i32> zeroinitializer
  %95 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 6), align 8
  %96 = and i32 %95, 65535
  %97 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 58), align 8
  %98 = shl i32 %97, 16
  %99 = sub i32 0, %98
  %100 = or i32 %96, %99
  %101 = insertelement <4 x i32> undef, i32 %100, i32 0
  %102 = shufflevector <4 x i32> %101, <4 x i32> undef, <4 x i32> zeroinitializer
  %103 = and i32 %97, 65535
  %104 = shl i32 %95, 16
  %105 = or i32 %103, %104
  %106 = insertelement <4 x i32> undef, i32 %105, i32 0
  %107 = shufflevector <4 x i32> %106, <4 x i32> undef, <4 x i32> zeroinitializer
  %108 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 60), align 16
  %109 = and i32 %108, 65535
  %110 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 4), align 16
  %111 = shl i32 %110, 16
  %112 = sub i32 0, %111
  %113 = or i32 %109, %112
  %114 = insertelement <4 x i32> undef, i32 %113, i32 0
  %115 = shufflevector <4 x i32> %114, <4 x i32> undef, <4 x i32> zeroinitializer
  %116 = and i32 %110, 65535
  %117 = shl i32 %108, 16
  %118 = or i32 %116, %117
  %119 = insertelement <4 x i32> undef, i32 %118, i32 0
  %120 = shufflevector <4 x i32> %119, <4 x i32> undef, <4 x i32> zeroinitializer
  %121 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 28), align 16
  %122 = and i32 %121, 65535
  %123 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 36), align 16
  %124 = shl i32 %123, 16
  %125 = sub i32 0, %124
  %126 = or i32 %122, %125
  %127 = insertelement <4 x i32> undef, i32 %126, i32 0
  %128 = shufflevector <4 x i32> %127, <4 x i32> undef, <4 x i32> zeroinitializer
  %129 = and i32 %123, 65535
  %130 = shl i32 %121, 16
  %131 = or i32 %129, %130
  %132 = insertelement <4 x i32> undef, i32 %131, i32 0
  %133 = shufflevector <4 x i32> %132, <4 x i32> undef, <4 x i32> zeroinitializer
  %134 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 44), align 16
  %135 = and i32 %134, 65535
  %136 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 20), align 16
  %137 = shl i32 %136, 16
  %138 = sub i32 0, %137
  %139 = or i32 %135, %138
  %140 = insertelement <4 x i32> undef, i32 %139, i32 0
  %141 = shufflevector <4 x i32> %140, <4 x i32> undef, <4 x i32> zeroinitializer
  %142 = and i32 %136, 65535
  %143 = shl i32 %134, 16
  %144 = or i32 %142, %143
  %145 = insertelement <4 x i32> undef, i32 %144, i32 0
  %146 = shufflevector <4 x i32> %145, <4 x i32> undef, <4 x i32> zeroinitializer
  %147 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 12), align 16
  %148 = and i32 %147, 65535
  %149 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 52), align 16
  %150 = shl i32 %149, 16
  %151 = sub i32 0, %150
  %152 = or i32 %148, %151
  %153 = insertelement <4 x i32> undef, i32 %152, i32 0
  %154 = shufflevector <4 x i32> %153, <4 x i32> undef, <4 x i32> zeroinitializer
  %155 = and i32 %149, 65535
  %156 = shl i32 %147, 16
  %157 = or i32 %155, %156
  %158 = insertelement <4 x i32> undef, i32 %157, i32 0
  %159 = shufflevector <4 x i32> %158, <4 x i32> undef, <4 x i32> zeroinitializer
  %160 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 56), align 16
  %161 = and i32 %160, 65535
  %162 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 8), align 16
  %163 = shl i32 %162, 16
  %164 = sub i32 0, %163
  %165 = or i32 %161, %164
  %166 = insertelement <4 x i32> undef, i32 %165, i32 0
  %167 = shufflevector <4 x i32> %166, <4 x i32> undef, <4 x i32> zeroinitializer
  %168 = and i32 %162, 65535
  %169 = shl i32 %160, 16
  %170 = or i32 %168, %169
  %171 = insertelement <4 x i32> undef, i32 %170, i32 0
  %172 = shufflevector <4 x i32> %171, <4 x i32> undef, <4 x i32> zeroinitializer
  %173 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 24), align 16
  %174 = and i32 %173, 65535
  %175 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 40), align 16
  %176 = shl i32 %175, 16
  %177 = sub i32 0, %176
  %178 = or i32 %174, %177
  %179 = insertelement <4 x i32> undef, i32 %178, i32 0
  %180 = shufflevector <4 x i32> %179, <4 x i32> undef, <4 x i32> zeroinitializer
  %181 = and i32 %175, 65535
  %182 = shl i32 %173, 16
  %183 = or i32 %181, %182
  %184 = insertelement <4 x i32> undef, i32 %183, i32 0
  %185 = shufflevector <4 x i32> %184, <4 x i32> undef, <4 x i32> zeroinitializer
  %186 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 32), align 16
  %187 = and i32 %186, 65535
  %188 = shl i32 %186, 16
  %189 = or i32 %187, %188
  %190 = insertelement <4 x i32> undef, i32 %189, i32 0
  %191 = shufflevector <4 x i32> %190, <4 x i32> undef, <4 x i32> zeroinitializer
  %192 = sub i32 0, %188
  %193 = or i32 %187, %192
  %194 = insertelement <4 x i32> undef, i32 %193, i32 0
  %195 = shufflevector <4 x i32> %194, <4 x i32> undef, <4 x i32> zeroinitializer
  %196 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 48), align 16
  %197 = and i32 %196, 65535
  %198 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 16), align 16
  %199 = shl i32 %198, 16
  %200 = sub i32 0, %199
  %201 = or i32 %197, %200
  %202 = insertelement <4 x i32> undef, i32 %201, i32 0
  %203 = shufflevector <4 x i32> %202, <4 x i32> undef, <4 x i32> zeroinitializer
  %204 = and i32 %198, 65535
  %205 = shl i32 %196, 16
  %206 = or i32 %204, %205
  %207 = insertelement <4 x i32> undef, i32 %206, i32 0
  %208 = shufflevector <4 x i32> %207, <4 x i32> undef, <4 x i32> zeroinitializer
  %209 = bitcast <2 x i64>* %0 to <8 x i16>*
  %210 = load <8 x i16>, <8 x i16>* %209, align 16
  %211 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 16
  %212 = bitcast <2 x i64>* %211 to <8 x i16>*
  %213 = load <8 x i16>, <8 x i16>* %212, align 16
  %214 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 8
  %215 = bitcast <2 x i64>* %214 to <8 x i16>*
  %216 = load <8 x i16>, <8 x i16>* %215, align 16
  %217 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 24
  %218 = bitcast <2 x i64>* %217 to <8 x i16>*
  %219 = load <8 x i16>, <8 x i16>* %218, align 16
  %220 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 4
  %221 = bitcast <2 x i64>* %220 to <8 x i16>*
  %222 = load <8 x i16>, <8 x i16>* %221, align 16
  %223 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 20
  %224 = bitcast <2 x i64>* %223 to <8 x i16>*
  %225 = load <8 x i16>, <8 x i16>* %224, align 16
  %226 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 12
  %227 = bitcast <2 x i64>* %226 to <8 x i16>*
  %228 = load <8 x i16>, <8 x i16>* %227, align 16
  %229 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 28
  %230 = bitcast <2 x i64>* %229 to <8 x i16>*
  %231 = load <8 x i16>, <8 x i16>* %230, align 16
  %232 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 2
  %233 = bitcast <2 x i64>* %232 to <8 x i16>*
  %234 = load <8 x i16>, <8 x i16>* %233, align 16
  %235 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 18
  %236 = bitcast <2 x i64>* %235 to <8 x i16>*
  %237 = load <8 x i16>, <8 x i16>* %236, align 16
  %238 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 10
  %239 = bitcast <2 x i64>* %238 to <8 x i16>*
  %240 = load <8 x i16>, <8 x i16>* %239, align 16
  %241 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 26
  %242 = bitcast <2 x i64>* %241 to <8 x i16>*
  %243 = load <8 x i16>, <8 x i16>* %242, align 16
  %244 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 6
  %245 = bitcast <2 x i64>* %244 to <8 x i16>*
  %246 = load <8 x i16>, <8 x i16>* %245, align 16
  %247 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 22
  %248 = bitcast <2 x i64>* %247 to <8 x i16>*
  %249 = load <8 x i16>, <8 x i16>* %248, align 16
  %250 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 14
  %251 = bitcast <2 x i64>* %250 to <8 x i16>*
  %252 = load <8 x i16>, <8 x i16>* %251, align 16
  %253 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 30
  %254 = bitcast <2 x i64>* %253 to <8 x i16>*
  %255 = load <8 x i16>, <8 x i16>* %254, align 16
  %256 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 1
  %257 = bitcast <2 x i64>* %256 to <8 x i16>*
  %258 = load <8 x i16>, <8 x i16>* %257, align 16
  %259 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 17
  %260 = bitcast <2 x i64>* %259 to <8 x i16>*
  %261 = load <8 x i16>, <8 x i16>* %260, align 16
  %262 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 9
  %263 = bitcast <2 x i64>* %262 to <8 x i16>*
  %264 = load <8 x i16>, <8 x i16>* %263, align 16
  %265 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 25
  %266 = bitcast <2 x i64>* %265 to <8 x i16>*
  %267 = load <8 x i16>, <8 x i16>* %266, align 16
  %268 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 5
  %269 = bitcast <2 x i64>* %268 to <8 x i16>*
  %270 = load <8 x i16>, <8 x i16>* %269, align 16
  %271 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 21
  %272 = bitcast <2 x i64>* %271 to <8 x i16>*
  %273 = load <8 x i16>, <8 x i16>* %272, align 16
  %274 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 13
  %275 = bitcast <2 x i64>* %274 to <8 x i16>*
  %276 = load <8 x i16>, <8 x i16>* %275, align 16
  %277 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 29
  %278 = bitcast <2 x i64>* %277 to <8 x i16>*
  %279 = load <8 x i16>, <8 x i16>* %278, align 16
  %280 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 3
  %281 = bitcast <2 x i64>* %280 to <8 x i16>*
  %282 = load <8 x i16>, <8 x i16>* %281, align 16
  %283 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 19
  %284 = bitcast <2 x i64>* %283 to <8 x i16>*
  %285 = load <8 x i16>, <8 x i16>* %284, align 16
  %286 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 11
  %287 = bitcast <2 x i64>* %286 to <8 x i16>*
  %288 = load <8 x i16>, <8 x i16>* %287, align 16
  %289 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 27
  %290 = bitcast <2 x i64>* %289 to <8 x i16>*
  %291 = load <8 x i16>, <8 x i16>* %290, align 16
  %292 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 7
  %293 = bitcast <2 x i64>* %292 to <8 x i16>*
  %294 = load <8 x i16>, <8 x i16>* %293, align 16
  %295 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 23
  %296 = bitcast <2 x i64>* %295 to <8 x i16>*
  %297 = load <8 x i16>, <8 x i16>* %296, align 16
  %298 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 15
  %299 = bitcast <2 x i64>* %298 to <8 x i16>*
  %300 = load <8 x i16>, <8 x i16>* %299, align 16
  %301 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 31
  %302 = bitcast <2 x i64>* %301 to <8 x i16>*
  %303 = load <8 x i16>, <8 x i16>* %302, align 16
  %304 = shufflevector <8 x i16> %258, <8 x i16> %303, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %305 = shufflevector <8 x i16> %258, <8 x i16> %303, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %306 = bitcast <4 x i32> %11 to <8 x i16>
  %307 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %304, <8 x i16> %306) #9
  %308 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %305, <8 x i16> %306) #9
  %309 = bitcast <4 x i32> %16 to <8 x i16>
  %310 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %304, <8 x i16> %309) #9
  %311 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %305, <8 x i16> %309) #9
  %312 = add <4 x i32> %307, <i32 2048, i32 2048, i32 2048, i32 2048>
  %313 = add <4 x i32> %308, <i32 2048, i32 2048, i32 2048, i32 2048>
  %314 = add <4 x i32> %310, <i32 2048, i32 2048, i32 2048, i32 2048>
  %315 = add <4 x i32> %311, <i32 2048, i32 2048, i32 2048, i32 2048>
  %316 = sext i8 %2 to i32
  %317 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %312, i32 %316) #9
  %318 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %313, i32 %316) #9
  %319 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %314, i32 %316) #9
  %320 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %315, i32 %316) #9
  %321 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %317, <4 x i32> %318) #9
  %322 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %319, <4 x i32> %320) #9
  %323 = shufflevector <8 x i16> %261, <8 x i16> %300, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %324 = shufflevector <8 x i16> %261, <8 x i16> %300, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %325 = bitcast <4 x i32> %24 to <8 x i16>
  %326 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %323, <8 x i16> %325) #9
  %327 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %324, <8 x i16> %325) #9
  %328 = bitcast <4 x i32> %29 to <8 x i16>
  %329 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %323, <8 x i16> %328) #9
  %330 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %324, <8 x i16> %328) #9
  %331 = add <4 x i32> %326, <i32 2048, i32 2048, i32 2048, i32 2048>
  %332 = add <4 x i32> %327, <i32 2048, i32 2048, i32 2048, i32 2048>
  %333 = add <4 x i32> %329, <i32 2048, i32 2048, i32 2048, i32 2048>
  %334 = add <4 x i32> %330, <i32 2048, i32 2048, i32 2048, i32 2048>
  %335 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %331, i32 %316) #9
  %336 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %332, i32 %316) #9
  %337 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %333, i32 %316) #9
  %338 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %334, i32 %316) #9
  %339 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %335, <4 x i32> %336) #9
  %340 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %337, <4 x i32> %338) #9
  %341 = shufflevector <8 x i16> %264, <8 x i16> %297, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %342 = shufflevector <8 x i16> %264, <8 x i16> %297, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %343 = bitcast <4 x i32> %37 to <8 x i16>
  %344 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %341, <8 x i16> %343) #9
  %345 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %342, <8 x i16> %343) #9
  %346 = bitcast <4 x i32> %42 to <8 x i16>
  %347 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %341, <8 x i16> %346) #9
  %348 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %342, <8 x i16> %346) #9
  %349 = add <4 x i32> %344, <i32 2048, i32 2048, i32 2048, i32 2048>
  %350 = add <4 x i32> %345, <i32 2048, i32 2048, i32 2048, i32 2048>
  %351 = add <4 x i32> %347, <i32 2048, i32 2048, i32 2048, i32 2048>
  %352 = add <4 x i32> %348, <i32 2048, i32 2048, i32 2048, i32 2048>
  %353 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %349, i32 %316) #9
  %354 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %350, i32 %316) #9
  %355 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %351, i32 %316) #9
  %356 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %352, i32 %316) #9
  %357 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %353, <4 x i32> %354) #9
  %358 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %355, <4 x i32> %356) #9
  %359 = shufflevector <8 x i16> %267, <8 x i16> %294, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %360 = shufflevector <8 x i16> %267, <8 x i16> %294, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %361 = bitcast <4 x i32> %50 to <8 x i16>
  %362 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %359, <8 x i16> %361) #9
  %363 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %360, <8 x i16> %361) #9
  %364 = bitcast <4 x i32> %55 to <8 x i16>
  %365 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %359, <8 x i16> %364) #9
  %366 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %360, <8 x i16> %364) #9
  %367 = add <4 x i32> %362, <i32 2048, i32 2048, i32 2048, i32 2048>
  %368 = add <4 x i32> %363, <i32 2048, i32 2048, i32 2048, i32 2048>
  %369 = add <4 x i32> %365, <i32 2048, i32 2048, i32 2048, i32 2048>
  %370 = add <4 x i32> %366, <i32 2048, i32 2048, i32 2048, i32 2048>
  %371 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %367, i32 %316) #9
  %372 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %368, i32 %316) #9
  %373 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %369, i32 %316) #9
  %374 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %370, i32 %316) #9
  %375 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %371, <4 x i32> %372) #9
  %376 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %373, <4 x i32> %374) #9
  %377 = shufflevector <8 x i16> %270, <8 x i16> %291, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %378 = shufflevector <8 x i16> %270, <8 x i16> %291, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %379 = bitcast <4 x i32> %63 to <8 x i16>
  %380 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %377, <8 x i16> %379) #9
  %381 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %378, <8 x i16> %379) #9
  %382 = bitcast <4 x i32> %68 to <8 x i16>
  %383 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %377, <8 x i16> %382) #9
  %384 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %378, <8 x i16> %382) #9
  %385 = add <4 x i32> %380, <i32 2048, i32 2048, i32 2048, i32 2048>
  %386 = add <4 x i32> %381, <i32 2048, i32 2048, i32 2048, i32 2048>
  %387 = add <4 x i32> %383, <i32 2048, i32 2048, i32 2048, i32 2048>
  %388 = add <4 x i32> %384, <i32 2048, i32 2048, i32 2048, i32 2048>
  %389 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %385, i32 %316) #9
  %390 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %386, i32 %316) #9
  %391 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %387, i32 %316) #9
  %392 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %388, i32 %316) #9
  %393 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %389, <4 x i32> %390) #9
  %394 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %391, <4 x i32> %392) #9
  %395 = shufflevector <8 x i16> %273, <8 x i16> %288, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %396 = shufflevector <8 x i16> %273, <8 x i16> %288, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %397 = bitcast <4 x i32> %76 to <8 x i16>
  %398 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %395, <8 x i16> %397) #9
  %399 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %396, <8 x i16> %397) #9
  %400 = bitcast <4 x i32> %81 to <8 x i16>
  %401 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %395, <8 x i16> %400) #9
  %402 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %396, <8 x i16> %400) #9
  %403 = add <4 x i32> %398, <i32 2048, i32 2048, i32 2048, i32 2048>
  %404 = add <4 x i32> %399, <i32 2048, i32 2048, i32 2048, i32 2048>
  %405 = add <4 x i32> %401, <i32 2048, i32 2048, i32 2048, i32 2048>
  %406 = add <4 x i32> %402, <i32 2048, i32 2048, i32 2048, i32 2048>
  %407 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %403, i32 %316) #9
  %408 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %404, i32 %316) #9
  %409 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %405, i32 %316) #9
  %410 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %406, i32 %316) #9
  %411 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %407, <4 x i32> %408) #9
  %412 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %409, <4 x i32> %410) #9
  %413 = shufflevector <8 x i16> %276, <8 x i16> %285, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %414 = shufflevector <8 x i16> %276, <8 x i16> %285, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %415 = bitcast <4 x i32> %89 to <8 x i16>
  %416 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %413, <8 x i16> %415) #9
  %417 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %414, <8 x i16> %415) #9
  %418 = bitcast <4 x i32> %94 to <8 x i16>
  %419 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %413, <8 x i16> %418) #9
  %420 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %414, <8 x i16> %418) #9
  %421 = add <4 x i32> %416, <i32 2048, i32 2048, i32 2048, i32 2048>
  %422 = add <4 x i32> %417, <i32 2048, i32 2048, i32 2048, i32 2048>
  %423 = add <4 x i32> %419, <i32 2048, i32 2048, i32 2048, i32 2048>
  %424 = add <4 x i32> %420, <i32 2048, i32 2048, i32 2048, i32 2048>
  %425 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %421, i32 %316) #9
  %426 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %422, i32 %316) #9
  %427 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %423, i32 %316) #9
  %428 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %424, i32 %316) #9
  %429 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %425, <4 x i32> %426) #9
  %430 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %427, <4 x i32> %428) #9
  %431 = shufflevector <8 x i16> %279, <8 x i16> %282, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %432 = shufflevector <8 x i16> %279, <8 x i16> %282, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %433 = bitcast <4 x i32> %102 to <8 x i16>
  %434 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %431, <8 x i16> %433) #9
  %435 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %432, <8 x i16> %433) #9
  %436 = bitcast <4 x i32> %107 to <8 x i16>
  %437 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %431, <8 x i16> %436) #9
  %438 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %432, <8 x i16> %436) #9
  %439 = add <4 x i32> %434, <i32 2048, i32 2048, i32 2048, i32 2048>
  %440 = add <4 x i32> %435, <i32 2048, i32 2048, i32 2048, i32 2048>
  %441 = add <4 x i32> %437, <i32 2048, i32 2048, i32 2048, i32 2048>
  %442 = add <4 x i32> %438, <i32 2048, i32 2048, i32 2048, i32 2048>
  %443 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %439, i32 %316) #9
  %444 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %440, i32 %316) #9
  %445 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %441, i32 %316) #9
  %446 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %442, i32 %316) #9
  %447 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %443, <4 x i32> %444) #9
  %448 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %445, <4 x i32> %446) #9
  %449 = shufflevector <8 x i16> %234, <8 x i16> %255, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %450 = shufflevector <8 x i16> %234, <8 x i16> %255, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %451 = bitcast <4 x i32> %115 to <8 x i16>
  %452 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %449, <8 x i16> %451) #9
  %453 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %450, <8 x i16> %451) #9
  %454 = bitcast <4 x i32> %120 to <8 x i16>
  %455 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %449, <8 x i16> %454) #9
  %456 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %450, <8 x i16> %454) #9
  %457 = add <4 x i32> %452, <i32 2048, i32 2048, i32 2048, i32 2048>
  %458 = add <4 x i32> %453, <i32 2048, i32 2048, i32 2048, i32 2048>
  %459 = add <4 x i32> %455, <i32 2048, i32 2048, i32 2048, i32 2048>
  %460 = add <4 x i32> %456, <i32 2048, i32 2048, i32 2048, i32 2048>
  %461 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %457, i32 %316) #9
  %462 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %458, i32 %316) #9
  %463 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %459, i32 %316) #9
  %464 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %460, i32 %316) #9
  %465 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %461, <4 x i32> %462) #9
  %466 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %463, <4 x i32> %464) #9
  %467 = shufflevector <8 x i16> %237, <8 x i16> %252, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %468 = shufflevector <8 x i16> %237, <8 x i16> %252, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %469 = bitcast <4 x i32> %128 to <8 x i16>
  %470 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %467, <8 x i16> %469) #9
  %471 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %468, <8 x i16> %469) #9
  %472 = bitcast <4 x i32> %133 to <8 x i16>
  %473 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %467, <8 x i16> %472) #9
  %474 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %468, <8 x i16> %472) #9
  %475 = add <4 x i32> %470, <i32 2048, i32 2048, i32 2048, i32 2048>
  %476 = add <4 x i32> %471, <i32 2048, i32 2048, i32 2048, i32 2048>
  %477 = add <4 x i32> %473, <i32 2048, i32 2048, i32 2048, i32 2048>
  %478 = add <4 x i32> %474, <i32 2048, i32 2048, i32 2048, i32 2048>
  %479 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %475, i32 %316) #9
  %480 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %476, i32 %316) #9
  %481 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %477, i32 %316) #9
  %482 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %478, i32 %316) #9
  %483 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %479, <4 x i32> %480) #9
  %484 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %481, <4 x i32> %482) #9
  %485 = shufflevector <8 x i16> %240, <8 x i16> %249, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %486 = shufflevector <8 x i16> %240, <8 x i16> %249, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %487 = bitcast <4 x i32> %141 to <8 x i16>
  %488 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %485, <8 x i16> %487) #9
  %489 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %486, <8 x i16> %487) #9
  %490 = bitcast <4 x i32> %146 to <8 x i16>
  %491 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %485, <8 x i16> %490) #9
  %492 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %486, <8 x i16> %490) #9
  %493 = add <4 x i32> %488, <i32 2048, i32 2048, i32 2048, i32 2048>
  %494 = add <4 x i32> %489, <i32 2048, i32 2048, i32 2048, i32 2048>
  %495 = add <4 x i32> %491, <i32 2048, i32 2048, i32 2048, i32 2048>
  %496 = add <4 x i32> %492, <i32 2048, i32 2048, i32 2048, i32 2048>
  %497 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %493, i32 %316) #9
  %498 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %494, i32 %316) #9
  %499 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %495, i32 %316) #9
  %500 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %496, i32 %316) #9
  %501 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %497, <4 x i32> %498) #9
  %502 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %499, <4 x i32> %500) #9
  %503 = shufflevector <8 x i16> %243, <8 x i16> %246, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %504 = shufflevector <8 x i16> %243, <8 x i16> %246, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %505 = bitcast <4 x i32> %154 to <8 x i16>
  %506 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %503, <8 x i16> %505) #9
  %507 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %504, <8 x i16> %505) #9
  %508 = bitcast <4 x i32> %159 to <8 x i16>
  %509 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %503, <8 x i16> %508) #9
  %510 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %504, <8 x i16> %508) #9
  %511 = add <4 x i32> %506, <i32 2048, i32 2048, i32 2048, i32 2048>
  %512 = add <4 x i32> %507, <i32 2048, i32 2048, i32 2048, i32 2048>
  %513 = add <4 x i32> %509, <i32 2048, i32 2048, i32 2048, i32 2048>
  %514 = add <4 x i32> %510, <i32 2048, i32 2048, i32 2048, i32 2048>
  %515 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %511, i32 %316) #9
  %516 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %512, i32 %316) #9
  %517 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %513, i32 %316) #9
  %518 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %514, i32 %316) #9
  %519 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %515, <4 x i32> %516) #9
  %520 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %517, <4 x i32> %518) #9
  %521 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %321, <8 x i16> %339) #9
  %522 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %321, <8 x i16> %339) #9
  %523 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %375, <8 x i16> %357) #9
  %524 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %375, <8 x i16> %357) #9
  %525 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %393, <8 x i16> %411) #9
  %526 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %393, <8 x i16> %411) #9
  %527 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %447, <8 x i16> %429) #9
  %528 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %447, <8 x i16> %429) #9
  %529 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %448, <8 x i16> %430) #9
  %530 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %448, <8 x i16> %430) #9
  %531 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %394, <8 x i16> %412) #9
  %532 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %394, <8 x i16> %412) #9
  %533 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %376, <8 x i16> %358) #9
  %534 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %376, <8 x i16> %358) #9
  %535 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %322, <8 x i16> %340) #9
  %536 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %322, <8 x i16> %340) #9
  %537 = shufflevector <8 x i16> %222, <8 x i16> %231, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %538 = shufflevector <8 x i16> %222, <8 x i16> %231, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %539 = bitcast <4 x i32> %167 to <8 x i16>
  %540 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %537, <8 x i16> %539) #9
  %541 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %538, <8 x i16> %539) #9
  %542 = bitcast <4 x i32> %172 to <8 x i16>
  %543 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %537, <8 x i16> %542) #9
  %544 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %538, <8 x i16> %542) #9
  %545 = add <4 x i32> %540, <i32 2048, i32 2048, i32 2048, i32 2048>
  %546 = add <4 x i32> %541, <i32 2048, i32 2048, i32 2048, i32 2048>
  %547 = add <4 x i32> %543, <i32 2048, i32 2048, i32 2048, i32 2048>
  %548 = add <4 x i32> %544, <i32 2048, i32 2048, i32 2048, i32 2048>
  %549 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %545, i32 %316) #9
  %550 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %546, i32 %316) #9
  %551 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %547, i32 %316) #9
  %552 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %548, i32 %316) #9
  %553 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %549, <4 x i32> %550) #9
  %554 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %551, <4 x i32> %552) #9
  %555 = shufflevector <8 x i16> %225, <8 x i16> %228, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %556 = shufflevector <8 x i16> %225, <8 x i16> %228, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %557 = bitcast <4 x i32> %180 to <8 x i16>
  %558 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %555, <8 x i16> %557) #9
  %559 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %556, <8 x i16> %557) #9
  %560 = bitcast <4 x i32> %185 to <8 x i16>
  %561 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %555, <8 x i16> %560) #9
  %562 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %556, <8 x i16> %560) #9
  %563 = add <4 x i32> %558, <i32 2048, i32 2048, i32 2048, i32 2048>
  %564 = add <4 x i32> %559, <i32 2048, i32 2048, i32 2048, i32 2048>
  %565 = add <4 x i32> %561, <i32 2048, i32 2048, i32 2048, i32 2048>
  %566 = add <4 x i32> %562, <i32 2048, i32 2048, i32 2048, i32 2048>
  %567 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %563, i32 %316) #9
  %568 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %564, i32 %316) #9
  %569 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %565, i32 %316) #9
  %570 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %566, i32 %316) #9
  %571 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %567, <4 x i32> %568) #9
  %572 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %569, <4 x i32> %570) #9
  %573 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %465, <8 x i16> %483) #9
  %574 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %465, <8 x i16> %483) #9
  %575 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %519, <8 x i16> %501) #9
  %576 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %519, <8 x i16> %501) #9
  %577 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %520, <8 x i16> %502) #9
  %578 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %520, <8 x i16> %502) #9
  %579 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %466, <8 x i16> %484) #9
  %580 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %466, <8 x i16> %484) #9
  %581 = sub i32 0, %162
  %582 = and i32 %581, 65535
  %583 = or i32 %582, %169
  %584 = insertelement <4 x i32> undef, i32 %583, i32 0
  %585 = shufflevector <4 x i32> %584, <4 x i32> undef, <4 x i32> zeroinitializer
  %586 = or i32 %163, %161
  %587 = insertelement <4 x i32> undef, i32 %586, i32 0
  %588 = shufflevector <4 x i32> %587, <4 x i32> undef, <4 x i32> zeroinitializer
  %589 = sub i32 0, %160
  %590 = and i32 %589, 65535
  %591 = or i32 %590, %164
  %592 = insertelement <4 x i32> undef, i32 %591, i32 0
  %593 = shufflevector <4 x i32> %592, <4 x i32> undef, <4 x i32> zeroinitializer
  %594 = sub i32 0, %175
  %595 = and i32 %594, 65535
  %596 = or i32 %595, %182
  %597 = insertelement <4 x i32> undef, i32 %596, i32 0
  %598 = shufflevector <4 x i32> %597, <4 x i32> undef, <4 x i32> zeroinitializer
  %599 = or i32 %176, %174
  %600 = insertelement <4 x i32> undef, i32 %599, i32 0
  %601 = shufflevector <4 x i32> %600, <4 x i32> undef, <4 x i32> zeroinitializer
  %602 = sub i32 0, %173
  %603 = and i32 %602, 65535
  %604 = or i32 %603, %177
  %605 = insertelement <4 x i32> undef, i32 %604, i32 0
  %606 = shufflevector <4 x i32> %605, <4 x i32> undef, <4 x i32> zeroinitializer
  %607 = shufflevector <8 x i16> %522, <8 x i16> %535, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %608 = shufflevector <8 x i16> %522, <8 x i16> %535, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %609 = bitcast <4 x i32> %585 to <8 x i16>
  %610 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %607, <8 x i16> %609) #9
  %611 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %608, <8 x i16> %609) #9
  %612 = bitcast <4 x i32> %588 to <8 x i16>
  %613 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %607, <8 x i16> %612) #9
  %614 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %608, <8 x i16> %612) #9
  %615 = add <4 x i32> %610, <i32 2048, i32 2048, i32 2048, i32 2048>
  %616 = add <4 x i32> %611, <i32 2048, i32 2048, i32 2048, i32 2048>
  %617 = add <4 x i32> %613, <i32 2048, i32 2048, i32 2048, i32 2048>
  %618 = add <4 x i32> %614, <i32 2048, i32 2048, i32 2048, i32 2048>
  %619 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %615, i32 %316) #9
  %620 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %616, i32 %316) #9
  %621 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %617, i32 %316) #9
  %622 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %618, i32 %316) #9
  %623 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %619, <4 x i32> %620) #9
  %624 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %621, <4 x i32> %622) #9
  %625 = shufflevector <8 x i16> %523, <8 x i16> %534, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %626 = shufflevector <8 x i16> %523, <8 x i16> %534, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %627 = bitcast <4 x i32> %593 to <8 x i16>
  %628 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %625, <8 x i16> %627) #9
  %629 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %626, <8 x i16> %627) #9
  %630 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %625, <8 x i16> %609) #9
  %631 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %626, <8 x i16> %609) #9
  %632 = add <4 x i32> %628, <i32 2048, i32 2048, i32 2048, i32 2048>
  %633 = add <4 x i32> %629, <i32 2048, i32 2048, i32 2048, i32 2048>
  %634 = add <4 x i32> %630, <i32 2048, i32 2048, i32 2048, i32 2048>
  %635 = add <4 x i32> %631, <i32 2048, i32 2048, i32 2048, i32 2048>
  %636 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %632, i32 %316) #9
  %637 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %633, i32 %316) #9
  %638 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %634, i32 %316) #9
  %639 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %635, i32 %316) #9
  %640 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %636, <4 x i32> %637) #9
  %641 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %638, <4 x i32> %639) #9
  %642 = shufflevector <8 x i16> %526, <8 x i16> %531, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %643 = shufflevector <8 x i16> %526, <8 x i16> %531, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %644 = bitcast <4 x i32> %598 to <8 x i16>
  %645 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %642, <8 x i16> %644) #9
  %646 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %643, <8 x i16> %644) #9
  %647 = bitcast <4 x i32> %601 to <8 x i16>
  %648 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %642, <8 x i16> %647) #9
  %649 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %643, <8 x i16> %647) #9
  %650 = add <4 x i32> %645, <i32 2048, i32 2048, i32 2048, i32 2048>
  %651 = add <4 x i32> %646, <i32 2048, i32 2048, i32 2048, i32 2048>
  %652 = add <4 x i32> %648, <i32 2048, i32 2048, i32 2048, i32 2048>
  %653 = add <4 x i32> %649, <i32 2048, i32 2048, i32 2048, i32 2048>
  %654 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %650, i32 %316) #9
  %655 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %651, i32 %316) #9
  %656 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %652, i32 %316) #9
  %657 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %653, i32 %316) #9
  %658 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %654, <4 x i32> %655) #9
  %659 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %656, <4 x i32> %657) #9
  %660 = shufflevector <8 x i16> %527, <8 x i16> %530, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %661 = shufflevector <8 x i16> %527, <8 x i16> %530, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %662 = bitcast <4 x i32> %606 to <8 x i16>
  %663 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %660, <8 x i16> %662) #9
  %664 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %661, <8 x i16> %662) #9
  %665 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %660, <8 x i16> %644) #9
  %666 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %661, <8 x i16> %644) #9
  %667 = add <4 x i32> %663, <i32 2048, i32 2048, i32 2048, i32 2048>
  %668 = add <4 x i32> %664, <i32 2048, i32 2048, i32 2048, i32 2048>
  %669 = add <4 x i32> %665, <i32 2048, i32 2048, i32 2048, i32 2048>
  %670 = add <4 x i32> %666, <i32 2048, i32 2048, i32 2048, i32 2048>
  %671 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %667, i32 %316) #9
  %672 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %668, i32 %316) #9
  %673 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %669, i32 %316) #9
  %674 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %670, i32 %316) #9
  %675 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %671, <4 x i32> %672) #9
  %676 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %673, <4 x i32> %674) #9
  %677 = shufflevector <8 x i16> %210, <8 x i16> %213, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %678 = shufflevector <8 x i16> %210, <8 x i16> %213, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %679 = bitcast <4 x i32> %191 to <8 x i16>
  %680 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %677, <8 x i16> %679) #9
  %681 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %678, <8 x i16> %679) #9
  %682 = bitcast <4 x i32> %195 to <8 x i16>
  %683 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %677, <8 x i16> %682) #9
  %684 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %678, <8 x i16> %682) #9
  %685 = add <4 x i32> %680, <i32 2048, i32 2048, i32 2048, i32 2048>
  %686 = add <4 x i32> %681, <i32 2048, i32 2048, i32 2048, i32 2048>
  %687 = add <4 x i32> %683, <i32 2048, i32 2048, i32 2048, i32 2048>
  %688 = add <4 x i32> %684, <i32 2048, i32 2048, i32 2048, i32 2048>
  %689 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %685, i32 %316) #9
  %690 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %686, i32 %316) #9
  %691 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %687, i32 %316) #9
  %692 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %688, i32 %316) #9
  %693 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %689, <4 x i32> %690) #9
  %694 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %691, <4 x i32> %692) #9
  %695 = shufflevector <8 x i16> %216, <8 x i16> %219, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %696 = shufflevector <8 x i16> %216, <8 x i16> %219, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %697 = bitcast <4 x i32> %203 to <8 x i16>
  %698 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %695, <8 x i16> %697) #9
  %699 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %696, <8 x i16> %697) #9
  %700 = bitcast <4 x i32> %208 to <8 x i16>
  %701 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %695, <8 x i16> %700) #9
  %702 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %696, <8 x i16> %700) #9
  %703 = add <4 x i32> %698, <i32 2048, i32 2048, i32 2048, i32 2048>
  %704 = add <4 x i32> %699, <i32 2048, i32 2048, i32 2048, i32 2048>
  %705 = add <4 x i32> %701, <i32 2048, i32 2048, i32 2048, i32 2048>
  %706 = add <4 x i32> %702, <i32 2048, i32 2048, i32 2048, i32 2048>
  %707 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %703, i32 %316) #9
  %708 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %704, i32 %316) #9
  %709 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %705, i32 %316) #9
  %710 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %706, i32 %316) #9
  %711 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %707, <4 x i32> %708) #9
  %712 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %709, <4 x i32> %710) #9
  %713 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %553, <8 x i16> %571) #9
  %714 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %553, <8 x i16> %571) #9
  %715 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %554, <8 x i16> %572) #9
  %716 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %554, <8 x i16> %572) #9
  %717 = sub i32 0, %198
  %718 = and i32 %717, 65535
  %719 = or i32 %718, %205
  %720 = insertelement <4 x i32> undef, i32 %719, i32 0
  %721 = shufflevector <4 x i32> %720, <4 x i32> undef, <4 x i32> zeroinitializer
  %722 = or i32 %199, %197
  %723 = insertelement <4 x i32> undef, i32 %722, i32 0
  %724 = shufflevector <4 x i32> %723, <4 x i32> undef, <4 x i32> zeroinitializer
  %725 = sub i32 0, %196
  %726 = and i32 %725, 65535
  %727 = or i32 %726, %200
  %728 = insertelement <4 x i32> undef, i32 %727, i32 0
  %729 = shufflevector <4 x i32> %728, <4 x i32> undef, <4 x i32> zeroinitializer
  %730 = shufflevector <8 x i16> %574, <8 x i16> %579, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %731 = shufflevector <8 x i16> %574, <8 x i16> %579, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %732 = bitcast <4 x i32> %721 to <8 x i16>
  %733 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %730, <8 x i16> %732) #9
  %734 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %731, <8 x i16> %732) #9
  %735 = bitcast <4 x i32> %724 to <8 x i16>
  %736 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %730, <8 x i16> %735) #9
  %737 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %731, <8 x i16> %735) #9
  %738 = add <4 x i32> %733, <i32 2048, i32 2048, i32 2048, i32 2048>
  %739 = add <4 x i32> %734, <i32 2048, i32 2048, i32 2048, i32 2048>
  %740 = add <4 x i32> %736, <i32 2048, i32 2048, i32 2048, i32 2048>
  %741 = add <4 x i32> %737, <i32 2048, i32 2048, i32 2048, i32 2048>
  %742 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %738, i32 %316) #9
  %743 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %739, i32 %316) #9
  %744 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %740, i32 %316) #9
  %745 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %741, i32 %316) #9
  %746 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %742, <4 x i32> %743) #9
  %747 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %744, <4 x i32> %745) #9
  %748 = shufflevector <8 x i16> %575, <8 x i16> %578, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %749 = shufflevector <8 x i16> %575, <8 x i16> %578, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %750 = bitcast <4 x i32> %729 to <8 x i16>
  %751 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %748, <8 x i16> %750) #9
  %752 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %749, <8 x i16> %750) #9
  %753 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %748, <8 x i16> %732) #9
  %754 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %749, <8 x i16> %732) #9
  %755 = add <4 x i32> %751, <i32 2048, i32 2048, i32 2048, i32 2048>
  %756 = add <4 x i32> %752, <i32 2048, i32 2048, i32 2048, i32 2048>
  %757 = add <4 x i32> %753, <i32 2048, i32 2048, i32 2048, i32 2048>
  %758 = add <4 x i32> %754, <i32 2048, i32 2048, i32 2048, i32 2048>
  %759 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %755, i32 %316) #9
  %760 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %756, i32 %316) #9
  %761 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %757, i32 %316) #9
  %762 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %758, i32 %316) #9
  %763 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %759, <4 x i32> %760) #9
  %764 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %761, <4 x i32> %762) #9
  %765 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %521, <8 x i16> %524) #9
  %766 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %521, <8 x i16> %524) #9
  %767 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %623, <8 x i16> %640) #9
  %768 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %623, <8 x i16> %640) #9
  %769 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %528, <8 x i16> %525) #9
  %770 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %528, <8 x i16> %525) #9
  %771 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %675, <8 x i16> %658) #9
  %772 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %675, <8 x i16> %658) #9
  %773 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %529, <8 x i16> %532) #9
  %774 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %529, <8 x i16> %532) #9
  %775 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %676, <8 x i16> %659) #9
  %776 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %676, <8 x i16> %659) #9
  %777 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %536, <8 x i16> %533) #9
  %778 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %536, <8 x i16> %533) #9
  %779 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %624, <8 x i16> %641) #9
  %780 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %624, <8 x i16> %641) #9
  %781 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %693, <8 x i16> %712) #9
  %782 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %693, <8 x i16> %712) #9
  %783 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %694, <8 x i16> %711) #9
  %784 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %694, <8 x i16> %711) #9
  %785 = sub i32 0, %186
  %786 = and i32 %785, 65535
  %787 = or i32 %786, %188
  %788 = insertelement <4 x i32> undef, i32 %787, i32 0
  %789 = shufflevector <4 x i32> %788, <4 x i32> undef, <4 x i32> zeroinitializer
  %790 = shufflevector <8 x i16> %714, <8 x i16> %716, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %791 = shufflevector <8 x i16> %714, <8 x i16> %716, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %792 = bitcast <4 x i32> %789 to <8 x i16>
  %793 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %790, <8 x i16> %792) #9
  %794 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %791, <8 x i16> %792) #9
  %795 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %790, <8 x i16> %679) #9
  %796 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %791, <8 x i16> %679) #9
  %797 = add <4 x i32> %793, <i32 2048, i32 2048, i32 2048, i32 2048>
  %798 = add <4 x i32> %794, <i32 2048, i32 2048, i32 2048, i32 2048>
  %799 = add <4 x i32> %795, <i32 2048, i32 2048, i32 2048, i32 2048>
  %800 = add <4 x i32> %796, <i32 2048, i32 2048, i32 2048, i32 2048>
  %801 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %797, i32 %316) #9
  %802 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %798, i32 %316) #9
  %803 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %799, i32 %316) #9
  %804 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %800, i32 %316) #9
  %805 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %801, <4 x i32> %802) #9
  %806 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %803, <4 x i32> %804) #9
  %807 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %573, <8 x i16> %576) #9
  %808 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %573, <8 x i16> %576) #9
  %809 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %746, <8 x i16> %763) #9
  %810 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %746, <8 x i16> %763) #9
  %811 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %580, <8 x i16> %577) #9
  %812 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %580, <8 x i16> %577) #9
  %813 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %747, <8 x i16> %764) #9
  %814 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %747, <8 x i16> %764) #9
  %815 = shufflevector <8 x i16> %768, <8 x i16> %779, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %816 = shufflevector <8 x i16> %768, <8 x i16> %779, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %817 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %815, <8 x i16> %732) #9
  %818 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %816, <8 x i16> %732) #9
  %819 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %815, <8 x i16> %735) #9
  %820 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %816, <8 x i16> %735) #9
  %821 = add <4 x i32> %817, <i32 2048, i32 2048, i32 2048, i32 2048>
  %822 = add <4 x i32> %818, <i32 2048, i32 2048, i32 2048, i32 2048>
  %823 = add <4 x i32> %819, <i32 2048, i32 2048, i32 2048, i32 2048>
  %824 = add <4 x i32> %820, <i32 2048, i32 2048, i32 2048, i32 2048>
  %825 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %821, i32 %316) #9
  %826 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %822, i32 %316) #9
  %827 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %823, i32 %316) #9
  %828 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %824, i32 %316) #9
  %829 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %825, <4 x i32> %826) #9
  %830 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %827, <4 x i32> %828) #9
  %831 = shufflevector <8 x i16> %766, <8 x i16> %777, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %832 = shufflevector <8 x i16> %766, <8 x i16> %777, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %833 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %831, <8 x i16> %732) #9
  %834 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %832, <8 x i16> %732) #9
  %835 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %831, <8 x i16> %735) #9
  %836 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %832, <8 x i16> %735) #9
  %837 = add <4 x i32> %833, <i32 2048, i32 2048, i32 2048, i32 2048>
  %838 = add <4 x i32> %834, <i32 2048, i32 2048, i32 2048, i32 2048>
  %839 = add <4 x i32> %835, <i32 2048, i32 2048, i32 2048, i32 2048>
  %840 = add <4 x i32> %836, <i32 2048, i32 2048, i32 2048, i32 2048>
  %841 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %837, i32 %316) #9
  %842 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %838, i32 %316) #9
  %843 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %839, i32 %316) #9
  %844 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %840, i32 %316) #9
  %845 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %841, <4 x i32> %842) #9
  %846 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %843, <4 x i32> %844) #9
  %847 = shufflevector <8 x i16> %769, <8 x i16> %774, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %848 = shufflevector <8 x i16> %769, <8 x i16> %774, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %849 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %847, <8 x i16> %750) #9
  %850 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %848, <8 x i16> %750) #9
  %851 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %847, <8 x i16> %732) #9
  %852 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %848, <8 x i16> %732) #9
  %853 = add <4 x i32> %849, <i32 2048, i32 2048, i32 2048, i32 2048>
  %854 = add <4 x i32> %850, <i32 2048, i32 2048, i32 2048, i32 2048>
  %855 = add <4 x i32> %851, <i32 2048, i32 2048, i32 2048, i32 2048>
  %856 = add <4 x i32> %852, <i32 2048, i32 2048, i32 2048, i32 2048>
  %857 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %853, i32 %316) #9
  %858 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %854, i32 %316) #9
  %859 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %855, i32 %316) #9
  %860 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %856, i32 %316) #9
  %861 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %857, <4 x i32> %858) #9
  %862 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %859, <4 x i32> %860) #9
  %863 = shufflevector <8 x i16> %771, <8 x i16> %776, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %864 = shufflevector <8 x i16> %771, <8 x i16> %776, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %865 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %863, <8 x i16> %750) #9
  %866 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %864, <8 x i16> %750) #9
  %867 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %863, <8 x i16> %732) #9
  %868 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %864, <8 x i16> %732) #9
  %869 = add <4 x i32> %865, <i32 2048, i32 2048, i32 2048, i32 2048>
  %870 = add <4 x i32> %866, <i32 2048, i32 2048, i32 2048, i32 2048>
  %871 = add <4 x i32> %867, <i32 2048, i32 2048, i32 2048, i32 2048>
  %872 = add <4 x i32> %868, <i32 2048, i32 2048, i32 2048, i32 2048>
  %873 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %869, i32 %316) #9
  %874 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %870, i32 %316) #9
  %875 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %871, i32 %316) #9
  %876 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %872, i32 %316) #9
  %877 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %873, <4 x i32> %874) #9
  %878 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %875, <4 x i32> %876) #9
  %879 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %781, <8 x i16> %715) #9
  %880 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %781, <8 x i16> %715) #9
  %881 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %783, <8 x i16> %806) #9
  %882 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %783, <8 x i16> %806) #9
  %883 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %784, <8 x i16> %805) #9
  %884 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %784, <8 x i16> %805) #9
  %885 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %782, <8 x i16> %713) #9
  %886 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %782, <8 x i16> %713) #9
  %887 = shufflevector <8 x i16> %810, <8 x i16> %813, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %888 = shufflevector <8 x i16> %810, <8 x i16> %813, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %889 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %887, <8 x i16> %792) #9
  %890 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %888, <8 x i16> %792) #9
  %891 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %887, <8 x i16> %679) #9
  %892 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %888, <8 x i16> %679) #9
  %893 = add <4 x i32> %889, <i32 2048, i32 2048, i32 2048, i32 2048>
  %894 = add <4 x i32> %890, <i32 2048, i32 2048, i32 2048, i32 2048>
  %895 = add <4 x i32> %891, <i32 2048, i32 2048, i32 2048, i32 2048>
  %896 = add <4 x i32> %892, <i32 2048, i32 2048, i32 2048, i32 2048>
  %897 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %893, i32 %316) #9
  %898 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %894, i32 %316) #9
  %899 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %895, i32 %316) #9
  %900 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %896, i32 %316) #9
  %901 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %897, <4 x i32> %898) #9
  %902 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %899, <4 x i32> %900) #9
  %903 = shufflevector <8 x i16> %808, <8 x i16> %811, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %904 = shufflevector <8 x i16> %808, <8 x i16> %811, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %905 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %903, <8 x i16> %792) #9
  %906 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %904, <8 x i16> %792) #9
  %907 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %903, <8 x i16> %679) #9
  %908 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %904, <8 x i16> %679) #9
  %909 = add <4 x i32> %905, <i32 2048, i32 2048, i32 2048, i32 2048>
  %910 = add <4 x i32> %906, <i32 2048, i32 2048, i32 2048, i32 2048>
  %911 = add <4 x i32> %907, <i32 2048, i32 2048, i32 2048, i32 2048>
  %912 = add <4 x i32> %908, <i32 2048, i32 2048, i32 2048, i32 2048>
  %913 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %909, i32 %316) #9
  %914 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %910, i32 %316) #9
  %915 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %911, i32 %316) #9
  %916 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %912, i32 %316) #9
  %917 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %913, <4 x i32> %914) #9
  %918 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %915, <4 x i32> %916) #9
  %919 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %765, <8 x i16> %770) #9
  %920 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %765, <8 x i16> %770) #9
  %921 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %767, <8 x i16> %772) #9
  %922 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %767, <8 x i16> %772) #9
  %923 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %829, <8 x i16> %877) #9
  %924 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %829, <8 x i16> %877) #9
  %925 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %845, <8 x i16> %861) #9
  %926 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %845, <8 x i16> %861) #9
  %927 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %778, <8 x i16> %773) #9
  %928 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %778, <8 x i16> %773) #9
  %929 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %780, <8 x i16> %775) #9
  %930 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %780, <8 x i16> %775) #9
  %931 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %830, <8 x i16> %878) #9
  %932 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %830, <8 x i16> %878) #9
  %933 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %846, <8 x i16> %862) #9
  %934 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %846, <8 x i16> %862) #9
  %935 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %879, <8 x i16> %812) #9
  %936 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %879, <8 x i16> %812) #9
  %937 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %881, <8 x i16> %814) #9
  %938 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %881, <8 x i16> %814) #9
  %939 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %883, <8 x i16> %902) #9
  %940 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %883, <8 x i16> %902) #9
  %941 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %885, <8 x i16> %918) #9
  %942 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %885, <8 x i16> %918) #9
  %943 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %886, <8 x i16> %917) #9
  %944 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %886, <8 x i16> %917) #9
  %945 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %884, <8 x i16> %901) #9
  %946 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %884, <8 x i16> %901) #9
  %947 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %882, <8 x i16> %809) #9
  %948 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %882, <8 x i16> %809) #9
  %949 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %880, <8 x i16> %807) #9
  %950 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %880, <8 x i16> %807) #9
  %951 = shufflevector <8 x i16> %926, <8 x i16> %933, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %952 = shufflevector <8 x i16> %926, <8 x i16> %933, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %953 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %951, <8 x i16> %792) #9
  %954 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %952, <8 x i16> %792) #9
  %955 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %951, <8 x i16> %679) #9
  %956 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %952, <8 x i16> %679) #9
  %957 = add <4 x i32> %953, <i32 2048, i32 2048, i32 2048, i32 2048>
  %958 = add <4 x i32> %954, <i32 2048, i32 2048, i32 2048, i32 2048>
  %959 = add <4 x i32> %955, <i32 2048, i32 2048, i32 2048, i32 2048>
  %960 = add <4 x i32> %956, <i32 2048, i32 2048, i32 2048, i32 2048>
  %961 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %957, i32 %316) #9
  %962 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %958, i32 %316) #9
  %963 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %959, i32 %316) #9
  %964 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %960, i32 %316) #9
  %965 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %961, <4 x i32> %962) #9
  %966 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %963, <4 x i32> %964) #9
  %967 = shufflevector <8 x i16> %924, <8 x i16> %931, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %968 = shufflevector <8 x i16> %924, <8 x i16> %931, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %969 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %967, <8 x i16> %792) #9
  %970 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %968, <8 x i16> %792) #9
  %971 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %967, <8 x i16> %679) #9
  %972 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %968, <8 x i16> %679) #9
  %973 = add <4 x i32> %969, <i32 2048, i32 2048, i32 2048, i32 2048>
  %974 = add <4 x i32> %970, <i32 2048, i32 2048, i32 2048, i32 2048>
  %975 = add <4 x i32> %971, <i32 2048, i32 2048, i32 2048, i32 2048>
  %976 = add <4 x i32> %972, <i32 2048, i32 2048, i32 2048, i32 2048>
  %977 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %973, i32 %316) #9
  %978 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %974, i32 %316) #9
  %979 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %975, i32 %316) #9
  %980 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %976, i32 %316) #9
  %981 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %977, <4 x i32> %978) #9
  %982 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %979, <4 x i32> %980) #9
  %983 = shufflevector <8 x i16> %922, <8 x i16> %929, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %984 = shufflevector <8 x i16> %922, <8 x i16> %929, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %985 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %983, <8 x i16> %792) #9
  %986 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %984, <8 x i16> %792) #9
  %987 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %983, <8 x i16> %679) #9
  %988 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %984, <8 x i16> %679) #9
  %989 = add <4 x i32> %985, <i32 2048, i32 2048, i32 2048, i32 2048>
  %990 = add <4 x i32> %986, <i32 2048, i32 2048, i32 2048, i32 2048>
  %991 = add <4 x i32> %987, <i32 2048, i32 2048, i32 2048, i32 2048>
  %992 = add <4 x i32> %988, <i32 2048, i32 2048, i32 2048, i32 2048>
  %993 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %989, i32 %316) #9
  %994 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %990, i32 %316) #9
  %995 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %991, i32 %316) #9
  %996 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %992, i32 %316) #9
  %997 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %993, <4 x i32> %994) #9
  %998 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %995, <4 x i32> %996) #9
  %999 = shufflevector <8 x i16> %920, <8 x i16> %927, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1000 = shufflevector <8 x i16> %920, <8 x i16> %927, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1001 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %999, <8 x i16> %792) #9
  %1002 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1000, <8 x i16> %792) #9
  %1003 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %999, <8 x i16> %679) #9
  %1004 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1000, <8 x i16> %679) #9
  %1005 = add <4 x i32> %1001, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1006 = add <4 x i32> %1002, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1007 = add <4 x i32> %1003, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1008 = add <4 x i32> %1004, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1009 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1005, i32 %316) #9
  %1010 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1006, i32 %316) #9
  %1011 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1007, i32 %316) #9
  %1012 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1008, i32 %316) #9
  %1013 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1009, <4 x i32> %1010) #9
  %1014 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1011, <4 x i32> %1012) #9
  %1015 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %935, <8 x i16> %928) #9
  %1016 = bitcast <2 x i64>* %1 to <8 x i16>*
  store <8 x i16> %1015, <8 x i16>* %1016, align 16
  %1017 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %935, <8 x i16> %928) #9
  %1018 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 31
  %1019 = bitcast <2 x i64>* %1018 to <8 x i16>*
  store <8 x i16> %1017, <8 x i16>* %1019, align 16
  %1020 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %937, <8 x i16> %930) #9
  %1021 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 1
  %1022 = bitcast <2 x i64>* %1021 to <8 x i16>*
  store <8 x i16> %1020, <8 x i16>* %1022, align 16
  %1023 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %937, <8 x i16> %930) #9
  %1024 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 30
  %1025 = bitcast <2 x i64>* %1024 to <8 x i16>*
  store <8 x i16> %1023, <8 x i16>* %1025, align 16
  %1026 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %939, <8 x i16> %932) #9
  %1027 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 2
  %1028 = bitcast <2 x i64>* %1027 to <8 x i16>*
  store <8 x i16> %1026, <8 x i16>* %1028, align 16
  %1029 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %939, <8 x i16> %932) #9
  %1030 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 29
  %1031 = bitcast <2 x i64>* %1030 to <8 x i16>*
  store <8 x i16> %1029, <8 x i16>* %1031, align 16
  %1032 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %941, <8 x i16> %934) #9
  %1033 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 3
  %1034 = bitcast <2 x i64>* %1033 to <8 x i16>*
  store <8 x i16> %1032, <8 x i16>* %1034, align 16
  %1035 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %941, <8 x i16> %934) #9
  %1036 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 28
  %1037 = bitcast <2 x i64>* %1036 to <8 x i16>*
  store <8 x i16> %1035, <8 x i16>* %1037, align 16
  %1038 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %943, <8 x i16> %966) #9
  %1039 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 4
  %1040 = bitcast <2 x i64>* %1039 to <8 x i16>*
  store <8 x i16> %1038, <8 x i16>* %1040, align 16
  %1041 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %943, <8 x i16> %966) #9
  %1042 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 27
  %1043 = bitcast <2 x i64>* %1042 to <8 x i16>*
  store <8 x i16> %1041, <8 x i16>* %1043, align 16
  %1044 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %945, <8 x i16> %982) #9
  %1045 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 5
  %1046 = bitcast <2 x i64>* %1045 to <8 x i16>*
  store <8 x i16> %1044, <8 x i16>* %1046, align 16
  %1047 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %945, <8 x i16> %982) #9
  %1048 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 26
  %1049 = bitcast <2 x i64>* %1048 to <8 x i16>*
  store <8 x i16> %1047, <8 x i16>* %1049, align 16
  %1050 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %947, <8 x i16> %998) #9
  %1051 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 6
  %1052 = bitcast <2 x i64>* %1051 to <8 x i16>*
  store <8 x i16> %1050, <8 x i16>* %1052, align 16
  %1053 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %947, <8 x i16> %998) #9
  %1054 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 25
  %1055 = bitcast <2 x i64>* %1054 to <8 x i16>*
  store <8 x i16> %1053, <8 x i16>* %1055, align 16
  %1056 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %949, <8 x i16> %1014) #9
  %1057 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 7
  %1058 = bitcast <2 x i64>* %1057 to <8 x i16>*
  store <8 x i16> %1056, <8 x i16>* %1058, align 16
  %1059 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %949, <8 x i16> %1014) #9
  %1060 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 24
  %1061 = bitcast <2 x i64>* %1060 to <8 x i16>*
  store <8 x i16> %1059, <8 x i16>* %1061, align 16
  %1062 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %950, <8 x i16> %1013) #9
  %1063 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 8
  %1064 = bitcast <2 x i64>* %1063 to <8 x i16>*
  store <8 x i16> %1062, <8 x i16>* %1064, align 16
  %1065 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %950, <8 x i16> %1013) #9
  %1066 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 23
  %1067 = bitcast <2 x i64>* %1066 to <8 x i16>*
  store <8 x i16> %1065, <8 x i16>* %1067, align 16
  %1068 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %948, <8 x i16> %997) #9
  %1069 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 9
  %1070 = bitcast <2 x i64>* %1069 to <8 x i16>*
  store <8 x i16> %1068, <8 x i16>* %1070, align 16
  %1071 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %948, <8 x i16> %997) #9
  %1072 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 22
  %1073 = bitcast <2 x i64>* %1072 to <8 x i16>*
  store <8 x i16> %1071, <8 x i16>* %1073, align 16
  %1074 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %946, <8 x i16> %981) #9
  %1075 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 10
  %1076 = bitcast <2 x i64>* %1075 to <8 x i16>*
  store <8 x i16> %1074, <8 x i16>* %1076, align 16
  %1077 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %946, <8 x i16> %981) #9
  %1078 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 21
  %1079 = bitcast <2 x i64>* %1078 to <8 x i16>*
  store <8 x i16> %1077, <8 x i16>* %1079, align 16
  %1080 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %944, <8 x i16> %965) #9
  %1081 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 11
  %1082 = bitcast <2 x i64>* %1081 to <8 x i16>*
  store <8 x i16> %1080, <8 x i16>* %1082, align 16
  %1083 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %944, <8 x i16> %965) #9
  %1084 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 20
  %1085 = bitcast <2 x i64>* %1084 to <8 x i16>*
  store <8 x i16> %1083, <8 x i16>* %1085, align 16
  %1086 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %942, <8 x i16> %925) #9
  %1087 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 12
  %1088 = bitcast <2 x i64>* %1087 to <8 x i16>*
  store <8 x i16> %1086, <8 x i16>* %1088, align 16
  %1089 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %942, <8 x i16> %925) #9
  %1090 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 19
  %1091 = bitcast <2 x i64>* %1090 to <8 x i16>*
  store <8 x i16> %1089, <8 x i16>* %1091, align 16
  %1092 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %940, <8 x i16> %923) #9
  %1093 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 13
  %1094 = bitcast <2 x i64>* %1093 to <8 x i16>*
  store <8 x i16> %1092, <8 x i16>* %1094, align 16
  %1095 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %940, <8 x i16> %923) #9
  %1096 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 18
  %1097 = bitcast <2 x i64>* %1096 to <8 x i16>*
  store <8 x i16> %1095, <8 x i16>* %1097, align 16
  %1098 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %938, <8 x i16> %921) #9
  %1099 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 14
  %1100 = bitcast <2 x i64>* %1099 to <8 x i16>*
  store <8 x i16> %1098, <8 x i16>* %1100, align 16
  %1101 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %938, <8 x i16> %921) #9
  %1102 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 17
  %1103 = bitcast <2 x i64>* %1102 to <8 x i16>*
  store <8 x i16> %1101, <8 x i16>* %1103, align 16
  %1104 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %936, <8 x i16> %919) #9
  %1105 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 15
  %1106 = bitcast <2 x i64>* %1105 to <8 x i16>*
  store <8 x i16> %1104, <8 x i16>* %1106, align 16
  %1107 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %936, <8 x i16> %919) #9
  %1108 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 16
  %1109 = bitcast <2 x i64>* %1108 to <8 x i16>*
  store <8 x i16> %1107, <8 x i16>* %1109, align 16
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal void @idct64_low32_ssse3(<2 x i64>* readonly, <2 x i64>*, i8 signext) #0 {
  %4 = alloca [64 x <2 x i64>], align 16
  %5 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 32), align 16
  %6 = and i32 %5, 65535
  %7 = shl i32 %5, 16
  %8 = or i32 %6, %7
  %9 = insertelement <4 x i32> undef, i32 %8, i32 0
  %10 = shufflevector <4 x i32> %9, <4 x i32> undef, <4 x i32> zeroinitializer
  %11 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 16), align 16
  %12 = sub i32 0, %11
  %13 = and i32 %12, 65535
  %14 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 48), align 16
  %15 = shl i32 %14, 16
  %16 = or i32 %15, %13
  %17 = insertelement <4 x i32> undef, i32 %16, i32 0
  %18 = shufflevector <4 x i32> %17, <4 x i32> undef, <4 x i32> zeroinitializer
  %19 = and i32 %14, 65535
  %20 = shl i32 %11, 16
  %21 = or i32 %19, %20
  %22 = insertelement <4 x i32> undef, i32 %21, i32 0
  %23 = shufflevector <4 x i32> %22, <4 x i32> undef, <4 x i32> zeroinitializer
  %24 = sub i32 0, %14
  %25 = and i32 %24, 65535
  %26 = sub i32 0, %20
  %27 = or i32 %25, %26
  %28 = insertelement <4 x i32> undef, i32 %27, i32 0
  %29 = shufflevector <4 x i32> %28, <4 x i32> undef, <4 x i32> zeroinitializer
  %30 = sub i32 0, %5
  %31 = and i32 %30, 65535
  %32 = or i32 %31, %7
  %33 = insertelement <4 x i32> undef, i32 %32, i32 0
  %34 = shufflevector <4 x i32> %33, <4 x i32> undef, <4 x i32> zeroinitializer
  %35 = bitcast [64 x <2 x i64>]* %4 to i8*
  call void @llvm.lifetime.start.p0i8(i64 1024, i8* nonnull %35) #9
  %36 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 1
  %37 = bitcast <2 x i64>* %36 to i8*
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %37, i8 -86, i64 1008, i1 false)
  %38 = load <2 x i64>, <2 x i64>* %0, align 16
  %39 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 0
  store <2 x i64> %38, <2 x i64>* %39, align 16
  %40 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 16
  %41 = load <2 x i64>, <2 x i64>* %40, align 16
  %42 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 2
  store <2 x i64> %41, <2 x i64>* %42, align 16
  %43 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 8
  %44 = load <2 x i64>, <2 x i64>* %43, align 16
  %45 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 4
  store <2 x i64> %44, <2 x i64>* %45, align 16
  %46 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 24
  %47 = load <2 x i64>, <2 x i64>* %46, align 16
  %48 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 6
  store <2 x i64> %47, <2 x i64>* %48, align 16
  %49 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 4
  %50 = load <2 x i64>, <2 x i64>* %49, align 16
  %51 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 8
  store <2 x i64> %50, <2 x i64>* %51, align 16
  %52 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 20
  %53 = load <2 x i64>, <2 x i64>* %52, align 16
  %54 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 10
  store <2 x i64> %53, <2 x i64>* %54, align 16
  %55 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 12
  %56 = load <2 x i64>, <2 x i64>* %55, align 16
  %57 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 12
  store <2 x i64> %56, <2 x i64>* %57, align 16
  %58 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 28
  %59 = load <2 x i64>, <2 x i64>* %58, align 16
  %60 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 14
  store <2 x i64> %59, <2 x i64>* %60, align 16
  %61 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 2
  %62 = load <2 x i64>, <2 x i64>* %61, align 16
  %63 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 16
  store <2 x i64> %62, <2 x i64>* %63, align 16
  %64 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 18
  %65 = load <2 x i64>, <2 x i64>* %64, align 16
  %66 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 18
  store <2 x i64> %65, <2 x i64>* %66, align 16
  %67 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 10
  %68 = load <2 x i64>, <2 x i64>* %67, align 16
  %69 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 20
  store <2 x i64> %68, <2 x i64>* %69, align 16
  %70 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 26
  %71 = load <2 x i64>, <2 x i64>* %70, align 16
  %72 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 22
  store <2 x i64> %71, <2 x i64>* %72, align 16
  %73 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 6
  %74 = load <2 x i64>, <2 x i64>* %73, align 16
  %75 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 24
  store <2 x i64> %74, <2 x i64>* %75, align 16
  %76 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 22
  %77 = load <2 x i64>, <2 x i64>* %76, align 16
  %78 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 26
  store <2 x i64> %77, <2 x i64>* %78, align 16
  %79 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 14
  %80 = load <2 x i64>, <2 x i64>* %79, align 16
  %81 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 28
  store <2 x i64> %80, <2 x i64>* %81, align 16
  %82 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 30
  %83 = load <2 x i64>, <2 x i64>* %82, align 16
  %84 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 30
  store <2 x i64> %83, <2 x i64>* %84, align 16
  %85 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 1
  %86 = bitcast <2 x i64>* %85 to <8 x i16>*
  %87 = load <8 x i16>, <8 x i16>* %86, align 16
  %88 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 32
  %89 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 17
  %90 = load <2 x i64>, <2 x i64>* %89, align 16
  %91 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 34
  store <2 x i64> %90, <2 x i64>* %91, align 16
  %92 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 9
  %93 = load <2 x i64>, <2 x i64>* %92, align 16
  %94 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 36
  store <2 x i64> %93, <2 x i64>* %94, align 16
  %95 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 25
  %96 = load <2 x i64>, <2 x i64>* %95, align 16
  %97 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 38
  store <2 x i64> %96, <2 x i64>* %97, align 16
  %98 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 5
  %99 = load <2 x i64>, <2 x i64>* %98, align 16
  %100 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 40
  store <2 x i64> %99, <2 x i64>* %100, align 16
  %101 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 21
  %102 = load <2 x i64>, <2 x i64>* %101, align 16
  %103 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 42
  store <2 x i64> %102, <2 x i64>* %103, align 16
  %104 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 13
  %105 = load <2 x i64>, <2 x i64>* %104, align 16
  %106 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 44
  store <2 x i64> %105, <2 x i64>* %106, align 16
  %107 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 29
  %108 = load <2 x i64>, <2 x i64>* %107, align 16
  %109 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 46
  store <2 x i64> %108, <2 x i64>* %109, align 16
  %110 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 3
  %111 = load <2 x i64>, <2 x i64>* %110, align 16
  %112 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 48
  store <2 x i64> %111, <2 x i64>* %112, align 16
  %113 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 19
  %114 = load <2 x i64>, <2 x i64>* %113, align 16
  %115 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 50
  store <2 x i64> %114, <2 x i64>* %115, align 16
  %116 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 11
  %117 = load <2 x i64>, <2 x i64>* %116, align 16
  %118 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 52
  store <2 x i64> %117, <2 x i64>* %118, align 16
  %119 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 27
  %120 = load <2 x i64>, <2 x i64>* %119, align 16
  %121 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 54
  store <2 x i64> %120, <2 x i64>* %121, align 16
  %122 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 7
  %123 = load <2 x i64>, <2 x i64>* %122, align 16
  %124 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 56
  store <2 x i64> %123, <2 x i64>* %124, align 16
  %125 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 23
  %126 = load <2 x i64>, <2 x i64>* %125, align 16
  %127 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 58
  store <2 x i64> %126, <2 x i64>* %127, align 16
  %128 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 15
  %129 = bitcast <2 x i64>* %128 to <8 x i16>*
  %130 = load <8 x i16>, <8 x i16>* %129, align 16
  %131 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 60
  %132 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 31
  %133 = bitcast <2 x i64>* %132 to <8 x i16>*
  %134 = load <8 x i16>, <8 x i16>* %133, align 16
  %135 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 62
  %136 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 63), align 4
  %137 = trunc i32 %136 to i16
  %138 = shl i16 %137, 3
  %139 = insertelement <8 x i16> undef, i16 %138, i32 0
  %140 = shufflevector <8 x i16> %139, <8 x i16> undef, <8 x i32> zeroinitializer
  %141 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 1), align 4
  %142 = trunc i32 %141 to i16
  %143 = shl i16 %142, 3
  %144 = insertelement <8 x i16> undef, i16 %143, i32 0
  %145 = shufflevector <8 x i16> %144, <8 x i16> undef, <8 x i32> zeroinitializer
  %146 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %87, <8 x i16> %140) #9
  %147 = bitcast <2 x i64>* %88 to <8 x i16>*
  store <8 x i16> %146, <8 x i16>* %147, align 16
  %148 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %87, <8 x i16> %145) #9
  %149 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 63
  %150 = bitcast <2 x i64>* %149 to <8 x i16>*
  store <8 x i16> %148, <8 x i16>* %150, align 16
  %151 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 33), align 4
  %152 = trunc i32 %151 to i16
  %153 = shl i16 %152, 3
  %154 = sub i16 0, %153
  %155 = insertelement <8 x i16> undef, i16 %154, i32 0
  %156 = shufflevector <8 x i16> %155, <8 x i16> undef, <8 x i32> zeroinitializer
  %157 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 31), align 4
  %158 = trunc i32 %157 to i16
  %159 = shl i16 %158, 3
  %160 = insertelement <8 x i16> undef, i16 %159, i32 0
  %161 = shufflevector <8 x i16> %160, <8 x i16> undef, <8 x i32> zeroinitializer
  %162 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %134, <8 x i16> %156) #9
  %163 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 33
  %164 = bitcast <2 x i64>* %163 to <8 x i16>*
  store <8 x i16> %162, <8 x i16>* %164, align 16
  %165 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %134, <8 x i16> %161) #9
  %166 = bitcast <2 x i64>* %135 to <8 x i16>*
  store <8 x i16> %165, <8 x i16>* %166, align 16
  %167 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 47), align 4
  %168 = trunc i32 %167 to i16
  %169 = shl i16 %168, 3
  %170 = insertelement <8 x i16> undef, i16 %169, i32 0
  %171 = shufflevector <8 x i16> %170, <8 x i16> undef, <8 x i32> zeroinitializer
  %172 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 17), align 4
  %173 = trunc i32 %172 to i16
  %174 = shl i16 %173, 3
  %175 = insertelement <8 x i16> undef, i16 %174, i32 0
  %176 = shufflevector <8 x i16> %175, <8 x i16> undef, <8 x i32> zeroinitializer
  %177 = bitcast <2 x i64> %90 to <8 x i16>
  %178 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %177, <8 x i16> %171) #9
  %179 = bitcast <2 x i64>* %91 to <8 x i16>*
  store <8 x i16> %178, <8 x i16>* %179, align 16
  %180 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %177, <8 x i16> %176) #9
  %181 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 61
  %182 = bitcast <2 x i64>* %181 to <8 x i16>*
  store <8 x i16> %180, <8 x i16>* %182, align 16
  %183 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 49), align 4
  %184 = trunc i32 %183 to i16
  %185 = shl i16 %184, 3
  %186 = sub i16 0, %185
  %187 = insertelement <8 x i16> undef, i16 %186, i32 0
  %188 = shufflevector <8 x i16> %187, <8 x i16> undef, <8 x i32> zeroinitializer
  %189 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 15), align 4
  %190 = trunc i32 %189 to i16
  %191 = shl i16 %190, 3
  %192 = insertelement <8 x i16> undef, i16 %191, i32 0
  %193 = shufflevector <8 x i16> %192, <8 x i16> undef, <8 x i32> zeroinitializer
  %194 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %130, <8 x i16> %188) #9
  %195 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 35
  %196 = bitcast <2 x i64>* %195 to <8 x i16>*
  store <8 x i16> %194, <8 x i16>* %196, align 16
  %197 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %130, <8 x i16> %193) #9
  %198 = bitcast <2 x i64>* %131 to <8 x i16>*
  store <8 x i16> %197, <8 x i16>* %198, align 16
  %199 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 55), align 4
  %200 = trunc i32 %199 to i16
  %201 = shl i16 %200, 3
  %202 = insertelement <8 x i16> undef, i16 %201, i32 0
  %203 = shufflevector <8 x i16> %202, <8 x i16> undef, <8 x i32> zeroinitializer
  %204 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 9), align 4
  %205 = trunc i32 %204 to i16
  %206 = shl i16 %205, 3
  %207 = insertelement <8 x i16> undef, i16 %206, i32 0
  %208 = shufflevector <8 x i16> %207, <8 x i16> undef, <8 x i32> zeroinitializer
  %209 = bitcast <2 x i64> %93 to <8 x i16>
  %210 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %209, <8 x i16> %203) #9
  %211 = bitcast <2 x i64>* %94 to <8 x i16>*
  store <8 x i16> %210, <8 x i16>* %211, align 16
  %212 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %209, <8 x i16> %208) #9
  %213 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 59
  %214 = bitcast <2 x i64>* %213 to <8 x i16>*
  store <8 x i16> %212, <8 x i16>* %214, align 16
  %215 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 41), align 4
  %216 = trunc i32 %215 to i16
  %217 = shl i16 %216, 3
  %218 = sub i16 0, %217
  %219 = insertelement <8 x i16> undef, i16 %218, i32 0
  %220 = shufflevector <8 x i16> %219, <8 x i16> undef, <8 x i32> zeroinitializer
  %221 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 23), align 4
  %222 = trunc i32 %221 to i16
  %223 = shl i16 %222, 3
  %224 = insertelement <8 x i16> undef, i16 %223, i32 0
  %225 = shufflevector <8 x i16> %224, <8 x i16> undef, <8 x i32> zeroinitializer
  %226 = bitcast <2 x i64> %126 to <8 x i16>
  %227 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %226, <8 x i16> %220) #9
  %228 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 37
  %229 = bitcast <2 x i64>* %228 to <8 x i16>*
  store <8 x i16> %227, <8 x i16>* %229, align 16
  %230 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %226, <8 x i16> %225) #9
  %231 = bitcast <2 x i64>* %127 to <8 x i16>*
  store <8 x i16> %230, <8 x i16>* %231, align 16
  %232 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 39), align 4
  %233 = trunc i32 %232 to i16
  %234 = shl i16 %233, 3
  %235 = insertelement <8 x i16> undef, i16 %234, i32 0
  %236 = shufflevector <8 x i16> %235, <8 x i16> undef, <8 x i32> zeroinitializer
  %237 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 25), align 4
  %238 = trunc i32 %237 to i16
  %239 = shl i16 %238, 3
  %240 = insertelement <8 x i16> undef, i16 %239, i32 0
  %241 = shufflevector <8 x i16> %240, <8 x i16> undef, <8 x i32> zeroinitializer
  %242 = bitcast <2 x i64> %96 to <8 x i16>
  %243 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %242, <8 x i16> %236) #9
  %244 = bitcast <2 x i64>* %97 to <8 x i16>*
  store <8 x i16> %243, <8 x i16>* %244, align 16
  %245 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %242, <8 x i16> %241) #9
  %246 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 57
  %247 = bitcast <2 x i64>* %246 to <8 x i16>*
  store <8 x i16> %245, <8 x i16>* %247, align 16
  %248 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 57), align 4
  %249 = trunc i32 %248 to i16
  %250 = shl i16 %249, 3
  %251 = sub i16 0, %250
  %252 = insertelement <8 x i16> undef, i16 %251, i32 0
  %253 = shufflevector <8 x i16> %252, <8 x i16> undef, <8 x i32> zeroinitializer
  %254 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 7), align 4
  %255 = trunc i32 %254 to i16
  %256 = shl i16 %255, 3
  %257 = insertelement <8 x i16> undef, i16 %256, i32 0
  %258 = shufflevector <8 x i16> %257, <8 x i16> undef, <8 x i32> zeroinitializer
  %259 = bitcast <2 x i64> %123 to <8 x i16>
  %260 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %259, <8 x i16> %253) #9
  %261 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 39
  %262 = bitcast <2 x i64>* %261 to <8 x i16>*
  store <8 x i16> %260, <8 x i16>* %262, align 16
  %263 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %259, <8 x i16> %258) #9
  %264 = bitcast <2 x i64>* %124 to <8 x i16>*
  store <8 x i16> %263, <8 x i16>* %264, align 16
  %265 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 59), align 4
  %266 = trunc i32 %265 to i16
  %267 = shl i16 %266, 3
  %268 = insertelement <8 x i16> undef, i16 %267, i32 0
  %269 = shufflevector <8 x i16> %268, <8 x i16> undef, <8 x i32> zeroinitializer
  %270 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 5), align 4
  %271 = trunc i32 %270 to i16
  %272 = shl i16 %271, 3
  %273 = insertelement <8 x i16> undef, i16 %272, i32 0
  %274 = shufflevector <8 x i16> %273, <8 x i16> undef, <8 x i32> zeroinitializer
  %275 = bitcast <2 x i64> %99 to <8 x i16>
  %276 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %275, <8 x i16> %269) #9
  %277 = bitcast <2 x i64>* %100 to <8 x i16>*
  store <8 x i16> %276, <8 x i16>* %277, align 16
  %278 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %275, <8 x i16> %274) #9
  %279 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 55
  %280 = bitcast <2 x i64>* %279 to <8 x i16>*
  store <8 x i16> %278, <8 x i16>* %280, align 16
  %281 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 37), align 4
  %282 = trunc i32 %281 to i16
  %283 = shl i16 %282, 3
  %284 = sub i16 0, %283
  %285 = insertelement <8 x i16> undef, i16 %284, i32 0
  %286 = shufflevector <8 x i16> %285, <8 x i16> undef, <8 x i32> zeroinitializer
  %287 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 27), align 4
  %288 = trunc i32 %287 to i16
  %289 = shl i16 %288, 3
  %290 = insertelement <8 x i16> undef, i16 %289, i32 0
  %291 = shufflevector <8 x i16> %290, <8 x i16> undef, <8 x i32> zeroinitializer
  %292 = bitcast <2 x i64> %120 to <8 x i16>
  %293 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %292, <8 x i16> %286) #9
  %294 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 41
  %295 = bitcast <2 x i64>* %294 to <8 x i16>*
  store <8 x i16> %293, <8 x i16>* %295, align 16
  %296 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %292, <8 x i16> %291) #9
  %297 = bitcast <2 x i64>* %121 to <8 x i16>*
  store <8 x i16> %296, <8 x i16>* %297, align 16
  %298 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 43), align 4
  %299 = trunc i32 %298 to i16
  %300 = shl i16 %299, 3
  %301 = insertelement <8 x i16> undef, i16 %300, i32 0
  %302 = shufflevector <8 x i16> %301, <8 x i16> undef, <8 x i32> zeroinitializer
  %303 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 21), align 4
  %304 = trunc i32 %303 to i16
  %305 = shl i16 %304, 3
  %306 = insertelement <8 x i16> undef, i16 %305, i32 0
  %307 = shufflevector <8 x i16> %306, <8 x i16> undef, <8 x i32> zeroinitializer
  %308 = bitcast <2 x i64> %102 to <8 x i16>
  %309 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %308, <8 x i16> %302) #9
  %310 = bitcast <2 x i64>* %103 to <8 x i16>*
  store <8 x i16> %309, <8 x i16>* %310, align 16
  %311 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %308, <8 x i16> %307) #9
  %312 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 53
  %313 = bitcast <2 x i64>* %312 to <8 x i16>*
  store <8 x i16> %311, <8 x i16>* %313, align 16
  %314 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 53), align 4
  %315 = trunc i32 %314 to i16
  %316 = shl i16 %315, 3
  %317 = sub i16 0, %316
  %318 = insertelement <8 x i16> undef, i16 %317, i32 0
  %319 = shufflevector <8 x i16> %318, <8 x i16> undef, <8 x i32> zeroinitializer
  %320 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 11), align 4
  %321 = trunc i32 %320 to i16
  %322 = shl i16 %321, 3
  %323 = insertelement <8 x i16> undef, i16 %322, i32 0
  %324 = shufflevector <8 x i16> %323, <8 x i16> undef, <8 x i32> zeroinitializer
  %325 = bitcast <2 x i64> %117 to <8 x i16>
  %326 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %325, <8 x i16> %319) #9
  %327 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 43
  %328 = bitcast <2 x i64>* %327 to <8 x i16>*
  store <8 x i16> %326, <8 x i16>* %328, align 16
  %329 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %325, <8 x i16> %324) #9
  %330 = bitcast <2 x i64>* %118 to <8 x i16>*
  store <8 x i16> %329, <8 x i16>* %330, align 16
  %331 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 51), align 4
  %332 = trunc i32 %331 to i16
  %333 = shl i16 %332, 3
  %334 = insertelement <8 x i16> undef, i16 %333, i32 0
  %335 = shufflevector <8 x i16> %334, <8 x i16> undef, <8 x i32> zeroinitializer
  %336 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 13), align 4
  %337 = trunc i32 %336 to i16
  %338 = shl i16 %337, 3
  %339 = insertelement <8 x i16> undef, i16 %338, i32 0
  %340 = shufflevector <8 x i16> %339, <8 x i16> undef, <8 x i32> zeroinitializer
  %341 = bitcast <2 x i64> %105 to <8 x i16>
  %342 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %341, <8 x i16> %335) #9
  %343 = bitcast <2 x i64>* %106 to <8 x i16>*
  store <8 x i16> %342, <8 x i16>* %343, align 16
  %344 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %341, <8 x i16> %340) #9
  %345 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 51
  %346 = bitcast <2 x i64>* %345 to <8 x i16>*
  store <8 x i16> %344, <8 x i16>* %346, align 16
  %347 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 45), align 4
  %348 = trunc i32 %347 to i16
  %349 = shl i16 %348, 3
  %350 = sub i16 0, %349
  %351 = insertelement <8 x i16> undef, i16 %350, i32 0
  %352 = shufflevector <8 x i16> %351, <8 x i16> undef, <8 x i32> zeroinitializer
  %353 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 19), align 4
  %354 = trunc i32 %353 to i16
  %355 = shl i16 %354, 3
  %356 = insertelement <8 x i16> undef, i16 %355, i32 0
  %357 = shufflevector <8 x i16> %356, <8 x i16> undef, <8 x i32> zeroinitializer
  %358 = bitcast <2 x i64> %114 to <8 x i16>
  %359 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %358, <8 x i16> %352) #9
  %360 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 45
  %361 = bitcast <2 x i64>* %360 to <8 x i16>*
  store <8 x i16> %359, <8 x i16>* %361, align 16
  %362 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %358, <8 x i16> %357) #9
  %363 = bitcast <2 x i64>* %115 to <8 x i16>*
  store <8 x i16> %362, <8 x i16>* %363, align 16
  %364 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 35), align 4
  %365 = trunc i32 %364 to i16
  %366 = shl i16 %365, 3
  %367 = insertelement <8 x i16> undef, i16 %366, i32 0
  %368 = shufflevector <8 x i16> %367, <8 x i16> undef, <8 x i32> zeroinitializer
  %369 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 29), align 4
  %370 = trunc i32 %369 to i16
  %371 = shl i16 %370, 3
  %372 = insertelement <8 x i16> undef, i16 %371, i32 0
  %373 = shufflevector <8 x i16> %372, <8 x i16> undef, <8 x i32> zeroinitializer
  %374 = bitcast <2 x i64> %108 to <8 x i16>
  %375 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %374, <8 x i16> %368) #9
  %376 = bitcast <2 x i64>* %109 to <8 x i16>*
  store <8 x i16> %375, <8 x i16>* %376, align 16
  %377 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %374, <8 x i16> %373) #9
  %378 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 49
  %379 = bitcast <2 x i64>* %378 to <8 x i16>*
  store <8 x i16> %377, <8 x i16>* %379, align 16
  %380 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 61), align 4
  %381 = trunc i32 %380 to i16
  %382 = shl i16 %381, 3
  %383 = sub i16 0, %382
  %384 = insertelement <8 x i16> undef, i16 %383, i32 0
  %385 = shufflevector <8 x i16> %384, <8 x i16> undef, <8 x i32> zeroinitializer
  %386 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 3), align 4
  %387 = trunc i32 %386 to i16
  %388 = shl i16 %387, 3
  %389 = insertelement <8 x i16> undef, i16 %388, i32 0
  %390 = shufflevector <8 x i16> %389, <8 x i16> undef, <8 x i32> zeroinitializer
  %391 = bitcast <2 x i64> %111 to <8 x i16>
  %392 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %391, <8 x i16> %385) #9
  %393 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 47
  %394 = bitcast <2 x i64>* %393 to <8 x i16>*
  store <8 x i16> %392, <8 x i16>* %394, align 16
  %395 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %391, <8 x i16> %390) #9
  %396 = bitcast <2 x i64>* %112 to <8 x i16>*
  store <8 x i16> %395, <8 x i16>* %396, align 16
  %397 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 62), align 8
  %398 = trunc i32 %397 to i16
  %399 = shl i16 %398, 3
  %400 = insertelement <8 x i16> undef, i16 %399, i32 0
  %401 = shufflevector <8 x i16> %400, <8 x i16> undef, <8 x i32> zeroinitializer
  %402 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 2), align 8
  %403 = trunc i32 %402 to i16
  %404 = shl i16 %403, 3
  %405 = insertelement <8 x i16> undef, i16 %404, i32 0
  %406 = shufflevector <8 x i16> %405, <8 x i16> undef, <8 x i32> zeroinitializer
  %407 = bitcast <2 x i64> %62 to <8 x i16>
  %408 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %407, <8 x i16> %401) #9
  %409 = bitcast <2 x i64>* %63 to <8 x i16>*
  store <8 x i16> %408, <8 x i16>* %409, align 16
  %410 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %407, <8 x i16> %406) #9
  %411 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 31
  %412 = bitcast <2 x i64>* %411 to <8 x i16>*
  store <8 x i16> %410, <8 x i16>* %412, align 16
  %413 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 34), align 8
  %414 = trunc i32 %413 to i16
  %415 = shl i16 %414, 3
  %416 = sub i16 0, %415
  %417 = insertelement <8 x i16> undef, i16 %416, i32 0
  %418 = shufflevector <8 x i16> %417, <8 x i16> undef, <8 x i32> zeroinitializer
  %419 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 30), align 8
  %420 = trunc i32 %419 to i16
  %421 = shl i16 %420, 3
  %422 = insertelement <8 x i16> undef, i16 %421, i32 0
  %423 = shufflevector <8 x i16> %422, <8 x i16> undef, <8 x i32> zeroinitializer
  %424 = bitcast <2 x i64> %83 to <8 x i16>
  %425 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %424, <8 x i16> %418) #9
  %426 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 17
  %427 = bitcast <2 x i64>* %426 to <8 x i16>*
  store <8 x i16> %425, <8 x i16>* %427, align 16
  %428 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %424, <8 x i16> %423) #9
  %429 = bitcast <2 x i64>* %84 to <8 x i16>*
  store <8 x i16> %428, <8 x i16>* %429, align 16
  %430 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 46), align 8
  %431 = trunc i32 %430 to i16
  %432 = shl i16 %431, 3
  %433 = insertelement <8 x i16> undef, i16 %432, i32 0
  %434 = shufflevector <8 x i16> %433, <8 x i16> undef, <8 x i32> zeroinitializer
  %435 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 18), align 8
  %436 = trunc i32 %435 to i16
  %437 = shl i16 %436, 3
  %438 = insertelement <8 x i16> undef, i16 %437, i32 0
  %439 = shufflevector <8 x i16> %438, <8 x i16> undef, <8 x i32> zeroinitializer
  %440 = bitcast <2 x i64> %65 to <8 x i16>
  %441 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %440, <8 x i16> %434) #9
  %442 = bitcast <2 x i64>* %66 to <8 x i16>*
  store <8 x i16> %441, <8 x i16>* %442, align 16
  %443 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %440, <8 x i16> %439) #9
  %444 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 29
  %445 = bitcast <2 x i64>* %444 to <8 x i16>*
  store <8 x i16> %443, <8 x i16>* %445, align 16
  %446 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 50), align 8
  %447 = trunc i32 %446 to i16
  %448 = shl i16 %447, 3
  %449 = sub i16 0, %448
  %450 = insertelement <8 x i16> undef, i16 %449, i32 0
  %451 = shufflevector <8 x i16> %450, <8 x i16> undef, <8 x i32> zeroinitializer
  %452 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 14), align 8
  %453 = trunc i32 %452 to i16
  %454 = shl i16 %453, 3
  %455 = insertelement <8 x i16> undef, i16 %454, i32 0
  %456 = shufflevector <8 x i16> %455, <8 x i16> undef, <8 x i32> zeroinitializer
  %457 = bitcast <2 x i64> %80 to <8 x i16>
  %458 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %457, <8 x i16> %451) #9
  %459 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 19
  %460 = bitcast <2 x i64>* %459 to <8 x i16>*
  store <8 x i16> %458, <8 x i16>* %460, align 16
  %461 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %457, <8 x i16> %456) #9
  %462 = bitcast <2 x i64>* %81 to <8 x i16>*
  store <8 x i16> %461, <8 x i16>* %462, align 16
  %463 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 54), align 8
  %464 = trunc i32 %463 to i16
  %465 = shl i16 %464, 3
  %466 = insertelement <8 x i16> undef, i16 %465, i32 0
  %467 = shufflevector <8 x i16> %466, <8 x i16> undef, <8 x i32> zeroinitializer
  %468 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 10), align 8
  %469 = trunc i32 %468 to i16
  %470 = shl i16 %469, 3
  %471 = insertelement <8 x i16> undef, i16 %470, i32 0
  %472 = shufflevector <8 x i16> %471, <8 x i16> undef, <8 x i32> zeroinitializer
  %473 = bitcast <2 x i64> %68 to <8 x i16>
  %474 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %473, <8 x i16> %467) #9
  %475 = bitcast <2 x i64>* %69 to <8 x i16>*
  store <8 x i16> %474, <8 x i16>* %475, align 16
  %476 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %473, <8 x i16> %472) #9
  %477 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 27
  %478 = bitcast <2 x i64>* %477 to <8 x i16>*
  store <8 x i16> %476, <8 x i16>* %478, align 16
  %479 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 42), align 8
  %480 = trunc i32 %479 to i16
  %481 = shl i16 %480, 3
  %482 = sub i16 0, %481
  %483 = insertelement <8 x i16> undef, i16 %482, i32 0
  %484 = shufflevector <8 x i16> %483, <8 x i16> undef, <8 x i32> zeroinitializer
  %485 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 22), align 8
  %486 = trunc i32 %485 to i16
  %487 = shl i16 %486, 3
  %488 = insertelement <8 x i16> undef, i16 %487, i32 0
  %489 = shufflevector <8 x i16> %488, <8 x i16> undef, <8 x i32> zeroinitializer
  %490 = bitcast <2 x i64> %77 to <8 x i16>
  %491 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %490, <8 x i16> %484) #9
  %492 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 21
  %493 = bitcast <2 x i64>* %492 to <8 x i16>*
  store <8 x i16> %491, <8 x i16>* %493, align 16
  %494 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %490, <8 x i16> %489) #9
  %495 = bitcast <2 x i64>* %78 to <8 x i16>*
  store <8 x i16> %494, <8 x i16>* %495, align 16
  %496 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 38), align 8
  %497 = trunc i32 %496 to i16
  %498 = shl i16 %497, 3
  %499 = insertelement <8 x i16> undef, i16 %498, i32 0
  %500 = shufflevector <8 x i16> %499, <8 x i16> undef, <8 x i32> zeroinitializer
  %501 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 26), align 8
  %502 = trunc i32 %501 to i16
  %503 = shl i16 %502, 3
  %504 = insertelement <8 x i16> undef, i16 %503, i32 0
  %505 = shufflevector <8 x i16> %504, <8 x i16> undef, <8 x i32> zeroinitializer
  %506 = bitcast <2 x i64> %71 to <8 x i16>
  %507 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %506, <8 x i16> %500) #9
  %508 = bitcast <2 x i64>* %72 to <8 x i16>*
  store <8 x i16> %507, <8 x i16>* %508, align 16
  %509 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %506, <8 x i16> %505) #9
  %510 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 25
  %511 = bitcast <2 x i64>* %510 to <8 x i16>*
  store <8 x i16> %509, <8 x i16>* %511, align 16
  %512 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 58), align 8
  %513 = trunc i32 %512 to i16
  %514 = shl i16 %513, 3
  %515 = sub i16 0, %514
  %516 = insertelement <8 x i16> undef, i16 %515, i32 0
  %517 = shufflevector <8 x i16> %516, <8 x i16> undef, <8 x i32> zeroinitializer
  %518 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 6), align 8
  %519 = trunc i32 %518 to i16
  %520 = shl i16 %519, 3
  %521 = insertelement <8 x i16> undef, i16 %520, i32 0
  %522 = shufflevector <8 x i16> %521, <8 x i16> undef, <8 x i32> zeroinitializer
  %523 = bitcast <2 x i64> %74 to <8 x i16>
  %524 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %523, <8 x i16> %517) #9
  %525 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 23
  %526 = bitcast <2 x i64>* %525 to <8 x i16>*
  store <8 x i16> %524, <8 x i16>* %526, align 16
  %527 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %523, <8 x i16> %522) #9
  %528 = bitcast <2 x i64>* %75 to <8 x i16>*
  store <8 x i16> %527, <8 x i16>* %528, align 16
  %529 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %146, <8 x i16> %162) #9
  store <8 x i16> %529, <8 x i16>* %147, align 16
  %530 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %146, <8 x i16> %162) #9
  store <8 x i16> %530, <8 x i16>* %164, align 16
  %531 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %194, <8 x i16> %178) #9
  store <8 x i16> %531, <8 x i16>* %179, align 16
  %532 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %194, <8 x i16> %178) #9
  store <8 x i16> %532, <8 x i16>* %196, align 16
  %533 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %210, <8 x i16> %227) #9
  store <8 x i16> %533, <8 x i16>* %211, align 16
  %534 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %210, <8 x i16> %227) #9
  store <8 x i16> %534, <8 x i16>* %229, align 16
  %535 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %260, <8 x i16> %243) #9
  store <8 x i16> %535, <8 x i16>* %244, align 16
  %536 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %260, <8 x i16> %243) #9
  store <8 x i16> %536, <8 x i16>* %262, align 16
  %537 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %276, <8 x i16> %293) #9
  store <8 x i16> %537, <8 x i16>* %277, align 16
  %538 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %276, <8 x i16> %293) #9
  store <8 x i16> %538, <8 x i16>* %295, align 16
  %539 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %326, <8 x i16> %309) #9
  store <8 x i16> %539, <8 x i16>* %310, align 16
  %540 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %326, <8 x i16> %309) #9
  store <8 x i16> %540, <8 x i16>* %328, align 16
  %541 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %342, <8 x i16> %359) #9
  store <8 x i16> %541, <8 x i16>* %343, align 16
  %542 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %342, <8 x i16> %359) #9
  store <8 x i16> %542, <8 x i16>* %361, align 16
  %543 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %392, <8 x i16> %375) #9
  store <8 x i16> %543, <8 x i16>* %376, align 16
  %544 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %392, <8 x i16> %375) #9
  store <8 x i16> %544, <8 x i16>* %394, align 16
  %545 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %395, <8 x i16> %377) #9
  store <8 x i16> %545, <8 x i16>* %396, align 16
  %546 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %395, <8 x i16> %377) #9
  store <8 x i16> %546, <8 x i16>* %379, align 16
  %547 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %344, <8 x i16> %362) #9
  store <8 x i16> %547, <8 x i16>* %363, align 16
  %548 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %344, <8 x i16> %362) #9
  store <8 x i16> %548, <8 x i16>* %346, align 16
  %549 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %329, <8 x i16> %311) #9
  store <8 x i16> %549, <8 x i16>* %330, align 16
  %550 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %329, <8 x i16> %311) #9
  store <8 x i16> %550, <8 x i16>* %313, align 16
  %551 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %278, <8 x i16> %296) #9
  store <8 x i16> %551, <8 x i16>* %297, align 16
  %552 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %278, <8 x i16> %296) #9
  store <8 x i16> %552, <8 x i16>* %280, align 16
  %553 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %263, <8 x i16> %245) #9
  store <8 x i16> %553, <8 x i16>* %264, align 16
  %554 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %263, <8 x i16> %245) #9
  store <8 x i16> %554, <8 x i16>* %247, align 16
  %555 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %212, <8 x i16> %230) #9
  store <8 x i16> %555, <8 x i16>* %231, align 16
  %556 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %212, <8 x i16> %230) #9
  store <8 x i16> %556, <8 x i16>* %214, align 16
  %557 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %197, <8 x i16> %180) #9
  store <8 x i16> %557, <8 x i16>* %198, align 16
  %558 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %197, <8 x i16> %180) #9
  store <8 x i16> %558, <8 x i16>* %182, align 16
  %559 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %148, <8 x i16> %165) #9
  store <8 x i16> %559, <8 x i16>* %166, align 16
  %560 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %148, <8 x i16> %165) #9
  store <8 x i16> %560, <8 x i16>* %150, align 16
  %561 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 60), align 16
  %562 = trunc i32 %561 to i16
  %563 = shl i16 %562, 3
  %564 = insertelement <8 x i16> undef, i16 %563, i32 0
  %565 = shufflevector <8 x i16> %564, <8 x i16> undef, <8 x i32> zeroinitializer
  %566 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 4), align 16
  %567 = trunc i32 %566 to i16
  %568 = shl i16 %567, 3
  %569 = insertelement <8 x i16> undef, i16 %568, i32 0
  %570 = shufflevector <8 x i16> %569, <8 x i16> undef, <8 x i32> zeroinitializer
  %571 = bitcast <2 x i64>* %51 to <8 x i16>*
  %572 = load <8 x i16>, <8 x i16>* %571, align 16
  %573 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %572, <8 x i16> %565) #9
  store <8 x i16> %573, <8 x i16>* %571, align 16
  %574 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %572, <8 x i16> %570) #9
  %575 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 15
  %576 = bitcast <2 x i64>* %575 to <8 x i16>*
  store <8 x i16> %574, <8 x i16>* %576, align 16
  %577 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 36), align 16
  %578 = trunc i32 %577 to i16
  %579 = shl i16 %578, 3
  %580 = sub i16 0, %579
  %581 = insertelement <8 x i16> undef, i16 %580, i32 0
  %582 = shufflevector <8 x i16> %581, <8 x i16> undef, <8 x i32> zeroinitializer
  %583 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 28), align 16
  %584 = trunc i32 %583 to i16
  %585 = shl i16 %584, 3
  %586 = insertelement <8 x i16> undef, i16 %585, i32 0
  %587 = shufflevector <8 x i16> %586, <8 x i16> undef, <8 x i32> zeroinitializer
  %588 = bitcast <2 x i64>* %60 to <8 x i16>*
  %589 = load <8 x i16>, <8 x i16>* %588, align 16
  %590 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %589, <8 x i16> %582) #9
  %591 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 9
  %592 = bitcast <2 x i64>* %591 to <8 x i16>*
  store <8 x i16> %590, <8 x i16>* %592, align 16
  %593 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %589, <8 x i16> %587) #9
  store <8 x i16> %593, <8 x i16>* %588, align 16
  %594 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 44), align 16
  %595 = trunc i32 %594 to i16
  %596 = shl i16 %595, 3
  %597 = insertelement <8 x i16> undef, i16 %596, i32 0
  %598 = shufflevector <8 x i16> %597, <8 x i16> undef, <8 x i32> zeroinitializer
  %599 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 20), align 16
  %600 = trunc i32 %599 to i16
  %601 = shl i16 %600, 3
  %602 = insertelement <8 x i16> undef, i16 %601, i32 0
  %603 = shufflevector <8 x i16> %602, <8 x i16> undef, <8 x i32> zeroinitializer
  %604 = bitcast <2 x i64>* %54 to <8 x i16>*
  %605 = load <8 x i16>, <8 x i16>* %604, align 16
  %606 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %605, <8 x i16> %598) #9
  store <8 x i16> %606, <8 x i16>* %604, align 16
  %607 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %605, <8 x i16> %603) #9
  %608 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 13
  %609 = bitcast <2 x i64>* %608 to <8 x i16>*
  store <8 x i16> %607, <8 x i16>* %609, align 16
  %610 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 52), align 16
  %611 = trunc i32 %610 to i16
  %612 = shl i16 %611, 3
  %613 = sub i16 0, %612
  %614 = insertelement <8 x i16> undef, i16 %613, i32 0
  %615 = shufflevector <8 x i16> %614, <8 x i16> undef, <8 x i32> zeroinitializer
  %616 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 12), align 16
  %617 = trunc i32 %616 to i16
  %618 = shl i16 %617, 3
  %619 = insertelement <8 x i16> undef, i16 %618, i32 0
  %620 = shufflevector <8 x i16> %619, <8 x i16> undef, <8 x i32> zeroinitializer
  %621 = bitcast <2 x i64>* %57 to <8 x i16>*
  %622 = load <8 x i16>, <8 x i16>* %621, align 16
  %623 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %622, <8 x i16> %615) #9
  %624 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 11
  %625 = bitcast <2 x i64>* %624 to <8 x i16>*
  store <8 x i16> %623, <8 x i16>* %625, align 16
  %626 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %622, <8 x i16> %620) #9
  store <8 x i16> %626, <8 x i16>* %621, align 16
  %627 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %408, <8 x i16> %425) #9
  store <8 x i16> %627, <8 x i16>* %409, align 16
  %628 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %408, <8 x i16> %425) #9
  store <8 x i16> %628, <8 x i16>* %427, align 16
  %629 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %458, <8 x i16> %441) #9
  store <8 x i16> %629, <8 x i16>* %442, align 16
  %630 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %458, <8 x i16> %441) #9
  store <8 x i16> %630, <8 x i16>* %460, align 16
  %631 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %474, <8 x i16> %491) #9
  store <8 x i16> %631, <8 x i16>* %475, align 16
  %632 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %474, <8 x i16> %491) #9
  store <8 x i16> %632, <8 x i16>* %493, align 16
  %633 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %524, <8 x i16> %507) #9
  store <8 x i16> %633, <8 x i16>* %508, align 16
  %634 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %524, <8 x i16> %507) #9
  store <8 x i16> %634, <8 x i16>* %526, align 16
  %635 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %527, <8 x i16> %509) #9
  store <8 x i16> %635, <8 x i16>* %528, align 16
  %636 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %527, <8 x i16> %509) #9
  store <8 x i16> %636, <8 x i16>* %511, align 16
  %637 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %476, <8 x i16> %494) #9
  store <8 x i16> %637, <8 x i16>* %495, align 16
  %638 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %476, <8 x i16> %494) #9
  store <8 x i16> %638, <8 x i16>* %478, align 16
  %639 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %461, <8 x i16> %443) #9
  store <8 x i16> %639, <8 x i16>* %462, align 16
  %640 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %461, <8 x i16> %443) #9
  store <8 x i16> %640, <8 x i16>* %445, align 16
  %641 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %410, <8 x i16> %428) #9
  store <8 x i16> %641, <8 x i16>* %429, align 16
  %642 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %410, <8 x i16> %428) #9
  store <8 x i16> %642, <8 x i16>* %412, align 16
  call fastcc void @idct64_stage4_high32_sse2(<2 x i64>* nonnull %39, <2 x i64> <i64 8796093024256, i64 8796093024256>, i8 signext %2)
  %643 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 56), align 16
  %644 = trunc i32 %643 to i16
  %645 = shl i16 %644, 3
  %646 = insertelement <8 x i16> undef, i16 %645, i32 0
  %647 = shufflevector <8 x i16> %646, <8 x i16> undef, <8 x i32> zeroinitializer
  %648 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 8), align 16
  %649 = trunc i32 %648 to i16
  %650 = shl i16 %649, 3
  %651 = insertelement <8 x i16> undef, i16 %650, i32 0
  %652 = shufflevector <8 x i16> %651, <8 x i16> undef, <8 x i32> zeroinitializer
  %653 = bitcast <2 x i64>* %45 to <8 x i16>*
  %654 = load <8 x i16>, <8 x i16>* %653, align 16
  %655 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %654, <8 x i16> %647) #9
  store <8 x i16> %655, <8 x i16>* %653, align 16
  %656 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %654, <8 x i16> %652) #9
  %657 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 7
  %658 = bitcast <2 x i64>* %657 to <8 x i16>*
  store <8 x i16> %656, <8 x i16>* %658, align 16
  %659 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 40), align 16
  %660 = trunc i32 %659 to i16
  %661 = shl i16 %660, 3
  %662 = sub i16 0, %661
  %663 = insertelement <8 x i16> undef, i16 %662, i32 0
  %664 = shufflevector <8 x i16> %663, <8 x i16> undef, <8 x i32> zeroinitializer
  %665 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 24), align 16
  %666 = trunc i32 %665 to i16
  %667 = shl i16 %666, 3
  %668 = insertelement <8 x i16> undef, i16 %667, i32 0
  %669 = shufflevector <8 x i16> %668, <8 x i16> undef, <8 x i32> zeroinitializer
  %670 = bitcast <2 x i64>* %48 to <8 x i16>*
  %671 = load <8 x i16>, <8 x i16>* %670, align 16
  %672 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %671, <8 x i16> %664) #9
  %673 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 5
  %674 = bitcast <2 x i64>* %673 to <8 x i16>*
  store <8 x i16> %672, <8 x i16>* %674, align 16
  %675 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %671, <8 x i16> %669) #9
  store <8 x i16> %675, <8 x i16>* %670, align 16
  %676 = load <8 x i16>, <8 x i16>* %571, align 16
  %677 = load <8 x i16>, <8 x i16>* %592, align 16
  %678 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %676, <8 x i16> %677) #9
  store <8 x i16> %678, <8 x i16>* %571, align 16
  %679 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %676, <8 x i16> %677) #9
  store <8 x i16> %679, <8 x i16>* %592, align 16
  %680 = load <8 x i16>, <8 x i16>* %625, align 16
  %681 = load <8 x i16>, <8 x i16>* %604, align 16
  %682 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %680, <8 x i16> %681) #9
  store <8 x i16> %682, <8 x i16>* %604, align 16
  %683 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %680, <8 x i16> %681) #9
  store <8 x i16> %683, <8 x i16>* %625, align 16
  %684 = load <8 x i16>, <8 x i16>* %621, align 16
  %685 = load <8 x i16>, <8 x i16>* %609, align 16
  %686 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %684, <8 x i16> %685) #9
  store <8 x i16> %686, <8 x i16>* %621, align 16
  %687 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %684, <8 x i16> %685) #9
  store <8 x i16> %687, <8 x i16>* %609, align 16
  %688 = load <8 x i16>, <8 x i16>* %576, align 16
  %689 = load <8 x i16>, <8 x i16>* %588, align 16
  %690 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %688, <8 x i16> %689) #9
  store <8 x i16> %690, <8 x i16>* %588, align 16
  %691 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %688, <8 x i16> %689) #9
  store <8 x i16> %691, <8 x i16>* %576, align 16
  %692 = sub i32 0, %648
  %693 = and i32 %692, 65535
  %694 = shl i32 %643, 16
  %695 = or i32 %693, %694
  %696 = insertelement <4 x i32> undef, i32 %695, i32 0
  %697 = shufflevector <4 x i32> %696, <4 x i32> undef, <4 x i32> zeroinitializer
  %698 = and i32 %643, 65535
  %699 = shl i32 %648, 16
  %700 = or i32 %699, %698
  %701 = insertelement <4 x i32> undef, i32 %700, i32 0
  %702 = shufflevector <4 x i32> %701, <4 x i32> undef, <4 x i32> zeroinitializer
  %703 = sub i32 0, %643
  %704 = and i32 %703, 65535
  %705 = sub i32 0, %699
  %706 = or i32 %704, %705
  %707 = insertelement <4 x i32> undef, i32 %706, i32 0
  %708 = shufflevector <4 x i32> %707, <4 x i32> undef, <4 x i32> zeroinitializer
  %709 = sub i32 0, %659
  %710 = and i32 %709, 65535
  %711 = shl i32 %665, 16
  %712 = or i32 %711, %710
  %713 = insertelement <4 x i32> undef, i32 %712, i32 0
  %714 = shufflevector <4 x i32> %713, <4 x i32> undef, <4 x i32> zeroinitializer
  %715 = and i32 %665, 65535
  %716 = shl i32 %659, 16
  %717 = or i32 %715, %716
  %718 = insertelement <4 x i32> undef, i32 %717, i32 0
  %719 = shufflevector <4 x i32> %718, <4 x i32> undef, <4 x i32> zeroinitializer
  %720 = sub i32 0, %665
  %721 = and i32 %720, 65535
  %722 = sub i32 0, %716
  %723 = or i32 %721, %722
  %724 = insertelement <4 x i32> undef, i32 %723, i32 0
  %725 = shufflevector <4 x i32> %724, <4 x i32> undef, <4 x i32> zeroinitializer
  %726 = load <8 x i16>, <8 x i16>* %427, align 16
  %727 = load <8 x i16>, <8 x i16>* %429, align 16
  %728 = shufflevector <8 x i16> %726, <8 x i16> %727, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %729 = shufflevector <8 x i16> %726, <8 x i16> %727, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %730 = bitcast <4 x i32> %697 to <8 x i16>
  %731 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %728, <8 x i16> %730) #9
  %732 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %729, <8 x i16> %730) #9
  %733 = bitcast <4 x i32> %702 to <8 x i16>
  %734 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %728, <8 x i16> %733) #9
  %735 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %729, <8 x i16> %733) #9
  %736 = add <4 x i32> %731, <i32 2048, i32 2048, i32 2048, i32 2048>
  %737 = add <4 x i32> %732, <i32 2048, i32 2048, i32 2048, i32 2048>
  %738 = add <4 x i32> %734, <i32 2048, i32 2048, i32 2048, i32 2048>
  %739 = add <4 x i32> %735, <i32 2048, i32 2048, i32 2048, i32 2048>
  %740 = sext i8 %2 to i32
  %741 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %736, i32 %740) #9
  %742 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %737, i32 %740) #9
  %743 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %738, i32 %740) #9
  %744 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %739, i32 %740) #9
  %745 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %741, <4 x i32> %742) #9
  store <8 x i16> %745, <8 x i16>* %427, align 16
  %746 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %743, <4 x i32> %744) #9
  store <8 x i16> %746, <8 x i16>* %429, align 16
  %747 = load <8 x i16>, <8 x i16>* %442, align 16
  %748 = load <8 x i16>, <8 x i16>* %445, align 16
  %749 = shufflevector <8 x i16> %747, <8 x i16> %748, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %750 = shufflevector <8 x i16> %747, <8 x i16> %748, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %751 = bitcast <4 x i32> %708 to <8 x i16>
  %752 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %749, <8 x i16> %751) #9
  %753 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %750, <8 x i16> %751) #9
  %754 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %749, <8 x i16> %730) #9
  %755 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %750, <8 x i16> %730) #9
  %756 = add <4 x i32> %752, <i32 2048, i32 2048, i32 2048, i32 2048>
  %757 = add <4 x i32> %753, <i32 2048, i32 2048, i32 2048, i32 2048>
  %758 = add <4 x i32> %754, <i32 2048, i32 2048, i32 2048, i32 2048>
  %759 = add <4 x i32> %755, <i32 2048, i32 2048, i32 2048, i32 2048>
  %760 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %756, i32 %740) #9
  %761 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %757, i32 %740) #9
  %762 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %758, i32 %740) #9
  %763 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %759, i32 %740) #9
  %764 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %760, <4 x i32> %761) #9
  store <8 x i16> %764, <8 x i16>* %442, align 16
  %765 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %762, <4 x i32> %763) #9
  store <8 x i16> %765, <8 x i16>* %445, align 16
  %766 = load <8 x i16>, <8 x i16>* %493, align 16
  %767 = load <8 x i16>, <8 x i16>* %495, align 16
  %768 = shufflevector <8 x i16> %766, <8 x i16> %767, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %769 = shufflevector <8 x i16> %766, <8 x i16> %767, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %770 = bitcast <4 x i32> %714 to <8 x i16>
  %771 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %768, <8 x i16> %770) #9
  %772 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %769, <8 x i16> %770) #9
  %773 = bitcast <4 x i32> %719 to <8 x i16>
  %774 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %768, <8 x i16> %773) #9
  %775 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %769, <8 x i16> %773) #9
  %776 = add <4 x i32> %771, <i32 2048, i32 2048, i32 2048, i32 2048>
  %777 = add <4 x i32> %772, <i32 2048, i32 2048, i32 2048, i32 2048>
  %778 = add <4 x i32> %774, <i32 2048, i32 2048, i32 2048, i32 2048>
  %779 = add <4 x i32> %775, <i32 2048, i32 2048, i32 2048, i32 2048>
  %780 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %776, i32 %740) #9
  %781 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %777, i32 %740) #9
  %782 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %778, i32 %740) #9
  %783 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %779, i32 %740) #9
  %784 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %780, <4 x i32> %781) #9
  store <8 x i16> %784, <8 x i16>* %493, align 16
  %785 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %782, <4 x i32> %783) #9
  store <8 x i16> %785, <8 x i16>* %495, align 16
  %786 = load <8 x i16>, <8 x i16>* %508, align 16
  %787 = load <8 x i16>, <8 x i16>* %511, align 16
  %788 = shufflevector <8 x i16> %786, <8 x i16> %787, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %789 = shufflevector <8 x i16> %786, <8 x i16> %787, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %790 = bitcast <4 x i32> %725 to <8 x i16>
  %791 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %788, <8 x i16> %790) #9
  %792 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %789, <8 x i16> %790) #9
  %793 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %788, <8 x i16> %770) #9
  %794 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %789, <8 x i16> %770) #9
  %795 = add <4 x i32> %791, <i32 2048, i32 2048, i32 2048, i32 2048>
  %796 = add <4 x i32> %792, <i32 2048, i32 2048, i32 2048, i32 2048>
  %797 = add <4 x i32> %793, <i32 2048, i32 2048, i32 2048, i32 2048>
  %798 = add <4 x i32> %794, <i32 2048, i32 2048, i32 2048, i32 2048>
  %799 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %795, i32 %740) #9
  %800 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %796, i32 %740) #9
  %801 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %797, i32 %740) #9
  %802 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %798, i32 %740) #9
  %803 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %799, <4 x i32> %800) #9
  store <8 x i16> %803, <8 x i16>* %508, align 16
  %804 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %801, <4 x i32> %802) #9
  store <8 x i16> %804, <8 x i16>* %511, align 16
  %805 = load <8 x i16>, <8 x i16>* %147, align 16
  %806 = load <8 x i16>, <8 x i16>* %196, align 16
  %807 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %805, <8 x i16> %806) #9
  store <8 x i16> %807, <8 x i16>* %147, align 16
  %808 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %805, <8 x i16> %806) #9
  store <8 x i16> %808, <8 x i16>* %196, align 16
  %809 = load <8 x i16>, <8 x i16>* %164, align 16
  %810 = load <8 x i16>, <8 x i16>* %179, align 16
  %811 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %809, <8 x i16> %810) #9
  store <8 x i16> %811, <8 x i16>* %164, align 16
  %812 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %809, <8 x i16> %810) #9
  store <8 x i16> %812, <8 x i16>* %179, align 16
  %813 = load <8 x i16>, <8 x i16>* %262, align 16
  %814 = load <8 x i16>, <8 x i16>* %211, align 16
  %815 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %813, <8 x i16> %814) #9
  store <8 x i16> %815, <8 x i16>* %211, align 16
  %816 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %813, <8 x i16> %814) #9
  store <8 x i16> %816, <8 x i16>* %262, align 16
  %817 = load <8 x i16>, <8 x i16>* %244, align 16
  %818 = load <8 x i16>, <8 x i16>* %229, align 16
  %819 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %817, <8 x i16> %818) #9
  store <8 x i16> %819, <8 x i16>* %229, align 16
  %820 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %817, <8 x i16> %818) #9
  store <8 x i16> %820, <8 x i16>* %244, align 16
  %821 = load <8 x i16>, <8 x i16>* %277, align 16
  %822 = load <8 x i16>, <8 x i16>* %328, align 16
  %823 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %821, <8 x i16> %822) #9
  store <8 x i16> %823, <8 x i16>* %277, align 16
  %824 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %821, <8 x i16> %822) #9
  store <8 x i16> %824, <8 x i16>* %328, align 16
  %825 = load <8 x i16>, <8 x i16>* %295, align 16
  %826 = load <8 x i16>, <8 x i16>* %310, align 16
  %827 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %825, <8 x i16> %826) #9
  store <8 x i16> %827, <8 x i16>* %295, align 16
  %828 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %825, <8 x i16> %826) #9
  store <8 x i16> %828, <8 x i16>* %310, align 16
  %829 = load <8 x i16>, <8 x i16>* %394, align 16
  %830 = load <8 x i16>, <8 x i16>* %343, align 16
  %831 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %829, <8 x i16> %830) #9
  store <8 x i16> %831, <8 x i16>* %343, align 16
  %832 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %829, <8 x i16> %830) #9
  store <8 x i16> %832, <8 x i16>* %394, align 16
  %833 = load <8 x i16>, <8 x i16>* %376, align 16
  %834 = load <8 x i16>, <8 x i16>* %361, align 16
  %835 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %833, <8 x i16> %834) #9
  store <8 x i16> %835, <8 x i16>* %361, align 16
  %836 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %833, <8 x i16> %834) #9
  store <8 x i16> %836, <8 x i16>* %376, align 16
  %837 = load <8 x i16>, <8 x i16>* %396, align 16
  %838 = load <8 x i16>, <8 x i16>* %346, align 16
  %839 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %837, <8 x i16> %838) #9
  store <8 x i16> %839, <8 x i16>* %396, align 16
  %840 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %837, <8 x i16> %838) #9
  store <8 x i16> %840, <8 x i16>* %346, align 16
  %841 = load <8 x i16>, <8 x i16>* %379, align 16
  %842 = load <8 x i16>, <8 x i16>* %363, align 16
  %843 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %841, <8 x i16> %842) #9
  store <8 x i16> %843, <8 x i16>* %379, align 16
  %844 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %841, <8 x i16> %842) #9
  store <8 x i16> %844, <8 x i16>* %363, align 16
  %845 = load <8 x i16>, <8 x i16>* %280, align 16
  %846 = load <8 x i16>, <8 x i16>* %330, align 16
  %847 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %845, <8 x i16> %846) #9
  store <8 x i16> %847, <8 x i16>* %330, align 16
  %848 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %845, <8 x i16> %846) #9
  store <8 x i16> %848, <8 x i16>* %280, align 16
  %849 = load <8 x i16>, <8 x i16>* %297, align 16
  %850 = load <8 x i16>, <8 x i16>* %313, align 16
  %851 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %849, <8 x i16> %850) #9
  store <8 x i16> %851, <8 x i16>* %313, align 16
  %852 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %849, <8 x i16> %850) #9
  store <8 x i16> %852, <8 x i16>* %297, align 16
  %853 = load <8 x i16>, <8 x i16>* %264, align 16
  %854 = load <8 x i16>, <8 x i16>* %214, align 16
  %855 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %853, <8 x i16> %854) #9
  store <8 x i16> %855, <8 x i16>* %264, align 16
  %856 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %853, <8 x i16> %854) #9
  store <8 x i16> %856, <8 x i16>* %214, align 16
  %857 = load <8 x i16>, <8 x i16>* %247, align 16
  %858 = load <8 x i16>, <8 x i16>* %231, align 16
  %859 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %857, <8 x i16> %858) #9
  store <8 x i16> %859, <8 x i16>* %247, align 16
  %860 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %857, <8 x i16> %858) #9
  store <8 x i16> %860, <8 x i16>* %231, align 16
  %861 = load <8 x i16>, <8 x i16>* %150, align 16
  %862 = load <8 x i16>, <8 x i16>* %198, align 16
  %863 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %861, <8 x i16> %862) #9
  store <8 x i16> %863, <8 x i16>* %198, align 16
  %864 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %861, <8 x i16> %862) #9
  store <8 x i16> %864, <8 x i16>* %150, align 16
  %865 = load <8 x i16>, <8 x i16>* %166, align 16
  %866 = load <8 x i16>, <8 x i16>* %182, align 16
  %867 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %865, <8 x i16> %866) #9
  store <8 x i16> %867, <8 x i16>* %182, align 16
  %868 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %865, <8 x i16> %866) #9
  store <8 x i16> %868, <8 x i16>* %166, align 16
  %869 = trunc i32 %5 to i16
  %870 = shl i16 %869, 3
  %871 = insertelement <8 x i16> undef, i16 %870, i32 0
  %872 = shufflevector <8 x i16> %871, <8 x i16> undef, <8 x i32> zeroinitializer
  %873 = bitcast [64 x <2 x i64>]* %4 to <8 x i16>*
  %874 = load <8 x i16>, <8 x i16>* %873, align 16
  %875 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %874, <8 x i16> %872) #9
  store <8 x i16> %875, <8 x i16>* %873, align 16
  %876 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 1
  %877 = bitcast <2 x i64>* %876 to <8 x i16>*
  store <8 x i16> %875, <8 x i16>* %877, align 16
  %878 = trunc i32 %14 to i16
  %879 = shl i16 %878, 3
  %880 = insertelement <8 x i16> undef, i16 %879, i32 0
  %881 = shufflevector <8 x i16> %880, <8 x i16> undef, <8 x i32> zeroinitializer
  %882 = trunc i32 %11 to i16
  %883 = shl i16 %882, 3
  %884 = insertelement <8 x i16> undef, i16 %883, i32 0
  %885 = shufflevector <8 x i16> %884, <8 x i16> undef, <8 x i32> zeroinitializer
  %886 = bitcast <2 x i64>* %42 to <8 x i16>*
  %887 = load <8 x i16>, <8 x i16>* %886, align 16
  %888 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %887, <8 x i16> %881) #9
  store <8 x i16> %888, <8 x i16>* %886, align 16
  %889 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %887, <8 x i16> %885) #9
  %890 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 3
  %891 = bitcast <2 x i64>* %890 to <8 x i16>*
  store <8 x i16> %889, <8 x i16>* %891, align 16
  %892 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %655, <8 x i16> %672) #9
  store <8 x i16> %892, <8 x i16>* %653, align 16
  %893 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %655, <8 x i16> %672) #9
  store <8 x i16> %893, <8 x i16>* %674, align 16
  %894 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %656, <8 x i16> %675) #9
  store <8 x i16> %894, <8 x i16>* %670, align 16
  %895 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %656, <8 x i16> %675) #9
  store <8 x i16> %895, <8 x i16>* %658, align 16
  %896 = shufflevector <8 x i16> %679, <8 x i16> %690, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %897 = shufflevector <8 x i16> %679, <8 x i16> %690, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %898 = bitcast <4 x i32> %18 to <8 x i16>
  %899 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %896, <8 x i16> %898) #9
  %900 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %897, <8 x i16> %898) #9
  %901 = bitcast <4 x i32> %23 to <8 x i16>
  %902 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %896, <8 x i16> %901) #9
  %903 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %897, <8 x i16> %901) #9
  %904 = add <4 x i32> %899, <i32 2048, i32 2048, i32 2048, i32 2048>
  %905 = add <4 x i32> %900, <i32 2048, i32 2048, i32 2048, i32 2048>
  %906 = add <4 x i32> %902, <i32 2048, i32 2048, i32 2048, i32 2048>
  %907 = add <4 x i32> %903, <i32 2048, i32 2048, i32 2048, i32 2048>
  %908 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %904, i32 %740) #9
  %909 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %905, i32 %740) #9
  %910 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %906, i32 %740) #9
  %911 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %907, i32 %740) #9
  %912 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %908, <4 x i32> %909) #9
  store <8 x i16> %912, <8 x i16>* %592, align 16
  %913 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %910, <4 x i32> %911) #9
  store <8 x i16> %913, <8 x i16>* %588, align 16
  %914 = shufflevector <8 x i16> %682, <8 x i16> %687, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %915 = shufflevector <8 x i16> %682, <8 x i16> %687, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %916 = bitcast <4 x i32> %29 to <8 x i16>
  %917 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %914, <8 x i16> %916) #9
  %918 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %915, <8 x i16> %916) #9
  %919 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %914, <8 x i16> %898) #9
  %920 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %915, <8 x i16> %898) #9
  %921 = add <4 x i32> %917, <i32 2048, i32 2048, i32 2048, i32 2048>
  %922 = add <4 x i32> %918, <i32 2048, i32 2048, i32 2048, i32 2048>
  %923 = add <4 x i32> %919, <i32 2048, i32 2048, i32 2048, i32 2048>
  %924 = add <4 x i32> %920, <i32 2048, i32 2048, i32 2048, i32 2048>
  %925 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %921, i32 %740) #9
  %926 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %922, i32 %740) #9
  %927 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %923, i32 %740) #9
  %928 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %924, i32 %740) #9
  %929 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %925, <4 x i32> %926) #9
  store <8 x i16> %929, <8 x i16>* %604, align 16
  %930 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %927, <4 x i32> %928) #9
  store <8 x i16> %930, <8 x i16>* %609, align 16
  %931 = load <8 x i16>, <8 x i16>* %409, align 16
  %932 = load <8 x i16>, <8 x i16>* %460, align 16
  %933 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %931, <8 x i16> %932) #9
  store <8 x i16> %933, <8 x i16>* %409, align 16
  %934 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %931, <8 x i16> %932) #9
  store <8 x i16> %934, <8 x i16>* %460, align 16
  %935 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %745, <8 x i16> %764) #9
  store <8 x i16> %935, <8 x i16>* %427, align 16
  %936 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %745, <8 x i16> %764) #9
  store <8 x i16> %936, <8 x i16>* %442, align 16
  %937 = load <8 x i16>, <8 x i16>* %526, align 16
  %938 = load <8 x i16>, <8 x i16>* %475, align 16
  %939 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %937, <8 x i16> %938) #9
  store <8 x i16> %939, <8 x i16>* %475, align 16
  %940 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %937, <8 x i16> %938) #9
  store <8 x i16> %940, <8 x i16>* %526, align 16
  %941 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %803, <8 x i16> %784) #9
  store <8 x i16> %941, <8 x i16>* %493, align 16
  %942 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %803, <8 x i16> %784) #9
  store <8 x i16> %942, <8 x i16>* %508, align 16
  %943 = load <8 x i16>, <8 x i16>* %528, align 16
  %944 = load <8 x i16>, <8 x i16>* %478, align 16
  %945 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %943, <8 x i16> %944) #9
  store <8 x i16> %945, <8 x i16>* %528, align 16
  %946 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %943, <8 x i16> %944) #9
  store <8 x i16> %946, <8 x i16>* %478, align 16
  %947 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %804, <8 x i16> %785) #9
  store <8 x i16> %947, <8 x i16>* %511, align 16
  %948 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %804, <8 x i16> %785) #9
  store <8 x i16> %948, <8 x i16>* %495, align 16
  %949 = load <8 x i16>, <8 x i16>* %412, align 16
  %950 = load <8 x i16>, <8 x i16>* %462, align 16
  %951 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %949, <8 x i16> %950) #9
  store <8 x i16> %951, <8 x i16>* %462, align 16
  %952 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %949, <8 x i16> %950) #9
  store <8 x i16> %952, <8 x i16>* %412, align 16
  %953 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %746, <8 x i16> %765) #9
  store <8 x i16> %953, <8 x i16>* %445, align 16
  %954 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %746, <8 x i16> %765) #9
  store <8 x i16> %954, <8 x i16>* %429, align 16
  %955 = shufflevector <8 x i16> %812, <8 x i16> %867, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %956 = shufflevector <8 x i16> %812, <8 x i16> %867, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %957 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %955, <8 x i16> %730) #9
  %958 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %956, <8 x i16> %730) #9
  %959 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %955, <8 x i16> %733) #9
  %960 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %956, <8 x i16> %733) #9
  %961 = add <4 x i32> %957, <i32 2048, i32 2048, i32 2048, i32 2048>
  %962 = add <4 x i32> %958, <i32 2048, i32 2048, i32 2048, i32 2048>
  %963 = add <4 x i32> %959, <i32 2048, i32 2048, i32 2048, i32 2048>
  %964 = add <4 x i32> %960, <i32 2048, i32 2048, i32 2048, i32 2048>
  %965 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %961, i32 %740) #9
  %966 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %962, i32 %740) #9
  %967 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %963, i32 %740) #9
  %968 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %964, i32 %740) #9
  %969 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %965, <4 x i32> %966) #9
  store <8 x i16> %969, <8 x i16>* %179, align 16
  %970 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %967, <4 x i32> %968) #9
  store <8 x i16> %970, <8 x i16>* %182, align 16
  %971 = shufflevector <8 x i16> %808, <8 x i16> %863, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %972 = shufflevector <8 x i16> %808, <8 x i16> %863, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %973 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %971, <8 x i16> %730) #9
  %974 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %972, <8 x i16> %730) #9
  %975 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %971, <8 x i16> %733) #9
  %976 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %972, <8 x i16> %733) #9
  %977 = add <4 x i32> %973, <i32 2048, i32 2048, i32 2048, i32 2048>
  %978 = add <4 x i32> %974, <i32 2048, i32 2048, i32 2048, i32 2048>
  %979 = add <4 x i32> %975, <i32 2048, i32 2048, i32 2048, i32 2048>
  %980 = add <4 x i32> %976, <i32 2048, i32 2048, i32 2048, i32 2048>
  %981 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %977, i32 %740) #9
  %982 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %978, i32 %740) #9
  %983 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %979, i32 %740) #9
  %984 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %980, i32 %740) #9
  %985 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %981, <4 x i32> %982) #9
  store <8 x i16> %985, <8 x i16>* %196, align 16
  %986 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %983, <4 x i32> %984) #9
  store <8 x i16> %986, <8 x i16>* %198, align 16
  %987 = shufflevector <8 x i16> %815, <8 x i16> %856, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %988 = shufflevector <8 x i16> %815, <8 x i16> %856, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %989 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %987, <8 x i16> %751) #9
  %990 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %988, <8 x i16> %751) #9
  %991 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %987, <8 x i16> %730) #9
  %992 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %988, <8 x i16> %730) #9
  %993 = add <4 x i32> %989, <i32 2048, i32 2048, i32 2048, i32 2048>
  %994 = add <4 x i32> %990, <i32 2048, i32 2048, i32 2048, i32 2048>
  %995 = add <4 x i32> %991, <i32 2048, i32 2048, i32 2048, i32 2048>
  %996 = add <4 x i32> %992, <i32 2048, i32 2048, i32 2048, i32 2048>
  %997 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %993, i32 %740) #9
  %998 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %994, i32 %740) #9
  %999 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %995, i32 %740) #9
  %1000 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %996, i32 %740) #9
  %1001 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %997, <4 x i32> %998) #9
  store <8 x i16> %1001, <8 x i16>* %211, align 16
  %1002 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %999, <4 x i32> %1000) #9
  store <8 x i16> %1002, <8 x i16>* %214, align 16
  %1003 = shufflevector <8 x i16> %819, <8 x i16> %860, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1004 = shufflevector <8 x i16> %819, <8 x i16> %860, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1005 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1003, <8 x i16> %751) #9
  %1006 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1004, <8 x i16> %751) #9
  %1007 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1003, <8 x i16> %730) #9
  %1008 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1004, <8 x i16> %730) #9
  %1009 = add <4 x i32> %1005, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1010 = add <4 x i32> %1006, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1011 = add <4 x i32> %1007, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1012 = add <4 x i32> %1008, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1013 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1009, i32 %740) #9
  %1014 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1010, i32 %740) #9
  %1015 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1011, i32 %740) #9
  %1016 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1012, i32 %740) #9
  %1017 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1013, <4 x i32> %1014) #9
  store <8 x i16> %1017, <8 x i16>* %229, align 16
  %1018 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1015, <4 x i32> %1016) #9
  store <8 x i16> %1018, <8 x i16>* %231, align 16
  %1019 = shufflevector <8 x i16> %828, <8 x i16> %851, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1020 = shufflevector <8 x i16> %828, <8 x i16> %851, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1021 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1019, <8 x i16> %770) #9
  %1022 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1020, <8 x i16> %770) #9
  %1023 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1019, <8 x i16> %773) #9
  %1024 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1020, <8 x i16> %773) #9
  %1025 = add <4 x i32> %1021, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1026 = add <4 x i32> %1022, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1027 = add <4 x i32> %1023, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1028 = add <4 x i32> %1024, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1029 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1025, i32 %740) #9
  %1030 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1026, i32 %740) #9
  %1031 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1027, i32 %740) #9
  %1032 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1028, i32 %740) #9
  %1033 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1029, <4 x i32> %1030) #9
  store <8 x i16> %1033, <8 x i16>* %310, align 16
  %1034 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1031, <4 x i32> %1032) #9
  store <8 x i16> %1034, <8 x i16>* %313, align 16
  %1035 = shufflevector <8 x i16> %824, <8 x i16> %847, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1036 = shufflevector <8 x i16> %824, <8 x i16> %847, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1037 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1035, <8 x i16> %770) #9
  %1038 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1036, <8 x i16> %770) #9
  %1039 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1035, <8 x i16> %773) #9
  %1040 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1036, <8 x i16> %773) #9
  %1041 = add <4 x i32> %1037, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1042 = add <4 x i32> %1038, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1043 = add <4 x i32> %1039, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1044 = add <4 x i32> %1040, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1045 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1041, i32 %740) #9
  %1046 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1042, i32 %740) #9
  %1047 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1043, i32 %740) #9
  %1048 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1044, i32 %740) #9
  %1049 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1045, <4 x i32> %1046) #9
  store <8 x i16> %1049, <8 x i16>* %328, align 16
  %1050 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1047, <4 x i32> %1048) #9
  store <8 x i16> %1050, <8 x i16>* %330, align 16
  %1051 = shufflevector <8 x i16> %831, <8 x i16> %840, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1052 = shufflevector <8 x i16> %831, <8 x i16> %840, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1053 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1051, <8 x i16> %790) #9
  %1054 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1052, <8 x i16> %790) #9
  %1055 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1051, <8 x i16> %770) #9
  %1056 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1052, <8 x i16> %770) #9
  %1057 = add <4 x i32> %1053, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1058 = add <4 x i32> %1054, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1059 = add <4 x i32> %1055, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1060 = add <4 x i32> %1056, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1061 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1057, i32 %740) #9
  %1062 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1058, i32 %740) #9
  %1063 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1059, i32 %740) #9
  %1064 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1060, i32 %740) #9
  %1065 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1061, <4 x i32> %1062) #9
  store <8 x i16> %1065, <8 x i16>* %343, align 16
  %1066 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1063, <4 x i32> %1064) #9
  store <8 x i16> %1066, <8 x i16>* %346, align 16
  %1067 = shufflevector <8 x i16> %835, <8 x i16> %844, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1068 = shufflevector <8 x i16> %835, <8 x i16> %844, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1069 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1067, <8 x i16> %790) #9
  %1070 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1068, <8 x i16> %790) #9
  %1071 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1067, <8 x i16> %770) #9
  %1072 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1068, <8 x i16> %770) #9
  %1073 = add <4 x i32> %1069, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1074 = add <4 x i32> %1070, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1075 = add <4 x i32> %1071, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1076 = add <4 x i32> %1072, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1077 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1073, i32 %740) #9
  %1078 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1074, i32 %740) #9
  %1079 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1075, i32 %740) #9
  %1080 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1076, i32 %740) #9
  %1081 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1077, <4 x i32> %1078) #9
  store <8 x i16> %1081, <8 x i16>* %361, align 16
  %1082 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1079, <4 x i32> %1080) #9
  store <8 x i16> %1082, <8 x i16>* %363, align 16
  %1083 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %875, <8 x i16> %889) #9
  store <8 x i16> %1083, <8 x i16>* %873, align 16
  %1084 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %875, <8 x i16> %889) #9
  store <8 x i16> %1084, <8 x i16>* %891, align 16
  %1085 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %875, <8 x i16> %888) #9
  store <8 x i16> %1085, <8 x i16>* %877, align 16
  %1086 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %875, <8 x i16> %888) #9
  store <8 x i16> %1086, <8 x i16>* %886, align 16
  %1087 = shufflevector <8 x i16> %893, <8 x i16> %894, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1088 = shufflevector <8 x i16> %893, <8 x i16> %894, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1089 = bitcast <4 x i32> %34 to <8 x i16>
  %1090 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1087, <8 x i16> %1089) #9
  %1091 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1088, <8 x i16> %1089) #9
  %1092 = bitcast <4 x i32> %10 to <8 x i16>
  %1093 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1087, <8 x i16> %1092) #9
  %1094 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1088, <8 x i16> %1092) #9
  %1095 = add <4 x i32> %1090, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1096 = add <4 x i32> %1091, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1097 = add <4 x i32> %1093, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1098 = add <4 x i32> %1094, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1099 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1095, i32 %740) #9
  %1100 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1096, i32 %740) #9
  %1101 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1097, i32 %740) #9
  %1102 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1098, i32 %740) #9
  %1103 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1099, <4 x i32> %1100) #9
  store <8 x i16> %1103, <8 x i16>* %674, align 16
  %1104 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1101, <4 x i32> %1102) #9
  store <8 x i16> %1104, <8 x i16>* %670, align 16
  %1105 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %678, <8 x i16> %683) #9
  store <8 x i16> %1105, <8 x i16>* %571, align 16
  %1106 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %678, <8 x i16> %683) #9
  store <8 x i16> %1106, <8 x i16>* %625, align 16
  %1107 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %912, <8 x i16> %929) #9
  store <8 x i16> %1107, <8 x i16>* %592, align 16
  %1108 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %912, <8 x i16> %929) #9
  store <8 x i16> %1108, <8 x i16>* %604, align 16
  %1109 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %691, <8 x i16> %686) #9
  store <8 x i16> %1109, <8 x i16>* %621, align 16
  %1110 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %691, <8 x i16> %686) #9
  store <8 x i16> %1110, <8 x i16>* %576, align 16
  %1111 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %913, <8 x i16> %930) #9
  store <8 x i16> %1111, <8 x i16>* %609, align 16
  %1112 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %913, <8 x i16> %930) #9
  store <8 x i16> %1112, <8 x i16>* %588, align 16
  %1113 = shufflevector <8 x i16> %936, <8 x i16> %953, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1114 = shufflevector <8 x i16> %936, <8 x i16> %953, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1115 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1113, <8 x i16> %898) #9
  %1116 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1114, <8 x i16> %898) #9
  %1117 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1113, <8 x i16> %901) #9
  %1118 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1114, <8 x i16> %901) #9
  %1119 = add <4 x i32> %1115, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1120 = add <4 x i32> %1116, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1121 = add <4 x i32> %1117, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1122 = add <4 x i32> %1118, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1123 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1119, i32 %740) #9
  %1124 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1120, i32 %740) #9
  %1125 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1121, i32 %740) #9
  %1126 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1122, i32 %740) #9
  %1127 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1123, <4 x i32> %1124) #9
  store <8 x i16> %1127, <8 x i16>* %442, align 16
  %1128 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1125, <4 x i32> %1126) #9
  store <8 x i16> %1128, <8 x i16>* %445, align 16
  %1129 = shufflevector <8 x i16> %934, <8 x i16> %951, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1130 = shufflevector <8 x i16> %934, <8 x i16> %951, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1131 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1129, <8 x i16> %898) #9
  %1132 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1130, <8 x i16> %898) #9
  %1133 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1129, <8 x i16> %901) #9
  %1134 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1130, <8 x i16> %901) #9
  %1135 = add <4 x i32> %1131, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1136 = add <4 x i32> %1132, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1137 = add <4 x i32> %1133, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1138 = add <4 x i32> %1134, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1139 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1135, i32 %740) #9
  %1140 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1136, i32 %740) #9
  %1141 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1137, i32 %740) #9
  %1142 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1138, i32 %740) #9
  %1143 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1139, <4 x i32> %1140) #9
  store <8 x i16> %1143, <8 x i16>* %460, align 16
  %1144 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1141, <4 x i32> %1142) #9
  store <8 x i16> %1144, <8 x i16>* %462, align 16
  %1145 = shufflevector <8 x i16> %939, <8 x i16> %946, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1146 = shufflevector <8 x i16> %939, <8 x i16> %946, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1147 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1145, <8 x i16> %916) #9
  %1148 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1146, <8 x i16> %916) #9
  %1149 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1145, <8 x i16> %898) #9
  %1150 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1146, <8 x i16> %898) #9
  %1151 = add <4 x i32> %1147, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1152 = add <4 x i32> %1148, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1153 = add <4 x i32> %1149, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1154 = add <4 x i32> %1150, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1155 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1151, i32 %740) #9
  %1156 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1152, i32 %740) #9
  %1157 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1153, i32 %740) #9
  %1158 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1154, i32 %740) #9
  %1159 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1155, <4 x i32> %1156) #9
  store <8 x i16> %1159, <8 x i16>* %475, align 16
  %1160 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1157, <4 x i32> %1158) #9
  store <8 x i16> %1160, <8 x i16>* %478, align 16
  %1161 = shufflevector <8 x i16> %941, <8 x i16> %948, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1162 = shufflevector <8 x i16> %941, <8 x i16> %948, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1163 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1161, <8 x i16> %916) #9
  %1164 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1162, <8 x i16> %916) #9
  %1165 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1161, <8 x i16> %898) #9
  %1166 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1162, <8 x i16> %898) #9
  %1167 = add <4 x i32> %1163, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1168 = add <4 x i32> %1164, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1169 = add <4 x i32> %1165, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1170 = add <4 x i32> %1166, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1171 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1167, i32 %740) #9
  %1172 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1168, i32 %740) #9
  %1173 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1169, i32 %740) #9
  %1174 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1170, i32 %740) #9
  %1175 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1171, <4 x i32> %1172) #9
  store <8 x i16> %1175, <8 x i16>* %493, align 16
  %1176 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1173, <4 x i32> %1174) #9
  store <8 x i16> %1176, <8 x i16>* %495, align 16
  %1177 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %807, <8 x i16> %816) #9
  store <8 x i16> %1177, <8 x i16>* %147, align 16
  %1178 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %807, <8 x i16> %816) #9
  store <8 x i16> %1178, <8 x i16>* %262, align 16
  %1179 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %811, <8 x i16> %820) #9
  store <8 x i16> %1179, <8 x i16>* %164, align 16
  %1180 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %811, <8 x i16> %820) #9
  store <8 x i16> %1180, <8 x i16>* %244, align 16
  %1181 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %969, <8 x i16> %1017) #9
  store <8 x i16> %1181, <8 x i16>* %179, align 16
  %1182 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %969, <8 x i16> %1017) #9
  store <8 x i16> %1182, <8 x i16>* %229, align 16
  %1183 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %985, <8 x i16> %1001) #9
  store <8 x i16> %1183, <8 x i16>* %196, align 16
  %1184 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %985, <8 x i16> %1001) #9
  store <8 x i16> %1184, <8 x i16>* %211, align 16
  %1185 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %832, <8 x i16> %823) #9
  store <8 x i16> %1185, <8 x i16>* %277, align 16
  %1186 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %832, <8 x i16> %823) #9
  store <8 x i16> %1186, <8 x i16>* %394, align 16
  %1187 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %836, <8 x i16> %827) #9
  store <8 x i16> %1187, <8 x i16>* %295, align 16
  %1188 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %836, <8 x i16> %827) #9
  store <8 x i16> %1188, <8 x i16>* %376, align 16
  %1189 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1081, <8 x i16> %1033) #9
  store <8 x i16> %1189, <8 x i16>* %310, align 16
  %1190 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1081, <8 x i16> %1033) #9
  store <8 x i16> %1190, <8 x i16>* %361, align 16
  %1191 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1065, <8 x i16> %1049) #9
  store <8 x i16> %1191, <8 x i16>* %328, align 16
  %1192 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1065, <8 x i16> %1049) #9
  store <8 x i16> %1192, <8 x i16>* %343, align 16
  %1193 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %839, <8 x i16> %848) #9
  store <8 x i16> %1193, <8 x i16>* %396, align 16
  %1194 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %839, <8 x i16> %848) #9
  store <8 x i16> %1194, <8 x i16>* %280, align 16
  %1195 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %843, <8 x i16> %852) #9
  store <8 x i16> %1195, <8 x i16>* %379, align 16
  %1196 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %843, <8 x i16> %852) #9
  store <8 x i16> %1196, <8 x i16>* %297, align 16
  %1197 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1082, <8 x i16> %1034) #9
  store <8 x i16> %1197, <8 x i16>* %363, align 16
  %1198 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1082, <8 x i16> %1034) #9
  store <8 x i16> %1198, <8 x i16>* %313, align 16
  %1199 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1066, <8 x i16> %1050) #9
  store <8 x i16> %1199, <8 x i16>* %346, align 16
  %1200 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1066, <8 x i16> %1050) #9
  store <8 x i16> %1200, <8 x i16>* %330, align 16
  %1201 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %864, <8 x i16> %855) #9
  store <8 x i16> %1201, <8 x i16>* %264, align 16
  %1202 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %864, <8 x i16> %855) #9
  store <8 x i16> %1202, <8 x i16>* %150, align 16
  %1203 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %868, <8 x i16> %859) #9
  store <8 x i16> %1203, <8 x i16>* %247, align 16
  %1204 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %868, <8 x i16> %859) #9
  store <8 x i16> %1204, <8 x i16>* %166, align 16
  %1205 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %970, <8 x i16> %1018) #9
  store <8 x i16> %1205, <8 x i16>* %231, align 16
  %1206 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %970, <8 x i16> %1018) #9
  store <8 x i16> %1206, <8 x i16>* %182, align 16
  %1207 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %986, <8 x i16> %1002) #9
  store <8 x i16> %1207, <8 x i16>* %214, align 16
  %1208 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %986, <8 x i16> %1002) #9
  store <8 x i16> %1208, <8 x i16>* %198, align 16
  %1209 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1083, <8 x i16> %895) #9
  store <8 x i16> %1209, <8 x i16>* %873, align 16
  %1210 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1083, <8 x i16> %895) #9
  store <8 x i16> %1210, <8 x i16>* %658, align 16
  %1211 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1085, <8 x i16> %1104) #9
  store <8 x i16> %1211, <8 x i16>* %877, align 16
  %1212 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1085, <8 x i16> %1104) #9
  store <8 x i16> %1212, <8 x i16>* %670, align 16
  %1213 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1086, <8 x i16> %1103) #9
  store <8 x i16> %1213, <8 x i16>* %886, align 16
  %1214 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1086, <8 x i16> %1103) #9
  store <8 x i16> %1214, <8 x i16>* %674, align 16
  %1215 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1084, <8 x i16> %892) #9
  store <8 x i16> %1215, <8 x i16>* %891, align 16
  %1216 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1084, <8 x i16> %892) #9
  store <8 x i16> %1216, <8 x i16>* %653, align 16
  %1217 = shufflevector <8 x i16> %1108, <8 x i16> %1111, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1218 = shufflevector <8 x i16> %1108, <8 x i16> %1111, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1219 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1217, <8 x i16> %1089) #9
  %1220 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1218, <8 x i16> %1089) #9
  %1221 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1217, <8 x i16> %1092) #9
  %1222 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1218, <8 x i16> %1092) #9
  %1223 = add <4 x i32> %1219, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1224 = add <4 x i32> %1220, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1225 = add <4 x i32> %1221, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1226 = add <4 x i32> %1222, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1227 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1223, i32 %740) #9
  %1228 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1224, i32 %740) #9
  %1229 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1225, i32 %740) #9
  %1230 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1226, i32 %740) #9
  %1231 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1227, <4 x i32> %1228) #9
  store <8 x i16> %1231, <8 x i16>* %604, align 16
  %1232 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1229, <4 x i32> %1230) #9
  store <8 x i16> %1232, <8 x i16>* %609, align 16
  %1233 = shufflevector <8 x i16> %1106, <8 x i16> %1109, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1234 = shufflevector <8 x i16> %1106, <8 x i16> %1109, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1235 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1233, <8 x i16> %1089) #9
  %1236 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1234, <8 x i16> %1089) #9
  %1237 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1233, <8 x i16> %1092) #9
  %1238 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1234, <8 x i16> %1092) #9
  %1239 = add <4 x i32> %1235, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1240 = add <4 x i32> %1236, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1241 = add <4 x i32> %1237, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1242 = add <4 x i32> %1238, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1243 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1239, i32 %740) #9
  %1244 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1240, i32 %740) #9
  %1245 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1241, i32 %740) #9
  %1246 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1242, i32 %740) #9
  %1247 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1243, <4 x i32> %1244) #9
  store <8 x i16> %1247, <8 x i16>* %625, align 16
  %1248 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1245, <4 x i32> %1246) #9
  store <8 x i16> %1248, <8 x i16>* %621, align 16
  %1249 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %933, <8 x i16> %940) #9
  store <8 x i16> %1249, <8 x i16>* %409, align 16
  %1250 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %933, <8 x i16> %940) #9
  store <8 x i16> %1250, <8 x i16>* %526, align 16
  %1251 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %935, <8 x i16> %942) #9
  store <8 x i16> %1251, <8 x i16>* %427, align 16
  %1252 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %935, <8 x i16> %942) #9
  store <8 x i16> %1252, <8 x i16>* %508, align 16
  %1253 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1127, <8 x i16> %1175) #9
  store <8 x i16> %1253, <8 x i16>* %442, align 16
  %1254 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1127, <8 x i16> %1175) #9
  store <8 x i16> %1254, <8 x i16>* %493, align 16
  %1255 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1143, <8 x i16> %1159) #9
  store <8 x i16> %1255, <8 x i16>* %460, align 16
  %1256 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1143, <8 x i16> %1159) #9
  store <8 x i16> %1256, <8 x i16>* %475, align 16
  %1257 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %952, <8 x i16> %945) #9
  store <8 x i16> %1257, <8 x i16>* %528, align 16
  %1258 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %952, <8 x i16> %945) #9
  store <8 x i16> %1258, <8 x i16>* %412, align 16
  %1259 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %954, <8 x i16> %947) #9
  store <8 x i16> %1259, <8 x i16>* %511, align 16
  %1260 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %954, <8 x i16> %947) #9
  store <8 x i16> %1260, <8 x i16>* %429, align 16
  %1261 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1128, <8 x i16> %1176) #9
  store <8 x i16> %1261, <8 x i16>* %495, align 16
  %1262 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1128, <8 x i16> %1176) #9
  store <8 x i16> %1262, <8 x i16>* %445, align 16
  %1263 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1144, <8 x i16> %1160) #9
  store <8 x i16> %1263, <8 x i16>* %478, align 16
  %1264 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1144, <8 x i16> %1160) #9
  store <8 x i16> %1264, <8 x i16>* %462, align 16
  %1265 = shufflevector <8 x i16> %1184, <8 x i16> %1207, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1266 = shufflevector <8 x i16> %1184, <8 x i16> %1207, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1267 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1265, <8 x i16> %898) #9
  %1268 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1266, <8 x i16> %898) #9
  %1269 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1265, <8 x i16> %901) #9
  %1270 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1266, <8 x i16> %901) #9
  %1271 = add <4 x i32> %1267, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1272 = add <4 x i32> %1268, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1273 = add <4 x i32> %1269, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1274 = add <4 x i32> %1270, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1275 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1271, i32 %740) #9
  %1276 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1272, i32 %740) #9
  %1277 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1273, i32 %740) #9
  %1278 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1274, i32 %740) #9
  %1279 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1275, <4 x i32> %1276) #9
  store <8 x i16> %1279, <8 x i16>* %211, align 16
  %1280 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1277, <4 x i32> %1278) #9
  store <8 x i16> %1280, <8 x i16>* %214, align 16
  %1281 = shufflevector <8 x i16> %1182, <8 x i16> %1205, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1282 = shufflevector <8 x i16> %1182, <8 x i16> %1205, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1283 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1281, <8 x i16> %898) #9
  %1284 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1282, <8 x i16> %898) #9
  %1285 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1281, <8 x i16> %901) #9
  %1286 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1282, <8 x i16> %901) #9
  %1287 = add <4 x i32> %1283, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1288 = add <4 x i32> %1284, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1289 = add <4 x i32> %1285, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1290 = add <4 x i32> %1286, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1291 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1287, i32 %740) #9
  %1292 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1288, i32 %740) #9
  %1293 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1289, i32 %740) #9
  %1294 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1290, i32 %740) #9
  %1295 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1291, <4 x i32> %1292) #9
  store <8 x i16> %1295, <8 x i16>* %229, align 16
  %1296 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1293, <4 x i32> %1294) #9
  store <8 x i16> %1296, <8 x i16>* %231, align 16
  %1297 = shufflevector <8 x i16> %1180, <8 x i16> %1203, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1298 = shufflevector <8 x i16> %1180, <8 x i16> %1203, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1299 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1297, <8 x i16> %898) #9
  %1300 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1298, <8 x i16> %898) #9
  %1301 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1297, <8 x i16> %901) #9
  %1302 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1298, <8 x i16> %901) #9
  %1303 = add <4 x i32> %1299, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1304 = add <4 x i32> %1300, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1305 = add <4 x i32> %1301, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1306 = add <4 x i32> %1302, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1307 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1303, i32 %740) #9
  %1308 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1304, i32 %740) #9
  %1309 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1305, i32 %740) #9
  %1310 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1306, i32 %740) #9
  %1311 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1307, <4 x i32> %1308) #9
  store <8 x i16> %1311, <8 x i16>* %244, align 16
  %1312 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1309, <4 x i32> %1310) #9
  store <8 x i16> %1312, <8 x i16>* %247, align 16
  %1313 = shufflevector <8 x i16> %1178, <8 x i16> %1201, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1314 = shufflevector <8 x i16> %1178, <8 x i16> %1201, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1315 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1313, <8 x i16> %898) #9
  %1316 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1314, <8 x i16> %898) #9
  %1317 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1313, <8 x i16> %901) #9
  %1318 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1314, <8 x i16> %901) #9
  %1319 = add <4 x i32> %1315, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1320 = add <4 x i32> %1316, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1321 = add <4 x i32> %1317, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1322 = add <4 x i32> %1318, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1323 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1319, i32 %740) #9
  %1324 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1320, i32 %740) #9
  %1325 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1321, i32 %740) #9
  %1326 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1322, i32 %740) #9
  %1327 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1323, <4 x i32> %1324) #9
  store <8 x i16> %1327, <8 x i16>* %262, align 16
  %1328 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1325, <4 x i32> %1326) #9
  store <8 x i16> %1328, <8 x i16>* %264, align 16
  %1329 = shufflevector <8 x i16> %1185, <8 x i16> %1194, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1330 = shufflevector <8 x i16> %1185, <8 x i16> %1194, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1331 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1329, <8 x i16> %916) #9
  %1332 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1330, <8 x i16> %916) #9
  %1333 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1329, <8 x i16> %898) #9
  %1334 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1330, <8 x i16> %898) #9
  %1335 = add <4 x i32> %1331, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1336 = add <4 x i32> %1332, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1337 = add <4 x i32> %1333, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1338 = add <4 x i32> %1334, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1339 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1335, i32 %740) #9
  %1340 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1336, i32 %740) #9
  %1341 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1337, i32 %740) #9
  %1342 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1338, i32 %740) #9
  %1343 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1339, <4 x i32> %1340) #9
  store <8 x i16> %1343, <8 x i16>* %277, align 16
  %1344 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1341, <4 x i32> %1342) #9
  store <8 x i16> %1344, <8 x i16>* %280, align 16
  %1345 = shufflevector <8 x i16> %1187, <8 x i16> %1196, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1346 = shufflevector <8 x i16> %1187, <8 x i16> %1196, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1347 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1345, <8 x i16> %916) #9
  %1348 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1346, <8 x i16> %916) #9
  %1349 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1345, <8 x i16> %898) #9
  %1350 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1346, <8 x i16> %898) #9
  %1351 = add <4 x i32> %1347, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1352 = add <4 x i32> %1348, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1353 = add <4 x i32> %1349, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1354 = add <4 x i32> %1350, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1355 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1351, i32 %740) #9
  %1356 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1352, i32 %740) #9
  %1357 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1353, i32 %740) #9
  %1358 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1354, i32 %740) #9
  %1359 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1355, <4 x i32> %1356) #9
  store <8 x i16> %1359, <8 x i16>* %295, align 16
  %1360 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1357, <4 x i32> %1358) #9
  store <8 x i16> %1360, <8 x i16>* %297, align 16
  %1361 = shufflevector <8 x i16> %1189, <8 x i16> %1198, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1362 = shufflevector <8 x i16> %1189, <8 x i16> %1198, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1363 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1361, <8 x i16> %916) #9
  %1364 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1362, <8 x i16> %916) #9
  %1365 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1361, <8 x i16> %898) #9
  %1366 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1362, <8 x i16> %898) #9
  %1367 = add <4 x i32> %1363, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1368 = add <4 x i32> %1364, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1369 = add <4 x i32> %1365, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1370 = add <4 x i32> %1366, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1371 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1367, i32 %740) #9
  %1372 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1368, i32 %740) #9
  %1373 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1369, i32 %740) #9
  %1374 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1370, i32 %740) #9
  %1375 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1371, <4 x i32> %1372) #9
  store <8 x i16> %1375, <8 x i16>* %310, align 16
  %1376 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1373, <4 x i32> %1374) #9
  store <8 x i16> %1376, <8 x i16>* %313, align 16
  %1377 = shufflevector <8 x i16> %1191, <8 x i16> %1200, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1378 = shufflevector <8 x i16> %1191, <8 x i16> %1200, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1379 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1377, <8 x i16> %916) #9
  %1380 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1378, <8 x i16> %916) #9
  %1381 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1377, <8 x i16> %898) #9
  %1382 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1378, <8 x i16> %898) #9
  %1383 = add <4 x i32> %1379, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1384 = add <4 x i32> %1380, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1385 = add <4 x i32> %1381, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1386 = add <4 x i32> %1382, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1387 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1383, i32 %740) #9
  %1388 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1384, i32 %740) #9
  %1389 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1385, i32 %740) #9
  %1390 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1386, i32 %740) #9
  %1391 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1387, <4 x i32> %1388) #9
  store <8 x i16> %1391, <8 x i16>* %328, align 16
  %1392 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1389, <4 x i32> %1390) #9
  store <8 x i16> %1392, <8 x i16>* %330, align 16
  %1393 = load <8 x i16>, <8 x i16>* %873, align 16
  %1394 = load <8 x i16>, <8 x i16>* %576, align 16
  %1395 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1393, <8 x i16> %1394) #9
  store <8 x i16> %1395, <8 x i16>* %873, align 16
  %1396 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1393, <8 x i16> %1394) #9
  store <8 x i16> %1396, <8 x i16>* %576, align 16
  %1397 = load <8 x i16>, <8 x i16>* %877, align 16
  %1398 = load <8 x i16>, <8 x i16>* %588, align 16
  %1399 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1397, <8 x i16> %1398) #9
  store <8 x i16> %1399, <8 x i16>* %877, align 16
  %1400 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1397, <8 x i16> %1398) #9
  store <8 x i16> %1400, <8 x i16>* %588, align 16
  %1401 = load <8 x i16>, <8 x i16>* %886, align 16
  %1402 = load <8 x i16>, <8 x i16>* %609, align 16
  %1403 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1401, <8 x i16> %1402) #9
  store <8 x i16> %1403, <8 x i16>* %886, align 16
  %1404 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1401, <8 x i16> %1402) #9
  store <8 x i16> %1404, <8 x i16>* %609, align 16
  %1405 = load <8 x i16>, <8 x i16>* %891, align 16
  %1406 = load <8 x i16>, <8 x i16>* %621, align 16
  %1407 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1405, <8 x i16> %1406) #9
  store <8 x i16> %1407, <8 x i16>* %891, align 16
  %1408 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1405, <8 x i16> %1406) #9
  store <8 x i16> %1408, <8 x i16>* %621, align 16
  %1409 = load <8 x i16>, <8 x i16>* %653, align 16
  %1410 = load <8 x i16>, <8 x i16>* %625, align 16
  %1411 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1409, <8 x i16> %1410) #9
  store <8 x i16> %1411, <8 x i16>* %653, align 16
  %1412 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1409, <8 x i16> %1410) #9
  store <8 x i16> %1412, <8 x i16>* %625, align 16
  %1413 = load <8 x i16>, <8 x i16>* %674, align 16
  %1414 = load <8 x i16>, <8 x i16>* %604, align 16
  %1415 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1413, <8 x i16> %1414) #9
  store <8 x i16> %1415, <8 x i16>* %674, align 16
  %1416 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1413, <8 x i16> %1414) #9
  store <8 x i16> %1416, <8 x i16>* %604, align 16
  %1417 = load <8 x i16>, <8 x i16>* %670, align 16
  %1418 = load <8 x i16>, <8 x i16>* %592, align 16
  %1419 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1417, <8 x i16> %1418) #9
  store <8 x i16> %1419, <8 x i16>* %670, align 16
  %1420 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1417, <8 x i16> %1418) #9
  store <8 x i16> %1420, <8 x i16>* %592, align 16
  %1421 = load <8 x i16>, <8 x i16>* %658, align 16
  %1422 = load <8 x i16>, <8 x i16>* %571, align 16
  %1423 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1421, <8 x i16> %1422) #9
  store <8 x i16> %1423, <8 x i16>* %658, align 16
  %1424 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1421, <8 x i16> %1422) #9
  store <8 x i16> %1424, <8 x i16>* %571, align 16
  %1425 = shufflevector <8 x i16> %1256, <8 x i16> %1263, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1426 = shufflevector <8 x i16> %1256, <8 x i16> %1263, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1427 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1425, <8 x i16> %1089) #9
  %1428 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1426, <8 x i16> %1089) #9
  %1429 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1425, <8 x i16> %1092) #9
  %1430 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1426, <8 x i16> %1092) #9
  %1431 = add <4 x i32> %1427, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1432 = add <4 x i32> %1428, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1433 = add <4 x i32> %1429, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1434 = add <4 x i32> %1430, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1435 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1431, i32 %740) #9
  %1436 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1432, i32 %740) #9
  %1437 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1433, i32 %740) #9
  %1438 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1434, i32 %740) #9
  %1439 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1435, <4 x i32> %1436) #9
  store <8 x i16> %1439, <8 x i16>* %475, align 16
  %1440 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1437, <4 x i32> %1438) #9
  store <8 x i16> %1440, <8 x i16>* %478, align 16
  %1441 = shufflevector <8 x i16> %1254, <8 x i16> %1261, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1442 = shufflevector <8 x i16> %1254, <8 x i16> %1261, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1443 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1441, <8 x i16> %1089) #9
  %1444 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1442, <8 x i16> %1089) #9
  %1445 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1441, <8 x i16> %1092) #9
  %1446 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1442, <8 x i16> %1092) #9
  %1447 = add <4 x i32> %1443, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1448 = add <4 x i32> %1444, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1449 = add <4 x i32> %1445, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1450 = add <4 x i32> %1446, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1451 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1447, i32 %740) #9
  %1452 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1448, i32 %740) #9
  %1453 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1449, i32 %740) #9
  %1454 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1450, i32 %740) #9
  %1455 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1451, <4 x i32> %1452) #9
  store <8 x i16> %1455, <8 x i16>* %493, align 16
  %1456 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1453, <4 x i32> %1454) #9
  store <8 x i16> %1456, <8 x i16>* %495, align 16
  %1457 = shufflevector <8 x i16> %1252, <8 x i16> %1259, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1458 = shufflevector <8 x i16> %1252, <8 x i16> %1259, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1459 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1457, <8 x i16> %1089) #9
  %1460 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1458, <8 x i16> %1089) #9
  %1461 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1457, <8 x i16> %1092) #9
  %1462 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1458, <8 x i16> %1092) #9
  %1463 = add <4 x i32> %1459, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1464 = add <4 x i32> %1460, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1465 = add <4 x i32> %1461, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1466 = add <4 x i32> %1462, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1467 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1463, i32 %740) #9
  %1468 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1464, i32 %740) #9
  %1469 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1465, i32 %740) #9
  %1470 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1466, i32 %740) #9
  %1471 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1467, <4 x i32> %1468) #9
  store <8 x i16> %1471, <8 x i16>* %508, align 16
  %1472 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1469, <4 x i32> %1470) #9
  store <8 x i16> %1472, <8 x i16>* %511, align 16
  %1473 = shufflevector <8 x i16> %1250, <8 x i16> %1257, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1474 = shufflevector <8 x i16> %1250, <8 x i16> %1257, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1475 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1473, <8 x i16> %1089) #9
  %1476 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1474, <8 x i16> %1089) #9
  %1477 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1473, <8 x i16> %1092) #9
  %1478 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1474, <8 x i16> %1092) #9
  %1479 = add <4 x i32> %1475, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1480 = add <4 x i32> %1476, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1481 = add <4 x i32> %1477, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1482 = add <4 x i32> %1478, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1483 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1479, i32 %740) #9
  %1484 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1480, i32 %740) #9
  %1485 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1481, i32 %740) #9
  %1486 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1482, i32 %740) #9
  %1487 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1483, <4 x i32> %1484) #9
  store <8 x i16> %1487, <8 x i16>* %526, align 16
  %1488 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1485, <4 x i32> %1486) #9
  %1489 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1177, <8 x i16> %1186) #9
  store <8 x i16> %1489, <8 x i16>* %147, align 16
  %1490 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1177, <8 x i16> %1186) #9
  store <8 x i16> %1490, <8 x i16>* %394, align 16
  %1491 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1179, <8 x i16> %1188) #9
  store <8 x i16> %1491, <8 x i16>* %164, align 16
  %1492 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1179, <8 x i16> %1188) #9
  store <8 x i16> %1492, <8 x i16>* %376, align 16
  %1493 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1181, <8 x i16> %1190) #9
  store <8 x i16> %1493, <8 x i16>* %179, align 16
  %1494 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1181, <8 x i16> %1190) #9
  store <8 x i16> %1494, <8 x i16>* %361, align 16
  %1495 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1183, <8 x i16> %1192) #9
  store <8 x i16> %1495, <8 x i16>* %196, align 16
  %1496 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1183, <8 x i16> %1192) #9
  store <8 x i16> %1496, <8 x i16>* %343, align 16
  %1497 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1279, <8 x i16> %1391) #9
  store <8 x i16> %1497, <8 x i16>* %211, align 16
  %1498 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1279, <8 x i16> %1391) #9
  store <8 x i16> %1498, <8 x i16>* %328, align 16
  %1499 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1295, <8 x i16> %1375) #9
  store <8 x i16> %1499, <8 x i16>* %229, align 16
  %1500 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1295, <8 x i16> %1375) #9
  store <8 x i16> %1500, <8 x i16>* %310, align 16
  %1501 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1311, <8 x i16> %1359) #9
  store <8 x i16> %1501, <8 x i16>* %244, align 16
  %1502 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1311, <8 x i16> %1359) #9
  store <8 x i16> %1502, <8 x i16>* %295, align 16
  %1503 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1327, <8 x i16> %1343) #9
  store <8 x i16> %1503, <8 x i16>* %262, align 16
  %1504 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1327, <8 x i16> %1343) #9
  store <8 x i16> %1504, <8 x i16>* %277, align 16
  %1505 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1202, <8 x i16> %1193) #9
  store <8 x i16> %1505, <8 x i16>* %396, align 16
  %1506 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1202, <8 x i16> %1193) #9
  store <8 x i16> %1506, <8 x i16>* %150, align 16
  %1507 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1204, <8 x i16> %1195) #9
  store <8 x i16> %1507, <8 x i16>* %379, align 16
  %1508 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1204, <8 x i16> %1195) #9
  store <8 x i16> %1508, <8 x i16>* %166, align 16
  %1509 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1206, <8 x i16> %1197) #9
  store <8 x i16> %1509, <8 x i16>* %363, align 16
  %1510 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1206, <8 x i16> %1197) #9
  store <8 x i16> %1510, <8 x i16>* %182, align 16
  %1511 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1208, <8 x i16> %1199) #9
  store <8 x i16> %1511, <8 x i16>* %346, align 16
  %1512 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1208, <8 x i16> %1199) #9
  store <8 x i16> %1512, <8 x i16>* %198, align 16
  %1513 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1280, <8 x i16> %1392) #9
  store <8 x i16> %1513, <8 x i16>* %330, align 16
  %1514 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1280, <8 x i16> %1392) #9
  store <8 x i16> %1514, <8 x i16>* %214, align 16
  %1515 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1296, <8 x i16> %1376) #9
  store <8 x i16> %1515, <8 x i16>* %313, align 16
  %1516 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1296, <8 x i16> %1376) #9
  store <8 x i16> %1516, <8 x i16>* %231, align 16
  %1517 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1312, <8 x i16> %1360) #9
  store <8 x i16> %1517, <8 x i16>* %297, align 16
  %1518 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1312, <8 x i16> %1360) #9
  store <8 x i16> %1518, <8 x i16>* %247, align 16
  %1519 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1328, <8 x i16> %1344) #9
  %1520 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1328, <8 x i16> %1344) #9
  store <8 x i16> %1520, <8 x i16>* %264, align 16
  %1521 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1395, <8 x i16> %1258) #9
  store <8 x i16> %1521, <8 x i16>* %873, align 16
  %1522 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1395, <8 x i16> %1258) #9
  store <8 x i16> %1522, <8 x i16>* %412, align 16
  %1523 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1399, <8 x i16> %1260) #9
  store <8 x i16> %1523, <8 x i16>* %877, align 16
  %1524 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1399, <8 x i16> %1260) #9
  store <8 x i16> %1524, <8 x i16>* %429, align 16
  %1525 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1403, <8 x i16> %1262) #9
  store <8 x i16> %1525, <8 x i16>* %886, align 16
  %1526 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1403, <8 x i16> %1262) #9
  store <8 x i16> %1526, <8 x i16>* %445, align 16
  %1527 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1407, <8 x i16> %1264) #9
  store <8 x i16> %1527, <8 x i16>* %891, align 16
  %1528 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1407, <8 x i16> %1264) #9
  store <8 x i16> %1528, <8 x i16>* %462, align 16
  %1529 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1411, <8 x i16> %1440) #9
  store <8 x i16> %1529, <8 x i16>* %653, align 16
  %1530 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1411, <8 x i16> %1440) #9
  store <8 x i16> %1530, <8 x i16>* %478, align 16
  %1531 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1415, <8 x i16> %1456) #9
  store <8 x i16> %1531, <8 x i16>* %674, align 16
  %1532 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1415, <8 x i16> %1456) #9
  store <8 x i16> %1532, <8 x i16>* %495, align 16
  %1533 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1419, <8 x i16> %1472) #9
  store <8 x i16> %1533, <8 x i16>* %670, align 16
  %1534 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1419, <8 x i16> %1472) #9
  store <8 x i16> %1534, <8 x i16>* %511, align 16
  %1535 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1423, <8 x i16> %1488) #9
  store <8 x i16> %1535, <8 x i16>* %658, align 16
  %1536 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1423, <8 x i16> %1488) #9
  store <8 x i16> %1536, <8 x i16>* %528, align 16
  %1537 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1424, <8 x i16> %1487) #9
  store <8 x i16> %1537, <8 x i16>* %571, align 16
  %1538 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1424, <8 x i16> %1487) #9
  store <8 x i16> %1538, <8 x i16>* %526, align 16
  %1539 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1420, <8 x i16> %1471) #9
  store <8 x i16> %1539, <8 x i16>* %592, align 16
  %1540 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1420, <8 x i16> %1471) #9
  store <8 x i16> %1540, <8 x i16>* %508, align 16
  %1541 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1416, <8 x i16> %1455) #9
  store <8 x i16> %1541, <8 x i16>* %604, align 16
  %1542 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1416, <8 x i16> %1455) #9
  store <8 x i16> %1542, <8 x i16>* %493, align 16
  %1543 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1412, <8 x i16> %1439) #9
  store <8 x i16> %1543, <8 x i16>* %625, align 16
  %1544 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1412, <8 x i16> %1439) #9
  store <8 x i16> %1544, <8 x i16>* %475, align 16
  %1545 = load <8 x i16>, <8 x i16>* %460, align 16
  %1546 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1408, <8 x i16> %1545) #9
  store <8 x i16> %1546, <8 x i16>* %621, align 16
  %1547 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1408, <8 x i16> %1545) #9
  store <8 x i16> %1547, <8 x i16>* %460, align 16
  %1548 = load <8 x i16>, <8 x i16>* %442, align 16
  %1549 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1404, <8 x i16> %1548) #9
  store <8 x i16> %1549, <8 x i16>* %609, align 16
  %1550 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1404, <8 x i16> %1548) #9
  store <8 x i16> %1550, <8 x i16>* %442, align 16
  %1551 = load <8 x i16>, <8 x i16>* %427, align 16
  %1552 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1400, <8 x i16> %1551) #9
  store <8 x i16> %1552, <8 x i16>* %588, align 16
  %1553 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1400, <8 x i16> %1551) #9
  store <8 x i16> %1553, <8 x i16>* %427, align 16
  %1554 = load <8 x i16>, <8 x i16>* %409, align 16
  %1555 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1396, <8 x i16> %1554) #9
  store <8 x i16> %1555, <8 x i16>* %576, align 16
  %1556 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1396, <8 x i16> %1554) #9
  store <8 x i16> %1556, <8 x i16>* %409, align 16
  %1557 = shufflevector <8 x i16> %1504, <8 x i16> %1519, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1558 = shufflevector <8 x i16> %1504, <8 x i16> %1519, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1559 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1557, <8 x i16> %1089) #9
  %1560 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1558, <8 x i16> %1089) #9
  %1561 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1557, <8 x i16> %1092) #9
  %1562 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1558, <8 x i16> %1092) #9
  %1563 = add <4 x i32> %1559, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1564 = add <4 x i32> %1560, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1565 = add <4 x i32> %1561, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1566 = add <4 x i32> %1562, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1567 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1563, i32 %740) #9
  %1568 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1564, i32 %740) #9
  %1569 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1565, i32 %740) #9
  %1570 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1566, i32 %740) #9
  %1571 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1567, <4 x i32> %1568) #9
  store <8 x i16> %1571, <8 x i16>* %277, align 16
  %1572 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1569, <4 x i32> %1570) #9
  store <8 x i16> %1572, <8 x i16>* %280, align 16
  %1573 = shufflevector <8 x i16> %1502, <8 x i16> %1517, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1574 = shufflevector <8 x i16> %1502, <8 x i16> %1517, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1575 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1573, <8 x i16> %1089) #9
  %1576 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1574, <8 x i16> %1089) #9
  %1577 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1573, <8 x i16> %1092) #9
  %1578 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1574, <8 x i16> %1092) #9
  %1579 = add <4 x i32> %1575, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1580 = add <4 x i32> %1576, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1581 = add <4 x i32> %1577, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1582 = add <4 x i32> %1578, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1583 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1579, i32 %740) #9
  %1584 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1580, i32 %740) #9
  %1585 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1581, i32 %740) #9
  %1586 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1582, i32 %740) #9
  %1587 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1583, <4 x i32> %1584) #9
  store <8 x i16> %1587, <8 x i16>* %295, align 16
  %1588 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1585, <4 x i32> %1586) #9
  store <8 x i16> %1588, <8 x i16>* %297, align 16
  %1589 = shufflevector <8 x i16> %1500, <8 x i16> %1515, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1590 = shufflevector <8 x i16> %1500, <8 x i16> %1515, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1591 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1589, <8 x i16> %1089) #9
  %1592 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1590, <8 x i16> %1089) #9
  %1593 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1589, <8 x i16> %1092) #9
  %1594 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1590, <8 x i16> %1092) #9
  %1595 = add <4 x i32> %1591, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1596 = add <4 x i32> %1592, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1597 = add <4 x i32> %1593, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1598 = add <4 x i32> %1594, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1599 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1595, i32 %740) #9
  %1600 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1596, i32 %740) #9
  %1601 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1597, i32 %740) #9
  %1602 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1598, i32 %740) #9
  %1603 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1599, <4 x i32> %1600) #9
  store <8 x i16> %1603, <8 x i16>* %310, align 16
  %1604 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1601, <4 x i32> %1602) #9
  store <8 x i16> %1604, <8 x i16>* %313, align 16
  %1605 = shufflevector <8 x i16> %1498, <8 x i16> %1513, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1606 = shufflevector <8 x i16> %1498, <8 x i16> %1513, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1607 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1605, <8 x i16> %1089) #9
  %1608 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1606, <8 x i16> %1089) #9
  %1609 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1605, <8 x i16> %1092) #9
  %1610 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1606, <8 x i16> %1092) #9
  %1611 = add <4 x i32> %1607, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1612 = add <4 x i32> %1608, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1613 = add <4 x i32> %1609, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1614 = add <4 x i32> %1610, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1615 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1611, i32 %740) #9
  %1616 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1612, i32 %740) #9
  %1617 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1613, i32 %740) #9
  %1618 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1614, i32 %740) #9
  %1619 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1615, <4 x i32> %1616) #9
  store <8 x i16> %1619, <8 x i16>* %328, align 16
  %1620 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1617, <4 x i32> %1618) #9
  store <8 x i16> %1620, <8 x i16>* %330, align 16
  %1621 = shufflevector <8 x i16> %1496, <8 x i16> %1511, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1622 = shufflevector <8 x i16> %1496, <8 x i16> %1511, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1623 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1621, <8 x i16> %1089) #9
  %1624 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1622, <8 x i16> %1089) #9
  %1625 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1621, <8 x i16> %1092) #9
  %1626 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1622, <8 x i16> %1092) #9
  %1627 = add <4 x i32> %1623, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1628 = add <4 x i32> %1624, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1629 = add <4 x i32> %1625, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1630 = add <4 x i32> %1626, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1631 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1627, i32 %740) #9
  %1632 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1628, i32 %740) #9
  %1633 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1629, i32 %740) #9
  %1634 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1630, i32 %740) #9
  %1635 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1631, <4 x i32> %1632) #9
  store <8 x i16> %1635, <8 x i16>* %343, align 16
  %1636 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1633, <4 x i32> %1634) #9
  store <8 x i16> %1636, <8 x i16>* %346, align 16
  %1637 = shufflevector <8 x i16> %1494, <8 x i16> %1509, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1638 = shufflevector <8 x i16> %1494, <8 x i16> %1509, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1639 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1637, <8 x i16> %1089) #9
  %1640 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1638, <8 x i16> %1089) #9
  %1641 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1637, <8 x i16> %1092) #9
  %1642 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1638, <8 x i16> %1092) #9
  %1643 = add <4 x i32> %1639, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1644 = add <4 x i32> %1640, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1645 = add <4 x i32> %1641, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1646 = add <4 x i32> %1642, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1647 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1643, i32 %740) #9
  %1648 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1644, i32 %740) #9
  %1649 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1645, i32 %740) #9
  %1650 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1646, i32 %740) #9
  %1651 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1647, <4 x i32> %1648) #9
  store <8 x i16> %1651, <8 x i16>* %361, align 16
  %1652 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1649, <4 x i32> %1650) #9
  store <8 x i16> %1652, <8 x i16>* %363, align 16
  %1653 = shufflevector <8 x i16> %1492, <8 x i16> %1507, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1654 = shufflevector <8 x i16> %1492, <8 x i16> %1507, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1655 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1653, <8 x i16> %1089) #9
  %1656 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1654, <8 x i16> %1089) #9
  %1657 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1653, <8 x i16> %1092) #9
  %1658 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1654, <8 x i16> %1092) #9
  %1659 = add <4 x i32> %1655, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1660 = add <4 x i32> %1656, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1661 = add <4 x i32> %1657, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1662 = add <4 x i32> %1658, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1663 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1659, i32 %740) #9
  %1664 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1660, i32 %740) #9
  %1665 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1661, i32 %740) #9
  %1666 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1662, i32 %740) #9
  %1667 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1663, <4 x i32> %1664) #9
  store <8 x i16> %1667, <8 x i16>* %376, align 16
  %1668 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1665, <4 x i32> %1666) #9
  store <8 x i16> %1668, <8 x i16>* %379, align 16
  %1669 = shufflevector <8 x i16> %1490, <8 x i16> %1505, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1670 = shufflevector <8 x i16> %1490, <8 x i16> %1505, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1671 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1669, <8 x i16> %1089) #9
  %1672 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1670, <8 x i16> %1089) #9
  %1673 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1669, <8 x i16> %1092) #9
  %1674 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1670, <8 x i16> %1092) #9
  %1675 = add <4 x i32> %1671, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1676 = add <4 x i32> %1672, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1677 = add <4 x i32> %1673, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1678 = add <4 x i32> %1674, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1679 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1675, i32 %740) #9
  %1680 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1676, i32 %740) #9
  %1681 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1677, i32 %740) #9
  %1682 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1678, i32 %740) #9
  %1683 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1679, <4 x i32> %1680) #9
  store <8 x i16> %1683, <8 x i16>* %394, align 16
  %1684 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1681, <4 x i32> %1682) #9
  store <8 x i16> %1684, <8 x i16>* %396, align 16
  %1685 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1521, <8 x i16> %1506) #9
  %1686 = bitcast <2 x i64>* %1 to <8 x i16>*
  store <8 x i16> %1685, <8 x i16>* %1686, align 16
  %1687 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1521, <8 x i16> %1506) #9
  %1688 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 63
  %1689 = bitcast <2 x i64>* %1688 to <8 x i16>*
  store <8 x i16> %1687, <8 x i16>* %1689, align 16
  %1690 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1523, <8 x i16> %1508) #9
  %1691 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 1
  %1692 = bitcast <2 x i64>* %1691 to <8 x i16>*
  store <8 x i16> %1690, <8 x i16>* %1692, align 16
  %1693 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1523, <8 x i16> %1508) #9
  %1694 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 62
  %1695 = bitcast <2 x i64>* %1694 to <8 x i16>*
  store <8 x i16> %1693, <8 x i16>* %1695, align 16
  %1696 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1525, <8 x i16> %1510) #9
  %1697 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 2
  %1698 = bitcast <2 x i64>* %1697 to <8 x i16>*
  store <8 x i16> %1696, <8 x i16>* %1698, align 16
  %1699 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1525, <8 x i16> %1510) #9
  %1700 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 61
  %1701 = bitcast <2 x i64>* %1700 to <8 x i16>*
  store <8 x i16> %1699, <8 x i16>* %1701, align 16
  %1702 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1527, <8 x i16> %1512) #9
  %1703 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 3
  %1704 = bitcast <2 x i64>* %1703 to <8 x i16>*
  store <8 x i16> %1702, <8 x i16>* %1704, align 16
  %1705 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1527, <8 x i16> %1512) #9
  %1706 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 60
  %1707 = bitcast <2 x i64>* %1706 to <8 x i16>*
  store <8 x i16> %1705, <8 x i16>* %1707, align 16
  %1708 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1529, <8 x i16> %1514) #9
  %1709 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 4
  %1710 = bitcast <2 x i64>* %1709 to <8 x i16>*
  store <8 x i16> %1708, <8 x i16>* %1710, align 16
  %1711 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1529, <8 x i16> %1514) #9
  %1712 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 59
  %1713 = bitcast <2 x i64>* %1712 to <8 x i16>*
  store <8 x i16> %1711, <8 x i16>* %1713, align 16
  %1714 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1531, <8 x i16> %1516) #9
  %1715 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 5
  %1716 = bitcast <2 x i64>* %1715 to <8 x i16>*
  store <8 x i16> %1714, <8 x i16>* %1716, align 16
  %1717 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1531, <8 x i16> %1516) #9
  %1718 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 58
  %1719 = bitcast <2 x i64>* %1718 to <8 x i16>*
  store <8 x i16> %1717, <8 x i16>* %1719, align 16
  %1720 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1533, <8 x i16> %1518) #9
  %1721 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 6
  %1722 = bitcast <2 x i64>* %1721 to <8 x i16>*
  store <8 x i16> %1720, <8 x i16>* %1722, align 16
  %1723 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1533, <8 x i16> %1518) #9
  %1724 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 57
  %1725 = bitcast <2 x i64>* %1724 to <8 x i16>*
  store <8 x i16> %1723, <8 x i16>* %1725, align 16
  %1726 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1535, <8 x i16> %1520) #9
  %1727 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 7
  %1728 = bitcast <2 x i64>* %1727 to <8 x i16>*
  store <8 x i16> %1726, <8 x i16>* %1728, align 16
  %1729 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1535, <8 x i16> %1520) #9
  %1730 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 56
  %1731 = bitcast <2 x i64>* %1730 to <8 x i16>*
  store <8 x i16> %1729, <8 x i16>* %1731, align 16
  %1732 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1537, <8 x i16> %1572) #9
  %1733 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 8
  %1734 = bitcast <2 x i64>* %1733 to <8 x i16>*
  store <8 x i16> %1732, <8 x i16>* %1734, align 16
  %1735 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1537, <8 x i16> %1572) #9
  %1736 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 55
  %1737 = bitcast <2 x i64>* %1736 to <8 x i16>*
  store <8 x i16> %1735, <8 x i16>* %1737, align 16
  %1738 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1539, <8 x i16> %1588) #9
  %1739 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 9
  %1740 = bitcast <2 x i64>* %1739 to <8 x i16>*
  store <8 x i16> %1738, <8 x i16>* %1740, align 16
  %1741 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1539, <8 x i16> %1588) #9
  %1742 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 54
  %1743 = bitcast <2 x i64>* %1742 to <8 x i16>*
  store <8 x i16> %1741, <8 x i16>* %1743, align 16
  %1744 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1541, <8 x i16> %1604) #9
  %1745 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 10
  %1746 = bitcast <2 x i64>* %1745 to <8 x i16>*
  store <8 x i16> %1744, <8 x i16>* %1746, align 16
  %1747 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1541, <8 x i16> %1604) #9
  %1748 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 53
  %1749 = bitcast <2 x i64>* %1748 to <8 x i16>*
  store <8 x i16> %1747, <8 x i16>* %1749, align 16
  %1750 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1543, <8 x i16> %1620) #9
  %1751 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 11
  %1752 = bitcast <2 x i64>* %1751 to <8 x i16>*
  store <8 x i16> %1750, <8 x i16>* %1752, align 16
  %1753 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1543, <8 x i16> %1620) #9
  %1754 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 52
  %1755 = bitcast <2 x i64>* %1754 to <8 x i16>*
  store <8 x i16> %1753, <8 x i16>* %1755, align 16
  %1756 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1546, <8 x i16> %1636) #9
  %1757 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 12
  %1758 = bitcast <2 x i64>* %1757 to <8 x i16>*
  store <8 x i16> %1756, <8 x i16>* %1758, align 16
  %1759 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1546, <8 x i16> %1636) #9
  %1760 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 51
  %1761 = bitcast <2 x i64>* %1760 to <8 x i16>*
  store <8 x i16> %1759, <8 x i16>* %1761, align 16
  %1762 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1549, <8 x i16> %1652) #9
  %1763 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 13
  %1764 = bitcast <2 x i64>* %1763 to <8 x i16>*
  store <8 x i16> %1762, <8 x i16>* %1764, align 16
  %1765 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1549, <8 x i16> %1652) #9
  %1766 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 50
  %1767 = bitcast <2 x i64>* %1766 to <8 x i16>*
  store <8 x i16> %1765, <8 x i16>* %1767, align 16
  %1768 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1552, <8 x i16> %1668) #9
  %1769 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 14
  %1770 = bitcast <2 x i64>* %1769 to <8 x i16>*
  store <8 x i16> %1768, <8 x i16>* %1770, align 16
  %1771 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1552, <8 x i16> %1668) #9
  %1772 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 49
  %1773 = bitcast <2 x i64>* %1772 to <8 x i16>*
  store <8 x i16> %1771, <8 x i16>* %1773, align 16
  %1774 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1555, <8 x i16> %1684) #9
  %1775 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 15
  %1776 = bitcast <2 x i64>* %1775 to <8 x i16>*
  store <8 x i16> %1774, <8 x i16>* %1776, align 16
  %1777 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1555, <8 x i16> %1684) #9
  %1778 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 48
  %1779 = bitcast <2 x i64>* %1778 to <8 x i16>*
  store <8 x i16> %1777, <8 x i16>* %1779, align 16
  %1780 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1556, <8 x i16> %1683) #9
  %1781 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 16
  %1782 = bitcast <2 x i64>* %1781 to <8 x i16>*
  store <8 x i16> %1780, <8 x i16>* %1782, align 16
  %1783 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1556, <8 x i16> %1683) #9
  %1784 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 47
  %1785 = bitcast <2 x i64>* %1784 to <8 x i16>*
  store <8 x i16> %1783, <8 x i16>* %1785, align 16
  %1786 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1553, <8 x i16> %1667) #9
  %1787 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 17
  %1788 = bitcast <2 x i64>* %1787 to <8 x i16>*
  store <8 x i16> %1786, <8 x i16>* %1788, align 16
  %1789 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1553, <8 x i16> %1667) #9
  %1790 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 46
  %1791 = bitcast <2 x i64>* %1790 to <8 x i16>*
  store <8 x i16> %1789, <8 x i16>* %1791, align 16
  %1792 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1550, <8 x i16> %1651) #9
  %1793 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 18
  %1794 = bitcast <2 x i64>* %1793 to <8 x i16>*
  store <8 x i16> %1792, <8 x i16>* %1794, align 16
  %1795 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1550, <8 x i16> %1651) #9
  %1796 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 45
  %1797 = bitcast <2 x i64>* %1796 to <8 x i16>*
  store <8 x i16> %1795, <8 x i16>* %1797, align 16
  %1798 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1547, <8 x i16> %1635) #9
  %1799 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 19
  %1800 = bitcast <2 x i64>* %1799 to <8 x i16>*
  store <8 x i16> %1798, <8 x i16>* %1800, align 16
  %1801 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1547, <8 x i16> %1635) #9
  %1802 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 44
  %1803 = bitcast <2 x i64>* %1802 to <8 x i16>*
  store <8 x i16> %1801, <8 x i16>* %1803, align 16
  %1804 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1544, <8 x i16> %1619) #9
  %1805 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 20
  %1806 = bitcast <2 x i64>* %1805 to <8 x i16>*
  store <8 x i16> %1804, <8 x i16>* %1806, align 16
  %1807 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1544, <8 x i16> %1619) #9
  %1808 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 43
  %1809 = bitcast <2 x i64>* %1808 to <8 x i16>*
  store <8 x i16> %1807, <8 x i16>* %1809, align 16
  %1810 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1542, <8 x i16> %1603) #9
  %1811 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 21
  %1812 = bitcast <2 x i64>* %1811 to <8 x i16>*
  store <8 x i16> %1810, <8 x i16>* %1812, align 16
  %1813 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1542, <8 x i16> %1603) #9
  %1814 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 42
  %1815 = bitcast <2 x i64>* %1814 to <8 x i16>*
  store <8 x i16> %1813, <8 x i16>* %1815, align 16
  %1816 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1540, <8 x i16> %1587) #9
  %1817 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 22
  %1818 = bitcast <2 x i64>* %1817 to <8 x i16>*
  store <8 x i16> %1816, <8 x i16>* %1818, align 16
  %1819 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1540, <8 x i16> %1587) #9
  %1820 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 41
  %1821 = bitcast <2 x i64>* %1820 to <8 x i16>*
  store <8 x i16> %1819, <8 x i16>* %1821, align 16
  %1822 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1538, <8 x i16> %1571) #9
  %1823 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 23
  %1824 = bitcast <2 x i64>* %1823 to <8 x i16>*
  store <8 x i16> %1822, <8 x i16>* %1824, align 16
  %1825 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1538, <8 x i16> %1571) #9
  %1826 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 40
  %1827 = bitcast <2 x i64>* %1826 to <8 x i16>*
  store <8 x i16> %1825, <8 x i16>* %1827, align 16
  %1828 = load <8 x i16>, <8 x i16>* %262, align 16
  %1829 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1536, <8 x i16> %1828) #9
  %1830 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 24
  %1831 = bitcast <2 x i64>* %1830 to <8 x i16>*
  store <8 x i16> %1829, <8 x i16>* %1831, align 16
  %1832 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1536, <8 x i16> %1828) #9
  %1833 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 39
  %1834 = bitcast <2 x i64>* %1833 to <8 x i16>*
  store <8 x i16> %1832, <8 x i16>* %1834, align 16
  %1835 = load <8 x i16>, <8 x i16>* %244, align 16
  %1836 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1534, <8 x i16> %1835) #9
  %1837 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 25
  %1838 = bitcast <2 x i64>* %1837 to <8 x i16>*
  store <8 x i16> %1836, <8 x i16>* %1838, align 16
  %1839 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1534, <8 x i16> %1835) #9
  %1840 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 38
  %1841 = bitcast <2 x i64>* %1840 to <8 x i16>*
  store <8 x i16> %1839, <8 x i16>* %1841, align 16
  %1842 = load <8 x i16>, <8 x i16>* %229, align 16
  %1843 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1532, <8 x i16> %1842) #9
  %1844 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 26
  %1845 = bitcast <2 x i64>* %1844 to <8 x i16>*
  store <8 x i16> %1843, <8 x i16>* %1845, align 16
  %1846 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1532, <8 x i16> %1842) #9
  %1847 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 37
  %1848 = bitcast <2 x i64>* %1847 to <8 x i16>*
  store <8 x i16> %1846, <8 x i16>* %1848, align 16
  %1849 = load <8 x i16>, <8 x i16>* %211, align 16
  %1850 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1530, <8 x i16> %1849) #9
  %1851 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 27
  %1852 = bitcast <2 x i64>* %1851 to <8 x i16>*
  store <8 x i16> %1850, <8 x i16>* %1852, align 16
  %1853 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1530, <8 x i16> %1849) #9
  %1854 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 36
  %1855 = bitcast <2 x i64>* %1854 to <8 x i16>*
  store <8 x i16> %1853, <8 x i16>* %1855, align 16
  %1856 = load <8 x i16>, <8 x i16>* %196, align 16
  %1857 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1528, <8 x i16> %1856) #9
  %1858 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 28
  %1859 = bitcast <2 x i64>* %1858 to <8 x i16>*
  store <8 x i16> %1857, <8 x i16>* %1859, align 16
  %1860 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1528, <8 x i16> %1856) #9
  %1861 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 35
  %1862 = bitcast <2 x i64>* %1861 to <8 x i16>*
  store <8 x i16> %1860, <8 x i16>* %1862, align 16
  %1863 = load <8 x i16>, <8 x i16>* %445, align 16
  %1864 = load <8 x i16>, <8 x i16>* %179, align 16
  %1865 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1863, <8 x i16> %1864) #9
  %1866 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 29
  %1867 = bitcast <2 x i64>* %1866 to <8 x i16>*
  store <8 x i16> %1865, <8 x i16>* %1867, align 16
  %1868 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1863, <8 x i16> %1864) #9
  %1869 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 34
  %1870 = bitcast <2 x i64>* %1869 to <8 x i16>*
  store <8 x i16> %1868, <8 x i16>* %1870, align 16
  %1871 = load <8 x i16>, <8 x i16>* %429, align 16
  %1872 = load <8 x i16>, <8 x i16>* %164, align 16
  %1873 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1871, <8 x i16> %1872) #9
  %1874 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 30
  %1875 = bitcast <2 x i64>* %1874 to <8 x i16>*
  store <8 x i16> %1873, <8 x i16>* %1875, align 16
  %1876 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1871, <8 x i16> %1872) #9
  %1877 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 33
  %1878 = bitcast <2 x i64>* %1877 to <8 x i16>*
  store <8 x i16> %1876, <8 x i16>* %1878, align 16
  %1879 = load <8 x i16>, <8 x i16>* %412, align 16
  %1880 = load <8 x i16>, <8 x i16>* %147, align 16
  %1881 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1879, <8 x i16> %1880) #9
  %1882 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 31
  %1883 = bitcast <2 x i64>* %1882 to <8 x i16>*
  store <8 x i16> %1881, <8 x i16>* %1883, align 16
  %1884 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1879, <8 x i16> %1880) #9
  %1885 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 32
  %1886 = bitcast <2 x i64>* %1885 to <8 x i16>*
  store <8 x i16> %1884, <8 x i16>* %1886, align 16
  call void @llvm.lifetime.end.p0i8(i64 1024, i8* nonnull %35) #9
  ret void
}

; Function Attrs: inlinehint nofree nounwind ssp uwtable
define internal fastcc void @idct64_stage4_high32_sse2(<2 x i64>* nocapture, <2 x i64>, i8 signext) unnamed_addr #7 {
  %4 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 4), align 16
  %5 = sub i32 0, %4
  %6 = and i32 %5, 65535
  %7 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 60), align 16
  %8 = shl i32 %7, 16
  %9 = or i32 %8, %6
  %10 = insertelement <4 x i32> undef, i32 %9, i32 0
  %11 = shufflevector <4 x i32> %10, <4 x i32> undef, <4 x i32> zeroinitializer
  %12 = and i32 %7, 65535
  %13 = shl i32 %4, 16
  %14 = or i32 %12, %13
  %15 = insertelement <4 x i32> undef, i32 %14, i32 0
  %16 = shufflevector <4 x i32> %15, <4 x i32> undef, <4 x i32> zeroinitializer
  %17 = sub i32 0, %7
  %18 = and i32 %17, 65535
  %19 = sub i32 0, %13
  %20 = or i32 %18, %19
  %21 = insertelement <4 x i32> undef, i32 %20, i32 0
  %22 = shufflevector <4 x i32> %21, <4 x i32> undef, <4 x i32> zeroinitializer
  %23 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 36), align 16
  %24 = sub i32 0, %23
  %25 = and i32 %24, 65535
  %26 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 28), align 16
  %27 = shl i32 %26, 16
  %28 = or i32 %27, %25
  %29 = insertelement <4 x i32> undef, i32 %28, i32 0
  %30 = shufflevector <4 x i32> %29, <4 x i32> undef, <4 x i32> zeroinitializer
  %31 = and i32 %26, 65535
  %32 = shl i32 %23, 16
  %33 = or i32 %31, %32
  %34 = insertelement <4 x i32> undef, i32 %33, i32 0
  %35 = shufflevector <4 x i32> %34, <4 x i32> undef, <4 x i32> zeroinitializer
  %36 = sub i32 0, %26
  %37 = and i32 %36, 65535
  %38 = sub i32 0, %32
  %39 = or i32 %37, %38
  %40 = insertelement <4 x i32> undef, i32 %39, i32 0
  %41 = shufflevector <4 x i32> %40, <4 x i32> undef, <4 x i32> zeroinitializer
  %42 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 20), align 16
  %43 = sub i32 0, %42
  %44 = and i32 %43, 65535
  %45 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 44), align 16
  %46 = shl i32 %45, 16
  %47 = or i32 %46, %44
  %48 = insertelement <4 x i32> undef, i32 %47, i32 0
  %49 = shufflevector <4 x i32> %48, <4 x i32> undef, <4 x i32> zeroinitializer
  %50 = and i32 %45, 65535
  %51 = shl i32 %42, 16
  %52 = or i32 %50, %51
  %53 = insertelement <4 x i32> undef, i32 %52, i32 0
  %54 = shufflevector <4 x i32> %53, <4 x i32> undef, <4 x i32> zeroinitializer
  %55 = sub i32 0, %45
  %56 = and i32 %55, 65535
  %57 = sub i32 0, %51
  %58 = or i32 %56, %57
  %59 = insertelement <4 x i32> undef, i32 %58, i32 0
  %60 = shufflevector <4 x i32> %59, <4 x i32> undef, <4 x i32> zeroinitializer
  %61 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 52), align 16
  %62 = sub i32 0, %61
  %63 = and i32 %62, 65535
  %64 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 12), align 16
  %65 = shl i32 %64, 16
  %66 = or i32 %65, %63
  %67 = insertelement <4 x i32> undef, i32 %66, i32 0
  %68 = shufflevector <4 x i32> %67, <4 x i32> undef, <4 x i32> zeroinitializer
  %69 = and i32 %64, 65535
  %70 = shl i32 %61, 16
  %71 = or i32 %69, %70
  %72 = insertelement <4 x i32> undef, i32 %71, i32 0
  %73 = shufflevector <4 x i32> %72, <4 x i32> undef, <4 x i32> zeroinitializer
  %74 = sub i32 0, %64
  %75 = and i32 %74, 65535
  %76 = sub i32 0, %70
  %77 = or i32 %75, %76
  %78 = insertelement <4 x i32> undef, i32 %77, i32 0
  %79 = shufflevector <4 x i32> %78, <4 x i32> undef, <4 x i32> zeroinitializer
  %80 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 33
  %81 = bitcast <2 x i64>* %80 to <8 x i16>*
  %82 = load <8 x i16>, <8 x i16>* %81, align 16
  %83 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 62
  %84 = bitcast <2 x i64>* %83 to <8 x i16>*
  %85 = load <8 x i16>, <8 x i16>* %84, align 16
  %86 = shufflevector <8 x i16> %82, <8 x i16> %85, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %87 = shufflevector <8 x i16> %82, <8 x i16> %85, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %88 = bitcast <4 x i32> %11 to <8 x i16>
  %89 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %86, <8 x i16> %88) #9
  %90 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %87, <8 x i16> %88) #9
  %91 = bitcast <4 x i32> %16 to <8 x i16>
  %92 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %86, <8 x i16> %91) #9
  %93 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %87, <8 x i16> %91) #9
  %94 = bitcast <2 x i64> %1 to <4 x i32>
  %95 = add <4 x i32> %89, %94
  %96 = add <4 x i32> %90, %94
  %97 = add <4 x i32> %92, %94
  %98 = add <4 x i32> %93, %94
  %99 = sext i8 %2 to i32
  %100 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %95, i32 %99) #9
  %101 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %96, i32 %99) #9
  %102 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %97, i32 %99) #9
  %103 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %98, i32 %99) #9
  %104 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %100, <4 x i32> %101) #9
  store <8 x i16> %104, <8 x i16>* %81, align 16
  %105 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %102, <4 x i32> %103) #9
  store <8 x i16> %105, <8 x i16>* %84, align 16
  %106 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 34
  %107 = bitcast <2 x i64>* %106 to <8 x i16>*
  %108 = load <8 x i16>, <8 x i16>* %107, align 16
  %109 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 61
  %110 = bitcast <2 x i64>* %109 to <8 x i16>*
  %111 = load <8 x i16>, <8 x i16>* %110, align 16
  %112 = shufflevector <8 x i16> %108, <8 x i16> %111, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %113 = shufflevector <8 x i16> %108, <8 x i16> %111, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %114 = bitcast <4 x i32> %22 to <8 x i16>
  %115 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %112, <8 x i16> %114) #9
  %116 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %113, <8 x i16> %114) #9
  %117 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %112, <8 x i16> %88) #9
  %118 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %113, <8 x i16> %88) #9
  %119 = add <4 x i32> %115, %94
  %120 = add <4 x i32> %116, %94
  %121 = add <4 x i32> %117, %94
  %122 = add <4 x i32> %118, %94
  %123 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %119, i32 %99) #9
  %124 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %120, i32 %99) #9
  %125 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %121, i32 %99) #9
  %126 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %122, i32 %99) #9
  %127 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %123, <4 x i32> %124) #9
  store <8 x i16> %127, <8 x i16>* %107, align 16
  %128 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %125, <4 x i32> %126) #9
  store <8 x i16> %128, <8 x i16>* %110, align 16
  %129 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 37
  %130 = bitcast <2 x i64>* %129 to <8 x i16>*
  %131 = load <8 x i16>, <8 x i16>* %130, align 16
  %132 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 58
  %133 = bitcast <2 x i64>* %132 to <8 x i16>*
  %134 = load <8 x i16>, <8 x i16>* %133, align 16
  %135 = shufflevector <8 x i16> %131, <8 x i16> %134, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %136 = shufflevector <8 x i16> %131, <8 x i16> %134, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %137 = bitcast <4 x i32> %30 to <8 x i16>
  %138 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %135, <8 x i16> %137) #9
  %139 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %136, <8 x i16> %137) #9
  %140 = bitcast <4 x i32> %35 to <8 x i16>
  %141 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %135, <8 x i16> %140) #9
  %142 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %136, <8 x i16> %140) #9
  %143 = add <4 x i32> %138, %94
  %144 = add <4 x i32> %139, %94
  %145 = add <4 x i32> %141, %94
  %146 = add <4 x i32> %142, %94
  %147 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %143, i32 %99) #9
  %148 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %144, i32 %99) #9
  %149 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %145, i32 %99) #9
  %150 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %146, i32 %99) #9
  %151 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %147, <4 x i32> %148) #9
  store <8 x i16> %151, <8 x i16>* %130, align 16
  %152 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %149, <4 x i32> %150) #9
  store <8 x i16> %152, <8 x i16>* %133, align 16
  %153 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 38
  %154 = bitcast <2 x i64>* %153 to <8 x i16>*
  %155 = load <8 x i16>, <8 x i16>* %154, align 16
  %156 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 57
  %157 = bitcast <2 x i64>* %156 to <8 x i16>*
  %158 = load <8 x i16>, <8 x i16>* %157, align 16
  %159 = shufflevector <8 x i16> %155, <8 x i16> %158, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %160 = shufflevector <8 x i16> %155, <8 x i16> %158, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %161 = bitcast <4 x i32> %41 to <8 x i16>
  %162 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %159, <8 x i16> %161) #9
  %163 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %160, <8 x i16> %161) #9
  %164 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %159, <8 x i16> %137) #9
  %165 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %160, <8 x i16> %137) #9
  %166 = add <4 x i32> %162, %94
  %167 = add <4 x i32> %163, %94
  %168 = add <4 x i32> %164, %94
  %169 = add <4 x i32> %165, %94
  %170 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %166, i32 %99) #9
  %171 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %167, i32 %99) #9
  %172 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %168, i32 %99) #9
  %173 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %169, i32 %99) #9
  %174 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %170, <4 x i32> %171) #9
  store <8 x i16> %174, <8 x i16>* %154, align 16
  %175 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %172, <4 x i32> %173) #9
  store <8 x i16> %175, <8 x i16>* %157, align 16
  %176 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 41
  %177 = bitcast <2 x i64>* %176 to <8 x i16>*
  %178 = load <8 x i16>, <8 x i16>* %177, align 16
  %179 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 54
  %180 = bitcast <2 x i64>* %179 to <8 x i16>*
  %181 = load <8 x i16>, <8 x i16>* %180, align 16
  %182 = shufflevector <8 x i16> %178, <8 x i16> %181, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %183 = shufflevector <8 x i16> %178, <8 x i16> %181, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %184 = bitcast <4 x i32> %49 to <8 x i16>
  %185 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %182, <8 x i16> %184) #9
  %186 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %183, <8 x i16> %184) #9
  %187 = bitcast <4 x i32> %54 to <8 x i16>
  %188 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %182, <8 x i16> %187) #9
  %189 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %183, <8 x i16> %187) #9
  %190 = add <4 x i32> %185, %94
  %191 = add <4 x i32> %186, %94
  %192 = add <4 x i32> %188, %94
  %193 = add <4 x i32> %189, %94
  %194 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %190, i32 %99) #9
  %195 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %191, i32 %99) #9
  %196 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %192, i32 %99) #9
  %197 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %193, i32 %99) #9
  %198 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %194, <4 x i32> %195) #9
  store <8 x i16> %198, <8 x i16>* %177, align 16
  %199 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %196, <4 x i32> %197) #9
  store <8 x i16> %199, <8 x i16>* %180, align 16
  %200 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 42
  %201 = bitcast <2 x i64>* %200 to <8 x i16>*
  %202 = load <8 x i16>, <8 x i16>* %201, align 16
  %203 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 53
  %204 = bitcast <2 x i64>* %203 to <8 x i16>*
  %205 = load <8 x i16>, <8 x i16>* %204, align 16
  %206 = shufflevector <8 x i16> %202, <8 x i16> %205, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %207 = shufflevector <8 x i16> %202, <8 x i16> %205, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %208 = bitcast <4 x i32> %60 to <8 x i16>
  %209 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %206, <8 x i16> %208) #9
  %210 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %207, <8 x i16> %208) #9
  %211 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %206, <8 x i16> %184) #9
  %212 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %207, <8 x i16> %184) #9
  %213 = add <4 x i32> %209, %94
  %214 = add <4 x i32> %210, %94
  %215 = add <4 x i32> %211, %94
  %216 = add <4 x i32> %212, %94
  %217 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %213, i32 %99) #9
  %218 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %214, i32 %99) #9
  %219 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %215, i32 %99) #9
  %220 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %216, i32 %99) #9
  %221 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %217, <4 x i32> %218) #9
  store <8 x i16> %221, <8 x i16>* %201, align 16
  %222 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %219, <4 x i32> %220) #9
  store <8 x i16> %222, <8 x i16>* %204, align 16
  %223 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 45
  %224 = bitcast <2 x i64>* %223 to <8 x i16>*
  %225 = load <8 x i16>, <8 x i16>* %224, align 16
  %226 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 50
  %227 = bitcast <2 x i64>* %226 to <8 x i16>*
  %228 = load <8 x i16>, <8 x i16>* %227, align 16
  %229 = shufflevector <8 x i16> %225, <8 x i16> %228, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %230 = shufflevector <8 x i16> %225, <8 x i16> %228, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %231 = bitcast <4 x i32> %68 to <8 x i16>
  %232 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %229, <8 x i16> %231) #9
  %233 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %230, <8 x i16> %231) #9
  %234 = bitcast <4 x i32> %73 to <8 x i16>
  %235 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %229, <8 x i16> %234) #9
  %236 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %230, <8 x i16> %234) #9
  %237 = add <4 x i32> %232, %94
  %238 = add <4 x i32> %233, %94
  %239 = add <4 x i32> %235, %94
  %240 = add <4 x i32> %236, %94
  %241 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %237, i32 %99) #9
  %242 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %238, i32 %99) #9
  %243 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %239, i32 %99) #9
  %244 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %240, i32 %99) #9
  %245 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %241, <4 x i32> %242) #9
  store <8 x i16> %245, <8 x i16>* %224, align 16
  %246 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %243, <4 x i32> %244) #9
  store <8 x i16> %246, <8 x i16>* %227, align 16
  %247 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 46
  %248 = bitcast <2 x i64>* %247 to <8 x i16>*
  %249 = load <8 x i16>, <8 x i16>* %248, align 16
  %250 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 49
  %251 = bitcast <2 x i64>* %250 to <8 x i16>*
  %252 = load <8 x i16>, <8 x i16>* %251, align 16
  %253 = shufflevector <8 x i16> %249, <8 x i16> %252, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %254 = shufflevector <8 x i16> %249, <8 x i16> %252, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %255 = bitcast <4 x i32> %79 to <8 x i16>
  %256 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %253, <8 x i16> %255) #9
  %257 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %254, <8 x i16> %255) #9
  %258 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %253, <8 x i16> %231) #9
  %259 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %254, <8 x i16> %231) #9
  %260 = add <4 x i32> %256, %94
  %261 = add <4 x i32> %257, %94
  %262 = add <4 x i32> %258, %94
  %263 = add <4 x i32> %259, %94
  %264 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %260, i32 %99) #9
  %265 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %261, i32 %99) #9
  %266 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %262, i32 %99) #9
  %267 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %263, i32 %99) #9
  %268 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %264, <4 x i32> %265) #9
  store <8 x i16> %268, <8 x i16>* %248, align 16
  %269 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %266, <4 x i32> %267) #9
  store <8 x i16> %269, <8 x i16>* %251, align 16
  ret void
}

; Function Attrs: inlinehint nounwind ssp uwtable
define internal fastcc void @lowbd_inv_txfm2d_add_no_identity_ssse3(i32* nocapture readonly, i8* nocapture, i32, i8 zeroext, i8 zeroext, i32) unnamed_addr #8 {
  %7 = alloca [512 x <2 x i64>], align 16
  %8 = alloca [64 x <2 x i64>], align 16
  %9 = bitcast [512 x <2 x i64>]* %7 to i8*
  call void @llvm.lifetime.start.p0i8(i64 8192, i8* nonnull %9) #9
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %9, i8 -86, i64 8192, i1 false)
  %10 = icmp eq i32 %5, 1
  %11 = zext i8 %4 to i64
  br i1 %10, label %25, label %12

12:                                               ; preds = %6
  %13 = getelementptr inbounds [19 x i32], [19 x i32]* @tx_size_wide_log2_eob, i64 0, i64 %11
  %14 = load i32, i32* %13, align 4
  %15 = add nsw i32 %5, -1
  %16 = ashr i32 %15, %14
  %17 = getelementptr inbounds [19 x i16*], [19 x i16*]* @av1_eob_to_eobxy_default, i64 0, i64 %11
  %18 = load i16*, i16** %17, align 8
  %19 = sext i32 %16 to i64
  %20 = getelementptr inbounds i16, i16* %18, i64 %19
  %21 = load i16, i16* %20, align 2
  %22 = sext i16 %21 to i32
  %23 = and i32 %22, 255
  %24 = ashr i32 %22, 8
  br label %25

25:                                               ; preds = %6, %12
  %26 = phi i32 [ %23, %12 ], [ 0, %6 ]
  %27 = phi i32 [ %24, %12 ], [ 0, %6 ]
  %28 = getelementptr inbounds [19 x i8*], [19 x i8*]* @av1_inv_txfm_shift_ls, i64 0, i64 %11
  %29 = load i8*, i8** %28, align 8
  %30 = getelementptr inbounds [19 x i32], [19 x i32]* @tx_size_wide_log2, i64 0, i64 %11
  %31 = load i32, i32* %30, align 4
  %32 = add nsw i32 %31, -2
  %33 = getelementptr inbounds [19 x i32], [19 x i32]* @tx_size_high_log2, i64 0, i64 %11
  %34 = load i32, i32* %33, align 4
  %35 = add nsw i32 %34, -2
  %36 = sext i32 %32 to i64
  %37 = sext i32 %35 to i64
  %38 = getelementptr inbounds [5 x [5 x i8]], [5 x [5 x i8]]* @av1_inv_cos_bit_col, i64 0, i64 %36, i64 %37
  %39 = load i8, i8* %38, align 1
  %40 = getelementptr inbounds [5 x [5 x i8]], [5 x [5 x i8]]* @av1_inv_cos_bit_row, i64 0, i64 %36, i64 %37
  %41 = load i8, i8* %40, align 1
  %42 = getelementptr inbounds [19 x i32], [19 x i32]* @tx_size_wide, i64 0, i64 %11
  %43 = load i32, i32* %42, align 4
  %44 = getelementptr inbounds [19 x i32], [19 x i32]* @tx_size_high, i64 0, i64 %11
  %45 = load i32, i32* %44, align 4
  %46 = ashr i32 %43, 3
  %47 = add nuw nsw i32 %26, 8
  %48 = lshr i32 %47, 3
  %49 = add nsw i32 %27, 8
  %50 = ashr i32 %49, 3
  %51 = icmp slt i32 %43, 32
  %52 = select i1 %51, i32 %43, i32 32
  %53 = icmp eq i32 %43, %45
  br i1 %53, label %69, label %54

54:                                               ; preds = %25
  %55 = icmp sgt i32 %43, %45
  br i1 %55, label %56, label %62

56:                                               ; preds = %54
  %57 = shl nsw i32 %45, 1
  %58 = icmp eq i32 %57, %43
  br i1 %58, label %69, label %59

59:                                               ; preds = %56
  %60 = shl nsw i32 %45, 2
  %61 = icmp eq i32 %60, %43
  br i1 %61, label %69, label %68

62:                                               ; preds = %54
  %63 = shl nsw i32 %43, 1
  %64 = icmp eq i32 %63, %45
  br i1 %64, label %69, label %65

65:                                               ; preds = %62
  %66 = shl nsw i32 %43, 2
  %67 = icmp eq i32 %66, %45
  br i1 %67, label %69, label %68

68:                                               ; preds = %65, %59
  br label %69

69:                                               ; preds = %25, %56, %59, %62, %65, %68
  %70 = phi i32 [ 0, %68 ], [ 0, %25 ], [ 1, %56 ], [ 2, %59 ], [ -1, %62 ], [ -2, %65 ]
  %71 = zext i32 %26 to i64
  %72 = getelementptr inbounds [32 x i32], [32 x i32]* @lowbd_txfm_all_1d_zeros_idx, i64 0, i64 %71
  %73 = load i32, i32* %72, align 4
  %74 = sext i32 %27 to i64
  %75 = getelementptr inbounds [32 x i32], [32 x i32]* @lowbd_txfm_all_1d_zeros_idx, i64 0, i64 %74
  %76 = load i32, i32* %75, align 4
  %77 = zext i8 %3 to i64
  %78 = getelementptr inbounds [16 x i8], [16 x i8]* @hitx_1d_tab, i64 0, i64 %77
  %79 = load i8, i8* %78, align 1
  %80 = zext i8 %79 to i64
  %81 = sext i32 %73 to i64
  %82 = getelementptr inbounds [5 x [3 x [4 x void (<2 x i64>*, <2 x i64>*, i8)*]]], [5 x [3 x [4 x void (<2 x i64>*, <2 x i64>*, i8)*]]]* @lowbd_txfm_all_1d_zeros_w8_arr, i64 0, i64 %36, i64 %80, i64 %81
  %83 = load void (<2 x i64>*, <2 x i64>*, i8)*, void (<2 x i64>*, <2 x i64>*, i8)** %82, align 8
  %84 = getelementptr inbounds [16 x i8], [16 x i8]* @vitx_1d_tab, i64 0, i64 %77
  %85 = load i8, i8* %84, align 1
  %86 = zext i8 %85 to i64
  %87 = sext i32 %76 to i64
  %88 = getelementptr inbounds [5 x [3 x [4 x void (<2 x i64>*, <2 x i64>*, i8)*]]], [5 x [3 x [4 x void (<2 x i64>*, <2 x i64>*, i8)*]]]* @lowbd_txfm_all_1d_zeros_w8_arr, i64 0, i64 %37, i64 %86, i64 %87
  %89 = load void (<2 x i64>*, <2 x i64>*, i8)*, void (<2 x i64>*, <2 x i64>*, i8)** %88, align 8
  %90 = add i8 %3, -4
  %91 = icmp ult i8 %90, 12
  br i1 %91, label %92, label %99

92:                                               ; preds = %69
  %93 = sext i8 %90 to i64
  %94 = getelementptr inbounds [12 x i32], [12 x i32]* @switch.table.lowbd_inv_txfm2d_add_no_identity_ssse3, i64 0, i64 %93
  %95 = load i32, i32* %94, align 4
  %96 = sext i8 %90 to i64
  %97 = getelementptr inbounds [12 x i32], [12 x i32]* @switch.table.lowbd_inv_txfm2d_add_no_identity_ssse3.3, i64 0, i64 %96
  %98 = load i32, i32* %97, align 4
  br label %99

99:                                               ; preds = %92, %69
  %100 = phi i32 [ 0, %69 ], [ %95, %92 ]
  %101 = phi i32 [ 0, %69 ], [ %98, %92 ]
  %102 = icmp sgt i32 %49, 7
  br i1 %102, label %106, label %103

103:                                              ; preds = %99
  %104 = lshr i64 516062, %11
  %105 = and i64 %104, 1
  br label %141

106:                                              ; preds = %99
  %107 = bitcast [64 x <2 x i64>]* %8 to i8*
  %108 = shl i32 %52, 3
  %109 = sext i32 %52 to i64
  %110 = zext i32 %52 to i64
  %111 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %8, i64 0, i64 0
  %112 = zext i32 %43 to i64
  %113 = icmp eq i32 %101, 0
  %114 = lshr i64 516062, %11
  %115 = and i64 %114, 1
  %116 = icmp eq i64 %115, 0
  %117 = sext i32 %46 to i64
  %118 = sext i32 %45 to i64
  %119 = sext i32 %50 to i64
  %120 = zext i32 %48 to i64
  %121 = shl nsw i64 %109, 1
  %122 = mul nsw i64 %109, 3
  %123 = shl nsw i64 %109, 2
  %124 = mul nsw i64 %109, 5
  %125 = mul nsw i64 %109, 6
  %126 = mul nsw i64 %109, 7
  %127 = add nsw i64 %110, -1
  %128 = add nsw i64 %112, -1
  %129 = and i64 %110, 3
  %130 = icmp ult i64 %127, 3
  %131 = sub nsw i64 %110, %129
  %132 = icmp eq i64 %129, 0
  %133 = and i64 %112, 3
  %134 = icmp ult i64 %128, 3
  %135 = sub nsw i64 %112, %133
  %136 = icmp eq i64 %133, 0
  %137 = and i64 %112, 3
  %138 = icmp ult i64 %128, 3
  %139 = sub nsw i64 %112, %137
  %140 = icmp eq i64 %137, 0
  br label %158

141:                                              ; preds = %564, %103
  %142 = phi i64 [ %105, %103 ], [ %115, %564 ]
  %143 = icmp eq i64 %142, 0
  br i1 %143, label %567, label %144

144:                                              ; preds = %141
  %145 = getelementptr inbounds i8, i8* %29, i64 1
  %146 = zext i32 %45 to i64
  %147 = sext i32 %45 to i64
  %148 = sext i32 %46 to i64
  %149 = add nsw i64 %146, -1
  %150 = and i64 %146, 3
  %151 = icmp ult i64 %149, 3
  %152 = sub nsw i64 %146, %150
  %153 = icmp eq i64 %150, 0
  %154 = and i64 %146, 3
  %155 = icmp ult i64 %149, 3
  %156 = sub nsw i64 %146, %154
  %157 = icmp eq i64 %154, 0
  br label %583

158:                                              ; preds = %106, %564
  %159 = phi i64 [ 0, %106 ], [ %565, %564 ]
  call void @llvm.lifetime.start.p0i8(i64 1024, i8* nonnull %107) #9
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %107, i8 -86, i64 1024, i1 false)
  %160 = trunc i64 %159 to i32
  %161 = mul i32 %108, %160
  %162 = sext i32 %161 to i64
  %163 = getelementptr inbounds i32, i32* %0, i64 %162
  br label %166

164:                                              ; preds = %166
  switch i32 %70, label %314 [
    i32 -1, label %165
    i32 1, label %165
  ]

165:                                              ; preds = %164, %164
  br i1 %130, label %302, label %277

166:                                              ; preds = %166, %158
  %167 = phi i64 [ 0, %158 ], [ %275, %166 ]
  %168 = shl nsw i64 %167, 3
  %169 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %8, i64 0, i64 %168
  %170 = getelementptr inbounds i32, i32* %163, i64 %168
  %171 = bitcast i32* %170 to <4 x i32>*
  %172 = load <4 x i32>, <4 x i32>* %171, align 16
  %173 = getelementptr inbounds i32, i32* %170, i64 4
  %174 = bitcast i32* %173 to <4 x i32>*
  %175 = load <4 x i32>, <4 x i32>* %174, align 16
  %176 = call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %172, <4 x i32> %175) #9
  %177 = bitcast <2 x i64>* %169 to <8 x i16>*
  store <8 x i16> %176, <8 x i16>* %177, align 16
  %178 = getelementptr inbounds i32, i32* %170, i64 %109
  %179 = bitcast i32* %178 to <4 x i32>*
  %180 = load <4 x i32>, <4 x i32>* %179, align 16
  %181 = getelementptr inbounds i32, i32* %178, i64 4
  %182 = bitcast i32* %181 to <4 x i32>*
  %183 = load <4 x i32>, <4 x i32>* %182, align 16
  %184 = call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %180, <4 x i32> %183) #9
  %185 = getelementptr inbounds <2 x i64>, <2 x i64>* %169, i64 1
  %186 = bitcast <2 x i64>* %185 to <8 x i16>*
  store <8 x i16> %184, <8 x i16>* %186, align 16
  %187 = getelementptr inbounds i32, i32* %170, i64 %121
  %188 = bitcast i32* %187 to <4 x i32>*
  %189 = load <4 x i32>, <4 x i32>* %188, align 16
  %190 = getelementptr inbounds i32, i32* %187, i64 4
  %191 = bitcast i32* %190 to <4 x i32>*
  %192 = load <4 x i32>, <4 x i32>* %191, align 16
  %193 = call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %189, <4 x i32> %192) #9
  %194 = getelementptr inbounds <2 x i64>, <2 x i64>* %169, i64 2
  %195 = getelementptr inbounds i32, i32* %170, i64 %122
  %196 = bitcast i32* %195 to <4 x i32>*
  %197 = load <4 x i32>, <4 x i32>* %196, align 16
  %198 = getelementptr inbounds i32, i32* %195, i64 4
  %199 = bitcast i32* %198 to <4 x i32>*
  %200 = load <4 x i32>, <4 x i32>* %199, align 16
  %201 = call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %197, <4 x i32> %200) #9
  %202 = getelementptr inbounds <2 x i64>, <2 x i64>* %169, i64 3
  %203 = getelementptr inbounds i32, i32* %170, i64 %123
  %204 = bitcast i32* %203 to <4 x i32>*
  %205 = load <4 x i32>, <4 x i32>* %204, align 16
  %206 = getelementptr inbounds i32, i32* %203, i64 4
  %207 = bitcast i32* %206 to <4 x i32>*
  %208 = load <4 x i32>, <4 x i32>* %207, align 16
  %209 = call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %205, <4 x i32> %208) #9
  %210 = getelementptr inbounds <2 x i64>, <2 x i64>* %169, i64 4
  %211 = getelementptr inbounds i32, i32* %170, i64 %124
  %212 = bitcast i32* %211 to <4 x i32>*
  %213 = load <4 x i32>, <4 x i32>* %212, align 16
  %214 = getelementptr inbounds i32, i32* %211, i64 4
  %215 = bitcast i32* %214 to <4 x i32>*
  %216 = load <4 x i32>, <4 x i32>* %215, align 16
  %217 = call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %213, <4 x i32> %216) #9
  %218 = getelementptr inbounds <2 x i64>, <2 x i64>* %169, i64 5
  %219 = getelementptr inbounds i32, i32* %170, i64 %125
  %220 = bitcast i32* %219 to <4 x i32>*
  %221 = load <4 x i32>, <4 x i32>* %220, align 16
  %222 = getelementptr inbounds i32, i32* %219, i64 4
  %223 = bitcast i32* %222 to <4 x i32>*
  %224 = load <4 x i32>, <4 x i32>* %223, align 16
  %225 = call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %221, <4 x i32> %224) #9
  %226 = getelementptr inbounds <2 x i64>, <2 x i64>* %169, i64 6
  %227 = getelementptr inbounds i32, i32* %170, i64 %126
  %228 = bitcast i32* %227 to <4 x i32>*
  %229 = load <4 x i32>, <4 x i32>* %228, align 16
  %230 = getelementptr inbounds i32, i32* %227, i64 4
  %231 = bitcast i32* %230 to <4 x i32>*
  %232 = load <4 x i32>, <4 x i32>* %231, align 16
  %233 = call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %229, <4 x i32> %232) #9
  %234 = getelementptr inbounds <2 x i64>, <2 x i64>* %169, i64 7
  %235 = shufflevector <8 x i16> %176, <8 x i16> %184, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %236 = shufflevector <8 x i16> %193, <8 x i16> %201, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %237 = shufflevector <8 x i16> %209, <8 x i16> %217, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %238 = shufflevector <8 x i16> %225, <8 x i16> %233, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %239 = shufflevector <8 x i16> %176, <8 x i16> %184, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %240 = shufflevector <8 x i16> %193, <8 x i16> %201, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %241 = shufflevector <8 x i16> %209, <8 x i16> %217, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %242 = shufflevector <8 x i16> %225, <8 x i16> %233, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %243 = bitcast <8 x i16> %235 to <4 x i32>
  %244 = bitcast <8 x i16> %236 to <4 x i32>
  %245 = shufflevector <4 x i32> %243, <4 x i32> %244, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %246 = bitcast <4 x i32> %245 to <2 x i64>
  %247 = bitcast <8 x i16> %237 to <4 x i32>
  %248 = bitcast <8 x i16> %238 to <4 x i32>
  %249 = shufflevector <4 x i32> %247, <4 x i32> %248, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %250 = bitcast <4 x i32> %249 to <2 x i64>
  %251 = bitcast <8 x i16> %239 to <4 x i32>
  %252 = bitcast <8 x i16> %240 to <4 x i32>
  %253 = shufflevector <4 x i32> %251, <4 x i32> %252, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %254 = bitcast <4 x i32> %253 to <2 x i64>
  %255 = bitcast <8 x i16> %241 to <4 x i32>
  %256 = bitcast <8 x i16> %242 to <4 x i32>
  %257 = shufflevector <4 x i32> %255, <4 x i32> %256, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %258 = bitcast <4 x i32> %257 to <2 x i64>
  %259 = shufflevector <4 x i32> %243, <4 x i32> %244, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %260 = bitcast <4 x i32> %259 to <2 x i64>
  %261 = shufflevector <4 x i32> %247, <4 x i32> %248, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %262 = bitcast <4 x i32> %261 to <2 x i64>
  %263 = shufflevector <4 x i32> %251, <4 x i32> %252, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %264 = bitcast <4 x i32> %263 to <2 x i64>
  %265 = shufflevector <4 x i32> %255, <4 x i32> %256, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %266 = bitcast <4 x i32> %265 to <2 x i64>
  %267 = shufflevector <2 x i64> %246, <2 x i64> %250, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %267, <2 x i64>* %169, align 16
  %268 = shufflevector <2 x i64> %246, <2 x i64> %250, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %268, <2 x i64>* %185, align 16
  %269 = shufflevector <2 x i64> %260, <2 x i64> %262, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %269, <2 x i64>* %194, align 16
  %270 = shufflevector <2 x i64> %260, <2 x i64> %262, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %270, <2 x i64>* %202, align 16
  %271 = shufflevector <2 x i64> %254, <2 x i64> %258, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %271, <2 x i64>* %210, align 16
  %272 = shufflevector <2 x i64> %254, <2 x i64> %258, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %272, <2 x i64>* %218, align 16
  %273 = shufflevector <2 x i64> %264, <2 x i64> %266, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %273, <2 x i64>* %226, align 16
  %274 = shufflevector <2 x i64> %264, <2 x i64> %266, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %274, <2 x i64>* %234, align 16
  %275 = add nuw nsw i64 %167, 1
  %276 = icmp eq i64 %275, %120
  br i1 %276, label %164, label %166

277:                                              ; preds = %165, %277
  %278 = phi i64 [ %299, %277 ], [ 0, %165 ]
  %279 = phi i64 [ %300, %277 ], [ %131, %165 ]
  %280 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %8, i64 0, i64 %278
  %281 = bitcast <2 x i64>* %280 to <8 x i16>*
  %282 = load <8 x i16>, <8 x i16>* %281, align 16
  %283 = call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %282, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #9
  store <8 x i16> %283, <8 x i16>* %281, align 16
  %284 = or i64 %278, 1
  %285 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %8, i64 0, i64 %284
  %286 = bitcast <2 x i64>* %285 to <8 x i16>*
  %287 = load <8 x i16>, <8 x i16>* %286, align 16
  %288 = call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %287, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #9
  store <8 x i16> %288, <8 x i16>* %286, align 16
  %289 = or i64 %278, 2
  %290 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %8, i64 0, i64 %289
  %291 = bitcast <2 x i64>* %290 to <8 x i16>*
  %292 = load <8 x i16>, <8 x i16>* %291, align 16
  %293 = call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %292, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #9
  store <8 x i16> %293, <8 x i16>* %291, align 16
  %294 = or i64 %278, 3
  %295 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %8, i64 0, i64 %294
  %296 = bitcast <2 x i64>* %295 to <8 x i16>*
  %297 = load <8 x i16>, <8 x i16>* %296, align 16
  %298 = call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %297, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #9
  store <8 x i16> %298, <8 x i16>* %296, align 16
  %299 = add nuw nsw i64 %278, 4
  %300 = add i64 %279, -4
  %301 = icmp eq i64 %300, 0
  br i1 %301, label %302, label %277

302:                                              ; preds = %277, %165
  %303 = phi i64 [ 0, %165 ], [ %299, %277 ]
  br i1 %132, label %314, label %304

304:                                              ; preds = %302, %304
  %305 = phi i64 [ %311, %304 ], [ %303, %302 ]
  %306 = phi i64 [ %312, %304 ], [ %129, %302 ]
  %307 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %8, i64 0, i64 %305
  %308 = bitcast <2 x i64>* %307 to <8 x i16>*
  %309 = load <8 x i16>, <8 x i16>* %308, align 16
  %310 = call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %309, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #9
  store <8 x i16> %310, <8 x i16>* %308, align 16
  %311 = add nuw nsw i64 %305, 1
  %312 = add i64 %306, -1
  %313 = icmp eq i64 %312, 0
  br i1 %313, label %314, label %304, !llvm.loop !6

314:                                              ; preds = %302, %304, %164
  call void %83(<2 x i64>* nonnull %111, <2 x i64>* nonnull %111, i8 signext %41) #9
  %315 = load i8, i8* %29, align 1
  %316 = sext i8 %315 to i32
  %317 = icmp slt i8 %315, 0
  br i1 %317, label %318, label %349

318:                                              ; preds = %314
  %319 = add nsw i32 %316, 15
  %320 = shl i32 1, %319
  %321 = trunc i32 %320 to i16
  %322 = insertelement <8 x i16> undef, i16 %321, i32 0
  %323 = shufflevector <8 x i16> %322, <8 x i16> undef, <8 x i32> zeroinitializer
  br i1 %138, label %377, label %324

324:                                              ; preds = %318, %324
  %325 = phi i64 [ %346, %324 ], [ 0, %318 ]
  %326 = phi i64 [ %347, %324 ], [ %139, %318 ]
  %327 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %8, i64 0, i64 %325
  %328 = bitcast <2 x i64>* %327 to <8 x i16>*
  %329 = load <8 x i16>, <8 x i16>* %328, align 16
  %330 = call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %329, <8 x i16> %323) #9
  store <8 x i16> %330, <8 x i16>* %328, align 16
  %331 = or i64 %325, 1
  %332 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %8, i64 0, i64 %331
  %333 = bitcast <2 x i64>* %332 to <8 x i16>*
  %334 = load <8 x i16>, <8 x i16>* %333, align 16
  %335 = call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %334, <8 x i16> %323) #9
  store <8 x i16> %335, <8 x i16>* %333, align 16
  %336 = or i64 %325, 2
  %337 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %8, i64 0, i64 %336
  %338 = bitcast <2 x i64>* %337 to <8 x i16>*
  %339 = load <8 x i16>, <8 x i16>* %338, align 16
  %340 = call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %339, <8 x i16> %323) #9
  store <8 x i16> %340, <8 x i16>* %338, align 16
  %341 = or i64 %325, 3
  %342 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %8, i64 0, i64 %341
  %343 = bitcast <2 x i64>* %342 to <8 x i16>*
  %344 = load <8 x i16>, <8 x i16>* %343, align 16
  %345 = call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %344, <8 x i16> %323) #9
  store <8 x i16> %345, <8 x i16>* %343, align 16
  %346 = add nuw nsw i64 %325, 4
  %347 = add i64 %326, -4
  %348 = icmp eq i64 %347, 0
  br i1 %348, label %377, label %324

349:                                              ; preds = %314
  %350 = icmp eq i8 %315, 0
  br i1 %350, label %401, label %351

351:                                              ; preds = %349
  br i1 %134, label %389, label %352

352:                                              ; preds = %351, %352
  %353 = phi i64 [ %374, %352 ], [ 0, %351 ]
  %354 = phi i64 [ %375, %352 ], [ %135, %351 ]
  %355 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %8, i64 0, i64 %353
  %356 = bitcast <2 x i64>* %355 to <8 x i16>*
  %357 = load <8 x i16>, <8 x i16>* %356, align 16
  %358 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %357, i32 %316) #9
  store <8 x i16> %358, <8 x i16>* %356, align 16
  %359 = or i64 %353, 1
  %360 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %8, i64 0, i64 %359
  %361 = bitcast <2 x i64>* %360 to <8 x i16>*
  %362 = load <8 x i16>, <8 x i16>* %361, align 16
  %363 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %362, i32 %316) #9
  store <8 x i16> %363, <8 x i16>* %361, align 16
  %364 = or i64 %353, 2
  %365 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %8, i64 0, i64 %364
  %366 = bitcast <2 x i64>* %365 to <8 x i16>*
  %367 = load <8 x i16>, <8 x i16>* %366, align 16
  %368 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %367, i32 %316) #9
  store <8 x i16> %368, <8 x i16>* %366, align 16
  %369 = or i64 %353, 3
  %370 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %8, i64 0, i64 %369
  %371 = bitcast <2 x i64>* %370 to <8 x i16>*
  %372 = load <8 x i16>, <8 x i16>* %371, align 16
  %373 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %372, i32 %316) #9
  store <8 x i16> %373, <8 x i16>* %371, align 16
  %374 = add nuw nsw i64 %353, 4
  %375 = add i64 %354, -4
  %376 = icmp eq i64 %375, 0
  br i1 %376, label %389, label %352

377:                                              ; preds = %324, %318
  %378 = phi i64 [ 0, %318 ], [ %346, %324 ]
  br i1 %140, label %401, label %379

379:                                              ; preds = %377, %379
  %380 = phi i64 [ %386, %379 ], [ %378, %377 ]
  %381 = phi i64 [ %387, %379 ], [ %137, %377 ]
  %382 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %8, i64 0, i64 %380
  %383 = bitcast <2 x i64>* %382 to <8 x i16>*
  %384 = load <8 x i16>, <8 x i16>* %383, align 16
  %385 = call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %384, <8 x i16> %323) #9
  store <8 x i16> %385, <8 x i16>* %383, align 16
  %386 = add nuw nsw i64 %380, 1
  %387 = add i64 %381, -1
  %388 = icmp eq i64 %387, 0
  br i1 %388, label %401, label %379, !llvm.loop !7

389:                                              ; preds = %352, %351
  %390 = phi i64 [ 0, %351 ], [ %374, %352 ]
  br i1 %136, label %401, label %391

391:                                              ; preds = %389, %391
  %392 = phi i64 [ %398, %391 ], [ %390, %389 ]
  %393 = phi i64 [ %399, %391 ], [ %133, %389 ]
  %394 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %8, i64 0, i64 %392
  %395 = bitcast <2 x i64>* %394 to <8 x i16>*
  %396 = load <8 x i16>, <8 x i16>* %395, align 16
  %397 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %396, i32 %316) #9
  store <8 x i16> %397, <8 x i16>* %395, align 16
  %398 = add nuw nsw i64 %392, 1
  %399 = add i64 %393, -1
  %400 = icmp eq i64 %399, 0
  br i1 %400, label %401, label %391, !llvm.loop !8

401:                                              ; preds = %389, %391, %377, %379, %349
  %402 = shl nsw i64 %159, 3
  %403 = getelementptr inbounds [512 x <2 x i64>], [512 x <2 x i64>]* %7, i64 0, i64 %402
  br i1 %113, label %405, label %404

404:                                              ; preds = %401
  br i1 %116, label %564, label %406

405:                                              ; preds = %401
  br i1 %116, label %564, label %486

406:                                              ; preds = %404, %406
  %407 = phi i64 [ %484, %406 ], [ 0, %404 ]
  %408 = shl nsw i64 %407, 3
  %409 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %8, i64 0, i64 %408
  %410 = bitcast <2 x i64>* %409 to <8 x i16>*
  %411 = load <8 x i16>, <8 x i16>* %410, align 16
  %412 = getelementptr inbounds <2 x i64>, <2 x i64>* %409, i64 1
  %413 = bitcast <2 x i64>* %412 to <8 x i16>*
  %414 = load <8 x i16>, <8 x i16>* %413, align 16
  %415 = getelementptr inbounds <2 x i64>, <2 x i64>* %409, i64 2
  %416 = bitcast <2 x i64>* %415 to <8 x i16>*
  %417 = load <8 x i16>, <8 x i16>* %416, align 16
  %418 = getelementptr inbounds <2 x i64>, <2 x i64>* %409, i64 3
  %419 = bitcast <2 x i64>* %418 to <8 x i16>*
  %420 = load <8 x i16>, <8 x i16>* %419, align 16
  %421 = getelementptr inbounds <2 x i64>, <2 x i64>* %409, i64 4
  %422 = bitcast <2 x i64>* %421 to <8 x i16>*
  %423 = load <8 x i16>, <8 x i16>* %422, align 16
  %424 = getelementptr inbounds <2 x i64>, <2 x i64>* %409, i64 5
  %425 = bitcast <2 x i64>* %424 to <8 x i16>*
  %426 = load <8 x i16>, <8 x i16>* %425, align 16
  %427 = getelementptr inbounds <2 x i64>, <2 x i64>* %409, i64 6
  %428 = bitcast <2 x i64>* %427 to <8 x i16>*
  %429 = load <8 x i16>, <8 x i16>* %428, align 16
  %430 = getelementptr inbounds <2 x i64>, <2 x i64>* %409, i64 7
  %431 = bitcast <2 x i64>* %430 to <8 x i16>*
  %432 = load <8 x i16>, <8 x i16>* %431, align 16
  %433 = xor i64 %407, -1
  %434 = add nsw i64 %117, %433
  %435 = mul nsw i64 %434, %118
  %436 = getelementptr inbounds <2 x i64>, <2 x i64>* %403, i64 %435
  %437 = shufflevector <8 x i16> %432, <8 x i16> %429, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %438 = shufflevector <8 x i16> %426, <8 x i16> %423, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %439 = shufflevector <8 x i16> %420, <8 x i16> %417, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %440 = shufflevector <8 x i16> %414, <8 x i16> %411, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %441 = shufflevector <8 x i16> %432, <8 x i16> %429, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %442 = shufflevector <8 x i16> %426, <8 x i16> %423, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %443 = shufflevector <8 x i16> %420, <8 x i16> %417, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %444 = shufflevector <8 x i16> %414, <8 x i16> %411, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %445 = bitcast <8 x i16> %437 to <4 x i32>
  %446 = bitcast <8 x i16> %438 to <4 x i32>
  %447 = shufflevector <4 x i32> %445, <4 x i32> %446, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %448 = bitcast <4 x i32> %447 to <2 x i64>
  %449 = bitcast <8 x i16> %439 to <4 x i32>
  %450 = bitcast <8 x i16> %440 to <4 x i32>
  %451 = shufflevector <4 x i32> %449, <4 x i32> %450, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %452 = bitcast <4 x i32> %451 to <2 x i64>
  %453 = bitcast <8 x i16> %441 to <4 x i32>
  %454 = bitcast <8 x i16> %442 to <4 x i32>
  %455 = shufflevector <4 x i32> %453, <4 x i32> %454, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %456 = bitcast <4 x i32> %455 to <2 x i64>
  %457 = bitcast <8 x i16> %443 to <4 x i32>
  %458 = bitcast <8 x i16> %444 to <4 x i32>
  %459 = shufflevector <4 x i32> %457, <4 x i32> %458, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %460 = bitcast <4 x i32> %459 to <2 x i64>
  %461 = shufflevector <4 x i32> %445, <4 x i32> %446, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %462 = bitcast <4 x i32> %461 to <2 x i64>
  %463 = shufflevector <4 x i32> %449, <4 x i32> %450, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %464 = bitcast <4 x i32> %463 to <2 x i64>
  %465 = shufflevector <4 x i32> %453, <4 x i32> %454, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %466 = bitcast <4 x i32> %465 to <2 x i64>
  %467 = shufflevector <4 x i32> %457, <4 x i32> %458, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %468 = bitcast <4 x i32> %467 to <2 x i64>
  %469 = shufflevector <2 x i64> %448, <2 x i64> %452, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %469, <2 x i64>* %436, align 16
  %470 = shufflevector <2 x i64> %448, <2 x i64> %452, <2 x i32> <i32 1, i32 3>
  %471 = getelementptr inbounds <2 x i64>, <2 x i64>* %436, i64 1
  store <2 x i64> %470, <2 x i64>* %471, align 16
  %472 = shufflevector <2 x i64> %462, <2 x i64> %464, <2 x i32> <i32 0, i32 2>
  %473 = getelementptr inbounds <2 x i64>, <2 x i64>* %436, i64 2
  store <2 x i64> %472, <2 x i64>* %473, align 16
  %474 = shufflevector <2 x i64> %462, <2 x i64> %464, <2 x i32> <i32 1, i32 3>
  %475 = getelementptr inbounds <2 x i64>, <2 x i64>* %436, i64 3
  store <2 x i64> %474, <2 x i64>* %475, align 16
  %476 = shufflevector <2 x i64> %456, <2 x i64> %460, <2 x i32> <i32 0, i32 2>
  %477 = getelementptr inbounds <2 x i64>, <2 x i64>* %436, i64 4
  store <2 x i64> %476, <2 x i64>* %477, align 16
  %478 = shufflevector <2 x i64> %456, <2 x i64> %460, <2 x i32> <i32 1, i32 3>
  %479 = getelementptr inbounds <2 x i64>, <2 x i64>* %436, i64 5
  store <2 x i64> %478, <2 x i64>* %479, align 16
  %480 = shufflevector <2 x i64> %466, <2 x i64> %468, <2 x i32> <i32 0, i32 2>
  %481 = getelementptr inbounds <2 x i64>, <2 x i64>* %436, i64 6
  store <2 x i64> %480, <2 x i64>* %481, align 16
  %482 = shufflevector <2 x i64> %466, <2 x i64> %468, <2 x i32> <i32 1, i32 3>
  %483 = getelementptr inbounds <2 x i64>, <2 x i64>* %436, i64 7
  store <2 x i64> %482, <2 x i64>* %483, align 16
  %484 = add nuw nsw i64 %407, 1
  %485 = icmp slt i64 %484, %117
  br i1 %485, label %406, label %564

486:                                              ; preds = %405, %486
  %487 = phi i64 [ %562, %486 ], [ 0, %405 ]
  %488 = shl nsw i64 %487, 3
  %489 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %8, i64 0, i64 %488
  %490 = mul nsw i64 %487, %118
  %491 = getelementptr inbounds <2 x i64>, <2 x i64>* %403, i64 %490
  %492 = bitcast <2 x i64>* %489 to <8 x i16>*
  %493 = load <8 x i16>, <8 x i16>* %492, align 16
  %494 = getelementptr inbounds <2 x i64>, <2 x i64>* %489, i64 1
  %495 = bitcast <2 x i64>* %494 to <8 x i16>*
  %496 = load <8 x i16>, <8 x i16>* %495, align 16
  %497 = shufflevector <8 x i16> %493, <8 x i16> %496, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %498 = getelementptr inbounds <2 x i64>, <2 x i64>* %489, i64 2
  %499 = bitcast <2 x i64>* %498 to <8 x i16>*
  %500 = load <8 x i16>, <8 x i16>* %499, align 16
  %501 = getelementptr inbounds <2 x i64>, <2 x i64>* %489, i64 3
  %502 = bitcast <2 x i64>* %501 to <8 x i16>*
  %503 = load <8 x i16>, <8 x i16>* %502, align 16
  %504 = shufflevector <8 x i16> %500, <8 x i16> %503, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %505 = getelementptr inbounds <2 x i64>, <2 x i64>* %489, i64 4
  %506 = bitcast <2 x i64>* %505 to <8 x i16>*
  %507 = load <8 x i16>, <8 x i16>* %506, align 16
  %508 = getelementptr inbounds <2 x i64>, <2 x i64>* %489, i64 5
  %509 = bitcast <2 x i64>* %508 to <8 x i16>*
  %510 = load <8 x i16>, <8 x i16>* %509, align 16
  %511 = shufflevector <8 x i16> %507, <8 x i16> %510, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %512 = getelementptr inbounds <2 x i64>, <2 x i64>* %489, i64 6
  %513 = bitcast <2 x i64>* %512 to <8 x i16>*
  %514 = load <8 x i16>, <8 x i16>* %513, align 16
  %515 = getelementptr inbounds <2 x i64>, <2 x i64>* %489, i64 7
  %516 = bitcast <2 x i64>* %515 to <8 x i16>*
  %517 = load <8 x i16>, <8 x i16>* %516, align 16
  %518 = shufflevector <8 x i16> %514, <8 x i16> %517, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %519 = shufflevector <8 x i16> %493, <8 x i16> %496, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %520 = shufflevector <8 x i16> %500, <8 x i16> %503, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %521 = shufflevector <8 x i16> %507, <8 x i16> %510, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %522 = shufflevector <8 x i16> %514, <8 x i16> %517, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %523 = bitcast <8 x i16> %497 to <4 x i32>
  %524 = bitcast <8 x i16> %504 to <4 x i32>
  %525 = shufflevector <4 x i32> %523, <4 x i32> %524, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %526 = bitcast <4 x i32> %525 to <2 x i64>
  %527 = bitcast <8 x i16> %511 to <4 x i32>
  %528 = bitcast <8 x i16> %518 to <4 x i32>
  %529 = shufflevector <4 x i32> %527, <4 x i32> %528, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %530 = bitcast <4 x i32> %529 to <2 x i64>
  %531 = bitcast <8 x i16> %519 to <4 x i32>
  %532 = bitcast <8 x i16> %520 to <4 x i32>
  %533 = shufflevector <4 x i32> %531, <4 x i32> %532, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %534 = bitcast <4 x i32> %533 to <2 x i64>
  %535 = bitcast <8 x i16> %521 to <4 x i32>
  %536 = bitcast <8 x i16> %522 to <4 x i32>
  %537 = shufflevector <4 x i32> %535, <4 x i32> %536, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %538 = bitcast <4 x i32> %537 to <2 x i64>
  %539 = shufflevector <4 x i32> %523, <4 x i32> %524, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %540 = bitcast <4 x i32> %539 to <2 x i64>
  %541 = shufflevector <4 x i32> %527, <4 x i32> %528, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %542 = bitcast <4 x i32> %541 to <2 x i64>
  %543 = shufflevector <4 x i32> %531, <4 x i32> %532, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %544 = bitcast <4 x i32> %543 to <2 x i64>
  %545 = shufflevector <4 x i32> %535, <4 x i32> %536, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %546 = bitcast <4 x i32> %545 to <2 x i64>
  %547 = shufflevector <2 x i64> %526, <2 x i64> %530, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %547, <2 x i64>* %491, align 16
  %548 = shufflevector <2 x i64> %526, <2 x i64> %530, <2 x i32> <i32 1, i32 3>
  %549 = getelementptr inbounds <2 x i64>, <2 x i64>* %491, i64 1
  store <2 x i64> %548, <2 x i64>* %549, align 16
  %550 = shufflevector <2 x i64> %540, <2 x i64> %542, <2 x i32> <i32 0, i32 2>
  %551 = getelementptr inbounds <2 x i64>, <2 x i64>* %491, i64 2
  store <2 x i64> %550, <2 x i64>* %551, align 16
  %552 = shufflevector <2 x i64> %540, <2 x i64> %542, <2 x i32> <i32 1, i32 3>
  %553 = getelementptr inbounds <2 x i64>, <2 x i64>* %491, i64 3
  store <2 x i64> %552, <2 x i64>* %553, align 16
  %554 = shufflevector <2 x i64> %534, <2 x i64> %538, <2 x i32> <i32 0, i32 2>
  %555 = getelementptr inbounds <2 x i64>, <2 x i64>* %491, i64 4
  store <2 x i64> %554, <2 x i64>* %555, align 16
  %556 = shufflevector <2 x i64> %534, <2 x i64> %538, <2 x i32> <i32 1, i32 3>
  %557 = getelementptr inbounds <2 x i64>, <2 x i64>* %491, i64 5
  store <2 x i64> %556, <2 x i64>* %557, align 16
  %558 = shufflevector <2 x i64> %544, <2 x i64> %546, <2 x i32> <i32 0, i32 2>
  %559 = getelementptr inbounds <2 x i64>, <2 x i64>* %491, i64 6
  store <2 x i64> %558, <2 x i64>* %559, align 16
  %560 = shufflevector <2 x i64> %544, <2 x i64> %546, <2 x i32> <i32 1, i32 3>
  %561 = getelementptr inbounds <2 x i64>, <2 x i64>* %491, i64 7
  store <2 x i64> %560, <2 x i64>* %561, align 16
  %562 = add nuw nsw i64 %487, 1
  %563 = icmp slt i64 %562, %117
  br i1 %563, label %486, label %564

564:                                              ; preds = %406, %486, %405, %404
  call void @llvm.lifetime.end.p0i8(i64 1024, i8* nonnull %107) #9
  %565 = add nuw nsw i64 %159, 1
  %566 = icmp slt i64 %565, %119
  br i1 %566, label %158, label %141

567:                                              ; preds = %673, %141
  %568 = lshr i64 483100, %11
  %569 = and i64 %568, 1
  %570 = icmp eq i64 %569, 0
  br i1 %570, label %711, label %571

571:                                              ; preds = %567
  %572 = ashr i32 %43, 4
  %573 = shl i32 %45, 1
  %574 = icmp ne i32 %100, 0
  %575 = select i1 %574, i64 -1, i64 1
  %576 = add nsw i32 %45, -1
  %577 = select i1 %574, i32 %576, i32 0
  %578 = sext i32 %577 to i64
  %579 = sext i32 %45 to i64
  %580 = sext i32 %2 to i64
  %581 = zext i32 %45 to i64
  %582 = sext i32 %572 to i64
  br label %676

583:                                              ; preds = %144, %673
  %584 = phi i64 [ 0, %144 ], [ %674, %673 ]
  %585 = mul nsw i64 %584, %147
  %586 = getelementptr inbounds [512 x <2 x i64>], [512 x <2 x i64>]* %7, i64 0, i64 %585
  call void %89(<2 x i64>* %586, <2 x i64>* %586, i8 signext %39) #9
  %587 = load i8, i8* %145, align 1
  %588 = sext i8 %587 to i32
  %589 = icmp slt i8 %587, 0
  br i1 %589, label %590, label %621

590:                                              ; preds = %583
  %591 = add nsw i32 %588, 15
  %592 = shl i32 1, %591
  %593 = trunc i32 %592 to i16
  %594 = insertelement <8 x i16> undef, i16 %593, i32 0
  %595 = shufflevector <8 x i16> %594, <8 x i16> undef, <8 x i32> zeroinitializer
  br i1 %155, label %649, label %596

596:                                              ; preds = %590, %596
  %597 = phi i64 [ %618, %596 ], [ 0, %590 ]
  %598 = phi i64 [ %619, %596 ], [ %156, %590 ]
  %599 = getelementptr inbounds <2 x i64>, <2 x i64>* %586, i64 %597
  %600 = bitcast <2 x i64>* %599 to <8 x i16>*
  %601 = load <8 x i16>, <8 x i16>* %600, align 16
  %602 = call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %601, <8 x i16> %595) #9
  store <8 x i16> %602, <8 x i16>* %600, align 16
  %603 = or i64 %597, 1
  %604 = getelementptr inbounds <2 x i64>, <2 x i64>* %586, i64 %603
  %605 = bitcast <2 x i64>* %604 to <8 x i16>*
  %606 = load <8 x i16>, <8 x i16>* %605, align 16
  %607 = call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %606, <8 x i16> %595) #9
  store <8 x i16> %607, <8 x i16>* %605, align 16
  %608 = or i64 %597, 2
  %609 = getelementptr inbounds <2 x i64>, <2 x i64>* %586, i64 %608
  %610 = bitcast <2 x i64>* %609 to <8 x i16>*
  %611 = load <8 x i16>, <8 x i16>* %610, align 16
  %612 = call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %611, <8 x i16> %595) #9
  store <8 x i16> %612, <8 x i16>* %610, align 16
  %613 = or i64 %597, 3
  %614 = getelementptr inbounds <2 x i64>, <2 x i64>* %586, i64 %613
  %615 = bitcast <2 x i64>* %614 to <8 x i16>*
  %616 = load <8 x i16>, <8 x i16>* %615, align 16
  %617 = call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %616, <8 x i16> %595) #9
  store <8 x i16> %617, <8 x i16>* %615, align 16
  %618 = add nuw nsw i64 %597, 4
  %619 = add i64 %598, -4
  %620 = icmp eq i64 %619, 0
  br i1 %620, label %649, label %596

621:                                              ; preds = %583
  %622 = icmp eq i8 %587, 0
  br i1 %622, label %673, label %623

623:                                              ; preds = %621
  br i1 %151, label %661, label %624

624:                                              ; preds = %623, %624
  %625 = phi i64 [ %646, %624 ], [ 0, %623 ]
  %626 = phi i64 [ %647, %624 ], [ %152, %623 ]
  %627 = getelementptr inbounds <2 x i64>, <2 x i64>* %586, i64 %625
  %628 = bitcast <2 x i64>* %627 to <8 x i16>*
  %629 = load <8 x i16>, <8 x i16>* %628, align 16
  %630 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %629, i32 %588) #9
  store <8 x i16> %630, <8 x i16>* %628, align 16
  %631 = or i64 %625, 1
  %632 = getelementptr inbounds <2 x i64>, <2 x i64>* %586, i64 %631
  %633 = bitcast <2 x i64>* %632 to <8 x i16>*
  %634 = load <8 x i16>, <8 x i16>* %633, align 16
  %635 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %634, i32 %588) #9
  store <8 x i16> %635, <8 x i16>* %633, align 16
  %636 = or i64 %625, 2
  %637 = getelementptr inbounds <2 x i64>, <2 x i64>* %586, i64 %636
  %638 = bitcast <2 x i64>* %637 to <8 x i16>*
  %639 = load <8 x i16>, <8 x i16>* %638, align 16
  %640 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %639, i32 %588) #9
  store <8 x i16> %640, <8 x i16>* %638, align 16
  %641 = or i64 %625, 3
  %642 = getelementptr inbounds <2 x i64>, <2 x i64>* %586, i64 %641
  %643 = bitcast <2 x i64>* %642 to <8 x i16>*
  %644 = load <8 x i16>, <8 x i16>* %643, align 16
  %645 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %644, i32 %588) #9
  store <8 x i16> %645, <8 x i16>* %643, align 16
  %646 = add nuw nsw i64 %625, 4
  %647 = add i64 %626, -4
  %648 = icmp eq i64 %647, 0
  br i1 %648, label %661, label %624

649:                                              ; preds = %596, %590
  %650 = phi i64 [ 0, %590 ], [ %618, %596 ]
  br i1 %157, label %673, label %651

651:                                              ; preds = %649, %651
  %652 = phi i64 [ %658, %651 ], [ %650, %649 ]
  %653 = phi i64 [ %659, %651 ], [ %154, %649 ]
  %654 = getelementptr inbounds <2 x i64>, <2 x i64>* %586, i64 %652
  %655 = bitcast <2 x i64>* %654 to <8 x i16>*
  %656 = load <8 x i16>, <8 x i16>* %655, align 16
  %657 = call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %656, <8 x i16> %595) #9
  store <8 x i16> %657, <8 x i16>* %655, align 16
  %658 = add nuw nsw i64 %652, 1
  %659 = add i64 %653, -1
  %660 = icmp eq i64 %659, 0
  br i1 %660, label %673, label %651, !llvm.loop !9

661:                                              ; preds = %624, %623
  %662 = phi i64 [ 0, %623 ], [ %646, %624 ]
  br i1 %153, label %673, label %663

663:                                              ; preds = %661, %663
  %664 = phi i64 [ %670, %663 ], [ %662, %661 ]
  %665 = phi i64 [ %671, %663 ], [ %150, %661 ]
  %666 = getelementptr inbounds <2 x i64>, <2 x i64>* %586, i64 %664
  %667 = bitcast <2 x i64>* %666 to <8 x i16>*
  %668 = load <8 x i16>, <8 x i16>* %667, align 16
  %669 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %668, i32 %588) #9
  store <8 x i16> %669, <8 x i16>* %667, align 16
  %670 = add nuw nsw i64 %664, 1
  %671 = add i64 %665, -1
  %672 = icmp eq i64 %671, 0
  br i1 %672, label %673, label %663, !llvm.loop !10

673:                                              ; preds = %661, %663, %649, %651, %621
  %674 = add nuw nsw i64 %584, 1
  %675 = icmp slt i64 %674, %148
  br i1 %675, label %583, label %567

676:                                              ; preds = %571, %708
  %677 = phi i64 [ 0, %571 ], [ %709, %708 ]
  %678 = trunc i64 %677 to i32
  %679 = mul i32 %573, %678
  %680 = sext i32 %679 to i64
  %681 = getelementptr inbounds [512 x <2 x i64>], [512 x <2 x i64>]* %7, i64 0, i64 %680
  %682 = shl nsw i64 %677, 4
  %683 = getelementptr inbounds i8, i8* %1, i64 %682
  br label %684

684:                                              ; preds = %684, %676
  %685 = phi i64 [ 0, %676 ], [ %705, %684 ]
  %686 = phi i64 [ %578, %676 ], [ %706, %684 ]
  %687 = mul nsw i64 %685, %580
  %688 = getelementptr inbounds i8, i8* %683, i64 %687
  %689 = bitcast i8* %688 to <16 x i8>*
  %690 = load <16 x i8>, <16 x i8>* %689, align 1
  %691 = getelementptr inbounds <2 x i64>, <2 x i64>* %681, i64 %686
  %692 = bitcast <2 x i64>* %691 to <8 x i16>*
  %693 = load <8 x i16>, <8 x i16>* %692, align 16
  %694 = add nsw i64 %686, %579
  %695 = getelementptr inbounds <2 x i64>, <2 x i64>* %681, i64 %694
  %696 = bitcast <2 x i64>* %695 to <8 x i16>*
  %697 = load <8 x i16>, <8 x i16>* %696, align 16
  %698 = shufflevector <16 x i8> %690, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %699 = shufflevector <16 x i8> %690, <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %700 = bitcast <16 x i8> %698 to <8 x i16>
  %701 = call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %693, <8 x i16> %700) #9
  %702 = bitcast <16 x i8> %699 to <8 x i16>
  %703 = call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %697, <8 x i16> %702) #9
  %704 = call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %701, <8 x i16> %703) #9
  store <16 x i8> %704, <16 x i8>* %689, align 1
  %705 = add nuw nsw i64 %685, 1
  %706 = add i64 %686, %575
  %707 = icmp eq i64 %705, %581
  br i1 %707, label %708, label %684

708:                                              ; preds = %684
  %709 = add nuw nsw i64 %677, 1
  %710 = icmp slt i64 %709, %582
  br i1 %710, label %676, label %744

711:                                              ; preds = %567
  %712 = lshr i64 32962, %11
  %713 = and i64 %712, 1
  %714 = icmp eq i64 %713, 0
  br i1 %714, label %744, label %715

715:                                              ; preds = %711
  %716 = icmp ne i32 %100, 0
  %717 = select i1 %716, i64 -1, i64 1
  %718 = add nsw i32 %45, -1
  %719 = select i1 %716, i32 %718, i32 0
  %720 = sext i32 %719 to i64
  %721 = sext i32 %2 to i64
  %722 = zext i32 %45 to i64
  br label %723

723:                                              ; preds = %723, %715
  %724 = phi i64 [ 0, %715 ], [ %741, %723 ]
  %725 = phi i64 [ %720, %715 ], [ %742, %723 ]
  %726 = mul nsw i64 %724, %721
  %727 = getelementptr inbounds i8, i8* %1, i64 %726
  %728 = bitcast i8* %727 to i64*
  %729 = load i64, i64* %728, align 1
  %730 = insertelement <2 x i64> undef, i64 %729, i32 0
  %731 = getelementptr inbounds [512 x <2 x i64>], [512 x <2 x i64>]* %7, i64 0, i64 %725
  %732 = bitcast <2 x i64>* %731 to <8 x i16>*
  %733 = load <8 x i16>, <8 x i16>* %732, align 16
  %734 = bitcast <2 x i64> %730 to <16 x i8>
  %735 = shufflevector <16 x i8> %734, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %736 = bitcast <16 x i8> %735 to <8 x i16>
  %737 = call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %733, <8 x i16> %736) #9
  %738 = call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %737, <8 x i16> undef) #9
  %739 = bitcast <16 x i8> %738 to <2 x i64>
  %740 = extractelement <2 x i64> %739, i32 0
  store i64 %740, i64* %728, align 1
  %741 = add nuw nsw i64 %724, 1
  %742 = add i64 %725, %717
  %743 = icmp eq i64 %741, %722
  br i1 %743, label %744, label %723

744:                                              ; preds = %708, %723, %711
  call void @llvm.lifetime.end.p0i8(i64 8192, i8* nonnull %9) #9
  ret void
}

; Function Attrs: nofree nounwind ssp uwtable
define internal void @idct8_low1_ssse3(<2 x i64>* nocapture readonly, <2 x i64>* nocapture, i8 signext) #4 {
  %4 = bitcast <2 x i64>* %0 to <8 x i16>*
  %5 = load <8 x i16>, <8 x i16>* %4, align 16
  %6 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 32), align 16
  %7 = trunc i32 %6 to i16
  %8 = shl i16 %7, 3
  %9 = insertelement <8 x i16> undef, i16 %8, i32 0
  %10 = shufflevector <8 x i16> %9, <8 x i16> undef, <8 x i32> zeroinitializer
  %11 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %5, <8 x i16> %10) #9
  %12 = bitcast <2 x i64>* %1 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %12, align 16
  %13 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 7
  %14 = bitcast <2 x i64>* %13 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %14, align 16
  %15 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 1
  %16 = bitcast <2 x i64>* %15 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %16, align 16
  %17 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 6
  %18 = bitcast <2 x i64>* %17 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %18, align 16
  %19 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 2
  %20 = bitcast <2 x i64>* %19 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %20, align 16
  %21 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 5
  %22 = bitcast <2 x i64>* %21 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %22, align 16
  %23 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 3
  %24 = bitcast <2 x i64>* %23 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %24, align 16
  %25 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 4
  %26 = bitcast <2 x i64>* %25 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %26, align 16
  ret void
}

; Function Attrs: nofree nounwind ssp uwtable
define internal void @iadst8_low1_ssse3(<2 x i64>* nocapture readonly, <2 x i64>* nocapture, i8 signext) #4 {
  %4 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 16), align 16
  %5 = and i32 %4, 65535
  %6 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 48), align 16
  %7 = shl i32 %6, 16
  %8 = or i32 %7, %5
  %9 = insertelement <4 x i32> undef, i32 %8, i32 0
  %10 = shufflevector <4 x i32> %9, <4 x i32> undef, <4 x i32> zeroinitializer
  %11 = and i32 %6, 65535
  %12 = shl i32 %4, 16
  %13 = sub i32 0, %12
  %14 = or i32 %11, %13
  %15 = insertelement <4 x i32> undef, i32 %14, i32 0
  %16 = shufflevector <4 x i32> %15, <4 x i32> undef, <4 x i32> zeroinitializer
  %17 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 32), align 16
  %18 = and i32 %17, 65535
  %19 = shl i32 %17, 16
  %20 = or i32 %18, %19
  %21 = insertelement <4 x i32> undef, i32 %20, i32 0
  %22 = shufflevector <4 x i32> %21, <4 x i32> undef, <4 x i32> zeroinitializer
  %23 = sub i32 0, %19
  %24 = or i32 %18, %23
  %25 = insertelement <4 x i32> undef, i32 %24, i32 0
  %26 = shufflevector <4 x i32> %25, <4 x i32> undef, <4 x i32> zeroinitializer
  %27 = bitcast <2 x i64>* %0 to <8 x i16>*
  %28 = load <8 x i16>, <8 x i16>* %27, align 16
  %29 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 60), align 16
  %30 = trunc i32 %29 to i16
  %31 = shl i16 %30, 3
  %32 = insertelement <8 x i16> undef, i16 %31, i32 0
  %33 = shufflevector <8 x i16> %32, <8 x i16> undef, <8 x i32> zeroinitializer
  %34 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 4), align 16
  %35 = trunc i32 %34 to i16
  %36 = shl i16 %35, 3
  %37 = sub i16 0, %36
  %38 = insertelement <8 x i16> undef, i16 %37, i32 0
  %39 = shufflevector <8 x i16> %38, <8 x i16> undef, <8 x i32> zeroinitializer
  %40 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %28, <8 x i16> %33) #9
  %41 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %28, <8 x i16> %39) #9
  %42 = shufflevector <8 x i16> %40, <8 x i16> %41, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %43 = shufflevector <8 x i16> %40, <8 x i16> %41, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %44 = bitcast <4 x i32> %10 to <8 x i16>
  %45 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %42, <8 x i16> %44) #9
  %46 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %43, <8 x i16> %44) #9
  %47 = bitcast <4 x i32> %16 to <8 x i16>
  %48 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %42, <8 x i16> %47) #9
  %49 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %43, <8 x i16> %47) #9
  %50 = add <4 x i32> %45, <i32 2048, i32 2048, i32 2048, i32 2048>
  %51 = add <4 x i32> %46, <i32 2048, i32 2048, i32 2048, i32 2048>
  %52 = add <4 x i32> %48, <i32 2048, i32 2048, i32 2048, i32 2048>
  %53 = add <4 x i32> %49, <i32 2048, i32 2048, i32 2048, i32 2048>
  %54 = sext i8 %2 to i32
  %55 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %50, i32 %54) #9
  %56 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %51, i32 %54) #9
  %57 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %52, i32 %54) #9
  %58 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %53, i32 %54) #9
  %59 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %55, <4 x i32> %56) #9
  %60 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %57, <4 x i32> %58) #9
  %61 = bitcast <4 x i32> %22 to <8 x i16>
  %62 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %42, <8 x i16> %61) #9
  %63 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %43, <8 x i16> %61) #9
  %64 = bitcast <4 x i32> %26 to <8 x i16>
  %65 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %42, <8 x i16> %64) #9
  %66 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %43, <8 x i16> %64) #9
  %67 = add <4 x i32> %62, <i32 2048, i32 2048, i32 2048, i32 2048>
  %68 = add <4 x i32> %63, <i32 2048, i32 2048, i32 2048, i32 2048>
  %69 = add <4 x i32> %65, <i32 2048, i32 2048, i32 2048, i32 2048>
  %70 = add <4 x i32> %66, <i32 2048, i32 2048, i32 2048, i32 2048>
  %71 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %67, i32 %54) #9
  %72 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %68, i32 %54) #9
  %73 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %69, i32 %54) #9
  %74 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %70, i32 %54) #9
  %75 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %71, <4 x i32> %72) #9
  %76 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %73, <4 x i32> %74) #9
  %77 = shufflevector <8 x i16> %59, <8 x i16> %60, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %78 = shufflevector <8 x i16> %59, <8 x i16> %60, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %79 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %77, <8 x i16> %61) #9
  %80 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %78, <8 x i16> %61) #9
  %81 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %77, <8 x i16> %64) #9
  %82 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %78, <8 x i16> %64) #9
  %83 = add <4 x i32> %79, <i32 2048, i32 2048, i32 2048, i32 2048>
  %84 = add <4 x i32> %80, <i32 2048, i32 2048, i32 2048, i32 2048>
  %85 = add <4 x i32> %81, <i32 2048, i32 2048, i32 2048, i32 2048>
  %86 = add <4 x i32> %82, <i32 2048, i32 2048, i32 2048, i32 2048>
  %87 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %83, i32 %54) #9
  %88 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %84, i32 %54) #9
  %89 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %85, i32 %54) #9
  %90 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %86, i32 %54) #9
  %91 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %87, <4 x i32> %88) #9
  %92 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %89, <4 x i32> %90) #9
  %93 = bitcast <2 x i64>* %1 to <8 x i16>*
  store <8 x i16> %40, <8 x i16>* %93, align 16
  %94 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %59) #9
  %95 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 1
  %96 = bitcast <2 x i64>* %95 to <8 x i16>*
  store <8 x i16> %94, <8 x i16>* %96, align 16
  %97 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 2
  %98 = bitcast <2 x i64>* %97 to <8 x i16>*
  store <8 x i16> %91, <8 x i16>* %98, align 16
  %99 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %75) #9
  %100 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 3
  %101 = bitcast <2 x i64>* %100 to <8 x i16>*
  store <8 x i16> %99, <8 x i16>* %101, align 16
  %102 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 4
  %103 = bitcast <2 x i64>* %102 to <8 x i16>*
  store <8 x i16> %76, <8 x i16>* %103, align 16
  %104 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %92) #9
  %105 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 5
  %106 = bitcast <2 x i64>* %105 to <8 x i16>*
  store <8 x i16> %104, <8 x i16>* %106, align 16
  %107 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 6
  %108 = bitcast <2 x i64>* %107 to <8 x i16>*
  store <8 x i16> %60, <8 x i16>* %108, align 16
  %109 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %41) #9
  %110 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 7
  %111 = bitcast <2 x i64>* %110 to <8 x i16>*
  store <8 x i16> %109, <8 x i16>* %111, align 16
  ret void
}

; Function Attrs: nofree nounwind ssp uwtable
define internal void @idct16_low1_ssse3(<2 x i64>* nocapture readonly, <2 x i64>* nocapture, i8 signext) #4 {
  %4 = bitcast <2 x i64>* %0 to <8 x i16>*
  %5 = load <8 x i16>, <8 x i16>* %4, align 16
  %6 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 32), align 16
  %7 = trunc i32 %6 to i16
  %8 = shl i16 %7, 3
  %9 = insertelement <8 x i16> undef, i16 %8, i32 0
  %10 = shufflevector <8 x i16> %9, <8 x i16> undef, <8 x i32> zeroinitializer
  %11 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %5, <8 x i16> %10) #9
  %12 = bitcast <2 x i64>* %1 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %12, align 16
  %13 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 15
  %14 = bitcast <2 x i64>* %13 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %14, align 16
  %15 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 1
  %16 = bitcast <2 x i64>* %15 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %16, align 16
  %17 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 14
  %18 = bitcast <2 x i64>* %17 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %18, align 16
  %19 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 2
  %20 = bitcast <2 x i64>* %19 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %20, align 16
  %21 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 13
  %22 = bitcast <2 x i64>* %21 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %22, align 16
  %23 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 3
  %24 = bitcast <2 x i64>* %23 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %24, align 16
  %25 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 12
  %26 = bitcast <2 x i64>* %25 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %26, align 16
  %27 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 4
  %28 = bitcast <2 x i64>* %27 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %28, align 16
  %29 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 11
  %30 = bitcast <2 x i64>* %29 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %30, align 16
  %31 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 5
  %32 = bitcast <2 x i64>* %31 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %32, align 16
  %33 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 10
  %34 = bitcast <2 x i64>* %33 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %34, align 16
  %35 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 6
  %36 = bitcast <2 x i64>* %35 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %36, align 16
  %37 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 9
  %38 = bitcast <2 x i64>* %37 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %38, align 16
  %39 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 7
  %40 = bitcast <2 x i64>* %39 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %40, align 16
  %41 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 8
  %42 = bitcast <2 x i64>* %41 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %42, align 16
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal void @idct16_low8_ssse3(<2 x i64>* nocapture readonly, <2 x i64>* nocapture, i8 signext) #0 {
  %4 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 16), align 16
  %5 = sub i32 0, %4
  %6 = and i32 %5, 65535
  %7 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 48), align 16
  %8 = shl i32 %7, 16
  %9 = or i32 %8, %6
  %10 = insertelement <4 x i32> undef, i32 %9, i32 0
  %11 = shufflevector <4 x i32> %10, <4 x i32> undef, <4 x i32> zeroinitializer
  %12 = and i32 %7, 65535
  %13 = shl i32 %4, 16
  %14 = or i32 %12, %13
  %15 = insertelement <4 x i32> undef, i32 %14, i32 0
  %16 = shufflevector <4 x i32> %15, <4 x i32> undef, <4 x i32> zeroinitializer
  %17 = sub i32 0, %7
  %18 = and i32 %17, 65535
  %19 = sub i32 0, %13
  %20 = or i32 %18, %19
  %21 = insertelement <4 x i32> undef, i32 %20, i32 0
  %22 = shufflevector <4 x i32> %21, <4 x i32> undef, <4 x i32> zeroinitializer
  %23 = bitcast <2 x i64>* %0 to <8 x i16>*
  %24 = load <8 x i16>, <8 x i16>* %23, align 16
  %25 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 4
  %26 = bitcast <2 x i64>* %25 to <8 x i16>*
  %27 = load <8 x i16>, <8 x i16>* %26, align 16
  %28 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 2
  %29 = bitcast <2 x i64>* %28 to <8 x i16>*
  %30 = load <8 x i16>, <8 x i16>* %29, align 16
  %31 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 6
  %32 = bitcast <2 x i64>* %31 to <8 x i16>*
  %33 = load <8 x i16>, <8 x i16>* %32, align 16
  %34 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 1
  %35 = bitcast <2 x i64>* %34 to <8 x i16>*
  %36 = load <8 x i16>, <8 x i16>* %35, align 16
  %37 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 5
  %38 = bitcast <2 x i64>* %37 to <8 x i16>*
  %39 = load <8 x i16>, <8 x i16>* %38, align 16
  %40 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 3
  %41 = bitcast <2 x i64>* %40 to <8 x i16>*
  %42 = load <8 x i16>, <8 x i16>* %41, align 16
  %43 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 7
  %44 = bitcast <2 x i64>* %43 to <8 x i16>*
  %45 = load <8 x i16>, <8 x i16>* %44, align 16
  %46 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 60), align 16
  %47 = trunc i32 %46 to i16
  %48 = shl i16 %47, 3
  %49 = insertelement <8 x i16> undef, i16 %48, i32 0
  %50 = shufflevector <8 x i16> %49, <8 x i16> undef, <8 x i32> zeroinitializer
  %51 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 4), align 16
  %52 = trunc i32 %51 to i16
  %53 = shl i16 %52, 3
  %54 = insertelement <8 x i16> undef, i16 %53, i32 0
  %55 = shufflevector <8 x i16> %54, <8 x i16> undef, <8 x i32> zeroinitializer
  %56 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %36, <8 x i16> %50) #9
  %57 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %36, <8 x i16> %55) #9
  %58 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 36), align 16
  %59 = trunc i32 %58 to i16
  %60 = shl i16 %59, 3
  %61 = sub i16 0, %60
  %62 = insertelement <8 x i16> undef, i16 %61, i32 0
  %63 = shufflevector <8 x i16> %62, <8 x i16> undef, <8 x i32> zeroinitializer
  %64 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 28), align 16
  %65 = trunc i32 %64 to i16
  %66 = shl i16 %65, 3
  %67 = insertelement <8 x i16> undef, i16 %66, i32 0
  %68 = shufflevector <8 x i16> %67, <8 x i16> undef, <8 x i32> zeroinitializer
  %69 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %45, <8 x i16> %63) #9
  %70 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %45, <8 x i16> %68) #9
  %71 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 44), align 16
  %72 = trunc i32 %71 to i16
  %73 = shl i16 %72, 3
  %74 = insertelement <8 x i16> undef, i16 %73, i32 0
  %75 = shufflevector <8 x i16> %74, <8 x i16> undef, <8 x i32> zeroinitializer
  %76 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 20), align 16
  %77 = trunc i32 %76 to i16
  %78 = shl i16 %77, 3
  %79 = insertelement <8 x i16> undef, i16 %78, i32 0
  %80 = shufflevector <8 x i16> %79, <8 x i16> undef, <8 x i32> zeroinitializer
  %81 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %39, <8 x i16> %75) #9
  %82 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %39, <8 x i16> %80) #9
  %83 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 52), align 16
  %84 = trunc i32 %83 to i16
  %85 = shl i16 %84, 3
  %86 = sub i16 0, %85
  %87 = insertelement <8 x i16> undef, i16 %86, i32 0
  %88 = shufflevector <8 x i16> %87, <8 x i16> undef, <8 x i32> zeroinitializer
  %89 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 12), align 16
  %90 = trunc i32 %89 to i16
  %91 = shl i16 %90, 3
  %92 = insertelement <8 x i16> undef, i16 %91, i32 0
  %93 = shufflevector <8 x i16> %92, <8 x i16> undef, <8 x i32> zeroinitializer
  %94 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %42, <8 x i16> %88) #9
  %95 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %42, <8 x i16> %93) #9
  %96 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 56), align 16
  %97 = trunc i32 %96 to i16
  %98 = shl i16 %97, 3
  %99 = insertelement <8 x i16> undef, i16 %98, i32 0
  %100 = shufflevector <8 x i16> %99, <8 x i16> undef, <8 x i32> zeroinitializer
  %101 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 8), align 16
  %102 = trunc i32 %101 to i16
  %103 = shl i16 %102, 3
  %104 = insertelement <8 x i16> undef, i16 %103, i32 0
  %105 = shufflevector <8 x i16> %104, <8 x i16> undef, <8 x i32> zeroinitializer
  %106 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %30, <8 x i16> %100) #9
  %107 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %30, <8 x i16> %105) #9
  %108 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 40), align 16
  %109 = trunc i32 %108 to i16
  %110 = shl i16 %109, 3
  %111 = sub i16 0, %110
  %112 = insertelement <8 x i16> undef, i16 %111, i32 0
  %113 = shufflevector <8 x i16> %112, <8 x i16> undef, <8 x i32> zeroinitializer
  %114 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 24), align 16
  %115 = trunc i32 %114 to i16
  %116 = shl i16 %115, 3
  %117 = insertelement <8 x i16> undef, i16 %116, i32 0
  %118 = shufflevector <8 x i16> %117, <8 x i16> undef, <8 x i32> zeroinitializer
  %119 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %33, <8 x i16> %113) #9
  %120 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %33, <8 x i16> %118) #9
  %121 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %56, <8 x i16> %69) #9
  %122 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %56, <8 x i16> %69) #9
  %123 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %94, <8 x i16> %81) #9
  %124 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %94, <8 x i16> %81) #9
  %125 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %95, <8 x i16> %82) #9
  %126 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %95, <8 x i16> %82) #9
  %127 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %57, <8 x i16> %70) #9
  %128 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %57, <8 x i16> %70) #9
  %129 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 32), align 16
  %130 = trunc i32 %129 to i16
  %131 = shl i16 %130, 3
  %132 = insertelement <8 x i16> undef, i16 %131, i32 0
  %133 = shufflevector <8 x i16> %132, <8 x i16> undef, <8 x i32> zeroinitializer
  %134 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %24, <8 x i16> %133) #9
  %135 = trunc i32 %7 to i16
  %136 = shl i16 %135, 3
  %137 = insertelement <8 x i16> undef, i16 %136, i32 0
  %138 = shufflevector <8 x i16> %137, <8 x i16> undef, <8 x i32> zeroinitializer
  %139 = trunc i32 %4 to i16
  %140 = shl i16 %139, 3
  %141 = insertelement <8 x i16> undef, i16 %140, i32 0
  %142 = shufflevector <8 x i16> %141, <8 x i16> undef, <8 x i32> zeroinitializer
  %143 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %27, <8 x i16> %138) #9
  %144 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %27, <8 x i16> %142) #9
  %145 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %106, <8 x i16> %119) #9
  %146 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %106, <8 x i16> %119) #9
  %147 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %107, <8 x i16> %120) #9
  %148 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %107, <8 x i16> %120) #9
  %149 = shufflevector <8 x i16> %122, <8 x i16> %127, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %150 = shufflevector <8 x i16> %122, <8 x i16> %127, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %151 = bitcast <4 x i32> %11 to <8 x i16>
  %152 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %149, <8 x i16> %151) #9
  %153 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %150, <8 x i16> %151) #9
  %154 = bitcast <4 x i32> %16 to <8 x i16>
  %155 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %149, <8 x i16> %154) #9
  %156 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %150, <8 x i16> %154) #9
  %157 = add <4 x i32> %152, <i32 2048, i32 2048, i32 2048, i32 2048>
  %158 = add <4 x i32> %153, <i32 2048, i32 2048, i32 2048, i32 2048>
  %159 = add <4 x i32> %155, <i32 2048, i32 2048, i32 2048, i32 2048>
  %160 = add <4 x i32> %156, <i32 2048, i32 2048, i32 2048, i32 2048>
  %161 = sext i8 %2 to i32
  %162 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %157, i32 %161) #9
  %163 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %158, i32 %161) #9
  %164 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %159, i32 %161) #9
  %165 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %160, i32 %161) #9
  %166 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %162, <4 x i32> %163) #9
  %167 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %164, <4 x i32> %165) #9
  %168 = shufflevector <8 x i16> %123, <8 x i16> %126, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %169 = shufflevector <8 x i16> %123, <8 x i16> %126, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %170 = bitcast <4 x i32> %22 to <8 x i16>
  %171 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %168, <8 x i16> %170) #9
  %172 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %169, <8 x i16> %170) #9
  %173 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %168, <8 x i16> %151) #9
  %174 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %169, <8 x i16> %151) #9
  %175 = add <4 x i32> %171, <i32 2048, i32 2048, i32 2048, i32 2048>
  %176 = add <4 x i32> %172, <i32 2048, i32 2048, i32 2048, i32 2048>
  %177 = add <4 x i32> %173, <i32 2048, i32 2048, i32 2048, i32 2048>
  %178 = add <4 x i32> %174, <i32 2048, i32 2048, i32 2048, i32 2048>
  %179 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %175, i32 %161) #9
  %180 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %176, i32 %161) #9
  %181 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %177, i32 %161) #9
  %182 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %178, i32 %161) #9
  %183 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %179, <4 x i32> %180) #9
  %184 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %181, <4 x i32> %182) #9
  %185 = sub i32 0, %129
  %186 = and i32 %185, 65535
  %187 = shl i32 %129, 16
  %188 = or i32 %186, %187
  %189 = insertelement <4 x i32> undef, i32 %188, i32 0
  %190 = shufflevector <4 x i32> %189, <4 x i32> undef, <4 x i32> zeroinitializer
  %191 = and i32 %129, 65535
  %192 = or i32 %191, %187
  %193 = insertelement <4 x i32> undef, i32 %192, i32 0
  %194 = shufflevector <4 x i32> %193, <4 x i32> undef, <4 x i32> zeroinitializer
  %195 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %134, <8 x i16> %144) #9
  %196 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %134, <8 x i16> %144) #9
  %197 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %134, <8 x i16> %143) #9
  %198 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %134, <8 x i16> %143) #9
  %199 = shufflevector <8 x i16> %146, <8 x i16> %147, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %200 = shufflevector <8 x i16> %146, <8 x i16> %147, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %201 = bitcast <4 x i32> %190 to <8 x i16>
  %202 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %199, <8 x i16> %201) #9
  %203 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %200, <8 x i16> %201) #9
  %204 = bitcast <4 x i32> %194 to <8 x i16>
  %205 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %199, <8 x i16> %204) #9
  %206 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %200, <8 x i16> %204) #9
  %207 = add <4 x i32> %202, <i32 2048, i32 2048, i32 2048, i32 2048>
  %208 = add <4 x i32> %203, <i32 2048, i32 2048, i32 2048, i32 2048>
  %209 = add <4 x i32> %205, <i32 2048, i32 2048, i32 2048, i32 2048>
  %210 = add <4 x i32> %206, <i32 2048, i32 2048, i32 2048, i32 2048>
  %211 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %207, i32 %161) #9
  %212 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %208, i32 %161) #9
  %213 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %209, i32 %161) #9
  %214 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %210, i32 %161) #9
  %215 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %211, <4 x i32> %212) #9
  %216 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %213, <4 x i32> %214) #9
  %217 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %121, <8 x i16> %124) #9
  %218 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %121, <8 x i16> %124) #9
  %219 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %166, <8 x i16> %183) #9
  %220 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %166, <8 x i16> %183) #9
  %221 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %128, <8 x i16> %125) #9
  %222 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %128, <8 x i16> %125) #9
  %223 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %167, <8 x i16> %184) #9
  %224 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %167, <8 x i16> %184) #9
  %225 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %195, <8 x i16> %148) #9
  %226 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %195, <8 x i16> %148) #9
  %227 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %197, <8 x i16> %216) #9
  %228 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %197, <8 x i16> %216) #9
  %229 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %198, <8 x i16> %215) #9
  %230 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %198, <8 x i16> %215) #9
  %231 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %196, <8 x i16> %145) #9
  %232 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %196, <8 x i16> %145) #9
  %233 = shufflevector <8 x i16> %220, <8 x i16> %223, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %234 = shufflevector <8 x i16> %220, <8 x i16> %223, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %235 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %233, <8 x i16> %201) #9
  %236 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %234, <8 x i16> %201) #9
  %237 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %233, <8 x i16> %204) #9
  %238 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %234, <8 x i16> %204) #9
  %239 = add <4 x i32> %235, <i32 2048, i32 2048, i32 2048, i32 2048>
  %240 = add <4 x i32> %236, <i32 2048, i32 2048, i32 2048, i32 2048>
  %241 = add <4 x i32> %237, <i32 2048, i32 2048, i32 2048, i32 2048>
  %242 = add <4 x i32> %238, <i32 2048, i32 2048, i32 2048, i32 2048>
  %243 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %239, i32 %161) #9
  %244 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %240, i32 %161) #9
  %245 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %241, i32 %161) #9
  %246 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %242, i32 %161) #9
  %247 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %243, <4 x i32> %244) #9
  %248 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %245, <4 x i32> %246) #9
  %249 = shufflevector <8 x i16> %218, <8 x i16> %221, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %250 = shufflevector <8 x i16> %218, <8 x i16> %221, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %251 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %249, <8 x i16> %201) #9
  %252 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %250, <8 x i16> %201) #9
  %253 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %249, <8 x i16> %204) #9
  %254 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %250, <8 x i16> %204) #9
  %255 = add <4 x i32> %251, <i32 2048, i32 2048, i32 2048, i32 2048>
  %256 = add <4 x i32> %252, <i32 2048, i32 2048, i32 2048, i32 2048>
  %257 = add <4 x i32> %253, <i32 2048, i32 2048, i32 2048, i32 2048>
  %258 = add <4 x i32> %254, <i32 2048, i32 2048, i32 2048, i32 2048>
  %259 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %255, i32 %161) #9
  %260 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %256, i32 %161) #9
  %261 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %257, i32 %161) #9
  %262 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %258, i32 %161) #9
  %263 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %259, <4 x i32> %260) #9
  %264 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %261, <4 x i32> %262) #9
  %265 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %225, <8 x i16> %222) #9
  %266 = bitcast <2 x i64>* %1 to <8 x i16>*
  store <8 x i16> %265, <8 x i16>* %266, align 16
  %267 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %225, <8 x i16> %222) #9
  %268 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 15
  %269 = bitcast <2 x i64>* %268 to <8 x i16>*
  store <8 x i16> %267, <8 x i16>* %269, align 16
  %270 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %227, <8 x i16> %224) #9
  %271 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 1
  %272 = bitcast <2 x i64>* %271 to <8 x i16>*
  store <8 x i16> %270, <8 x i16>* %272, align 16
  %273 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %227, <8 x i16> %224) #9
  %274 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 14
  %275 = bitcast <2 x i64>* %274 to <8 x i16>*
  store <8 x i16> %273, <8 x i16>* %275, align 16
  %276 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %229, <8 x i16> %248) #9
  %277 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 2
  %278 = bitcast <2 x i64>* %277 to <8 x i16>*
  store <8 x i16> %276, <8 x i16>* %278, align 16
  %279 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %229, <8 x i16> %248) #9
  %280 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 13
  %281 = bitcast <2 x i64>* %280 to <8 x i16>*
  store <8 x i16> %279, <8 x i16>* %281, align 16
  %282 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %231, <8 x i16> %264) #9
  %283 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 3
  %284 = bitcast <2 x i64>* %283 to <8 x i16>*
  store <8 x i16> %282, <8 x i16>* %284, align 16
  %285 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %231, <8 x i16> %264) #9
  %286 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 12
  %287 = bitcast <2 x i64>* %286 to <8 x i16>*
  store <8 x i16> %285, <8 x i16>* %287, align 16
  %288 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %232, <8 x i16> %263) #9
  %289 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 4
  %290 = bitcast <2 x i64>* %289 to <8 x i16>*
  store <8 x i16> %288, <8 x i16>* %290, align 16
  %291 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %232, <8 x i16> %263) #9
  %292 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 11
  %293 = bitcast <2 x i64>* %292 to <8 x i16>*
  store <8 x i16> %291, <8 x i16>* %293, align 16
  %294 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %230, <8 x i16> %247) #9
  %295 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 5
  %296 = bitcast <2 x i64>* %295 to <8 x i16>*
  store <8 x i16> %294, <8 x i16>* %296, align 16
  %297 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %230, <8 x i16> %247) #9
  %298 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 10
  %299 = bitcast <2 x i64>* %298 to <8 x i16>*
  store <8 x i16> %297, <8 x i16>* %299, align 16
  %300 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %228, <8 x i16> %219) #9
  %301 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 6
  %302 = bitcast <2 x i64>* %301 to <8 x i16>*
  store <8 x i16> %300, <8 x i16>* %302, align 16
  %303 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %228, <8 x i16> %219) #9
  %304 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 9
  %305 = bitcast <2 x i64>* %304 to <8 x i16>*
  store <8 x i16> %303, <8 x i16>* %305, align 16
  %306 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %226, <8 x i16> %217) #9
  %307 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 7
  %308 = bitcast <2 x i64>* %307 to <8 x i16>*
  store <8 x i16> %306, <8 x i16>* %308, align 16
  %309 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %226, <8 x i16> %217) #9
  %310 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 8
  %311 = bitcast <2 x i64>* %310 to <8 x i16>*
  store <8 x i16> %309, <8 x i16>* %311, align 16
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal void @iadst16_low1_ssse3(<2 x i64>* nocapture readonly, <2 x i64>* nocapture, i8 signext) #0 {
  %4 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 8), align 16
  %5 = and i32 %4, 65535
  %6 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 56), align 16
  %7 = shl i32 %6, 16
  %8 = or i32 %7, %5
  %9 = insertelement <4 x i32> undef, i32 %8, i32 0
  %10 = shufflevector <4 x i32> %9, <4 x i32> undef, <4 x i32> zeroinitializer
  %11 = and i32 %6, 65535
  %12 = shl i32 %4, 16
  %13 = sub i32 0, %12
  %14 = or i32 %11, %13
  %15 = insertelement <4 x i32> undef, i32 %14, i32 0
  %16 = shufflevector <4 x i32> %15, <4 x i32> undef, <4 x i32> zeroinitializer
  %17 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 16), align 16
  %18 = and i32 %17, 65535
  %19 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 48), align 16
  %20 = shl i32 %19, 16
  %21 = or i32 %20, %18
  %22 = insertelement <4 x i32> undef, i32 %21, i32 0
  %23 = shufflevector <4 x i32> %22, <4 x i32> undef, <4 x i32> zeroinitializer
  %24 = and i32 %19, 65535
  %25 = shl i32 %17, 16
  %26 = sub i32 0, %25
  %27 = or i32 %24, %26
  %28 = insertelement <4 x i32> undef, i32 %27, i32 0
  %29 = shufflevector <4 x i32> %28, <4 x i32> undef, <4 x i32> zeroinitializer
  %30 = bitcast <2 x i64>* %0 to <8 x i16>*
  %31 = load <8 x i16>, <8 x i16>* %30, align 16
  %32 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 62), align 8
  %33 = trunc i32 %32 to i16
  %34 = shl i16 %33, 3
  %35 = insertelement <8 x i16> undef, i16 %34, i32 0
  %36 = shufflevector <8 x i16> %35, <8 x i16> undef, <8 x i32> zeroinitializer
  %37 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 2), align 8
  %38 = trunc i32 %37 to i16
  %39 = shl i16 %38, 3
  %40 = sub i16 0, %39
  %41 = insertelement <8 x i16> undef, i16 %40, i32 0
  %42 = shufflevector <8 x i16> %41, <8 x i16> undef, <8 x i32> zeroinitializer
  %43 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %31, <8 x i16> %36) #9
  %44 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %31, <8 x i16> %42) #9
  %45 = shufflevector <8 x i16> %43, <8 x i16> %44, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %46 = shufflevector <8 x i16> %43, <8 x i16> %44, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %47 = bitcast <4 x i32> %10 to <8 x i16>
  %48 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %45, <8 x i16> %47) #9
  %49 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %46, <8 x i16> %47) #9
  %50 = bitcast <4 x i32> %16 to <8 x i16>
  %51 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %45, <8 x i16> %50) #9
  %52 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %46, <8 x i16> %50) #9
  %53 = add <4 x i32> %48, <i32 2048, i32 2048, i32 2048, i32 2048>
  %54 = add <4 x i32> %49, <i32 2048, i32 2048, i32 2048, i32 2048>
  %55 = add <4 x i32> %51, <i32 2048, i32 2048, i32 2048, i32 2048>
  %56 = add <4 x i32> %52, <i32 2048, i32 2048, i32 2048, i32 2048>
  %57 = sext i8 %2 to i32
  %58 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %53, i32 %57) #9
  %59 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %54, i32 %57) #9
  %60 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %55, i32 %57) #9
  %61 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %56, i32 %57) #9
  %62 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %58, <4 x i32> %59) #9
  %63 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %60, <4 x i32> %61) #9
  %64 = bitcast <4 x i32> %23 to <8 x i16>
  %65 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %45, <8 x i16> %64) #9
  %66 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %46, <8 x i16> %64) #9
  %67 = bitcast <4 x i32> %29 to <8 x i16>
  %68 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %45, <8 x i16> %67) #9
  %69 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %46, <8 x i16> %67) #9
  %70 = add <4 x i32> %65, <i32 2048, i32 2048, i32 2048, i32 2048>
  %71 = add <4 x i32> %66, <i32 2048, i32 2048, i32 2048, i32 2048>
  %72 = add <4 x i32> %68, <i32 2048, i32 2048, i32 2048, i32 2048>
  %73 = add <4 x i32> %69, <i32 2048, i32 2048, i32 2048, i32 2048>
  %74 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %70, i32 %57) #9
  %75 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %71, i32 %57) #9
  %76 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %72, i32 %57) #9
  %77 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %73, i32 %57) #9
  %78 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %74, <4 x i32> %75) #9
  %79 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %76, <4 x i32> %77) #9
  %80 = shufflevector <8 x i16> %62, <8 x i16> %63, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %81 = shufflevector <8 x i16> %62, <8 x i16> %63, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %82 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %80, <8 x i16> %64) #9
  %83 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %81, <8 x i16> %64) #9
  %84 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %80, <8 x i16> %67) #9
  %85 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %81, <8 x i16> %67) #9
  %86 = add <4 x i32> %82, <i32 2048, i32 2048, i32 2048, i32 2048>
  %87 = add <4 x i32> %83, <i32 2048, i32 2048, i32 2048, i32 2048>
  %88 = add <4 x i32> %84, <i32 2048, i32 2048, i32 2048, i32 2048>
  %89 = add <4 x i32> %85, <i32 2048, i32 2048, i32 2048, i32 2048>
  %90 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %86, i32 %57) #9
  %91 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %87, i32 %57) #9
  %92 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %88, i32 %57) #9
  %93 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %89, i32 %57) #9
  %94 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %90, <4 x i32> %91) #9
  %95 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %92, <4 x i32> %93) #9
  %96 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 32), align 16
  %97 = and i32 %96, 65535
  %98 = shl i32 %96, 16
  %99 = or i32 %97, %98
  %100 = insertelement <4 x i32> undef, i32 %99, i32 0
  %101 = shufflevector <4 x i32> %100, <4 x i32> undef, <4 x i32> zeroinitializer
  %102 = sub i32 0, %98
  %103 = or i32 %97, %102
  %104 = insertelement <4 x i32> undef, i32 %103, i32 0
  %105 = shufflevector <4 x i32> %104, <4 x i32> undef, <4 x i32> zeroinitializer
  %106 = bitcast <4 x i32> %101 to <8 x i16>
  %107 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %45, <8 x i16> %106) #9
  %108 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %46, <8 x i16> %106) #9
  %109 = bitcast <4 x i32> %105 to <8 x i16>
  %110 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %45, <8 x i16> %109) #9
  %111 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %46, <8 x i16> %109) #9
  %112 = add <4 x i32> %107, <i32 2048, i32 2048, i32 2048, i32 2048>
  %113 = add <4 x i32> %108, <i32 2048, i32 2048, i32 2048, i32 2048>
  %114 = add <4 x i32> %110, <i32 2048, i32 2048, i32 2048, i32 2048>
  %115 = add <4 x i32> %111, <i32 2048, i32 2048, i32 2048, i32 2048>
  %116 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %112, i32 %57) #9
  %117 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %113, i32 %57) #9
  %118 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %114, i32 %57) #9
  %119 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %115, i32 %57) #9
  %120 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %116, <4 x i32> %117) #9
  %121 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %118, <4 x i32> %119) #9
  %122 = shufflevector <8 x i16> %78, <8 x i16> %79, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %123 = shufflevector <8 x i16> %78, <8 x i16> %79, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %124 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %122, <8 x i16> %106) #9
  %125 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %123, <8 x i16> %106) #9
  %126 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %122, <8 x i16> %109) #9
  %127 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %123, <8 x i16> %109) #9
  %128 = add <4 x i32> %124, <i32 2048, i32 2048, i32 2048, i32 2048>
  %129 = add <4 x i32> %125, <i32 2048, i32 2048, i32 2048, i32 2048>
  %130 = add <4 x i32> %126, <i32 2048, i32 2048, i32 2048, i32 2048>
  %131 = add <4 x i32> %127, <i32 2048, i32 2048, i32 2048, i32 2048>
  %132 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %128, i32 %57) #9
  %133 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %129, i32 %57) #9
  %134 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %130, i32 %57) #9
  %135 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %131, i32 %57) #9
  %136 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %132, <4 x i32> %133) #9
  %137 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %134, <4 x i32> %135) #9
  %138 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %80, <8 x i16> %106) #9
  %139 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %81, <8 x i16> %106) #9
  %140 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %80, <8 x i16> %109) #9
  %141 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %81, <8 x i16> %109) #9
  %142 = add <4 x i32> %138, <i32 2048, i32 2048, i32 2048, i32 2048>
  %143 = add <4 x i32> %139, <i32 2048, i32 2048, i32 2048, i32 2048>
  %144 = add <4 x i32> %140, <i32 2048, i32 2048, i32 2048, i32 2048>
  %145 = add <4 x i32> %141, <i32 2048, i32 2048, i32 2048, i32 2048>
  %146 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %142, i32 %57) #9
  %147 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %143, i32 %57) #9
  %148 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %144, i32 %57) #9
  %149 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %145, i32 %57) #9
  %150 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %146, <4 x i32> %147) #9
  %151 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %148, <4 x i32> %149) #9
  %152 = shufflevector <8 x i16> %94, <8 x i16> %95, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %153 = shufflevector <8 x i16> %94, <8 x i16> %95, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %154 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %152, <8 x i16> %106) #9
  %155 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %153, <8 x i16> %106) #9
  %156 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %152, <8 x i16> %109) #9
  %157 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %153, <8 x i16> %109) #9
  %158 = add <4 x i32> %154, <i32 2048, i32 2048, i32 2048, i32 2048>
  %159 = add <4 x i32> %155, <i32 2048, i32 2048, i32 2048, i32 2048>
  %160 = add <4 x i32> %156, <i32 2048, i32 2048, i32 2048, i32 2048>
  %161 = add <4 x i32> %157, <i32 2048, i32 2048, i32 2048, i32 2048>
  %162 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %158, i32 %57) #9
  %163 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %159, i32 %57) #9
  %164 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %160, i32 %57) #9
  %165 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %161, i32 %57) #9
  %166 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %162, <4 x i32> %163) #9
  %167 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %164, <4 x i32> %165) #9
  %168 = bitcast <2 x i64>* %1 to <8 x i16>*
  store <8 x i16> %43, <8 x i16>* %168, align 16
  %169 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %62) #9
  %170 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 1
  %171 = bitcast <2 x i64>* %170 to <8 x i16>*
  store <8 x i16> %169, <8 x i16>* %171, align 16
  %172 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 2
  %173 = bitcast <2 x i64>* %172 to <8 x i16>*
  store <8 x i16> %94, <8 x i16>* %173, align 16
  %174 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %78) #9
  %175 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 3
  %176 = bitcast <2 x i64>* %175 to <8 x i16>*
  store <8 x i16> %174, <8 x i16>* %176, align 16
  %177 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 4
  %178 = bitcast <2 x i64>* %177 to <8 x i16>*
  store <8 x i16> %136, <8 x i16>* %178, align 16
  %179 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %166) #9
  %180 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 5
  %181 = bitcast <2 x i64>* %180 to <8 x i16>*
  store <8 x i16> %179, <8 x i16>* %181, align 16
  %182 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 6
  %183 = bitcast <2 x i64>* %182 to <8 x i16>*
  store <8 x i16> %150, <8 x i16>* %183, align 16
  %184 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %120) #9
  %185 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 7
  %186 = bitcast <2 x i64>* %185 to <8 x i16>*
  store <8 x i16> %184, <8 x i16>* %186, align 16
  %187 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 8
  %188 = bitcast <2 x i64>* %187 to <8 x i16>*
  store <8 x i16> %121, <8 x i16>* %188, align 16
  %189 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %151) #9
  %190 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 9
  %191 = bitcast <2 x i64>* %190 to <8 x i16>*
  store <8 x i16> %189, <8 x i16>* %191, align 16
  %192 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 10
  %193 = bitcast <2 x i64>* %192 to <8 x i16>*
  store <8 x i16> %167, <8 x i16>* %193, align 16
  %194 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %137) #9
  %195 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 11
  %196 = bitcast <2 x i64>* %195 to <8 x i16>*
  store <8 x i16> %194, <8 x i16>* %196, align 16
  %197 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 12
  %198 = bitcast <2 x i64>* %197 to <8 x i16>*
  store <8 x i16> %79, <8 x i16>* %198, align 16
  %199 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %95) #9
  %200 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 13
  %201 = bitcast <2 x i64>* %200 to <8 x i16>*
  store <8 x i16> %199, <8 x i16>* %201, align 16
  %202 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 14
  %203 = bitcast <2 x i64>* %202 to <8 x i16>*
  store <8 x i16> %63, <8 x i16>* %203, align 16
  %204 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %44) #9
  %205 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 15
  %206 = bitcast <2 x i64>* %205 to <8 x i16>*
  store <8 x i16> %204, <8 x i16>* %206, align 16
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal void @iadst16_low8_ssse3(<2 x i64>* nocapture readonly, <2 x i64>* nocapture, i8 signext) #0 {
  %4 = bitcast <2 x i64>* %0 to <8 x i16>*
  %5 = load <8 x i16>, <8 x i16>* %4, align 16
  %6 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 2
  %7 = bitcast <2 x i64>* %6 to <8 x i16>*
  %8 = load <8 x i16>, <8 x i16>* %7, align 16
  %9 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 4
  %10 = bitcast <2 x i64>* %9 to <8 x i16>*
  %11 = load <8 x i16>, <8 x i16>* %10, align 16
  %12 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 6
  %13 = bitcast <2 x i64>* %12 to <8 x i16>*
  %14 = load <8 x i16>, <8 x i16>* %13, align 16
  %15 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 7
  %16 = bitcast <2 x i64>* %15 to <8 x i16>*
  %17 = load <8 x i16>, <8 x i16>* %16, align 16
  %18 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 5
  %19 = bitcast <2 x i64>* %18 to <8 x i16>*
  %20 = load <8 x i16>, <8 x i16>* %19, align 16
  %21 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 3
  %22 = bitcast <2 x i64>* %21 to <8 x i16>*
  %23 = load <8 x i16>, <8 x i16>* %22, align 16
  %24 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 1
  %25 = bitcast <2 x i64>* %24 to <8 x i16>*
  %26 = load <8 x i16>, <8 x i16>* %25, align 16
  %27 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 62), align 8
  %28 = trunc i32 %27 to i16
  %29 = shl i16 %28, 3
  %30 = insertelement <8 x i16> undef, i16 %29, i32 0
  %31 = shufflevector <8 x i16> %30, <8 x i16> undef, <8 x i32> zeroinitializer
  %32 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 2), align 8
  %33 = trunc i32 %32 to i16
  %34 = shl i16 %33, 3
  %35 = sub i16 0, %34
  %36 = insertelement <8 x i16> undef, i16 %35, i32 0
  %37 = shufflevector <8 x i16> %36, <8 x i16> undef, <8 x i32> zeroinitializer
  %38 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %5, <8 x i16> %31) #9
  %39 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %5, <8 x i16> %37) #9
  %40 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 54), align 8
  %41 = trunc i32 %40 to i16
  %42 = shl i16 %41, 3
  %43 = insertelement <8 x i16> undef, i16 %42, i32 0
  %44 = shufflevector <8 x i16> %43, <8 x i16> undef, <8 x i32> zeroinitializer
  %45 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 10), align 8
  %46 = trunc i32 %45 to i16
  %47 = shl i16 %46, 3
  %48 = sub i16 0, %47
  %49 = insertelement <8 x i16> undef, i16 %48, i32 0
  %50 = shufflevector <8 x i16> %49, <8 x i16> undef, <8 x i32> zeroinitializer
  %51 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %8, <8 x i16> %44) #9
  %52 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %8, <8 x i16> %50) #9
  %53 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 46), align 8
  %54 = trunc i32 %53 to i16
  %55 = shl i16 %54, 3
  %56 = insertelement <8 x i16> undef, i16 %55, i32 0
  %57 = shufflevector <8 x i16> %56, <8 x i16> undef, <8 x i32> zeroinitializer
  %58 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 18), align 8
  %59 = trunc i32 %58 to i16
  %60 = shl i16 %59, 3
  %61 = sub i16 0, %60
  %62 = insertelement <8 x i16> undef, i16 %61, i32 0
  %63 = shufflevector <8 x i16> %62, <8 x i16> undef, <8 x i32> zeroinitializer
  %64 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %11, <8 x i16> %57) #9
  %65 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %11, <8 x i16> %63) #9
  %66 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 38), align 8
  %67 = trunc i32 %66 to i16
  %68 = shl i16 %67, 3
  %69 = insertelement <8 x i16> undef, i16 %68, i32 0
  %70 = shufflevector <8 x i16> %69, <8 x i16> undef, <8 x i32> zeroinitializer
  %71 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 26), align 8
  %72 = trunc i32 %71 to i16
  %73 = shl i16 %72, 3
  %74 = sub i16 0, %73
  %75 = insertelement <8 x i16> undef, i16 %74, i32 0
  %76 = shufflevector <8 x i16> %75, <8 x i16> undef, <8 x i32> zeroinitializer
  %77 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %14, <8 x i16> %70) #9
  %78 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %14, <8 x i16> %76) #9
  %79 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 34), align 8
  %80 = trunc i32 %79 to i16
  %81 = shl i16 %80, 3
  %82 = insertelement <8 x i16> undef, i16 %81, i32 0
  %83 = shufflevector <8 x i16> %82, <8 x i16> undef, <8 x i32> zeroinitializer
  %84 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 30), align 8
  %85 = trunc i32 %84 to i16
  %86 = shl i16 %85, 3
  %87 = insertelement <8 x i16> undef, i16 %86, i32 0
  %88 = shufflevector <8 x i16> %87, <8 x i16> undef, <8 x i32> zeroinitializer
  %89 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %17, <8 x i16> %83) #9
  %90 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %17, <8 x i16> %88) #9
  %91 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 42), align 8
  %92 = trunc i32 %91 to i16
  %93 = shl i16 %92, 3
  %94 = insertelement <8 x i16> undef, i16 %93, i32 0
  %95 = shufflevector <8 x i16> %94, <8 x i16> undef, <8 x i32> zeroinitializer
  %96 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 22), align 8
  %97 = trunc i32 %96 to i16
  %98 = shl i16 %97, 3
  %99 = insertelement <8 x i16> undef, i16 %98, i32 0
  %100 = shufflevector <8 x i16> %99, <8 x i16> undef, <8 x i32> zeroinitializer
  %101 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %20, <8 x i16> %95) #9
  %102 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %20, <8 x i16> %100) #9
  %103 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 50), align 8
  %104 = trunc i32 %103 to i16
  %105 = shl i16 %104, 3
  %106 = insertelement <8 x i16> undef, i16 %105, i32 0
  %107 = shufflevector <8 x i16> %106, <8 x i16> undef, <8 x i32> zeroinitializer
  %108 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 14), align 8
  %109 = trunc i32 %108 to i16
  %110 = shl i16 %109, 3
  %111 = insertelement <8 x i16> undef, i16 %110, i32 0
  %112 = shufflevector <8 x i16> %111, <8 x i16> undef, <8 x i32> zeroinitializer
  %113 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %23, <8 x i16> %107) #9
  %114 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %23, <8 x i16> %112) #9
  %115 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 58), align 8
  %116 = trunc i32 %115 to i16
  %117 = shl i16 %116, 3
  %118 = insertelement <8 x i16> undef, i16 %117, i32 0
  %119 = shufflevector <8 x i16> %118, <8 x i16> undef, <8 x i32> zeroinitializer
  %120 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 6), align 8
  %121 = trunc i32 %120 to i16
  %122 = shl i16 %121, 3
  %123 = insertelement <8 x i16> undef, i16 %122, i32 0
  %124 = shufflevector <8 x i16> %123, <8 x i16> undef, <8 x i32> zeroinitializer
  %125 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %26, <8 x i16> %119) #9
  %126 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %26, <8 x i16> %124) #9
  %127 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %38, <8 x i16> %89) #9
  %128 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %38, <8 x i16> %89) #9
  %129 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %39, <8 x i16> %90) #9
  %130 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %39, <8 x i16> %90) #9
  %131 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %51, <8 x i16> %101) #9
  %132 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %51, <8 x i16> %101) #9
  %133 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %52, <8 x i16> %102) #9
  %134 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %52, <8 x i16> %102) #9
  %135 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %64, <8 x i16> %113) #9
  %136 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %64, <8 x i16> %113) #9
  %137 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %65, <8 x i16> %114) #9
  %138 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %65, <8 x i16> %114) #9
  %139 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %77, <8 x i16> %125) #9
  %140 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %77, <8 x i16> %125) #9
  %141 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %78, <8 x i16> %126) #9
  %142 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %78, <8 x i16> %126) #9
  %143 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 8), align 16
  %144 = and i32 %143, 65535
  %145 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 56), align 16
  %146 = shl i32 %145, 16
  %147 = or i32 %146, %144
  %148 = insertelement <4 x i32> undef, i32 %147, i32 0
  %149 = shufflevector <4 x i32> %148, <4 x i32> undef, <4 x i32> zeroinitializer
  %150 = and i32 %145, 65535
  %151 = shl i32 %143, 16
  %152 = sub i32 0, %151
  %153 = or i32 %150, %152
  %154 = insertelement <4 x i32> undef, i32 %153, i32 0
  %155 = shufflevector <4 x i32> %154, <4 x i32> undef, <4 x i32> zeroinitializer
  %156 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 40), align 16
  %157 = and i32 %156, 65535
  %158 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 24), align 16
  %159 = shl i32 %158, 16
  %160 = or i32 %159, %157
  %161 = insertelement <4 x i32> undef, i32 %160, i32 0
  %162 = shufflevector <4 x i32> %161, <4 x i32> undef, <4 x i32> zeroinitializer
  %163 = and i32 %158, 65535
  %164 = shl i32 %156, 16
  %165 = sub i32 0, %164
  %166 = or i32 %163, %165
  %167 = insertelement <4 x i32> undef, i32 %166, i32 0
  %168 = shufflevector <4 x i32> %167, <4 x i32> undef, <4 x i32> zeroinitializer
  %169 = sub i32 0, %145
  %170 = and i32 %169, 65535
  %171 = or i32 %170, %151
  %172 = insertelement <4 x i32> undef, i32 %171, i32 0
  %173 = shufflevector <4 x i32> %172, <4 x i32> undef, <4 x i32> zeroinitializer
  %174 = sub i32 0, %158
  %175 = and i32 %174, 65535
  %176 = or i32 %175, %164
  %177 = insertelement <4 x i32> undef, i32 %176, i32 0
  %178 = shufflevector <4 x i32> %177, <4 x i32> undef, <4 x i32> zeroinitializer
  %179 = shufflevector <8 x i16> %128, <8 x i16> %130, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %180 = shufflevector <8 x i16> %128, <8 x i16> %130, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %181 = bitcast <4 x i32> %149 to <8 x i16>
  %182 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %179, <8 x i16> %181) #9
  %183 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %180, <8 x i16> %181) #9
  %184 = bitcast <4 x i32> %155 to <8 x i16>
  %185 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %179, <8 x i16> %184) #9
  %186 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %180, <8 x i16> %184) #9
  %187 = add <4 x i32> %182, <i32 2048, i32 2048, i32 2048, i32 2048>
  %188 = add <4 x i32> %183, <i32 2048, i32 2048, i32 2048, i32 2048>
  %189 = add <4 x i32> %185, <i32 2048, i32 2048, i32 2048, i32 2048>
  %190 = add <4 x i32> %186, <i32 2048, i32 2048, i32 2048, i32 2048>
  %191 = sext i8 %2 to i32
  %192 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %187, i32 %191) #9
  %193 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %188, i32 %191) #9
  %194 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %189, i32 %191) #9
  %195 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %190, i32 %191) #9
  %196 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %192, <4 x i32> %193) #9
  %197 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %194, <4 x i32> %195) #9
  %198 = shufflevector <8 x i16> %132, <8 x i16> %134, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %199 = shufflevector <8 x i16> %132, <8 x i16> %134, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %200 = bitcast <4 x i32> %162 to <8 x i16>
  %201 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %198, <8 x i16> %200) #9
  %202 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %199, <8 x i16> %200) #9
  %203 = bitcast <4 x i32> %168 to <8 x i16>
  %204 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %198, <8 x i16> %203) #9
  %205 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %199, <8 x i16> %203) #9
  %206 = add <4 x i32> %201, <i32 2048, i32 2048, i32 2048, i32 2048>
  %207 = add <4 x i32> %202, <i32 2048, i32 2048, i32 2048, i32 2048>
  %208 = add <4 x i32> %204, <i32 2048, i32 2048, i32 2048, i32 2048>
  %209 = add <4 x i32> %205, <i32 2048, i32 2048, i32 2048, i32 2048>
  %210 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %206, i32 %191) #9
  %211 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %207, i32 %191) #9
  %212 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %208, i32 %191) #9
  %213 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %209, i32 %191) #9
  %214 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %210, <4 x i32> %211) #9
  %215 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %212, <4 x i32> %213) #9
  %216 = shufflevector <8 x i16> %136, <8 x i16> %138, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %217 = shufflevector <8 x i16> %136, <8 x i16> %138, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %218 = bitcast <4 x i32> %173 to <8 x i16>
  %219 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %216, <8 x i16> %218) #9
  %220 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %217, <8 x i16> %218) #9
  %221 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %216, <8 x i16> %181) #9
  %222 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %217, <8 x i16> %181) #9
  %223 = add <4 x i32> %219, <i32 2048, i32 2048, i32 2048, i32 2048>
  %224 = add <4 x i32> %220, <i32 2048, i32 2048, i32 2048, i32 2048>
  %225 = add <4 x i32> %221, <i32 2048, i32 2048, i32 2048, i32 2048>
  %226 = add <4 x i32> %222, <i32 2048, i32 2048, i32 2048, i32 2048>
  %227 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %223, i32 %191) #9
  %228 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %224, i32 %191) #9
  %229 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %225, i32 %191) #9
  %230 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %226, i32 %191) #9
  %231 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %227, <4 x i32> %228) #9
  %232 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %229, <4 x i32> %230) #9
  %233 = shufflevector <8 x i16> %140, <8 x i16> %142, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %234 = shufflevector <8 x i16> %140, <8 x i16> %142, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %235 = bitcast <4 x i32> %178 to <8 x i16>
  %236 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %233, <8 x i16> %235) #9
  %237 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %234, <8 x i16> %235) #9
  %238 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %233, <8 x i16> %200) #9
  %239 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %234, <8 x i16> %200) #9
  %240 = add <4 x i32> %236, <i32 2048, i32 2048, i32 2048, i32 2048>
  %241 = add <4 x i32> %237, <i32 2048, i32 2048, i32 2048, i32 2048>
  %242 = add <4 x i32> %238, <i32 2048, i32 2048, i32 2048, i32 2048>
  %243 = add <4 x i32> %239, <i32 2048, i32 2048, i32 2048, i32 2048>
  %244 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %240, i32 %191) #9
  %245 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %241, i32 %191) #9
  %246 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %242, i32 %191) #9
  %247 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %243, i32 %191) #9
  %248 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %244, <4 x i32> %245) #9
  %249 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %246, <4 x i32> %247) #9
  %250 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %127, <8 x i16> %135) #9
  %251 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %127, <8 x i16> %135) #9
  %252 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %129, <8 x i16> %137) #9
  %253 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %129, <8 x i16> %137) #9
  %254 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %131, <8 x i16> %139) #9
  %255 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %131, <8 x i16> %139) #9
  %256 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %133, <8 x i16> %141) #9
  %257 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %133, <8 x i16> %141) #9
  %258 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %196, <8 x i16> %231) #9
  %259 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %196, <8 x i16> %231) #9
  %260 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %197, <8 x i16> %232) #9
  %261 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %197, <8 x i16> %232) #9
  %262 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %214, <8 x i16> %248) #9
  %263 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %214, <8 x i16> %248) #9
  %264 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %215, <8 x i16> %249) #9
  %265 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %215, <8 x i16> %249) #9
  %266 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 16), align 16
  %267 = and i32 %266, 65535
  %268 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 48), align 16
  %269 = shl i32 %268, 16
  %270 = or i32 %269, %267
  %271 = insertelement <4 x i32> undef, i32 %270, i32 0
  %272 = shufflevector <4 x i32> %271, <4 x i32> undef, <4 x i32> zeroinitializer
  %273 = and i32 %268, 65535
  %274 = shl i32 %266, 16
  %275 = sub i32 0, %274
  %276 = or i32 %273, %275
  %277 = insertelement <4 x i32> undef, i32 %276, i32 0
  %278 = shufflevector <4 x i32> %277, <4 x i32> undef, <4 x i32> zeroinitializer
  %279 = sub i32 0, %268
  %280 = and i32 %279, 65535
  %281 = or i32 %280, %274
  %282 = insertelement <4 x i32> undef, i32 %281, i32 0
  %283 = shufflevector <4 x i32> %282, <4 x i32> undef, <4 x i32> zeroinitializer
  %284 = shufflevector <8 x i16> %251, <8 x i16> %253, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %285 = shufflevector <8 x i16> %251, <8 x i16> %253, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %286 = bitcast <4 x i32> %272 to <8 x i16>
  %287 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %284, <8 x i16> %286) #9
  %288 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %285, <8 x i16> %286) #9
  %289 = bitcast <4 x i32> %278 to <8 x i16>
  %290 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %284, <8 x i16> %289) #9
  %291 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %285, <8 x i16> %289) #9
  %292 = add <4 x i32> %287, <i32 2048, i32 2048, i32 2048, i32 2048>
  %293 = add <4 x i32> %288, <i32 2048, i32 2048, i32 2048, i32 2048>
  %294 = add <4 x i32> %290, <i32 2048, i32 2048, i32 2048, i32 2048>
  %295 = add <4 x i32> %291, <i32 2048, i32 2048, i32 2048, i32 2048>
  %296 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %292, i32 %191) #9
  %297 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %293, i32 %191) #9
  %298 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %294, i32 %191) #9
  %299 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %295, i32 %191) #9
  %300 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %296, <4 x i32> %297) #9
  %301 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %298, <4 x i32> %299) #9
  %302 = shufflevector <8 x i16> %255, <8 x i16> %257, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %303 = shufflevector <8 x i16> %255, <8 x i16> %257, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %304 = bitcast <4 x i32> %283 to <8 x i16>
  %305 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %302, <8 x i16> %304) #9
  %306 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %303, <8 x i16> %304) #9
  %307 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %302, <8 x i16> %286) #9
  %308 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %303, <8 x i16> %286) #9
  %309 = add <4 x i32> %305, <i32 2048, i32 2048, i32 2048, i32 2048>
  %310 = add <4 x i32> %306, <i32 2048, i32 2048, i32 2048, i32 2048>
  %311 = add <4 x i32> %307, <i32 2048, i32 2048, i32 2048, i32 2048>
  %312 = add <4 x i32> %308, <i32 2048, i32 2048, i32 2048, i32 2048>
  %313 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %309, i32 %191) #9
  %314 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %310, i32 %191) #9
  %315 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %311, i32 %191) #9
  %316 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %312, i32 %191) #9
  %317 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %313, <4 x i32> %314) #9
  %318 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %315, <4 x i32> %316) #9
  %319 = shufflevector <8 x i16> %259, <8 x i16> %261, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %320 = shufflevector <8 x i16> %259, <8 x i16> %261, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %321 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %319, <8 x i16> %286) #9
  %322 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %320, <8 x i16> %286) #9
  %323 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %319, <8 x i16> %289) #9
  %324 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %320, <8 x i16> %289) #9
  %325 = add <4 x i32> %321, <i32 2048, i32 2048, i32 2048, i32 2048>
  %326 = add <4 x i32> %322, <i32 2048, i32 2048, i32 2048, i32 2048>
  %327 = add <4 x i32> %323, <i32 2048, i32 2048, i32 2048, i32 2048>
  %328 = add <4 x i32> %324, <i32 2048, i32 2048, i32 2048, i32 2048>
  %329 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %325, i32 %191) #9
  %330 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %326, i32 %191) #9
  %331 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %327, i32 %191) #9
  %332 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %328, i32 %191) #9
  %333 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %329, <4 x i32> %330) #9
  %334 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %331, <4 x i32> %332) #9
  %335 = shufflevector <8 x i16> %263, <8 x i16> %265, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %336 = shufflevector <8 x i16> %263, <8 x i16> %265, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %337 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %335, <8 x i16> %304) #9
  %338 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %336, <8 x i16> %304) #9
  %339 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %335, <8 x i16> %286) #9
  %340 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %336, <8 x i16> %286) #9
  %341 = add <4 x i32> %337, <i32 2048, i32 2048, i32 2048, i32 2048>
  %342 = add <4 x i32> %338, <i32 2048, i32 2048, i32 2048, i32 2048>
  %343 = add <4 x i32> %339, <i32 2048, i32 2048, i32 2048, i32 2048>
  %344 = add <4 x i32> %340, <i32 2048, i32 2048, i32 2048, i32 2048>
  %345 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %341, i32 %191) #9
  %346 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %342, i32 %191) #9
  %347 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %343, i32 %191) #9
  %348 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %344, i32 %191) #9
  %349 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %345, <4 x i32> %346) #9
  %350 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %347, <4 x i32> %348) #9
  %351 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %250, <8 x i16> %254) #9
  %352 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %250, <8 x i16> %254) #9
  %353 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %252, <8 x i16> %256) #9
  %354 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %252, <8 x i16> %256) #9
  %355 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %300, <8 x i16> %317) #9
  %356 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %300, <8 x i16> %317) #9
  %357 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %301, <8 x i16> %318) #9
  %358 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %301, <8 x i16> %318) #9
  %359 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %258, <8 x i16> %262) #9
  %360 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %258, <8 x i16> %262) #9
  %361 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %260, <8 x i16> %264) #9
  %362 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %260, <8 x i16> %264) #9
  %363 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %333, <8 x i16> %349) #9
  %364 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %333, <8 x i16> %349) #9
  %365 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %334, <8 x i16> %350) #9
  %366 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %334, <8 x i16> %350) #9
  %367 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 32), align 16
  %368 = and i32 %367, 65535
  %369 = shl i32 %367, 16
  %370 = or i32 %368, %369
  %371 = insertelement <4 x i32> undef, i32 %370, i32 0
  %372 = shufflevector <4 x i32> %371, <4 x i32> undef, <4 x i32> zeroinitializer
  %373 = sub i32 0, %369
  %374 = or i32 %368, %373
  %375 = insertelement <4 x i32> undef, i32 %374, i32 0
  %376 = shufflevector <4 x i32> %375, <4 x i32> undef, <4 x i32> zeroinitializer
  %377 = shufflevector <8 x i16> %352, <8 x i16> %354, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %378 = shufflevector <8 x i16> %352, <8 x i16> %354, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %379 = bitcast <4 x i32> %372 to <8 x i16>
  %380 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %377, <8 x i16> %379) #9
  %381 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %378, <8 x i16> %379) #9
  %382 = bitcast <4 x i32> %376 to <8 x i16>
  %383 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %377, <8 x i16> %382) #9
  %384 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %378, <8 x i16> %382) #9
  %385 = add <4 x i32> %380, <i32 2048, i32 2048, i32 2048, i32 2048>
  %386 = add <4 x i32> %381, <i32 2048, i32 2048, i32 2048, i32 2048>
  %387 = add <4 x i32> %383, <i32 2048, i32 2048, i32 2048, i32 2048>
  %388 = add <4 x i32> %384, <i32 2048, i32 2048, i32 2048, i32 2048>
  %389 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %385, i32 %191) #9
  %390 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %386, i32 %191) #9
  %391 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %387, i32 %191) #9
  %392 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %388, i32 %191) #9
  %393 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %389, <4 x i32> %390) #9
  %394 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %391, <4 x i32> %392) #9
  %395 = shufflevector <8 x i16> %356, <8 x i16> %358, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %396 = shufflevector <8 x i16> %356, <8 x i16> %358, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %397 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %395, <8 x i16> %379) #9
  %398 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %396, <8 x i16> %379) #9
  %399 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %395, <8 x i16> %382) #9
  %400 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %396, <8 x i16> %382) #9
  %401 = add <4 x i32> %397, <i32 2048, i32 2048, i32 2048, i32 2048>
  %402 = add <4 x i32> %398, <i32 2048, i32 2048, i32 2048, i32 2048>
  %403 = add <4 x i32> %399, <i32 2048, i32 2048, i32 2048, i32 2048>
  %404 = add <4 x i32> %400, <i32 2048, i32 2048, i32 2048, i32 2048>
  %405 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %401, i32 %191) #9
  %406 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %402, i32 %191) #9
  %407 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %403, i32 %191) #9
  %408 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %404, i32 %191) #9
  %409 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %405, <4 x i32> %406) #9
  %410 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %407, <4 x i32> %408) #9
  %411 = shufflevector <8 x i16> %360, <8 x i16> %362, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %412 = shufflevector <8 x i16> %360, <8 x i16> %362, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %413 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %411, <8 x i16> %379) #9
  %414 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %412, <8 x i16> %379) #9
  %415 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %411, <8 x i16> %382) #9
  %416 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %412, <8 x i16> %382) #9
  %417 = add <4 x i32> %413, <i32 2048, i32 2048, i32 2048, i32 2048>
  %418 = add <4 x i32> %414, <i32 2048, i32 2048, i32 2048, i32 2048>
  %419 = add <4 x i32> %415, <i32 2048, i32 2048, i32 2048, i32 2048>
  %420 = add <4 x i32> %416, <i32 2048, i32 2048, i32 2048, i32 2048>
  %421 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %417, i32 %191) #9
  %422 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %418, i32 %191) #9
  %423 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %419, i32 %191) #9
  %424 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %420, i32 %191) #9
  %425 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %421, <4 x i32> %422) #9
  %426 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %423, <4 x i32> %424) #9
  %427 = shufflevector <8 x i16> %364, <8 x i16> %366, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %428 = shufflevector <8 x i16> %364, <8 x i16> %366, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %429 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %427, <8 x i16> %379) #9
  %430 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %428, <8 x i16> %379) #9
  %431 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %427, <8 x i16> %382) #9
  %432 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %428, <8 x i16> %382) #9
  %433 = add <4 x i32> %429, <i32 2048, i32 2048, i32 2048, i32 2048>
  %434 = add <4 x i32> %430, <i32 2048, i32 2048, i32 2048, i32 2048>
  %435 = add <4 x i32> %431, <i32 2048, i32 2048, i32 2048, i32 2048>
  %436 = add <4 x i32> %432, <i32 2048, i32 2048, i32 2048, i32 2048>
  %437 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %433, i32 %191) #9
  %438 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %434, i32 %191) #9
  %439 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %435, i32 %191) #9
  %440 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %436, i32 %191) #9
  %441 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %437, <4 x i32> %438) #9
  %442 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %439, <4 x i32> %440) #9
  %443 = bitcast <2 x i64>* %1 to <8 x i16>*
  store <8 x i16> %351, <8 x i16>* %443, align 16
  %444 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %359) #9
  %445 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 1
  %446 = bitcast <2 x i64>* %445 to <8 x i16>*
  store <8 x i16> %444, <8 x i16>* %446, align 16
  %447 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 2
  %448 = bitcast <2 x i64>* %447 to <8 x i16>*
  store <8 x i16> %363, <8 x i16>* %448, align 16
  %449 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %355) #9
  %450 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 3
  %451 = bitcast <2 x i64>* %450 to <8 x i16>*
  store <8 x i16> %449, <8 x i16>* %451, align 16
  %452 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 4
  %453 = bitcast <2 x i64>* %452 to <8 x i16>*
  store <8 x i16> %409, <8 x i16>* %453, align 16
  %454 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %441) #9
  %455 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 5
  %456 = bitcast <2 x i64>* %455 to <8 x i16>*
  store <8 x i16> %454, <8 x i16>* %456, align 16
  %457 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 6
  %458 = bitcast <2 x i64>* %457 to <8 x i16>*
  store <8 x i16> %425, <8 x i16>* %458, align 16
  %459 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %393) #9
  %460 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 7
  %461 = bitcast <2 x i64>* %460 to <8 x i16>*
  store <8 x i16> %459, <8 x i16>* %461, align 16
  %462 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 8
  %463 = bitcast <2 x i64>* %462 to <8 x i16>*
  store <8 x i16> %394, <8 x i16>* %463, align 16
  %464 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %426) #9
  %465 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 9
  %466 = bitcast <2 x i64>* %465 to <8 x i16>*
  store <8 x i16> %464, <8 x i16>* %466, align 16
  %467 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 10
  %468 = bitcast <2 x i64>* %467 to <8 x i16>*
  store <8 x i16> %442, <8 x i16>* %468, align 16
  %469 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %410) #9
  %470 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 11
  %471 = bitcast <2 x i64>* %470 to <8 x i16>*
  store <8 x i16> %469, <8 x i16>* %471, align 16
  %472 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 12
  %473 = bitcast <2 x i64>* %472 to <8 x i16>*
  store <8 x i16> %357, <8 x i16>* %473, align 16
  %474 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %365) #9
  %475 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 13
  %476 = bitcast <2 x i64>* %475 to <8 x i16>*
  store <8 x i16> %474, <8 x i16>* %476, align 16
  %477 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 14
  %478 = bitcast <2 x i64>* %477 to <8 x i16>*
  store <8 x i16> %361, <8 x i16>* %478, align 16
  %479 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %353) #9
  %480 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 15
  %481 = bitcast <2 x i64>* %480 to <8 x i16>*
  store <8 x i16> %479, <8 x i16>* %481, align 16
  ret void
}

; Function Attrs: nofree nounwind ssp uwtable
define internal void @idct32_low1_ssse3(<2 x i64>* nocapture readonly, <2 x i64>*, i8 signext) #4 {
  %4 = bitcast <2 x i64>* %0 to <8 x i16>*
  %5 = load <8 x i16>, <8 x i16>* %4, align 16
  %6 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 32), align 16
  %7 = trunc i32 %6 to i16
  %8 = shl i16 %7, 3
  %9 = insertelement <8 x i16> undef, i16 %8, i32 0
  %10 = shufflevector <8 x i16> %9, <8 x i16> undef, <8 x i32> zeroinitializer
  %11 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %5, <8 x i16> %10) #9
  %12 = bitcast <2 x i64>* %1 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %12, align 16
  %13 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 31
  %14 = bitcast <2 x i64>* %13 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %14, align 16
  %15 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 1
  %16 = bitcast <2 x i64>* %15 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %16, align 16
  %17 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 30
  %18 = bitcast <2 x i64>* %17 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %18, align 16
  %19 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 2
  %20 = bitcast <2 x i64>* %19 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %20, align 16
  %21 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 29
  %22 = bitcast <2 x i64>* %21 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %22, align 16
  %23 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 3
  %24 = bitcast <2 x i64>* %23 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %24, align 16
  %25 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 28
  %26 = bitcast <2 x i64>* %25 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %26, align 16
  %27 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 4
  %28 = bitcast <2 x i64>* %27 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %28, align 16
  %29 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 27
  %30 = bitcast <2 x i64>* %29 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %30, align 16
  %31 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 5
  %32 = bitcast <2 x i64>* %31 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %32, align 16
  %33 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 26
  %34 = bitcast <2 x i64>* %33 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %34, align 16
  %35 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 6
  %36 = bitcast <2 x i64>* %35 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %36, align 16
  %37 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 25
  %38 = bitcast <2 x i64>* %37 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %38, align 16
  %39 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 7
  %40 = bitcast <2 x i64>* %39 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %40, align 16
  %41 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 24
  %42 = bitcast <2 x i64>* %41 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %42, align 16
  %43 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 8
  %44 = bitcast <2 x i64>* %43 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %44, align 16
  %45 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 23
  %46 = bitcast <2 x i64>* %45 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %46, align 16
  %47 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 9
  %48 = bitcast <2 x i64>* %47 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %48, align 16
  %49 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 22
  %50 = bitcast <2 x i64>* %49 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %50, align 16
  %51 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 10
  %52 = bitcast <2 x i64>* %51 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %52, align 16
  %53 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 21
  %54 = bitcast <2 x i64>* %53 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %54, align 16
  %55 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 11
  %56 = bitcast <2 x i64>* %55 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %56, align 16
  %57 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 20
  %58 = bitcast <2 x i64>* %57 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %58, align 16
  %59 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 12
  %60 = bitcast <2 x i64>* %59 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %60, align 16
  %61 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 19
  %62 = bitcast <2 x i64>* %61 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %62, align 16
  %63 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 13
  %64 = bitcast <2 x i64>* %63 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %64, align 16
  %65 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 18
  %66 = bitcast <2 x i64>* %65 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %66, align 16
  %67 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 14
  %68 = bitcast <2 x i64>* %67 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %68, align 16
  %69 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 17
  %70 = bitcast <2 x i64>* %69 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %70, align 16
  %71 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 15
  %72 = bitcast <2 x i64>* %71 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %72, align 16
  %73 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 16
  %74 = bitcast <2 x i64>* %73 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %74, align 16
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal void @idct32_low8_ssse3(<2 x i64>* nocapture readonly, <2 x i64>*, i8 signext) #0 {
  %4 = bitcast <2 x i64>* %0 to <8 x i16>*
  %5 = load <8 x i16>, <8 x i16>* %4, align 16
  %6 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 4
  %7 = bitcast <2 x i64>* %6 to <8 x i16>*
  %8 = load <8 x i16>, <8 x i16>* %7, align 16
  %9 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 2
  %10 = bitcast <2 x i64>* %9 to <8 x i16>*
  %11 = load <8 x i16>, <8 x i16>* %10, align 16
  %12 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 6
  %13 = bitcast <2 x i64>* %12 to <8 x i16>*
  %14 = load <8 x i16>, <8 x i16>* %13, align 16
  %15 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 1
  %16 = bitcast <2 x i64>* %15 to <8 x i16>*
  %17 = load <8 x i16>, <8 x i16>* %16, align 16
  %18 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 5
  %19 = bitcast <2 x i64>* %18 to <8 x i16>*
  %20 = load <8 x i16>, <8 x i16>* %19, align 16
  %21 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 3
  %22 = bitcast <2 x i64>* %21 to <8 x i16>*
  %23 = load <8 x i16>, <8 x i16>* %22, align 16
  %24 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 7
  %25 = bitcast <2 x i64>* %24 to <8 x i16>*
  %26 = load <8 x i16>, <8 x i16>* %25, align 16
  %27 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 62), align 8
  %28 = trunc i32 %27 to i16
  %29 = shl i16 %28, 3
  %30 = insertelement <8 x i16> undef, i16 %29, i32 0
  %31 = shufflevector <8 x i16> %30, <8 x i16> undef, <8 x i32> zeroinitializer
  %32 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 2), align 8
  %33 = trunc i32 %32 to i16
  %34 = shl i16 %33, 3
  %35 = insertelement <8 x i16> undef, i16 %34, i32 0
  %36 = shufflevector <8 x i16> %35, <8 x i16> undef, <8 x i32> zeroinitializer
  %37 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %17, <8 x i16> %31) #9
  %38 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %17, <8 x i16> %36) #9
  %39 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 50), align 8
  %40 = trunc i32 %39 to i16
  %41 = shl i16 %40, 3
  %42 = sub i16 0, %41
  %43 = insertelement <8 x i16> undef, i16 %42, i32 0
  %44 = shufflevector <8 x i16> %43, <8 x i16> undef, <8 x i32> zeroinitializer
  %45 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 14), align 8
  %46 = trunc i32 %45 to i16
  %47 = shl i16 %46, 3
  %48 = insertelement <8 x i16> undef, i16 %47, i32 0
  %49 = shufflevector <8 x i16> %48, <8 x i16> undef, <8 x i32> zeroinitializer
  %50 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %26, <8 x i16> %44) #9
  %51 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %26, <8 x i16> %49) #9
  %52 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 54), align 8
  %53 = trunc i32 %52 to i16
  %54 = shl i16 %53, 3
  %55 = insertelement <8 x i16> undef, i16 %54, i32 0
  %56 = shufflevector <8 x i16> %55, <8 x i16> undef, <8 x i32> zeroinitializer
  %57 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 10), align 8
  %58 = trunc i32 %57 to i16
  %59 = shl i16 %58, 3
  %60 = insertelement <8 x i16> undef, i16 %59, i32 0
  %61 = shufflevector <8 x i16> %60, <8 x i16> undef, <8 x i32> zeroinitializer
  %62 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %20, <8 x i16> %56) #9
  %63 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %20, <8 x i16> %61) #9
  %64 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 58), align 8
  %65 = trunc i32 %64 to i16
  %66 = shl i16 %65, 3
  %67 = sub i16 0, %66
  %68 = insertelement <8 x i16> undef, i16 %67, i32 0
  %69 = shufflevector <8 x i16> %68, <8 x i16> undef, <8 x i32> zeroinitializer
  %70 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 6), align 8
  %71 = trunc i32 %70 to i16
  %72 = shl i16 %71, 3
  %73 = insertelement <8 x i16> undef, i16 %72, i32 0
  %74 = shufflevector <8 x i16> %73, <8 x i16> undef, <8 x i32> zeroinitializer
  %75 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %23, <8 x i16> %69) #9
  %76 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %23, <8 x i16> %74) #9
  %77 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 60), align 16
  %78 = trunc i32 %77 to i16
  %79 = shl i16 %78, 3
  %80 = insertelement <8 x i16> undef, i16 %79, i32 0
  %81 = shufflevector <8 x i16> %80, <8 x i16> undef, <8 x i32> zeroinitializer
  %82 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 4), align 16
  %83 = trunc i32 %82 to i16
  %84 = shl i16 %83, 3
  %85 = insertelement <8 x i16> undef, i16 %84, i32 0
  %86 = shufflevector <8 x i16> %85, <8 x i16> undef, <8 x i32> zeroinitializer
  %87 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %11, <8 x i16> %81) #9
  %88 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %11, <8 x i16> %86) #9
  %89 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 52), align 16
  %90 = trunc i32 %89 to i16
  %91 = shl i16 %90, 3
  %92 = sub i16 0, %91
  %93 = insertelement <8 x i16> undef, i16 %92, i32 0
  %94 = shufflevector <8 x i16> %93, <8 x i16> undef, <8 x i32> zeroinitializer
  %95 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 12), align 16
  %96 = trunc i32 %95 to i16
  %97 = shl i16 %96, 3
  %98 = insertelement <8 x i16> undef, i16 %97, i32 0
  %99 = shufflevector <8 x i16> %98, <8 x i16> undef, <8 x i32> zeroinitializer
  %100 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %14, <8 x i16> %94) #9
  %101 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %14, <8 x i16> %99) #9
  %102 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 56), align 16
  %103 = trunc i32 %102 to i16
  %104 = shl i16 %103, 3
  %105 = insertelement <8 x i16> undef, i16 %104, i32 0
  %106 = shufflevector <8 x i16> %105, <8 x i16> undef, <8 x i32> zeroinitializer
  %107 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 8), align 16
  %108 = trunc i32 %107 to i16
  %109 = shl i16 %108, 3
  %110 = insertelement <8 x i16> undef, i16 %109, i32 0
  %111 = shufflevector <8 x i16> %110, <8 x i16> undef, <8 x i32> zeroinitializer
  %112 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %8, <8 x i16> %106) #9
  %113 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %8, <8 x i16> %111) #9
  %114 = sub i32 0, %107
  %115 = and i32 %114, 65535
  %116 = shl i32 %102, 16
  %117 = or i32 %115, %116
  %118 = insertelement <4 x i32> undef, i32 %117, i32 0
  %119 = shufflevector <4 x i32> %118, <4 x i32> undef, <4 x i32> zeroinitializer
  %120 = and i32 %102, 65535
  %121 = shl i32 %107, 16
  %122 = or i32 %121, %120
  %123 = insertelement <4 x i32> undef, i32 %122, i32 0
  %124 = shufflevector <4 x i32> %123, <4 x i32> undef, <4 x i32> zeroinitializer
  %125 = sub i32 0, %102
  %126 = and i32 %125, 65535
  %127 = sub i32 0, %121
  %128 = or i32 %126, %127
  %129 = insertelement <4 x i32> undef, i32 %128, i32 0
  %130 = shufflevector <4 x i32> %129, <4 x i32> undef, <4 x i32> zeroinitializer
  %131 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 40), align 16
  %132 = sub i32 0, %131
  %133 = and i32 %132, 65535
  %134 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 24), align 16
  %135 = shl i32 %134, 16
  %136 = or i32 %135, %133
  %137 = insertelement <4 x i32> undef, i32 %136, i32 0
  %138 = shufflevector <4 x i32> %137, <4 x i32> undef, <4 x i32> zeroinitializer
  %139 = and i32 %134, 65535
  %140 = shl i32 %131, 16
  %141 = or i32 %139, %140
  %142 = insertelement <4 x i32> undef, i32 %141, i32 0
  %143 = shufflevector <4 x i32> %142, <4 x i32> undef, <4 x i32> zeroinitializer
  %144 = sub i32 0, %134
  %145 = and i32 %144, 65535
  %146 = sub i32 0, %140
  %147 = or i32 %145, %146
  %148 = insertelement <4 x i32> undef, i32 %147, i32 0
  %149 = shufflevector <4 x i32> %148, <4 x i32> undef, <4 x i32> zeroinitializer
  %150 = shufflevector <8 x i16> %37, <8 x i16> %38, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %151 = shufflevector <8 x i16> %37, <8 x i16> %38, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %152 = bitcast <4 x i32> %119 to <8 x i16>
  %153 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %150, <8 x i16> %152) #9
  %154 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %151, <8 x i16> %152) #9
  %155 = bitcast <4 x i32> %124 to <8 x i16>
  %156 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %150, <8 x i16> %155) #9
  %157 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %151, <8 x i16> %155) #9
  %158 = add <4 x i32> %153, <i32 2048, i32 2048, i32 2048, i32 2048>
  %159 = add <4 x i32> %154, <i32 2048, i32 2048, i32 2048, i32 2048>
  %160 = add <4 x i32> %156, <i32 2048, i32 2048, i32 2048, i32 2048>
  %161 = add <4 x i32> %157, <i32 2048, i32 2048, i32 2048, i32 2048>
  %162 = sext i8 %2 to i32
  %163 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %158, i32 %162) #9
  %164 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %159, i32 %162) #9
  %165 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %160, i32 %162) #9
  %166 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %161, i32 %162) #9
  %167 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %163, <4 x i32> %164) #9
  %168 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %165, <4 x i32> %166) #9
  %169 = shufflevector <8 x i16> %50, <8 x i16> %51, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %170 = shufflevector <8 x i16> %50, <8 x i16> %51, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %171 = bitcast <4 x i32> %130 to <8 x i16>
  %172 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %169, <8 x i16> %171) #9
  %173 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %170, <8 x i16> %171) #9
  %174 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %169, <8 x i16> %152) #9
  %175 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %170, <8 x i16> %152) #9
  %176 = add <4 x i32> %172, <i32 2048, i32 2048, i32 2048, i32 2048>
  %177 = add <4 x i32> %173, <i32 2048, i32 2048, i32 2048, i32 2048>
  %178 = add <4 x i32> %174, <i32 2048, i32 2048, i32 2048, i32 2048>
  %179 = add <4 x i32> %175, <i32 2048, i32 2048, i32 2048, i32 2048>
  %180 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %176, i32 %162) #9
  %181 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %177, i32 %162) #9
  %182 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %178, i32 %162) #9
  %183 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %179, i32 %162) #9
  %184 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %180, <4 x i32> %181) #9
  %185 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %182, <4 x i32> %183) #9
  %186 = shufflevector <8 x i16> %62, <8 x i16> %63, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %187 = shufflevector <8 x i16> %62, <8 x i16> %63, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %188 = bitcast <4 x i32> %138 to <8 x i16>
  %189 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %186, <8 x i16> %188) #9
  %190 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %187, <8 x i16> %188) #9
  %191 = bitcast <4 x i32> %143 to <8 x i16>
  %192 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %186, <8 x i16> %191) #9
  %193 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %187, <8 x i16> %191) #9
  %194 = add <4 x i32> %189, <i32 2048, i32 2048, i32 2048, i32 2048>
  %195 = add <4 x i32> %190, <i32 2048, i32 2048, i32 2048, i32 2048>
  %196 = add <4 x i32> %192, <i32 2048, i32 2048, i32 2048, i32 2048>
  %197 = add <4 x i32> %193, <i32 2048, i32 2048, i32 2048, i32 2048>
  %198 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %194, i32 %162) #9
  %199 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %195, i32 %162) #9
  %200 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %196, i32 %162) #9
  %201 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %197, i32 %162) #9
  %202 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %198, <4 x i32> %199) #9
  %203 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %200, <4 x i32> %201) #9
  %204 = shufflevector <8 x i16> %75, <8 x i16> %76, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %205 = shufflevector <8 x i16> %75, <8 x i16> %76, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %206 = bitcast <4 x i32> %149 to <8 x i16>
  %207 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %204, <8 x i16> %206) #9
  %208 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %205, <8 x i16> %206) #9
  %209 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %204, <8 x i16> %188) #9
  %210 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %205, <8 x i16> %188) #9
  %211 = add <4 x i32> %207, <i32 2048, i32 2048, i32 2048, i32 2048>
  %212 = add <4 x i32> %208, <i32 2048, i32 2048, i32 2048, i32 2048>
  %213 = add <4 x i32> %209, <i32 2048, i32 2048, i32 2048, i32 2048>
  %214 = add <4 x i32> %210, <i32 2048, i32 2048, i32 2048, i32 2048>
  %215 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %211, i32 %162) #9
  %216 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %212, i32 %162) #9
  %217 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %213, i32 %162) #9
  %218 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %214, i32 %162) #9
  %219 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %215, <4 x i32> %216) #9
  %220 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %217, <4 x i32> %218) #9
  %221 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 32), align 16
  %222 = trunc i32 %221 to i16
  %223 = shl i16 %222, 3
  %224 = insertelement <8 x i16> undef, i16 %223, i32 0
  %225 = shufflevector <8 x i16> %224, <8 x i16> undef, <8 x i32> zeroinitializer
  %226 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %5, <8 x i16> %225) #9
  %227 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 16), align 16
  %228 = sub i32 0, %227
  %229 = and i32 %228, 65535
  %230 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 48), align 16
  %231 = shl i32 %230, 16
  %232 = or i32 %231, %229
  %233 = insertelement <4 x i32> undef, i32 %232, i32 0
  %234 = shufflevector <4 x i32> %233, <4 x i32> undef, <4 x i32> zeroinitializer
  %235 = and i32 %230, 65535
  %236 = shl i32 %227, 16
  %237 = or i32 %235, %236
  %238 = insertelement <4 x i32> undef, i32 %237, i32 0
  %239 = shufflevector <4 x i32> %238, <4 x i32> undef, <4 x i32> zeroinitializer
  %240 = sub i32 0, %230
  %241 = and i32 %240, 65535
  %242 = sub i32 0, %236
  %243 = or i32 %241, %242
  %244 = insertelement <4 x i32> undef, i32 %243, i32 0
  %245 = shufflevector <4 x i32> %244, <4 x i32> undef, <4 x i32> zeroinitializer
  %246 = shufflevector <8 x i16> %87, <8 x i16> %88, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %247 = shufflevector <8 x i16> %87, <8 x i16> %88, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %248 = bitcast <4 x i32> %234 to <8 x i16>
  %249 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %246, <8 x i16> %248) #9
  %250 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %247, <8 x i16> %248) #9
  %251 = bitcast <4 x i32> %239 to <8 x i16>
  %252 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %246, <8 x i16> %251) #9
  %253 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %247, <8 x i16> %251) #9
  %254 = add <4 x i32> %249, <i32 2048, i32 2048, i32 2048, i32 2048>
  %255 = add <4 x i32> %250, <i32 2048, i32 2048, i32 2048, i32 2048>
  %256 = add <4 x i32> %252, <i32 2048, i32 2048, i32 2048, i32 2048>
  %257 = add <4 x i32> %253, <i32 2048, i32 2048, i32 2048, i32 2048>
  %258 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %254, i32 %162) #9
  %259 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %255, i32 %162) #9
  %260 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %256, i32 %162) #9
  %261 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %257, i32 %162) #9
  %262 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %258, <4 x i32> %259) #9
  %263 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %260, <4 x i32> %261) #9
  %264 = shufflevector <8 x i16> %100, <8 x i16> %101, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %265 = shufflevector <8 x i16> %100, <8 x i16> %101, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %266 = bitcast <4 x i32> %245 to <8 x i16>
  %267 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %264, <8 x i16> %266) #9
  %268 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %265, <8 x i16> %266) #9
  %269 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %264, <8 x i16> %248) #9
  %270 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %265, <8 x i16> %248) #9
  %271 = add <4 x i32> %267, <i32 2048, i32 2048, i32 2048, i32 2048>
  %272 = add <4 x i32> %268, <i32 2048, i32 2048, i32 2048, i32 2048>
  %273 = add <4 x i32> %269, <i32 2048, i32 2048, i32 2048, i32 2048>
  %274 = add <4 x i32> %270, <i32 2048, i32 2048, i32 2048, i32 2048>
  %275 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %271, i32 %162) #9
  %276 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %272, i32 %162) #9
  %277 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %273, i32 %162) #9
  %278 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %274, i32 %162) #9
  %279 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %275, <4 x i32> %276) #9
  %280 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %277, <4 x i32> %278) #9
  %281 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %37, <8 x i16> %50) #9
  %282 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %37, <8 x i16> %50) #9
  %283 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %167, <8 x i16> %184) #9
  %284 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %167, <8 x i16> %184) #9
  %285 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %75, <8 x i16> %62) #9
  %286 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %75, <8 x i16> %62) #9
  %287 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %219, <8 x i16> %202) #9
  %288 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %219, <8 x i16> %202) #9
  %289 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %76, <8 x i16> %63) #9
  %290 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %76, <8 x i16> %63) #9
  %291 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %220, <8 x i16> %203) #9
  %292 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %220, <8 x i16> %203) #9
  %293 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %38, <8 x i16> %51) #9
  %294 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %38, <8 x i16> %51) #9
  %295 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %168, <8 x i16> %185) #9
  %296 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %168, <8 x i16> %185) #9
  %297 = sub i32 0, %221
  %298 = and i32 %297, 65535
  %299 = shl i32 %221, 16
  %300 = or i32 %298, %299
  %301 = insertelement <4 x i32> undef, i32 %300, i32 0
  %302 = shufflevector <4 x i32> %301, <4 x i32> undef, <4 x i32> zeroinitializer
  %303 = and i32 %221, 65535
  %304 = or i32 %303, %299
  %305 = insertelement <4 x i32> undef, i32 %304, i32 0
  %306 = shufflevector <4 x i32> %305, <4 x i32> undef, <4 x i32> zeroinitializer
  %307 = shufflevector <8 x i16> %112, <8 x i16> %113, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %308 = shufflevector <8 x i16> %112, <8 x i16> %113, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %309 = bitcast <4 x i32> %302 to <8 x i16>
  %310 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %307, <8 x i16> %309) #9
  %311 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %308, <8 x i16> %309) #9
  %312 = bitcast <4 x i32> %306 to <8 x i16>
  %313 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %307, <8 x i16> %312) #9
  %314 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %308, <8 x i16> %312) #9
  %315 = add <4 x i32> %310, <i32 2048, i32 2048, i32 2048, i32 2048>
  %316 = add <4 x i32> %311, <i32 2048, i32 2048, i32 2048, i32 2048>
  %317 = add <4 x i32> %313, <i32 2048, i32 2048, i32 2048, i32 2048>
  %318 = add <4 x i32> %314, <i32 2048, i32 2048, i32 2048, i32 2048>
  %319 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %315, i32 %162) #9
  %320 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %316, i32 %162) #9
  %321 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %317, i32 %162) #9
  %322 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %318, i32 %162) #9
  %323 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %319, <4 x i32> %320) #9
  %324 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %321, <4 x i32> %322) #9
  %325 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %87, <8 x i16> %100) #9
  %326 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %87, <8 x i16> %100) #9
  %327 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %262, <8 x i16> %279) #9
  %328 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %262, <8 x i16> %279) #9
  %329 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %88, <8 x i16> %101) #9
  %330 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %88, <8 x i16> %101) #9
  %331 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %263, <8 x i16> %280) #9
  %332 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %263, <8 x i16> %280) #9
  %333 = shufflevector <8 x i16> %284, <8 x i16> %295, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %334 = shufflevector <8 x i16> %284, <8 x i16> %295, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %335 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %333, <8 x i16> %248) #9
  %336 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %334, <8 x i16> %248) #9
  %337 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %333, <8 x i16> %251) #9
  %338 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %334, <8 x i16> %251) #9
  %339 = add <4 x i32> %335, <i32 2048, i32 2048, i32 2048, i32 2048>
  %340 = add <4 x i32> %336, <i32 2048, i32 2048, i32 2048, i32 2048>
  %341 = add <4 x i32> %337, <i32 2048, i32 2048, i32 2048, i32 2048>
  %342 = add <4 x i32> %338, <i32 2048, i32 2048, i32 2048, i32 2048>
  %343 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %339, i32 %162) #9
  %344 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %340, i32 %162) #9
  %345 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %341, i32 %162) #9
  %346 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %342, i32 %162) #9
  %347 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %343, <4 x i32> %344) #9
  %348 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %345, <4 x i32> %346) #9
  %349 = shufflevector <8 x i16> %282, <8 x i16> %293, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %350 = shufflevector <8 x i16> %282, <8 x i16> %293, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %351 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %349, <8 x i16> %248) #9
  %352 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %350, <8 x i16> %248) #9
  %353 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %349, <8 x i16> %251) #9
  %354 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %350, <8 x i16> %251) #9
  %355 = add <4 x i32> %351, <i32 2048, i32 2048, i32 2048, i32 2048>
  %356 = add <4 x i32> %352, <i32 2048, i32 2048, i32 2048, i32 2048>
  %357 = add <4 x i32> %353, <i32 2048, i32 2048, i32 2048, i32 2048>
  %358 = add <4 x i32> %354, <i32 2048, i32 2048, i32 2048, i32 2048>
  %359 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %355, i32 %162) #9
  %360 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %356, i32 %162) #9
  %361 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %357, i32 %162) #9
  %362 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %358, i32 %162) #9
  %363 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %359, <4 x i32> %360) #9
  %364 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %361, <4 x i32> %362) #9
  %365 = shufflevector <8 x i16> %285, <8 x i16> %290, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %366 = shufflevector <8 x i16> %285, <8 x i16> %290, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %367 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %365, <8 x i16> %266) #9
  %368 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %366, <8 x i16> %266) #9
  %369 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %365, <8 x i16> %248) #9
  %370 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %366, <8 x i16> %248) #9
  %371 = add <4 x i32> %367, <i32 2048, i32 2048, i32 2048, i32 2048>
  %372 = add <4 x i32> %368, <i32 2048, i32 2048, i32 2048, i32 2048>
  %373 = add <4 x i32> %369, <i32 2048, i32 2048, i32 2048, i32 2048>
  %374 = add <4 x i32> %370, <i32 2048, i32 2048, i32 2048, i32 2048>
  %375 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %371, i32 %162) #9
  %376 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %372, i32 %162) #9
  %377 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %373, i32 %162) #9
  %378 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %374, i32 %162) #9
  %379 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %375, <4 x i32> %376) #9
  %380 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %377, <4 x i32> %378) #9
  %381 = shufflevector <8 x i16> %287, <8 x i16> %292, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %382 = shufflevector <8 x i16> %287, <8 x i16> %292, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %383 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %381, <8 x i16> %266) #9
  %384 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %382, <8 x i16> %266) #9
  %385 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %381, <8 x i16> %248) #9
  %386 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %382, <8 x i16> %248) #9
  %387 = add <4 x i32> %383, <i32 2048, i32 2048, i32 2048, i32 2048>
  %388 = add <4 x i32> %384, <i32 2048, i32 2048, i32 2048, i32 2048>
  %389 = add <4 x i32> %385, <i32 2048, i32 2048, i32 2048, i32 2048>
  %390 = add <4 x i32> %386, <i32 2048, i32 2048, i32 2048, i32 2048>
  %391 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %387, i32 %162) #9
  %392 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %388, i32 %162) #9
  %393 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %389, i32 %162) #9
  %394 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %390, i32 %162) #9
  %395 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %391, <4 x i32> %392) #9
  %396 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %393, <4 x i32> %394) #9
  %397 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %226, <8 x i16> %113) #9
  %398 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %226, <8 x i16> %113) #9
  %399 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %226, <8 x i16> %324) #9
  %400 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %226, <8 x i16> %324) #9
  %401 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %226, <8 x i16> %323) #9
  %402 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %226, <8 x i16> %323) #9
  %403 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %226, <8 x i16> %112) #9
  %404 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %226, <8 x i16> %112) #9
  %405 = shufflevector <8 x i16> %328, <8 x i16> %331, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %406 = shufflevector <8 x i16> %328, <8 x i16> %331, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %407 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %405, <8 x i16> %309) #9
  %408 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %406, <8 x i16> %309) #9
  %409 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %405, <8 x i16> %312) #9
  %410 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %406, <8 x i16> %312) #9
  %411 = add <4 x i32> %407, <i32 2048, i32 2048, i32 2048, i32 2048>
  %412 = add <4 x i32> %408, <i32 2048, i32 2048, i32 2048, i32 2048>
  %413 = add <4 x i32> %409, <i32 2048, i32 2048, i32 2048, i32 2048>
  %414 = add <4 x i32> %410, <i32 2048, i32 2048, i32 2048, i32 2048>
  %415 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %411, i32 %162) #9
  %416 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %412, i32 %162) #9
  %417 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %413, i32 %162) #9
  %418 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %414, i32 %162) #9
  %419 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %415, <4 x i32> %416) #9
  %420 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %417, <4 x i32> %418) #9
  %421 = shufflevector <8 x i16> %326, <8 x i16> %329, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %422 = shufflevector <8 x i16> %326, <8 x i16> %329, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %423 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %421, <8 x i16> %309) #9
  %424 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %422, <8 x i16> %309) #9
  %425 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %421, <8 x i16> %312) #9
  %426 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %422, <8 x i16> %312) #9
  %427 = add <4 x i32> %423, <i32 2048, i32 2048, i32 2048, i32 2048>
  %428 = add <4 x i32> %424, <i32 2048, i32 2048, i32 2048, i32 2048>
  %429 = add <4 x i32> %425, <i32 2048, i32 2048, i32 2048, i32 2048>
  %430 = add <4 x i32> %426, <i32 2048, i32 2048, i32 2048, i32 2048>
  %431 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %427, i32 %162) #9
  %432 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %428, i32 %162) #9
  %433 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %429, i32 %162) #9
  %434 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %430, i32 %162) #9
  %435 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %431, <4 x i32> %432) #9
  %436 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %433, <4 x i32> %434) #9
  %437 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %281, <8 x i16> %286) #9
  %438 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %281, <8 x i16> %286) #9
  %439 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %283, <8 x i16> %288) #9
  %440 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %283, <8 x i16> %288) #9
  %441 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %347, <8 x i16> %395) #9
  %442 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %347, <8 x i16> %395) #9
  %443 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %363, <8 x i16> %379) #9
  %444 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %363, <8 x i16> %379) #9
  %445 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %294, <8 x i16> %289) #9
  %446 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %294, <8 x i16> %289) #9
  %447 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %296, <8 x i16> %291) #9
  %448 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %296, <8 x i16> %291) #9
  %449 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %348, <8 x i16> %396) #9
  %450 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %348, <8 x i16> %396) #9
  %451 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %364, <8 x i16> %380) #9
  %452 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %364, <8 x i16> %380) #9
  %453 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %397, <8 x i16> %330) #9
  %454 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %397, <8 x i16> %330) #9
  %455 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %399, <8 x i16> %332) #9
  %456 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %399, <8 x i16> %332) #9
  %457 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %401, <8 x i16> %420) #9
  %458 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %401, <8 x i16> %420) #9
  %459 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %403, <8 x i16> %436) #9
  %460 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %403, <8 x i16> %436) #9
  %461 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %404, <8 x i16> %435) #9
  %462 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %404, <8 x i16> %435) #9
  %463 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %402, <8 x i16> %419) #9
  %464 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %402, <8 x i16> %419) #9
  %465 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %400, <8 x i16> %327) #9
  %466 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %400, <8 x i16> %327) #9
  %467 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %398, <8 x i16> %325) #9
  %468 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %398, <8 x i16> %325) #9
  %469 = shufflevector <8 x i16> %444, <8 x i16> %451, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %470 = shufflevector <8 x i16> %444, <8 x i16> %451, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %471 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %469, <8 x i16> %309) #9
  %472 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %470, <8 x i16> %309) #9
  %473 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %469, <8 x i16> %312) #9
  %474 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %470, <8 x i16> %312) #9
  %475 = add <4 x i32> %471, <i32 2048, i32 2048, i32 2048, i32 2048>
  %476 = add <4 x i32> %472, <i32 2048, i32 2048, i32 2048, i32 2048>
  %477 = add <4 x i32> %473, <i32 2048, i32 2048, i32 2048, i32 2048>
  %478 = add <4 x i32> %474, <i32 2048, i32 2048, i32 2048, i32 2048>
  %479 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %475, i32 %162) #9
  %480 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %476, i32 %162) #9
  %481 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %477, i32 %162) #9
  %482 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %478, i32 %162) #9
  %483 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %479, <4 x i32> %480) #9
  %484 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %481, <4 x i32> %482) #9
  %485 = shufflevector <8 x i16> %442, <8 x i16> %449, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %486 = shufflevector <8 x i16> %442, <8 x i16> %449, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %487 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %485, <8 x i16> %309) #9
  %488 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %486, <8 x i16> %309) #9
  %489 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %485, <8 x i16> %312) #9
  %490 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %486, <8 x i16> %312) #9
  %491 = add <4 x i32> %487, <i32 2048, i32 2048, i32 2048, i32 2048>
  %492 = add <4 x i32> %488, <i32 2048, i32 2048, i32 2048, i32 2048>
  %493 = add <4 x i32> %489, <i32 2048, i32 2048, i32 2048, i32 2048>
  %494 = add <4 x i32> %490, <i32 2048, i32 2048, i32 2048, i32 2048>
  %495 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %491, i32 %162) #9
  %496 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %492, i32 %162) #9
  %497 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %493, i32 %162) #9
  %498 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %494, i32 %162) #9
  %499 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %495, <4 x i32> %496) #9
  %500 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %497, <4 x i32> %498) #9
  %501 = shufflevector <8 x i16> %440, <8 x i16> %447, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %502 = shufflevector <8 x i16> %440, <8 x i16> %447, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %503 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %501, <8 x i16> %309) #9
  %504 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %502, <8 x i16> %309) #9
  %505 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %501, <8 x i16> %312) #9
  %506 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %502, <8 x i16> %312) #9
  %507 = add <4 x i32> %503, <i32 2048, i32 2048, i32 2048, i32 2048>
  %508 = add <4 x i32> %504, <i32 2048, i32 2048, i32 2048, i32 2048>
  %509 = add <4 x i32> %505, <i32 2048, i32 2048, i32 2048, i32 2048>
  %510 = add <4 x i32> %506, <i32 2048, i32 2048, i32 2048, i32 2048>
  %511 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %507, i32 %162) #9
  %512 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %508, i32 %162) #9
  %513 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %509, i32 %162) #9
  %514 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %510, i32 %162) #9
  %515 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %511, <4 x i32> %512) #9
  %516 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %513, <4 x i32> %514) #9
  %517 = shufflevector <8 x i16> %438, <8 x i16> %445, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %518 = shufflevector <8 x i16> %438, <8 x i16> %445, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %519 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %517, <8 x i16> %309) #9
  %520 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %518, <8 x i16> %309) #9
  %521 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %517, <8 x i16> %312) #9
  %522 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %518, <8 x i16> %312) #9
  %523 = add <4 x i32> %519, <i32 2048, i32 2048, i32 2048, i32 2048>
  %524 = add <4 x i32> %520, <i32 2048, i32 2048, i32 2048, i32 2048>
  %525 = add <4 x i32> %521, <i32 2048, i32 2048, i32 2048, i32 2048>
  %526 = add <4 x i32> %522, <i32 2048, i32 2048, i32 2048, i32 2048>
  %527 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %523, i32 %162) #9
  %528 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %524, i32 %162) #9
  %529 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %525, i32 %162) #9
  %530 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %526, i32 %162) #9
  %531 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %527, <4 x i32> %528) #9
  %532 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %529, <4 x i32> %530) #9
  %533 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %453, <8 x i16> %446) #9
  %534 = bitcast <2 x i64>* %1 to <8 x i16>*
  store <8 x i16> %533, <8 x i16>* %534, align 16
  %535 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %453, <8 x i16> %446) #9
  %536 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 31
  %537 = bitcast <2 x i64>* %536 to <8 x i16>*
  store <8 x i16> %535, <8 x i16>* %537, align 16
  %538 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %455, <8 x i16> %448) #9
  %539 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 1
  %540 = bitcast <2 x i64>* %539 to <8 x i16>*
  store <8 x i16> %538, <8 x i16>* %540, align 16
  %541 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %455, <8 x i16> %448) #9
  %542 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 30
  %543 = bitcast <2 x i64>* %542 to <8 x i16>*
  store <8 x i16> %541, <8 x i16>* %543, align 16
  %544 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %457, <8 x i16> %450) #9
  %545 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 2
  %546 = bitcast <2 x i64>* %545 to <8 x i16>*
  store <8 x i16> %544, <8 x i16>* %546, align 16
  %547 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %457, <8 x i16> %450) #9
  %548 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 29
  %549 = bitcast <2 x i64>* %548 to <8 x i16>*
  store <8 x i16> %547, <8 x i16>* %549, align 16
  %550 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %459, <8 x i16> %452) #9
  %551 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 3
  %552 = bitcast <2 x i64>* %551 to <8 x i16>*
  store <8 x i16> %550, <8 x i16>* %552, align 16
  %553 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %459, <8 x i16> %452) #9
  %554 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 28
  %555 = bitcast <2 x i64>* %554 to <8 x i16>*
  store <8 x i16> %553, <8 x i16>* %555, align 16
  %556 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %461, <8 x i16> %484) #9
  %557 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 4
  %558 = bitcast <2 x i64>* %557 to <8 x i16>*
  store <8 x i16> %556, <8 x i16>* %558, align 16
  %559 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %461, <8 x i16> %484) #9
  %560 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 27
  %561 = bitcast <2 x i64>* %560 to <8 x i16>*
  store <8 x i16> %559, <8 x i16>* %561, align 16
  %562 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %463, <8 x i16> %500) #9
  %563 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 5
  %564 = bitcast <2 x i64>* %563 to <8 x i16>*
  store <8 x i16> %562, <8 x i16>* %564, align 16
  %565 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %463, <8 x i16> %500) #9
  %566 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 26
  %567 = bitcast <2 x i64>* %566 to <8 x i16>*
  store <8 x i16> %565, <8 x i16>* %567, align 16
  %568 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %465, <8 x i16> %516) #9
  %569 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 6
  %570 = bitcast <2 x i64>* %569 to <8 x i16>*
  store <8 x i16> %568, <8 x i16>* %570, align 16
  %571 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %465, <8 x i16> %516) #9
  %572 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 25
  %573 = bitcast <2 x i64>* %572 to <8 x i16>*
  store <8 x i16> %571, <8 x i16>* %573, align 16
  %574 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %467, <8 x i16> %532) #9
  %575 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 7
  %576 = bitcast <2 x i64>* %575 to <8 x i16>*
  store <8 x i16> %574, <8 x i16>* %576, align 16
  %577 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %467, <8 x i16> %532) #9
  %578 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 24
  %579 = bitcast <2 x i64>* %578 to <8 x i16>*
  store <8 x i16> %577, <8 x i16>* %579, align 16
  %580 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %468, <8 x i16> %531) #9
  %581 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 8
  %582 = bitcast <2 x i64>* %581 to <8 x i16>*
  store <8 x i16> %580, <8 x i16>* %582, align 16
  %583 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %468, <8 x i16> %531) #9
  %584 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 23
  %585 = bitcast <2 x i64>* %584 to <8 x i16>*
  store <8 x i16> %583, <8 x i16>* %585, align 16
  %586 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %466, <8 x i16> %515) #9
  %587 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 9
  %588 = bitcast <2 x i64>* %587 to <8 x i16>*
  store <8 x i16> %586, <8 x i16>* %588, align 16
  %589 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %466, <8 x i16> %515) #9
  %590 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 22
  %591 = bitcast <2 x i64>* %590 to <8 x i16>*
  store <8 x i16> %589, <8 x i16>* %591, align 16
  %592 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %464, <8 x i16> %499) #9
  %593 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 10
  %594 = bitcast <2 x i64>* %593 to <8 x i16>*
  store <8 x i16> %592, <8 x i16>* %594, align 16
  %595 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %464, <8 x i16> %499) #9
  %596 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 21
  %597 = bitcast <2 x i64>* %596 to <8 x i16>*
  store <8 x i16> %595, <8 x i16>* %597, align 16
  %598 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %462, <8 x i16> %483) #9
  %599 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 11
  %600 = bitcast <2 x i64>* %599 to <8 x i16>*
  store <8 x i16> %598, <8 x i16>* %600, align 16
  %601 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %462, <8 x i16> %483) #9
  %602 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 20
  %603 = bitcast <2 x i64>* %602 to <8 x i16>*
  store <8 x i16> %601, <8 x i16>* %603, align 16
  %604 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %460, <8 x i16> %443) #9
  %605 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 12
  %606 = bitcast <2 x i64>* %605 to <8 x i16>*
  store <8 x i16> %604, <8 x i16>* %606, align 16
  %607 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %460, <8 x i16> %443) #9
  %608 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 19
  %609 = bitcast <2 x i64>* %608 to <8 x i16>*
  store <8 x i16> %607, <8 x i16>* %609, align 16
  %610 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %458, <8 x i16> %441) #9
  %611 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 13
  %612 = bitcast <2 x i64>* %611 to <8 x i16>*
  store <8 x i16> %610, <8 x i16>* %612, align 16
  %613 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %458, <8 x i16> %441) #9
  %614 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 18
  %615 = bitcast <2 x i64>* %614 to <8 x i16>*
  store <8 x i16> %613, <8 x i16>* %615, align 16
  %616 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %456, <8 x i16> %439) #9
  %617 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 14
  %618 = bitcast <2 x i64>* %617 to <8 x i16>*
  store <8 x i16> %616, <8 x i16>* %618, align 16
  %619 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %456, <8 x i16> %439) #9
  %620 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 17
  %621 = bitcast <2 x i64>* %620 to <8 x i16>*
  store <8 x i16> %619, <8 x i16>* %621, align 16
  %622 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %454, <8 x i16> %437) #9
  %623 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 15
  %624 = bitcast <2 x i64>* %623 to <8 x i16>*
  store <8 x i16> %622, <8 x i16>* %624, align 16
  %625 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %454, <8 x i16> %437) #9
  %626 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 16
  %627 = bitcast <2 x i64>* %626 to <8 x i16>*
  store <8 x i16> %625, <8 x i16>* %627, align 16
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal void @idct32_low16_ssse3(<2 x i64>* nocapture readonly, <2 x i64>*, i8 signext) #0 {
  %4 = bitcast <2 x i64>* %0 to <8 x i16>*
  %5 = load <8 x i16>, <8 x i16>* %4, align 16
  %6 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 8
  %7 = bitcast <2 x i64>* %6 to <8 x i16>*
  %8 = load <8 x i16>, <8 x i16>* %7, align 16
  %9 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 4
  %10 = bitcast <2 x i64>* %9 to <8 x i16>*
  %11 = load <8 x i16>, <8 x i16>* %10, align 16
  %12 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 12
  %13 = bitcast <2 x i64>* %12 to <8 x i16>*
  %14 = load <8 x i16>, <8 x i16>* %13, align 16
  %15 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 2
  %16 = bitcast <2 x i64>* %15 to <8 x i16>*
  %17 = load <8 x i16>, <8 x i16>* %16, align 16
  %18 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 10
  %19 = bitcast <2 x i64>* %18 to <8 x i16>*
  %20 = load <8 x i16>, <8 x i16>* %19, align 16
  %21 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 6
  %22 = bitcast <2 x i64>* %21 to <8 x i16>*
  %23 = load <8 x i16>, <8 x i16>* %22, align 16
  %24 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 14
  %25 = bitcast <2 x i64>* %24 to <8 x i16>*
  %26 = load <8 x i16>, <8 x i16>* %25, align 16
  %27 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 1
  %28 = bitcast <2 x i64>* %27 to <8 x i16>*
  %29 = load <8 x i16>, <8 x i16>* %28, align 16
  %30 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 9
  %31 = bitcast <2 x i64>* %30 to <8 x i16>*
  %32 = load <8 x i16>, <8 x i16>* %31, align 16
  %33 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 5
  %34 = bitcast <2 x i64>* %33 to <8 x i16>*
  %35 = load <8 x i16>, <8 x i16>* %34, align 16
  %36 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 13
  %37 = bitcast <2 x i64>* %36 to <8 x i16>*
  %38 = load <8 x i16>, <8 x i16>* %37, align 16
  %39 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 3
  %40 = bitcast <2 x i64>* %39 to <8 x i16>*
  %41 = load <8 x i16>, <8 x i16>* %40, align 16
  %42 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 11
  %43 = bitcast <2 x i64>* %42 to <8 x i16>*
  %44 = load <8 x i16>, <8 x i16>* %43, align 16
  %45 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 7
  %46 = bitcast <2 x i64>* %45 to <8 x i16>*
  %47 = load <8 x i16>, <8 x i16>* %46, align 16
  %48 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 15
  %49 = bitcast <2 x i64>* %48 to <8 x i16>*
  %50 = load <8 x i16>, <8 x i16>* %49, align 16
  %51 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 62), align 8
  %52 = trunc i32 %51 to i16
  %53 = shl i16 %52, 3
  %54 = insertelement <8 x i16> undef, i16 %53, i32 0
  %55 = shufflevector <8 x i16> %54, <8 x i16> undef, <8 x i32> zeroinitializer
  %56 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 2), align 8
  %57 = trunc i32 %56 to i16
  %58 = shl i16 %57, 3
  %59 = insertelement <8 x i16> undef, i16 %58, i32 0
  %60 = shufflevector <8 x i16> %59, <8 x i16> undef, <8 x i32> zeroinitializer
  %61 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %29, <8 x i16> %55) #9
  %62 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %29, <8 x i16> %60) #9
  %63 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 34), align 8
  %64 = trunc i32 %63 to i16
  %65 = shl i16 %64, 3
  %66 = sub i16 0, %65
  %67 = insertelement <8 x i16> undef, i16 %66, i32 0
  %68 = shufflevector <8 x i16> %67, <8 x i16> undef, <8 x i32> zeroinitializer
  %69 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 30), align 8
  %70 = trunc i32 %69 to i16
  %71 = shl i16 %70, 3
  %72 = insertelement <8 x i16> undef, i16 %71, i32 0
  %73 = shufflevector <8 x i16> %72, <8 x i16> undef, <8 x i32> zeroinitializer
  %74 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %50, <8 x i16> %68) #9
  %75 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %50, <8 x i16> %73) #9
  %76 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 46), align 8
  %77 = trunc i32 %76 to i16
  %78 = shl i16 %77, 3
  %79 = insertelement <8 x i16> undef, i16 %78, i32 0
  %80 = shufflevector <8 x i16> %79, <8 x i16> undef, <8 x i32> zeroinitializer
  %81 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 18), align 8
  %82 = trunc i32 %81 to i16
  %83 = shl i16 %82, 3
  %84 = insertelement <8 x i16> undef, i16 %83, i32 0
  %85 = shufflevector <8 x i16> %84, <8 x i16> undef, <8 x i32> zeroinitializer
  %86 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %32, <8 x i16> %80) #9
  %87 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %32, <8 x i16> %85) #9
  %88 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 50), align 8
  %89 = trunc i32 %88 to i16
  %90 = shl i16 %89, 3
  %91 = sub i16 0, %90
  %92 = insertelement <8 x i16> undef, i16 %91, i32 0
  %93 = shufflevector <8 x i16> %92, <8 x i16> undef, <8 x i32> zeroinitializer
  %94 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 14), align 8
  %95 = trunc i32 %94 to i16
  %96 = shl i16 %95, 3
  %97 = insertelement <8 x i16> undef, i16 %96, i32 0
  %98 = shufflevector <8 x i16> %97, <8 x i16> undef, <8 x i32> zeroinitializer
  %99 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %47, <8 x i16> %93) #9
  %100 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %47, <8 x i16> %98) #9
  %101 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 54), align 8
  %102 = trunc i32 %101 to i16
  %103 = shl i16 %102, 3
  %104 = insertelement <8 x i16> undef, i16 %103, i32 0
  %105 = shufflevector <8 x i16> %104, <8 x i16> undef, <8 x i32> zeroinitializer
  %106 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 10), align 8
  %107 = trunc i32 %106 to i16
  %108 = shl i16 %107, 3
  %109 = insertelement <8 x i16> undef, i16 %108, i32 0
  %110 = shufflevector <8 x i16> %109, <8 x i16> undef, <8 x i32> zeroinitializer
  %111 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %35, <8 x i16> %105) #9
  %112 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %35, <8 x i16> %110) #9
  %113 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 42), align 8
  %114 = trunc i32 %113 to i16
  %115 = shl i16 %114, 3
  %116 = sub i16 0, %115
  %117 = insertelement <8 x i16> undef, i16 %116, i32 0
  %118 = shufflevector <8 x i16> %117, <8 x i16> undef, <8 x i32> zeroinitializer
  %119 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 22), align 8
  %120 = trunc i32 %119 to i16
  %121 = shl i16 %120, 3
  %122 = insertelement <8 x i16> undef, i16 %121, i32 0
  %123 = shufflevector <8 x i16> %122, <8 x i16> undef, <8 x i32> zeroinitializer
  %124 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %44, <8 x i16> %118) #9
  %125 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %44, <8 x i16> %123) #9
  %126 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 38), align 8
  %127 = trunc i32 %126 to i16
  %128 = shl i16 %127, 3
  %129 = insertelement <8 x i16> undef, i16 %128, i32 0
  %130 = shufflevector <8 x i16> %129, <8 x i16> undef, <8 x i32> zeroinitializer
  %131 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 26), align 8
  %132 = trunc i32 %131 to i16
  %133 = shl i16 %132, 3
  %134 = insertelement <8 x i16> undef, i16 %133, i32 0
  %135 = shufflevector <8 x i16> %134, <8 x i16> undef, <8 x i32> zeroinitializer
  %136 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %38, <8 x i16> %130) #9
  %137 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %38, <8 x i16> %135) #9
  %138 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 58), align 8
  %139 = trunc i32 %138 to i16
  %140 = shl i16 %139, 3
  %141 = sub i16 0, %140
  %142 = insertelement <8 x i16> undef, i16 %141, i32 0
  %143 = shufflevector <8 x i16> %142, <8 x i16> undef, <8 x i32> zeroinitializer
  %144 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 6), align 8
  %145 = trunc i32 %144 to i16
  %146 = shl i16 %145, 3
  %147 = insertelement <8 x i16> undef, i16 %146, i32 0
  %148 = shufflevector <8 x i16> %147, <8 x i16> undef, <8 x i32> zeroinitializer
  %149 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %41, <8 x i16> %143) #9
  %150 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %41, <8 x i16> %148) #9
  %151 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 60), align 16
  %152 = trunc i32 %151 to i16
  %153 = shl i16 %152, 3
  %154 = insertelement <8 x i16> undef, i16 %153, i32 0
  %155 = shufflevector <8 x i16> %154, <8 x i16> undef, <8 x i32> zeroinitializer
  %156 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 4), align 16
  %157 = trunc i32 %156 to i16
  %158 = shl i16 %157, 3
  %159 = insertelement <8 x i16> undef, i16 %158, i32 0
  %160 = shufflevector <8 x i16> %159, <8 x i16> undef, <8 x i32> zeroinitializer
  %161 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %17, <8 x i16> %155) #9
  %162 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %17, <8 x i16> %160) #9
  %163 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 36), align 16
  %164 = trunc i32 %163 to i16
  %165 = shl i16 %164, 3
  %166 = sub i16 0, %165
  %167 = insertelement <8 x i16> undef, i16 %166, i32 0
  %168 = shufflevector <8 x i16> %167, <8 x i16> undef, <8 x i32> zeroinitializer
  %169 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 28), align 16
  %170 = trunc i32 %169 to i16
  %171 = shl i16 %170, 3
  %172 = insertelement <8 x i16> undef, i16 %171, i32 0
  %173 = shufflevector <8 x i16> %172, <8 x i16> undef, <8 x i32> zeroinitializer
  %174 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %26, <8 x i16> %168) #9
  %175 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %26, <8 x i16> %173) #9
  %176 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 44), align 16
  %177 = trunc i32 %176 to i16
  %178 = shl i16 %177, 3
  %179 = insertelement <8 x i16> undef, i16 %178, i32 0
  %180 = shufflevector <8 x i16> %179, <8 x i16> undef, <8 x i32> zeroinitializer
  %181 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 20), align 16
  %182 = trunc i32 %181 to i16
  %183 = shl i16 %182, 3
  %184 = insertelement <8 x i16> undef, i16 %183, i32 0
  %185 = shufflevector <8 x i16> %184, <8 x i16> undef, <8 x i32> zeroinitializer
  %186 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %20, <8 x i16> %180) #9
  %187 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %20, <8 x i16> %185) #9
  %188 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 52), align 16
  %189 = trunc i32 %188 to i16
  %190 = shl i16 %189, 3
  %191 = sub i16 0, %190
  %192 = insertelement <8 x i16> undef, i16 %191, i32 0
  %193 = shufflevector <8 x i16> %192, <8 x i16> undef, <8 x i32> zeroinitializer
  %194 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 12), align 16
  %195 = trunc i32 %194 to i16
  %196 = shl i16 %195, 3
  %197 = insertelement <8 x i16> undef, i16 %196, i32 0
  %198 = shufflevector <8 x i16> %197, <8 x i16> undef, <8 x i32> zeroinitializer
  %199 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %23, <8 x i16> %193) #9
  %200 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %23, <8 x i16> %198) #9
  %201 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %61, <8 x i16> %74) #9
  %202 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %61, <8 x i16> %74) #9
  %203 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %99, <8 x i16> %86) #9
  %204 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %99, <8 x i16> %86) #9
  %205 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %111, <8 x i16> %124) #9
  %206 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %111, <8 x i16> %124) #9
  %207 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %149, <8 x i16> %136) #9
  %208 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %149, <8 x i16> %136) #9
  %209 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %150, <8 x i16> %137) #9
  %210 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %150, <8 x i16> %137) #9
  %211 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %112, <8 x i16> %125) #9
  %212 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %112, <8 x i16> %125) #9
  %213 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %100, <8 x i16> %87) #9
  %214 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %100, <8 x i16> %87) #9
  %215 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %62, <8 x i16> %75) #9
  %216 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %62, <8 x i16> %75) #9
  %217 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 56), align 16
  %218 = trunc i32 %217 to i16
  %219 = shl i16 %218, 3
  %220 = insertelement <8 x i16> undef, i16 %219, i32 0
  %221 = shufflevector <8 x i16> %220, <8 x i16> undef, <8 x i32> zeroinitializer
  %222 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 8), align 16
  %223 = trunc i32 %222 to i16
  %224 = shl i16 %223, 3
  %225 = insertelement <8 x i16> undef, i16 %224, i32 0
  %226 = shufflevector <8 x i16> %225, <8 x i16> undef, <8 x i32> zeroinitializer
  %227 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %11, <8 x i16> %221) #9
  %228 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %11, <8 x i16> %226) #9
  %229 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 40), align 16
  %230 = trunc i32 %229 to i16
  %231 = shl i16 %230, 3
  %232 = sub i16 0, %231
  %233 = insertelement <8 x i16> undef, i16 %232, i32 0
  %234 = shufflevector <8 x i16> %233, <8 x i16> undef, <8 x i32> zeroinitializer
  %235 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 24), align 16
  %236 = trunc i32 %235 to i16
  %237 = shl i16 %236, 3
  %238 = insertelement <8 x i16> undef, i16 %237, i32 0
  %239 = shufflevector <8 x i16> %238, <8 x i16> undef, <8 x i32> zeroinitializer
  %240 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %14, <8 x i16> %234) #9
  %241 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %14, <8 x i16> %239) #9
  %242 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %161, <8 x i16> %174) #9
  %243 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %161, <8 x i16> %174) #9
  %244 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %199, <8 x i16> %186) #9
  %245 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %199, <8 x i16> %186) #9
  %246 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %200, <8 x i16> %187) #9
  %247 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %200, <8 x i16> %187) #9
  %248 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %162, <8 x i16> %175) #9
  %249 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %162, <8 x i16> %175) #9
  %250 = sub i32 0, %222
  %251 = and i32 %250, 65535
  %252 = shl i32 %217, 16
  %253 = or i32 %251, %252
  %254 = insertelement <4 x i32> undef, i32 %253, i32 0
  %255 = shufflevector <4 x i32> %254, <4 x i32> undef, <4 x i32> zeroinitializer
  %256 = and i32 %217, 65535
  %257 = shl i32 %222, 16
  %258 = or i32 %257, %256
  %259 = insertelement <4 x i32> undef, i32 %258, i32 0
  %260 = shufflevector <4 x i32> %259, <4 x i32> undef, <4 x i32> zeroinitializer
  %261 = sub i32 0, %217
  %262 = and i32 %261, 65535
  %263 = sub i32 0, %257
  %264 = or i32 %262, %263
  %265 = insertelement <4 x i32> undef, i32 %264, i32 0
  %266 = shufflevector <4 x i32> %265, <4 x i32> undef, <4 x i32> zeroinitializer
  %267 = sub i32 0, %229
  %268 = and i32 %267, 65535
  %269 = shl i32 %235, 16
  %270 = or i32 %269, %268
  %271 = insertelement <4 x i32> undef, i32 %270, i32 0
  %272 = shufflevector <4 x i32> %271, <4 x i32> undef, <4 x i32> zeroinitializer
  %273 = and i32 %235, 65535
  %274 = shl i32 %229, 16
  %275 = or i32 %273, %274
  %276 = insertelement <4 x i32> undef, i32 %275, i32 0
  %277 = shufflevector <4 x i32> %276, <4 x i32> undef, <4 x i32> zeroinitializer
  %278 = sub i32 0, %235
  %279 = and i32 %278, 65535
  %280 = sub i32 0, %274
  %281 = or i32 %279, %280
  %282 = insertelement <4 x i32> undef, i32 %281, i32 0
  %283 = shufflevector <4 x i32> %282, <4 x i32> undef, <4 x i32> zeroinitializer
  %284 = shufflevector <8 x i16> %202, <8 x i16> %215, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %285 = shufflevector <8 x i16> %202, <8 x i16> %215, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %286 = bitcast <4 x i32> %255 to <8 x i16>
  %287 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %284, <8 x i16> %286) #9
  %288 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %285, <8 x i16> %286) #9
  %289 = bitcast <4 x i32> %260 to <8 x i16>
  %290 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %284, <8 x i16> %289) #9
  %291 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %285, <8 x i16> %289) #9
  %292 = add <4 x i32> %287, <i32 2048, i32 2048, i32 2048, i32 2048>
  %293 = add <4 x i32> %288, <i32 2048, i32 2048, i32 2048, i32 2048>
  %294 = add <4 x i32> %290, <i32 2048, i32 2048, i32 2048, i32 2048>
  %295 = add <4 x i32> %291, <i32 2048, i32 2048, i32 2048, i32 2048>
  %296 = sext i8 %2 to i32
  %297 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %292, i32 %296) #9
  %298 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %293, i32 %296) #9
  %299 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %294, i32 %296) #9
  %300 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %295, i32 %296) #9
  %301 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %297, <4 x i32> %298) #9
  %302 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %299, <4 x i32> %300) #9
  %303 = shufflevector <8 x i16> %203, <8 x i16> %214, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %304 = shufflevector <8 x i16> %203, <8 x i16> %214, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %305 = bitcast <4 x i32> %266 to <8 x i16>
  %306 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %303, <8 x i16> %305) #9
  %307 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %304, <8 x i16> %305) #9
  %308 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %303, <8 x i16> %286) #9
  %309 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %304, <8 x i16> %286) #9
  %310 = add <4 x i32> %306, <i32 2048, i32 2048, i32 2048, i32 2048>
  %311 = add <4 x i32> %307, <i32 2048, i32 2048, i32 2048, i32 2048>
  %312 = add <4 x i32> %308, <i32 2048, i32 2048, i32 2048, i32 2048>
  %313 = add <4 x i32> %309, <i32 2048, i32 2048, i32 2048, i32 2048>
  %314 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %310, i32 %296) #9
  %315 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %311, i32 %296) #9
  %316 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %312, i32 %296) #9
  %317 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %313, i32 %296) #9
  %318 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %314, <4 x i32> %315) #9
  %319 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %316, <4 x i32> %317) #9
  %320 = shufflevector <8 x i16> %206, <8 x i16> %211, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %321 = shufflevector <8 x i16> %206, <8 x i16> %211, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %322 = bitcast <4 x i32> %272 to <8 x i16>
  %323 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %320, <8 x i16> %322) #9
  %324 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %321, <8 x i16> %322) #9
  %325 = bitcast <4 x i32> %277 to <8 x i16>
  %326 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %320, <8 x i16> %325) #9
  %327 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %321, <8 x i16> %325) #9
  %328 = add <4 x i32> %323, <i32 2048, i32 2048, i32 2048, i32 2048>
  %329 = add <4 x i32> %324, <i32 2048, i32 2048, i32 2048, i32 2048>
  %330 = add <4 x i32> %326, <i32 2048, i32 2048, i32 2048, i32 2048>
  %331 = add <4 x i32> %327, <i32 2048, i32 2048, i32 2048, i32 2048>
  %332 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %328, i32 %296) #9
  %333 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %329, i32 %296) #9
  %334 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %330, i32 %296) #9
  %335 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %331, i32 %296) #9
  %336 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %332, <4 x i32> %333) #9
  %337 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %334, <4 x i32> %335) #9
  %338 = shufflevector <8 x i16> %207, <8 x i16> %210, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %339 = shufflevector <8 x i16> %207, <8 x i16> %210, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %340 = bitcast <4 x i32> %283 to <8 x i16>
  %341 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %338, <8 x i16> %340) #9
  %342 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %339, <8 x i16> %340) #9
  %343 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %338, <8 x i16> %322) #9
  %344 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %339, <8 x i16> %322) #9
  %345 = add <4 x i32> %341, <i32 2048, i32 2048, i32 2048, i32 2048>
  %346 = add <4 x i32> %342, <i32 2048, i32 2048, i32 2048, i32 2048>
  %347 = add <4 x i32> %343, <i32 2048, i32 2048, i32 2048, i32 2048>
  %348 = add <4 x i32> %344, <i32 2048, i32 2048, i32 2048, i32 2048>
  %349 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %345, i32 %296) #9
  %350 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %346, i32 %296) #9
  %351 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %347, i32 %296) #9
  %352 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %348, i32 %296) #9
  %353 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %349, <4 x i32> %350) #9
  %354 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %351, <4 x i32> %352) #9
  %355 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 32), align 16
  %356 = trunc i32 %355 to i16
  %357 = shl i16 %356, 3
  %358 = insertelement <8 x i16> undef, i16 %357, i32 0
  %359 = shufflevector <8 x i16> %358, <8 x i16> undef, <8 x i32> zeroinitializer
  %360 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %5, <8 x i16> %359) #9
  %361 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 48), align 16
  %362 = trunc i32 %361 to i16
  %363 = shl i16 %362, 3
  %364 = insertelement <8 x i16> undef, i16 %363, i32 0
  %365 = shufflevector <8 x i16> %364, <8 x i16> undef, <8 x i32> zeroinitializer
  %366 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 16), align 16
  %367 = trunc i32 %366 to i16
  %368 = shl i16 %367, 3
  %369 = insertelement <8 x i16> undef, i16 %368, i32 0
  %370 = shufflevector <8 x i16> %369, <8 x i16> undef, <8 x i32> zeroinitializer
  %371 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %8, <8 x i16> %365) #9
  %372 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %8, <8 x i16> %370) #9
  %373 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %227, <8 x i16> %240) #9
  %374 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %227, <8 x i16> %240) #9
  %375 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %228, <8 x i16> %241) #9
  %376 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %228, <8 x i16> %241) #9
  %377 = sub i32 0, %366
  %378 = and i32 %377, 65535
  %379 = shl i32 %361, 16
  %380 = or i32 %378, %379
  %381 = insertelement <4 x i32> undef, i32 %380, i32 0
  %382 = shufflevector <4 x i32> %381, <4 x i32> undef, <4 x i32> zeroinitializer
  %383 = and i32 %361, 65535
  %384 = shl i32 %366, 16
  %385 = or i32 %384, %383
  %386 = insertelement <4 x i32> undef, i32 %385, i32 0
  %387 = shufflevector <4 x i32> %386, <4 x i32> undef, <4 x i32> zeroinitializer
  %388 = sub i32 0, %361
  %389 = and i32 %388, 65535
  %390 = sub i32 0, %384
  %391 = or i32 %389, %390
  %392 = insertelement <4 x i32> undef, i32 %391, i32 0
  %393 = shufflevector <4 x i32> %392, <4 x i32> undef, <4 x i32> zeroinitializer
  %394 = shufflevector <8 x i16> %243, <8 x i16> %248, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %395 = shufflevector <8 x i16> %243, <8 x i16> %248, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %396 = bitcast <4 x i32> %382 to <8 x i16>
  %397 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %394, <8 x i16> %396) #9
  %398 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %395, <8 x i16> %396) #9
  %399 = bitcast <4 x i32> %387 to <8 x i16>
  %400 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %394, <8 x i16> %399) #9
  %401 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %395, <8 x i16> %399) #9
  %402 = add <4 x i32> %397, <i32 2048, i32 2048, i32 2048, i32 2048>
  %403 = add <4 x i32> %398, <i32 2048, i32 2048, i32 2048, i32 2048>
  %404 = add <4 x i32> %400, <i32 2048, i32 2048, i32 2048, i32 2048>
  %405 = add <4 x i32> %401, <i32 2048, i32 2048, i32 2048, i32 2048>
  %406 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %402, i32 %296) #9
  %407 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %403, i32 %296) #9
  %408 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %404, i32 %296) #9
  %409 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %405, i32 %296) #9
  %410 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %406, <4 x i32> %407) #9
  %411 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %408, <4 x i32> %409) #9
  %412 = shufflevector <8 x i16> %244, <8 x i16> %247, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %413 = shufflevector <8 x i16> %244, <8 x i16> %247, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %414 = bitcast <4 x i32> %393 to <8 x i16>
  %415 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %412, <8 x i16> %414) #9
  %416 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %413, <8 x i16> %414) #9
  %417 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %412, <8 x i16> %396) #9
  %418 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %413, <8 x i16> %396) #9
  %419 = add <4 x i32> %415, <i32 2048, i32 2048, i32 2048, i32 2048>
  %420 = add <4 x i32> %416, <i32 2048, i32 2048, i32 2048, i32 2048>
  %421 = add <4 x i32> %417, <i32 2048, i32 2048, i32 2048, i32 2048>
  %422 = add <4 x i32> %418, <i32 2048, i32 2048, i32 2048, i32 2048>
  %423 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %419, i32 %296) #9
  %424 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %420, i32 %296) #9
  %425 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %421, i32 %296) #9
  %426 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %422, i32 %296) #9
  %427 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %423, <4 x i32> %424) #9
  %428 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %425, <4 x i32> %426) #9
  %429 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %201, <8 x i16> %204) #9
  %430 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %201, <8 x i16> %204) #9
  %431 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %301, <8 x i16> %318) #9
  %432 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %301, <8 x i16> %318) #9
  %433 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %208, <8 x i16> %205) #9
  %434 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %208, <8 x i16> %205) #9
  %435 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %353, <8 x i16> %336) #9
  %436 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %353, <8 x i16> %336) #9
  %437 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %209, <8 x i16> %212) #9
  %438 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %209, <8 x i16> %212) #9
  %439 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %354, <8 x i16> %337) #9
  %440 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %354, <8 x i16> %337) #9
  %441 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %216, <8 x i16> %213) #9
  %442 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %216, <8 x i16> %213) #9
  %443 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %302, <8 x i16> %319) #9
  %444 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %302, <8 x i16> %319) #9
  %445 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %360, <8 x i16> %372) #9
  %446 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %360, <8 x i16> %372) #9
  %447 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %360, <8 x i16> %371) #9
  %448 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %360, <8 x i16> %371) #9
  %449 = sub i32 0, %355
  %450 = and i32 %449, 65535
  %451 = shl i32 %355, 16
  %452 = or i32 %450, %451
  %453 = insertelement <4 x i32> undef, i32 %452, i32 0
  %454 = shufflevector <4 x i32> %453, <4 x i32> undef, <4 x i32> zeroinitializer
  %455 = and i32 %355, 65535
  %456 = or i32 %455, %451
  %457 = insertelement <4 x i32> undef, i32 %456, i32 0
  %458 = shufflevector <4 x i32> %457, <4 x i32> undef, <4 x i32> zeroinitializer
  %459 = shufflevector <8 x i16> %374, <8 x i16> %375, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %460 = shufflevector <8 x i16> %374, <8 x i16> %375, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %461 = bitcast <4 x i32> %454 to <8 x i16>
  %462 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %459, <8 x i16> %461) #9
  %463 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %460, <8 x i16> %461) #9
  %464 = bitcast <4 x i32> %458 to <8 x i16>
  %465 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %459, <8 x i16> %464) #9
  %466 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %460, <8 x i16> %464) #9
  %467 = add <4 x i32> %462, <i32 2048, i32 2048, i32 2048, i32 2048>
  %468 = add <4 x i32> %463, <i32 2048, i32 2048, i32 2048, i32 2048>
  %469 = add <4 x i32> %465, <i32 2048, i32 2048, i32 2048, i32 2048>
  %470 = add <4 x i32> %466, <i32 2048, i32 2048, i32 2048, i32 2048>
  %471 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %467, i32 %296) #9
  %472 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %468, i32 %296) #9
  %473 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %469, i32 %296) #9
  %474 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %470, i32 %296) #9
  %475 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %471, <4 x i32> %472) #9
  %476 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %473, <4 x i32> %474) #9
  %477 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %242, <8 x i16> %245) #9
  %478 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %242, <8 x i16> %245) #9
  %479 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %410, <8 x i16> %427) #9
  %480 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %410, <8 x i16> %427) #9
  %481 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %249, <8 x i16> %246) #9
  %482 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %249, <8 x i16> %246) #9
  %483 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %411, <8 x i16> %428) #9
  %484 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %411, <8 x i16> %428) #9
  %485 = shufflevector <8 x i16> %432, <8 x i16> %443, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %486 = shufflevector <8 x i16> %432, <8 x i16> %443, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %487 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %485, <8 x i16> %396) #9
  %488 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %486, <8 x i16> %396) #9
  %489 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %485, <8 x i16> %399) #9
  %490 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %486, <8 x i16> %399) #9
  %491 = add <4 x i32> %487, <i32 2048, i32 2048, i32 2048, i32 2048>
  %492 = add <4 x i32> %488, <i32 2048, i32 2048, i32 2048, i32 2048>
  %493 = add <4 x i32> %489, <i32 2048, i32 2048, i32 2048, i32 2048>
  %494 = add <4 x i32> %490, <i32 2048, i32 2048, i32 2048, i32 2048>
  %495 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %491, i32 %296) #9
  %496 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %492, i32 %296) #9
  %497 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %493, i32 %296) #9
  %498 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %494, i32 %296) #9
  %499 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %495, <4 x i32> %496) #9
  %500 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %497, <4 x i32> %498) #9
  %501 = shufflevector <8 x i16> %430, <8 x i16> %441, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %502 = shufflevector <8 x i16> %430, <8 x i16> %441, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %503 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %501, <8 x i16> %396) #9
  %504 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %502, <8 x i16> %396) #9
  %505 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %501, <8 x i16> %399) #9
  %506 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %502, <8 x i16> %399) #9
  %507 = add <4 x i32> %503, <i32 2048, i32 2048, i32 2048, i32 2048>
  %508 = add <4 x i32> %504, <i32 2048, i32 2048, i32 2048, i32 2048>
  %509 = add <4 x i32> %505, <i32 2048, i32 2048, i32 2048, i32 2048>
  %510 = add <4 x i32> %506, <i32 2048, i32 2048, i32 2048, i32 2048>
  %511 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %507, i32 %296) #9
  %512 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %508, i32 %296) #9
  %513 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %509, i32 %296) #9
  %514 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %510, i32 %296) #9
  %515 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %511, <4 x i32> %512) #9
  %516 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %513, <4 x i32> %514) #9
  %517 = shufflevector <8 x i16> %433, <8 x i16> %438, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %518 = shufflevector <8 x i16> %433, <8 x i16> %438, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %519 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %517, <8 x i16> %414) #9
  %520 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %518, <8 x i16> %414) #9
  %521 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %517, <8 x i16> %396) #9
  %522 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %518, <8 x i16> %396) #9
  %523 = add <4 x i32> %519, <i32 2048, i32 2048, i32 2048, i32 2048>
  %524 = add <4 x i32> %520, <i32 2048, i32 2048, i32 2048, i32 2048>
  %525 = add <4 x i32> %521, <i32 2048, i32 2048, i32 2048, i32 2048>
  %526 = add <4 x i32> %522, <i32 2048, i32 2048, i32 2048, i32 2048>
  %527 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %523, i32 %296) #9
  %528 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %524, i32 %296) #9
  %529 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %525, i32 %296) #9
  %530 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %526, i32 %296) #9
  %531 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %527, <4 x i32> %528) #9
  %532 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %529, <4 x i32> %530) #9
  %533 = shufflevector <8 x i16> %435, <8 x i16> %440, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %534 = shufflevector <8 x i16> %435, <8 x i16> %440, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %535 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %533, <8 x i16> %414) #9
  %536 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %534, <8 x i16> %414) #9
  %537 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %533, <8 x i16> %396) #9
  %538 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %534, <8 x i16> %396) #9
  %539 = add <4 x i32> %535, <i32 2048, i32 2048, i32 2048, i32 2048>
  %540 = add <4 x i32> %536, <i32 2048, i32 2048, i32 2048, i32 2048>
  %541 = add <4 x i32> %537, <i32 2048, i32 2048, i32 2048, i32 2048>
  %542 = add <4 x i32> %538, <i32 2048, i32 2048, i32 2048, i32 2048>
  %543 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %539, i32 %296) #9
  %544 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %540, i32 %296) #9
  %545 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %541, i32 %296) #9
  %546 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %542, i32 %296) #9
  %547 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %543, <4 x i32> %544) #9
  %548 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %545, <4 x i32> %546) #9
  %549 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %445, <8 x i16> %376) #9
  %550 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %445, <8 x i16> %376) #9
  %551 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %447, <8 x i16> %476) #9
  %552 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %447, <8 x i16> %476) #9
  %553 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %448, <8 x i16> %475) #9
  %554 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %448, <8 x i16> %475) #9
  %555 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %446, <8 x i16> %373) #9
  %556 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %446, <8 x i16> %373) #9
  %557 = shufflevector <8 x i16> %480, <8 x i16> %483, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %558 = shufflevector <8 x i16> %480, <8 x i16> %483, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %559 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %557, <8 x i16> %461) #9
  %560 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %558, <8 x i16> %461) #9
  %561 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %557, <8 x i16> %464) #9
  %562 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %558, <8 x i16> %464) #9
  %563 = add <4 x i32> %559, <i32 2048, i32 2048, i32 2048, i32 2048>
  %564 = add <4 x i32> %560, <i32 2048, i32 2048, i32 2048, i32 2048>
  %565 = add <4 x i32> %561, <i32 2048, i32 2048, i32 2048, i32 2048>
  %566 = add <4 x i32> %562, <i32 2048, i32 2048, i32 2048, i32 2048>
  %567 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %563, i32 %296) #9
  %568 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %564, i32 %296) #9
  %569 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %565, i32 %296) #9
  %570 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %566, i32 %296) #9
  %571 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %567, <4 x i32> %568) #9
  %572 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %569, <4 x i32> %570) #9
  %573 = shufflevector <8 x i16> %478, <8 x i16> %481, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %574 = shufflevector <8 x i16> %478, <8 x i16> %481, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %575 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %573, <8 x i16> %461) #9
  %576 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %574, <8 x i16> %461) #9
  %577 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %573, <8 x i16> %464) #9
  %578 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %574, <8 x i16> %464) #9
  %579 = add <4 x i32> %575, <i32 2048, i32 2048, i32 2048, i32 2048>
  %580 = add <4 x i32> %576, <i32 2048, i32 2048, i32 2048, i32 2048>
  %581 = add <4 x i32> %577, <i32 2048, i32 2048, i32 2048, i32 2048>
  %582 = add <4 x i32> %578, <i32 2048, i32 2048, i32 2048, i32 2048>
  %583 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %579, i32 %296) #9
  %584 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %580, i32 %296) #9
  %585 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %581, i32 %296) #9
  %586 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %582, i32 %296) #9
  %587 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %583, <4 x i32> %584) #9
  %588 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %585, <4 x i32> %586) #9
  %589 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %429, <8 x i16> %434) #9
  %590 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %429, <8 x i16> %434) #9
  %591 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %431, <8 x i16> %436) #9
  %592 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %431, <8 x i16> %436) #9
  %593 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %499, <8 x i16> %547) #9
  %594 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %499, <8 x i16> %547) #9
  %595 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %515, <8 x i16> %531) #9
  %596 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %515, <8 x i16> %531) #9
  %597 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %442, <8 x i16> %437) #9
  %598 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %442, <8 x i16> %437) #9
  %599 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %444, <8 x i16> %439) #9
  %600 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %444, <8 x i16> %439) #9
  %601 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %500, <8 x i16> %548) #9
  %602 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %500, <8 x i16> %548) #9
  %603 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %516, <8 x i16> %532) #9
  %604 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %516, <8 x i16> %532) #9
  %605 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %549, <8 x i16> %482) #9
  %606 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %549, <8 x i16> %482) #9
  %607 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %551, <8 x i16> %484) #9
  %608 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %551, <8 x i16> %484) #9
  %609 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %553, <8 x i16> %572) #9
  %610 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %553, <8 x i16> %572) #9
  %611 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %555, <8 x i16> %588) #9
  %612 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %555, <8 x i16> %588) #9
  %613 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %556, <8 x i16> %587) #9
  %614 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %556, <8 x i16> %587) #9
  %615 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %554, <8 x i16> %571) #9
  %616 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %554, <8 x i16> %571) #9
  %617 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %552, <8 x i16> %479) #9
  %618 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %552, <8 x i16> %479) #9
  %619 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %550, <8 x i16> %477) #9
  %620 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %550, <8 x i16> %477) #9
  %621 = shufflevector <8 x i16> %596, <8 x i16> %603, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %622 = shufflevector <8 x i16> %596, <8 x i16> %603, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %623 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %621, <8 x i16> %461) #9
  %624 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %622, <8 x i16> %461) #9
  %625 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %621, <8 x i16> %464) #9
  %626 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %622, <8 x i16> %464) #9
  %627 = add <4 x i32> %623, <i32 2048, i32 2048, i32 2048, i32 2048>
  %628 = add <4 x i32> %624, <i32 2048, i32 2048, i32 2048, i32 2048>
  %629 = add <4 x i32> %625, <i32 2048, i32 2048, i32 2048, i32 2048>
  %630 = add <4 x i32> %626, <i32 2048, i32 2048, i32 2048, i32 2048>
  %631 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %627, i32 %296) #9
  %632 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %628, i32 %296) #9
  %633 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %629, i32 %296) #9
  %634 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %630, i32 %296) #9
  %635 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %631, <4 x i32> %632) #9
  %636 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %633, <4 x i32> %634) #9
  %637 = shufflevector <8 x i16> %594, <8 x i16> %601, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %638 = shufflevector <8 x i16> %594, <8 x i16> %601, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %639 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %637, <8 x i16> %461) #9
  %640 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %638, <8 x i16> %461) #9
  %641 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %637, <8 x i16> %464) #9
  %642 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %638, <8 x i16> %464) #9
  %643 = add <4 x i32> %639, <i32 2048, i32 2048, i32 2048, i32 2048>
  %644 = add <4 x i32> %640, <i32 2048, i32 2048, i32 2048, i32 2048>
  %645 = add <4 x i32> %641, <i32 2048, i32 2048, i32 2048, i32 2048>
  %646 = add <4 x i32> %642, <i32 2048, i32 2048, i32 2048, i32 2048>
  %647 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %643, i32 %296) #9
  %648 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %644, i32 %296) #9
  %649 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %645, i32 %296) #9
  %650 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %646, i32 %296) #9
  %651 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %647, <4 x i32> %648) #9
  %652 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %649, <4 x i32> %650) #9
  %653 = shufflevector <8 x i16> %592, <8 x i16> %599, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %654 = shufflevector <8 x i16> %592, <8 x i16> %599, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %655 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %653, <8 x i16> %461) #9
  %656 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %654, <8 x i16> %461) #9
  %657 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %653, <8 x i16> %464) #9
  %658 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %654, <8 x i16> %464) #9
  %659 = add <4 x i32> %655, <i32 2048, i32 2048, i32 2048, i32 2048>
  %660 = add <4 x i32> %656, <i32 2048, i32 2048, i32 2048, i32 2048>
  %661 = add <4 x i32> %657, <i32 2048, i32 2048, i32 2048, i32 2048>
  %662 = add <4 x i32> %658, <i32 2048, i32 2048, i32 2048, i32 2048>
  %663 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %659, i32 %296) #9
  %664 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %660, i32 %296) #9
  %665 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %661, i32 %296) #9
  %666 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %662, i32 %296) #9
  %667 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %663, <4 x i32> %664) #9
  %668 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %665, <4 x i32> %666) #9
  %669 = shufflevector <8 x i16> %590, <8 x i16> %597, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %670 = shufflevector <8 x i16> %590, <8 x i16> %597, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %671 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %669, <8 x i16> %461) #9
  %672 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %670, <8 x i16> %461) #9
  %673 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %669, <8 x i16> %464) #9
  %674 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %670, <8 x i16> %464) #9
  %675 = add <4 x i32> %671, <i32 2048, i32 2048, i32 2048, i32 2048>
  %676 = add <4 x i32> %672, <i32 2048, i32 2048, i32 2048, i32 2048>
  %677 = add <4 x i32> %673, <i32 2048, i32 2048, i32 2048, i32 2048>
  %678 = add <4 x i32> %674, <i32 2048, i32 2048, i32 2048, i32 2048>
  %679 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %675, i32 %296) #9
  %680 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %676, i32 %296) #9
  %681 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %677, i32 %296) #9
  %682 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %678, i32 %296) #9
  %683 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %679, <4 x i32> %680) #9
  %684 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %681, <4 x i32> %682) #9
  %685 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %605, <8 x i16> %598) #9
  %686 = bitcast <2 x i64>* %1 to <8 x i16>*
  store <8 x i16> %685, <8 x i16>* %686, align 16
  %687 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %605, <8 x i16> %598) #9
  %688 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 31
  %689 = bitcast <2 x i64>* %688 to <8 x i16>*
  store <8 x i16> %687, <8 x i16>* %689, align 16
  %690 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %607, <8 x i16> %600) #9
  %691 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 1
  %692 = bitcast <2 x i64>* %691 to <8 x i16>*
  store <8 x i16> %690, <8 x i16>* %692, align 16
  %693 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %607, <8 x i16> %600) #9
  %694 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 30
  %695 = bitcast <2 x i64>* %694 to <8 x i16>*
  store <8 x i16> %693, <8 x i16>* %695, align 16
  %696 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %609, <8 x i16> %602) #9
  %697 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 2
  %698 = bitcast <2 x i64>* %697 to <8 x i16>*
  store <8 x i16> %696, <8 x i16>* %698, align 16
  %699 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %609, <8 x i16> %602) #9
  %700 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 29
  %701 = bitcast <2 x i64>* %700 to <8 x i16>*
  store <8 x i16> %699, <8 x i16>* %701, align 16
  %702 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %611, <8 x i16> %604) #9
  %703 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 3
  %704 = bitcast <2 x i64>* %703 to <8 x i16>*
  store <8 x i16> %702, <8 x i16>* %704, align 16
  %705 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %611, <8 x i16> %604) #9
  %706 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 28
  %707 = bitcast <2 x i64>* %706 to <8 x i16>*
  store <8 x i16> %705, <8 x i16>* %707, align 16
  %708 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %613, <8 x i16> %636) #9
  %709 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 4
  %710 = bitcast <2 x i64>* %709 to <8 x i16>*
  store <8 x i16> %708, <8 x i16>* %710, align 16
  %711 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %613, <8 x i16> %636) #9
  %712 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 27
  %713 = bitcast <2 x i64>* %712 to <8 x i16>*
  store <8 x i16> %711, <8 x i16>* %713, align 16
  %714 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %615, <8 x i16> %652) #9
  %715 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 5
  %716 = bitcast <2 x i64>* %715 to <8 x i16>*
  store <8 x i16> %714, <8 x i16>* %716, align 16
  %717 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %615, <8 x i16> %652) #9
  %718 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 26
  %719 = bitcast <2 x i64>* %718 to <8 x i16>*
  store <8 x i16> %717, <8 x i16>* %719, align 16
  %720 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %617, <8 x i16> %668) #9
  %721 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 6
  %722 = bitcast <2 x i64>* %721 to <8 x i16>*
  store <8 x i16> %720, <8 x i16>* %722, align 16
  %723 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %617, <8 x i16> %668) #9
  %724 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 25
  %725 = bitcast <2 x i64>* %724 to <8 x i16>*
  store <8 x i16> %723, <8 x i16>* %725, align 16
  %726 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %619, <8 x i16> %684) #9
  %727 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 7
  %728 = bitcast <2 x i64>* %727 to <8 x i16>*
  store <8 x i16> %726, <8 x i16>* %728, align 16
  %729 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %619, <8 x i16> %684) #9
  %730 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 24
  %731 = bitcast <2 x i64>* %730 to <8 x i16>*
  store <8 x i16> %729, <8 x i16>* %731, align 16
  %732 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %620, <8 x i16> %683) #9
  %733 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 8
  %734 = bitcast <2 x i64>* %733 to <8 x i16>*
  store <8 x i16> %732, <8 x i16>* %734, align 16
  %735 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %620, <8 x i16> %683) #9
  %736 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 23
  %737 = bitcast <2 x i64>* %736 to <8 x i16>*
  store <8 x i16> %735, <8 x i16>* %737, align 16
  %738 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %618, <8 x i16> %667) #9
  %739 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 9
  %740 = bitcast <2 x i64>* %739 to <8 x i16>*
  store <8 x i16> %738, <8 x i16>* %740, align 16
  %741 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %618, <8 x i16> %667) #9
  %742 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 22
  %743 = bitcast <2 x i64>* %742 to <8 x i16>*
  store <8 x i16> %741, <8 x i16>* %743, align 16
  %744 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %616, <8 x i16> %651) #9
  %745 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 10
  %746 = bitcast <2 x i64>* %745 to <8 x i16>*
  store <8 x i16> %744, <8 x i16>* %746, align 16
  %747 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %616, <8 x i16> %651) #9
  %748 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 21
  %749 = bitcast <2 x i64>* %748 to <8 x i16>*
  store <8 x i16> %747, <8 x i16>* %749, align 16
  %750 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %614, <8 x i16> %635) #9
  %751 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 11
  %752 = bitcast <2 x i64>* %751 to <8 x i16>*
  store <8 x i16> %750, <8 x i16>* %752, align 16
  %753 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %614, <8 x i16> %635) #9
  %754 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 20
  %755 = bitcast <2 x i64>* %754 to <8 x i16>*
  store <8 x i16> %753, <8 x i16>* %755, align 16
  %756 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %612, <8 x i16> %595) #9
  %757 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 12
  %758 = bitcast <2 x i64>* %757 to <8 x i16>*
  store <8 x i16> %756, <8 x i16>* %758, align 16
  %759 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %612, <8 x i16> %595) #9
  %760 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 19
  %761 = bitcast <2 x i64>* %760 to <8 x i16>*
  store <8 x i16> %759, <8 x i16>* %761, align 16
  %762 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %610, <8 x i16> %593) #9
  %763 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 13
  %764 = bitcast <2 x i64>* %763 to <8 x i16>*
  store <8 x i16> %762, <8 x i16>* %764, align 16
  %765 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %610, <8 x i16> %593) #9
  %766 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 18
  %767 = bitcast <2 x i64>* %766 to <8 x i16>*
  store <8 x i16> %765, <8 x i16>* %767, align 16
  %768 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %608, <8 x i16> %591) #9
  %769 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 14
  %770 = bitcast <2 x i64>* %769 to <8 x i16>*
  store <8 x i16> %768, <8 x i16>* %770, align 16
  %771 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %608, <8 x i16> %591) #9
  %772 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 17
  %773 = bitcast <2 x i64>* %772 to <8 x i16>*
  store <8 x i16> %771, <8 x i16>* %773, align 16
  %774 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %606, <8 x i16> %589) #9
  %775 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 15
  %776 = bitcast <2 x i64>* %775 to <8 x i16>*
  store <8 x i16> %774, <8 x i16>* %776, align 16
  %777 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %606, <8 x i16> %589) #9
  %778 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 16
  %779 = bitcast <2 x i64>* %778 to <8 x i16>*
  store <8 x i16> %777, <8 x i16>* %779, align 16
  ret void
}

; Function Attrs: nofree nounwind ssp uwtable
define internal void @idct64_low1_ssse3(<2 x i64>* nocapture readonly, <2 x i64>*, i8 signext) #4 {
  %4 = bitcast <2 x i64>* %0 to <8 x i16>*
  %5 = load <8 x i16>, <8 x i16>* %4, align 16
  %6 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 32), align 16
  %7 = trunc i32 %6 to i16
  %8 = shl i16 %7, 3
  %9 = insertelement <8 x i16> undef, i16 %8, i32 0
  %10 = shufflevector <8 x i16> %9, <8 x i16> undef, <8 x i32> zeroinitializer
  %11 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %5, <8 x i16> %10) #9
  %12 = bitcast <2 x i64>* %1 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %12, align 16
  %13 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 63
  %14 = bitcast <2 x i64>* %13 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %14, align 16
  %15 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 1
  %16 = bitcast <2 x i64>* %15 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %16, align 16
  %17 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 62
  %18 = bitcast <2 x i64>* %17 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %18, align 16
  %19 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 2
  %20 = bitcast <2 x i64>* %19 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %20, align 16
  %21 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 61
  %22 = bitcast <2 x i64>* %21 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %22, align 16
  %23 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 3
  %24 = bitcast <2 x i64>* %23 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %24, align 16
  %25 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 60
  %26 = bitcast <2 x i64>* %25 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %26, align 16
  %27 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 4
  %28 = bitcast <2 x i64>* %27 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %28, align 16
  %29 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 59
  %30 = bitcast <2 x i64>* %29 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %30, align 16
  %31 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 5
  %32 = bitcast <2 x i64>* %31 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %32, align 16
  %33 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 58
  %34 = bitcast <2 x i64>* %33 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %34, align 16
  %35 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 6
  %36 = bitcast <2 x i64>* %35 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %36, align 16
  %37 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 57
  %38 = bitcast <2 x i64>* %37 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %38, align 16
  %39 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 7
  %40 = bitcast <2 x i64>* %39 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %40, align 16
  %41 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 56
  %42 = bitcast <2 x i64>* %41 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %42, align 16
  %43 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 8
  %44 = bitcast <2 x i64>* %43 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %44, align 16
  %45 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 55
  %46 = bitcast <2 x i64>* %45 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %46, align 16
  %47 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 9
  %48 = bitcast <2 x i64>* %47 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %48, align 16
  %49 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 54
  %50 = bitcast <2 x i64>* %49 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %50, align 16
  %51 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 10
  %52 = bitcast <2 x i64>* %51 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %52, align 16
  %53 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 53
  %54 = bitcast <2 x i64>* %53 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %54, align 16
  %55 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 11
  %56 = bitcast <2 x i64>* %55 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %56, align 16
  %57 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 52
  %58 = bitcast <2 x i64>* %57 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %58, align 16
  %59 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 12
  %60 = bitcast <2 x i64>* %59 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %60, align 16
  %61 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 51
  %62 = bitcast <2 x i64>* %61 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %62, align 16
  %63 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 13
  %64 = bitcast <2 x i64>* %63 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %64, align 16
  %65 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 50
  %66 = bitcast <2 x i64>* %65 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %66, align 16
  %67 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 14
  %68 = bitcast <2 x i64>* %67 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %68, align 16
  %69 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 49
  %70 = bitcast <2 x i64>* %69 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %70, align 16
  %71 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 15
  %72 = bitcast <2 x i64>* %71 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %72, align 16
  %73 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 48
  %74 = bitcast <2 x i64>* %73 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %74, align 16
  %75 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 16
  %76 = bitcast <2 x i64>* %75 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %76, align 16
  %77 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 47
  %78 = bitcast <2 x i64>* %77 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %78, align 16
  %79 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 17
  %80 = bitcast <2 x i64>* %79 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %80, align 16
  %81 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 46
  %82 = bitcast <2 x i64>* %81 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %82, align 16
  %83 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 18
  %84 = bitcast <2 x i64>* %83 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %84, align 16
  %85 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 45
  %86 = bitcast <2 x i64>* %85 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %86, align 16
  %87 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 19
  %88 = bitcast <2 x i64>* %87 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %88, align 16
  %89 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 44
  %90 = bitcast <2 x i64>* %89 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %90, align 16
  %91 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 20
  %92 = bitcast <2 x i64>* %91 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %92, align 16
  %93 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 43
  %94 = bitcast <2 x i64>* %93 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %94, align 16
  %95 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 21
  %96 = bitcast <2 x i64>* %95 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %96, align 16
  %97 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 42
  %98 = bitcast <2 x i64>* %97 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %98, align 16
  %99 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 22
  %100 = bitcast <2 x i64>* %99 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %100, align 16
  %101 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 41
  %102 = bitcast <2 x i64>* %101 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %102, align 16
  %103 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 23
  %104 = bitcast <2 x i64>* %103 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %104, align 16
  %105 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 40
  %106 = bitcast <2 x i64>* %105 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %106, align 16
  %107 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 24
  %108 = bitcast <2 x i64>* %107 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %108, align 16
  %109 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 39
  %110 = bitcast <2 x i64>* %109 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %110, align 16
  %111 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 25
  %112 = bitcast <2 x i64>* %111 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %112, align 16
  %113 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 38
  %114 = bitcast <2 x i64>* %113 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %114, align 16
  %115 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 26
  %116 = bitcast <2 x i64>* %115 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %116, align 16
  %117 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 37
  %118 = bitcast <2 x i64>* %117 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %118, align 16
  %119 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 27
  %120 = bitcast <2 x i64>* %119 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %120, align 16
  %121 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 36
  %122 = bitcast <2 x i64>* %121 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %122, align 16
  %123 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 28
  %124 = bitcast <2 x i64>* %123 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %124, align 16
  %125 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 35
  %126 = bitcast <2 x i64>* %125 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %126, align 16
  %127 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 29
  %128 = bitcast <2 x i64>* %127 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %128, align 16
  %129 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 34
  %130 = bitcast <2 x i64>* %129 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %130, align 16
  %131 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 30
  %132 = bitcast <2 x i64>* %131 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %132, align 16
  %133 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 33
  %134 = bitcast <2 x i64>* %133 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %134, align 16
  %135 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 31
  %136 = bitcast <2 x i64>* %135 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %136, align 16
  %137 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 32
  %138 = bitcast <2 x i64>* %137 to <8 x i16>*
  store <8 x i16> %11, <8 x i16>* %138, align 16
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal void @idct64_low8_ssse3(<2 x i64>* nocapture readonly, <2 x i64>*, i8 signext) #0 {
  %4 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 4), align 16
  %5 = sub i32 0, %4
  %6 = and i32 %5, 65535
  %7 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 60), align 16
  %8 = shl i32 %7, 16
  %9 = or i32 %8, %6
  %10 = insertelement <4 x i32> undef, i32 %9, i32 0
  %11 = shufflevector <4 x i32> %10, <4 x i32> undef, <4 x i32> zeroinitializer
  %12 = and i32 %7, 65535
  %13 = shl i32 %4, 16
  %14 = or i32 %12, %13
  %15 = insertelement <4 x i32> undef, i32 %14, i32 0
  %16 = shufflevector <4 x i32> %15, <4 x i32> undef, <4 x i32> zeroinitializer
  %17 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 36), align 16
  %18 = sub i32 0, %17
  %19 = and i32 %18, 65535
  %20 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 28), align 16
  %21 = shl i32 %20, 16
  %22 = or i32 %21, %19
  %23 = insertelement <4 x i32> undef, i32 %22, i32 0
  %24 = shufflevector <4 x i32> %23, <4 x i32> undef, <4 x i32> zeroinitializer
  %25 = sub i32 0, %20
  %26 = and i32 %25, 65535
  %27 = shl i32 %17, 16
  %28 = sub i32 0, %27
  %29 = or i32 %26, %28
  %30 = insertelement <4 x i32> undef, i32 %29, i32 0
  %31 = shufflevector <4 x i32> %30, <4 x i32> undef, <4 x i32> zeroinitializer
  %32 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 20), align 16
  %33 = sub i32 0, %32
  %34 = and i32 %33, 65535
  %35 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 44), align 16
  %36 = shl i32 %35, 16
  %37 = or i32 %36, %34
  %38 = insertelement <4 x i32> undef, i32 %37, i32 0
  %39 = shufflevector <4 x i32> %38, <4 x i32> undef, <4 x i32> zeroinitializer
  %40 = and i32 %35, 65535
  %41 = shl i32 %32, 16
  %42 = or i32 %40, %41
  %43 = insertelement <4 x i32> undef, i32 %42, i32 0
  %44 = shufflevector <4 x i32> %43, <4 x i32> undef, <4 x i32> zeroinitializer
  %45 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 52), align 16
  %46 = sub i32 0, %45
  %47 = and i32 %46, 65535
  %48 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 12), align 16
  %49 = shl i32 %48, 16
  %50 = or i32 %49, %47
  %51 = insertelement <4 x i32> undef, i32 %50, i32 0
  %52 = shufflevector <4 x i32> %51, <4 x i32> undef, <4 x i32> zeroinitializer
  %53 = sub i32 0, %48
  %54 = and i32 %53, 65535
  %55 = shl i32 %45, 16
  %56 = sub i32 0, %55
  %57 = or i32 %54, %56
  %58 = insertelement <4 x i32> undef, i32 %57, i32 0
  %59 = shufflevector <4 x i32> %58, <4 x i32> undef, <4 x i32> zeroinitializer
  %60 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 8), align 16
  %61 = sub i32 0, %60
  %62 = and i32 %61, 65535
  %63 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 56), align 16
  %64 = shl i32 %63, 16
  %65 = or i32 %64, %62
  %66 = insertelement <4 x i32> undef, i32 %65, i32 0
  %67 = shufflevector <4 x i32> %66, <4 x i32> undef, <4 x i32> zeroinitializer
  %68 = and i32 %63, 65535
  %69 = shl i32 %60, 16
  %70 = or i32 %68, %69
  %71 = insertelement <4 x i32> undef, i32 %70, i32 0
  %72 = shufflevector <4 x i32> %71, <4 x i32> undef, <4 x i32> zeroinitializer
  %73 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 40), align 16
  %74 = sub i32 0, %73
  %75 = and i32 %74, 65535
  %76 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 24), align 16
  %77 = shl i32 %76, 16
  %78 = or i32 %77, %75
  %79 = insertelement <4 x i32> undef, i32 %78, i32 0
  %80 = shufflevector <4 x i32> %79, <4 x i32> undef, <4 x i32> zeroinitializer
  %81 = sub i32 0, %76
  %82 = and i32 %81, 65535
  %83 = shl i32 %73, 16
  %84 = sub i32 0, %83
  %85 = or i32 %82, %84
  %86 = insertelement <4 x i32> undef, i32 %85, i32 0
  %87 = shufflevector <4 x i32> %86, <4 x i32> undef, <4 x i32> zeroinitializer
  %88 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 32), align 16
  %89 = and i32 %88, 65535
  %90 = shl i32 %88, 16
  %91 = or i32 %89, %90
  %92 = insertelement <4 x i32> undef, i32 %91, i32 0
  %93 = shufflevector <4 x i32> %92, <4 x i32> undef, <4 x i32> zeroinitializer
  %94 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 16), align 16
  %95 = sub i32 0, %94
  %96 = and i32 %95, 65535
  %97 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 48), align 16
  %98 = shl i32 %97, 16
  %99 = or i32 %98, %96
  %100 = insertelement <4 x i32> undef, i32 %99, i32 0
  %101 = shufflevector <4 x i32> %100, <4 x i32> undef, <4 x i32> zeroinitializer
  %102 = and i32 %97, 65535
  %103 = shl i32 %94, 16
  %104 = or i32 %102, %103
  %105 = insertelement <4 x i32> undef, i32 %104, i32 0
  %106 = shufflevector <4 x i32> %105, <4 x i32> undef, <4 x i32> zeroinitializer
  %107 = sub i32 0, %88
  %108 = and i32 %107, 65535
  %109 = or i32 %108, %90
  %110 = insertelement <4 x i32> undef, i32 %109, i32 0
  %111 = shufflevector <4 x i32> %110, <4 x i32> undef, <4 x i32> zeroinitializer
  %112 = bitcast <2 x i64>* %0 to <8 x i16>*
  %113 = load <8 x i16>, <8 x i16>* %112, align 16
  %114 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 4
  %115 = bitcast <2 x i64>* %114 to <8 x i16>*
  %116 = load <8 x i16>, <8 x i16>* %115, align 16
  %117 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 2
  %118 = bitcast <2 x i64>* %117 to <8 x i16>*
  %119 = load <8 x i16>, <8 x i16>* %118, align 16
  %120 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 6
  %121 = bitcast <2 x i64>* %120 to <8 x i16>*
  %122 = load <8 x i16>, <8 x i16>* %121, align 16
  %123 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 1
  %124 = bitcast <2 x i64>* %123 to <8 x i16>*
  %125 = load <8 x i16>, <8 x i16>* %124, align 16
  %126 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 5
  %127 = bitcast <2 x i64>* %126 to <8 x i16>*
  %128 = load <8 x i16>, <8 x i16>* %127, align 16
  %129 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 3
  %130 = bitcast <2 x i64>* %129 to <8 x i16>*
  %131 = load <8 x i16>, <8 x i16>* %130, align 16
  %132 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 7
  %133 = bitcast <2 x i64>* %132 to <8 x i16>*
  %134 = load <8 x i16>, <8 x i16>* %133, align 16
  %135 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 63), align 4
  %136 = trunc i32 %135 to i16
  %137 = shl i16 %136, 3
  %138 = insertelement <8 x i16> undef, i16 %137, i32 0
  %139 = shufflevector <8 x i16> %138, <8 x i16> undef, <8 x i32> zeroinitializer
  %140 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 1), align 4
  %141 = trunc i32 %140 to i16
  %142 = shl i16 %141, 3
  %143 = insertelement <8 x i16> undef, i16 %142, i32 0
  %144 = shufflevector <8 x i16> %143, <8 x i16> undef, <8 x i32> zeroinitializer
  %145 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %125, <8 x i16> %139) #9
  %146 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %125, <8 x i16> %144) #9
  %147 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 57), align 4
  %148 = trunc i32 %147 to i16
  %149 = shl i16 %148, 3
  %150 = sub i16 0, %149
  %151 = insertelement <8 x i16> undef, i16 %150, i32 0
  %152 = shufflevector <8 x i16> %151, <8 x i16> undef, <8 x i32> zeroinitializer
  %153 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 7), align 4
  %154 = trunc i32 %153 to i16
  %155 = shl i16 %154, 3
  %156 = insertelement <8 x i16> undef, i16 %155, i32 0
  %157 = shufflevector <8 x i16> %156, <8 x i16> undef, <8 x i32> zeroinitializer
  %158 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %134, <8 x i16> %152) #9
  %159 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %134, <8 x i16> %157) #9
  %160 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 59), align 4
  %161 = trunc i32 %160 to i16
  %162 = shl i16 %161, 3
  %163 = insertelement <8 x i16> undef, i16 %162, i32 0
  %164 = shufflevector <8 x i16> %163, <8 x i16> undef, <8 x i32> zeroinitializer
  %165 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 5), align 4
  %166 = trunc i32 %165 to i16
  %167 = shl i16 %166, 3
  %168 = insertelement <8 x i16> undef, i16 %167, i32 0
  %169 = shufflevector <8 x i16> %168, <8 x i16> undef, <8 x i32> zeroinitializer
  %170 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %128, <8 x i16> %164) #9
  %171 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %128, <8 x i16> %169) #9
  %172 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 61), align 4
  %173 = trunc i32 %172 to i16
  %174 = shl i16 %173, 3
  %175 = sub i16 0, %174
  %176 = insertelement <8 x i16> undef, i16 %175, i32 0
  %177 = shufflevector <8 x i16> %176, <8 x i16> undef, <8 x i32> zeroinitializer
  %178 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 3), align 4
  %179 = trunc i32 %178 to i16
  %180 = shl i16 %179, 3
  %181 = insertelement <8 x i16> undef, i16 %180, i32 0
  %182 = shufflevector <8 x i16> %181, <8 x i16> undef, <8 x i32> zeroinitializer
  %183 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %131, <8 x i16> %177) #9
  %184 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %131, <8 x i16> %182) #9
  %185 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 62), align 8
  %186 = trunc i32 %185 to i16
  %187 = shl i16 %186, 3
  %188 = insertelement <8 x i16> undef, i16 %187, i32 0
  %189 = shufflevector <8 x i16> %188, <8 x i16> undef, <8 x i32> zeroinitializer
  %190 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 2), align 8
  %191 = trunc i32 %190 to i16
  %192 = shl i16 %191, 3
  %193 = insertelement <8 x i16> undef, i16 %192, i32 0
  %194 = shufflevector <8 x i16> %193, <8 x i16> undef, <8 x i32> zeroinitializer
  %195 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %119, <8 x i16> %189) #9
  %196 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %119, <8 x i16> %194) #9
  %197 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 58), align 8
  %198 = trunc i32 %197 to i16
  %199 = shl i16 %198, 3
  %200 = sub i16 0, %199
  %201 = insertelement <8 x i16> undef, i16 %200, i32 0
  %202 = shufflevector <8 x i16> %201, <8 x i16> undef, <8 x i32> zeroinitializer
  %203 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 6), align 8
  %204 = trunc i32 %203 to i16
  %205 = shl i16 %204, 3
  %206 = insertelement <8 x i16> undef, i16 %205, i32 0
  %207 = shufflevector <8 x i16> %206, <8 x i16> undef, <8 x i32> zeroinitializer
  %208 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %122, <8 x i16> %202) #9
  %209 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %122, <8 x i16> %207) #9
  %210 = trunc i32 %7 to i16
  %211 = shl i16 %210, 3
  %212 = insertelement <8 x i16> undef, i16 %211, i32 0
  %213 = shufflevector <8 x i16> %212, <8 x i16> undef, <8 x i32> zeroinitializer
  %214 = trunc i32 %4 to i16
  %215 = shl i16 %214, 3
  %216 = insertelement <8 x i16> undef, i16 %215, i32 0
  %217 = shufflevector <8 x i16> %216, <8 x i16> undef, <8 x i32> zeroinitializer
  %218 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %116, <8 x i16> %213) #9
  %219 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %116, <8 x i16> %217) #9
  %220 = shufflevector <8 x i16> %145, <8 x i16> %146, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %221 = shufflevector <8 x i16> %145, <8 x i16> %146, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %222 = bitcast <4 x i32> %11 to <8 x i16>
  %223 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %220, <8 x i16> %222) #9
  %224 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %221, <8 x i16> %222) #9
  %225 = bitcast <4 x i32> %16 to <8 x i16>
  %226 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %220, <8 x i16> %225) #9
  %227 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %221, <8 x i16> %225) #9
  %228 = add <4 x i32> %223, <i32 2048, i32 2048, i32 2048, i32 2048>
  %229 = add <4 x i32> %224, <i32 2048, i32 2048, i32 2048, i32 2048>
  %230 = add <4 x i32> %226, <i32 2048, i32 2048, i32 2048, i32 2048>
  %231 = add <4 x i32> %227, <i32 2048, i32 2048, i32 2048, i32 2048>
  %232 = sext i8 %2 to i32
  %233 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %228, i32 %232) #9
  %234 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %229, i32 %232) #9
  %235 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %230, i32 %232) #9
  %236 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %231, i32 %232) #9
  %237 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %233, <4 x i32> %234) #9
  %238 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %235, <4 x i32> %236) #9
  %239 = shufflevector <8 x i16> %158, <8 x i16> %159, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %240 = shufflevector <8 x i16> %158, <8 x i16> %159, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %241 = bitcast <4 x i32> %31 to <8 x i16>
  %242 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %239, <8 x i16> %241) #9
  %243 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %240, <8 x i16> %241) #9
  %244 = bitcast <4 x i32> %24 to <8 x i16>
  %245 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %239, <8 x i16> %244) #9
  %246 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %240, <8 x i16> %244) #9
  %247 = add <4 x i32> %242, <i32 2048, i32 2048, i32 2048, i32 2048>
  %248 = add <4 x i32> %243, <i32 2048, i32 2048, i32 2048, i32 2048>
  %249 = add <4 x i32> %245, <i32 2048, i32 2048, i32 2048, i32 2048>
  %250 = add <4 x i32> %246, <i32 2048, i32 2048, i32 2048, i32 2048>
  %251 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %247, i32 %232) #9
  %252 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %248, i32 %232) #9
  %253 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %249, i32 %232) #9
  %254 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %250, i32 %232) #9
  %255 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %251, <4 x i32> %252) #9
  %256 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %253, <4 x i32> %254) #9
  %257 = shufflevector <8 x i16> %170, <8 x i16> %171, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %258 = shufflevector <8 x i16> %170, <8 x i16> %171, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %259 = bitcast <4 x i32> %39 to <8 x i16>
  %260 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %257, <8 x i16> %259) #9
  %261 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %258, <8 x i16> %259) #9
  %262 = bitcast <4 x i32> %44 to <8 x i16>
  %263 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %257, <8 x i16> %262) #9
  %264 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %258, <8 x i16> %262) #9
  %265 = add <4 x i32> %260, <i32 2048, i32 2048, i32 2048, i32 2048>
  %266 = add <4 x i32> %261, <i32 2048, i32 2048, i32 2048, i32 2048>
  %267 = add <4 x i32> %263, <i32 2048, i32 2048, i32 2048, i32 2048>
  %268 = add <4 x i32> %264, <i32 2048, i32 2048, i32 2048, i32 2048>
  %269 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %265, i32 %232) #9
  %270 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %266, i32 %232) #9
  %271 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %267, i32 %232) #9
  %272 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %268, i32 %232) #9
  %273 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %269, <4 x i32> %270) #9
  %274 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %271, <4 x i32> %272) #9
  %275 = shufflevector <8 x i16> %183, <8 x i16> %184, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %276 = shufflevector <8 x i16> %183, <8 x i16> %184, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %277 = bitcast <4 x i32> %59 to <8 x i16>
  %278 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %275, <8 x i16> %277) #9
  %279 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %276, <8 x i16> %277) #9
  %280 = bitcast <4 x i32> %52 to <8 x i16>
  %281 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %275, <8 x i16> %280) #9
  %282 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %276, <8 x i16> %280) #9
  %283 = add <4 x i32> %278, <i32 2048, i32 2048, i32 2048, i32 2048>
  %284 = add <4 x i32> %279, <i32 2048, i32 2048, i32 2048, i32 2048>
  %285 = add <4 x i32> %281, <i32 2048, i32 2048, i32 2048, i32 2048>
  %286 = add <4 x i32> %282, <i32 2048, i32 2048, i32 2048, i32 2048>
  %287 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %283, i32 %232) #9
  %288 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %284, i32 %232) #9
  %289 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %285, i32 %232) #9
  %290 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %286, i32 %232) #9
  %291 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %287, <4 x i32> %288) #9
  %292 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %289, <4 x i32> %290) #9
  %293 = shufflevector <8 x i16> %195, <8 x i16> %196, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %294 = shufflevector <8 x i16> %195, <8 x i16> %196, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %295 = bitcast <4 x i32> %67 to <8 x i16>
  %296 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %293, <8 x i16> %295) #9
  %297 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %294, <8 x i16> %295) #9
  %298 = bitcast <4 x i32> %72 to <8 x i16>
  %299 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %293, <8 x i16> %298) #9
  %300 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %294, <8 x i16> %298) #9
  %301 = add <4 x i32> %296, <i32 2048, i32 2048, i32 2048, i32 2048>
  %302 = add <4 x i32> %297, <i32 2048, i32 2048, i32 2048, i32 2048>
  %303 = add <4 x i32> %299, <i32 2048, i32 2048, i32 2048, i32 2048>
  %304 = add <4 x i32> %300, <i32 2048, i32 2048, i32 2048, i32 2048>
  %305 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %301, i32 %232) #9
  %306 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %302, i32 %232) #9
  %307 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %303, i32 %232) #9
  %308 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %304, i32 %232) #9
  %309 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %305, <4 x i32> %306) #9
  %310 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %307, <4 x i32> %308) #9
  %311 = shufflevector <8 x i16> %208, <8 x i16> %209, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %312 = shufflevector <8 x i16> %208, <8 x i16> %209, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %313 = bitcast <4 x i32> %87 to <8 x i16>
  %314 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %311, <8 x i16> %313) #9
  %315 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %312, <8 x i16> %313) #9
  %316 = bitcast <4 x i32> %80 to <8 x i16>
  %317 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %311, <8 x i16> %316) #9
  %318 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %312, <8 x i16> %316) #9
  %319 = add <4 x i32> %314, <i32 2048, i32 2048, i32 2048, i32 2048>
  %320 = add <4 x i32> %315, <i32 2048, i32 2048, i32 2048, i32 2048>
  %321 = add <4 x i32> %317, <i32 2048, i32 2048, i32 2048, i32 2048>
  %322 = add <4 x i32> %318, <i32 2048, i32 2048, i32 2048, i32 2048>
  %323 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %319, i32 %232) #9
  %324 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %320, i32 %232) #9
  %325 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %321, i32 %232) #9
  %326 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %322, i32 %232) #9
  %327 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %323, <4 x i32> %324) #9
  %328 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %325, <4 x i32> %326) #9
  %329 = trunc i32 %88 to i16
  %330 = shl i16 %329, 3
  %331 = insertelement <8 x i16> undef, i16 %330, i32 0
  %332 = shufflevector <8 x i16> %331, <8 x i16> undef, <8 x i32> zeroinitializer
  %333 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %113, <8 x i16> %332) #9
  %334 = shufflevector <8 x i16> %218, <8 x i16> %219, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %335 = shufflevector <8 x i16> %218, <8 x i16> %219, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %336 = bitcast <4 x i32> %101 to <8 x i16>
  %337 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %334, <8 x i16> %336) #9
  %338 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %335, <8 x i16> %336) #9
  %339 = bitcast <4 x i32> %106 to <8 x i16>
  %340 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %334, <8 x i16> %339) #9
  %341 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %335, <8 x i16> %339) #9
  %342 = add <4 x i32> %337, <i32 2048, i32 2048, i32 2048, i32 2048>
  %343 = add <4 x i32> %338, <i32 2048, i32 2048, i32 2048, i32 2048>
  %344 = add <4 x i32> %340, <i32 2048, i32 2048, i32 2048, i32 2048>
  %345 = add <4 x i32> %341, <i32 2048, i32 2048, i32 2048, i32 2048>
  %346 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %342, i32 %232) #9
  %347 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %343, i32 %232) #9
  %348 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %344, i32 %232) #9
  %349 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %345, i32 %232) #9
  %350 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %346, <4 x i32> %347) #9
  %351 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %348, <4 x i32> %349) #9
  %352 = sub i32 0, %63
  %353 = and i32 %352, 65535
  %354 = sub i32 0, %69
  %355 = or i32 %353, %354
  %356 = insertelement <4 x i32> undef, i32 %355, i32 0
  %357 = shufflevector <4 x i32> %356, <4 x i32> undef, <4 x i32> zeroinitializer
  %358 = and i32 %76, 65535
  %359 = or i32 %358, %83
  %360 = insertelement <4 x i32> undef, i32 %359, i32 0
  %361 = shufflevector <4 x i32> %360, <4 x i32> undef, <4 x i32> zeroinitializer
  %362 = shufflevector <8 x i16> %237, <8 x i16> %238, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %363 = shufflevector <8 x i16> %237, <8 x i16> %238, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %364 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %362, <8 x i16> %295) #9
  %365 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %363, <8 x i16> %295) #9
  %366 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %362, <8 x i16> %298) #9
  %367 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %363, <8 x i16> %298) #9
  %368 = add <4 x i32> %364, <i32 2048, i32 2048, i32 2048, i32 2048>
  %369 = add <4 x i32> %365, <i32 2048, i32 2048, i32 2048, i32 2048>
  %370 = add <4 x i32> %366, <i32 2048, i32 2048, i32 2048, i32 2048>
  %371 = add <4 x i32> %367, <i32 2048, i32 2048, i32 2048, i32 2048>
  %372 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %368, i32 %232) #9
  %373 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %369, i32 %232) #9
  %374 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %370, i32 %232) #9
  %375 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %371, i32 %232) #9
  %376 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %372, <4 x i32> %373) #9
  %377 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %374, <4 x i32> %375) #9
  %378 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %220, <8 x i16> %295) #9
  %379 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %221, <8 x i16> %295) #9
  %380 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %220, <8 x i16> %298) #9
  %381 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %221, <8 x i16> %298) #9
  %382 = add <4 x i32> %378, <i32 2048, i32 2048, i32 2048, i32 2048>
  %383 = add <4 x i32> %379, <i32 2048, i32 2048, i32 2048, i32 2048>
  %384 = add <4 x i32> %380, <i32 2048, i32 2048, i32 2048, i32 2048>
  %385 = add <4 x i32> %381, <i32 2048, i32 2048, i32 2048, i32 2048>
  %386 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %382, i32 %232) #9
  %387 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %383, i32 %232) #9
  %388 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %384, i32 %232) #9
  %389 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %385, i32 %232) #9
  %390 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %386, <4 x i32> %387) #9
  %391 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %388, <4 x i32> %389) #9
  %392 = bitcast <4 x i32> %357 to <8 x i16>
  %393 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %239, <8 x i16> %392) #9
  %394 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %240, <8 x i16> %392) #9
  %395 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %239, <8 x i16> %295) #9
  %396 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %240, <8 x i16> %295) #9
  %397 = add <4 x i32> %393, <i32 2048, i32 2048, i32 2048, i32 2048>
  %398 = add <4 x i32> %394, <i32 2048, i32 2048, i32 2048, i32 2048>
  %399 = add <4 x i32> %395, <i32 2048, i32 2048, i32 2048, i32 2048>
  %400 = add <4 x i32> %396, <i32 2048, i32 2048, i32 2048, i32 2048>
  %401 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %397, i32 %232) #9
  %402 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %398, i32 %232) #9
  %403 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %399, i32 %232) #9
  %404 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %400, i32 %232) #9
  %405 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %401, <4 x i32> %402) #9
  %406 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %403, <4 x i32> %404) #9
  %407 = shufflevector <8 x i16> %255, <8 x i16> %256, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %408 = shufflevector <8 x i16> %255, <8 x i16> %256, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %409 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %407, <8 x i16> %392) #9
  %410 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %408, <8 x i16> %392) #9
  %411 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %407, <8 x i16> %295) #9
  %412 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %408, <8 x i16> %295) #9
  %413 = add <4 x i32> %409, <i32 2048, i32 2048, i32 2048, i32 2048>
  %414 = add <4 x i32> %410, <i32 2048, i32 2048, i32 2048, i32 2048>
  %415 = add <4 x i32> %411, <i32 2048, i32 2048, i32 2048, i32 2048>
  %416 = add <4 x i32> %412, <i32 2048, i32 2048, i32 2048, i32 2048>
  %417 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %413, i32 %232) #9
  %418 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %414, i32 %232) #9
  %419 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %415, i32 %232) #9
  %420 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %416, i32 %232) #9
  %421 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %417, <4 x i32> %418) #9
  %422 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %419, <4 x i32> %420) #9
  %423 = shufflevector <8 x i16> %273, <8 x i16> %274, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %424 = shufflevector <8 x i16> %273, <8 x i16> %274, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %425 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %423, <8 x i16> %316) #9
  %426 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %424, <8 x i16> %316) #9
  %427 = bitcast <4 x i32> %361 to <8 x i16>
  %428 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %423, <8 x i16> %427) #9
  %429 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %424, <8 x i16> %427) #9
  %430 = add <4 x i32> %425, <i32 2048, i32 2048, i32 2048, i32 2048>
  %431 = add <4 x i32> %426, <i32 2048, i32 2048, i32 2048, i32 2048>
  %432 = add <4 x i32> %428, <i32 2048, i32 2048, i32 2048, i32 2048>
  %433 = add <4 x i32> %429, <i32 2048, i32 2048, i32 2048, i32 2048>
  %434 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %430, i32 %232) #9
  %435 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %431, i32 %232) #9
  %436 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %432, i32 %232) #9
  %437 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %433, i32 %232) #9
  %438 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %434, <4 x i32> %435) #9
  %439 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %436, <4 x i32> %437) #9
  %440 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %257, <8 x i16> %316) #9
  %441 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %258, <8 x i16> %316) #9
  %442 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %257, <8 x i16> %427) #9
  %443 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %258, <8 x i16> %427) #9
  %444 = add <4 x i32> %440, <i32 2048, i32 2048, i32 2048, i32 2048>
  %445 = add <4 x i32> %441, <i32 2048, i32 2048, i32 2048, i32 2048>
  %446 = add <4 x i32> %442, <i32 2048, i32 2048, i32 2048, i32 2048>
  %447 = add <4 x i32> %443, <i32 2048, i32 2048, i32 2048, i32 2048>
  %448 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %444, i32 %232) #9
  %449 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %445, i32 %232) #9
  %450 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %446, i32 %232) #9
  %451 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %447, i32 %232) #9
  %452 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %448, <4 x i32> %449) #9
  %453 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %450, <4 x i32> %451) #9
  %454 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %275, <8 x i16> %313) #9
  %455 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %276, <8 x i16> %313) #9
  %456 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %275, <8 x i16> %316) #9
  %457 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %276, <8 x i16> %316) #9
  %458 = add <4 x i32> %454, <i32 2048, i32 2048, i32 2048, i32 2048>
  %459 = add <4 x i32> %455, <i32 2048, i32 2048, i32 2048, i32 2048>
  %460 = add <4 x i32> %456, <i32 2048, i32 2048, i32 2048, i32 2048>
  %461 = add <4 x i32> %457, <i32 2048, i32 2048, i32 2048, i32 2048>
  %462 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %458, i32 %232) #9
  %463 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %459, i32 %232) #9
  %464 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %460, i32 %232) #9
  %465 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %461, i32 %232) #9
  %466 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %462, <4 x i32> %463) #9
  %467 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %464, <4 x i32> %465) #9
  %468 = shufflevector <8 x i16> %291, <8 x i16> %292, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %469 = shufflevector <8 x i16> %291, <8 x i16> %292, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %470 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %468, <8 x i16> %313) #9
  %471 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %469, <8 x i16> %313) #9
  %472 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %468, <8 x i16> %316) #9
  %473 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %469, <8 x i16> %316) #9
  %474 = add <4 x i32> %470, <i32 2048, i32 2048, i32 2048, i32 2048>
  %475 = add <4 x i32> %471, <i32 2048, i32 2048, i32 2048, i32 2048>
  %476 = add <4 x i32> %472, <i32 2048, i32 2048, i32 2048, i32 2048>
  %477 = add <4 x i32> %473, <i32 2048, i32 2048, i32 2048, i32 2048>
  %478 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %474, i32 %232) #9
  %479 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %475, i32 %232) #9
  %480 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %476, i32 %232) #9
  %481 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %477, i32 %232) #9
  %482 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %478, <4 x i32> %479) #9
  %483 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %480, <4 x i32> %481) #9
  %484 = sub i32 0, %97
  %485 = and i32 %484, 65535
  %486 = sub i32 0, %103
  %487 = or i32 %485, %486
  %488 = insertelement <4 x i32> undef, i32 %487, i32 0
  %489 = shufflevector <4 x i32> %488, <4 x i32> undef, <4 x i32> zeroinitializer
  %490 = shufflevector <8 x i16> %309, <8 x i16> %310, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %491 = shufflevector <8 x i16> %309, <8 x i16> %310, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %492 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %490, <8 x i16> %336) #9
  %493 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %491, <8 x i16> %336) #9
  %494 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %490, <8 x i16> %339) #9
  %495 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %491, <8 x i16> %339) #9
  %496 = add <4 x i32> %492, <i32 2048, i32 2048, i32 2048, i32 2048>
  %497 = add <4 x i32> %493, <i32 2048, i32 2048, i32 2048, i32 2048>
  %498 = add <4 x i32> %494, <i32 2048, i32 2048, i32 2048, i32 2048>
  %499 = add <4 x i32> %495, <i32 2048, i32 2048, i32 2048, i32 2048>
  %500 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %496, i32 %232) #9
  %501 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %497, i32 %232) #9
  %502 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %498, i32 %232) #9
  %503 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %499, i32 %232) #9
  %504 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %500, <4 x i32> %501) #9
  %505 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %502, <4 x i32> %503) #9
  %506 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %293, <8 x i16> %336) #9
  %507 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %294, <8 x i16> %336) #9
  %508 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %293, <8 x i16> %339) #9
  %509 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %294, <8 x i16> %339) #9
  %510 = add <4 x i32> %506, <i32 2048, i32 2048, i32 2048, i32 2048>
  %511 = add <4 x i32> %507, <i32 2048, i32 2048, i32 2048, i32 2048>
  %512 = add <4 x i32> %508, <i32 2048, i32 2048, i32 2048, i32 2048>
  %513 = add <4 x i32> %509, <i32 2048, i32 2048, i32 2048, i32 2048>
  %514 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %510, i32 %232) #9
  %515 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %511, i32 %232) #9
  %516 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %512, i32 %232) #9
  %517 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %513, i32 %232) #9
  %518 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %514, <4 x i32> %515) #9
  %519 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %516, <4 x i32> %517) #9
  %520 = bitcast <4 x i32> %489 to <8 x i16>
  %521 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %311, <8 x i16> %520) #9
  %522 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %312, <8 x i16> %520) #9
  %523 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %311, <8 x i16> %336) #9
  %524 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %312, <8 x i16> %336) #9
  %525 = add <4 x i32> %521, <i32 2048, i32 2048, i32 2048, i32 2048>
  %526 = add <4 x i32> %522, <i32 2048, i32 2048, i32 2048, i32 2048>
  %527 = add <4 x i32> %523, <i32 2048, i32 2048, i32 2048, i32 2048>
  %528 = add <4 x i32> %524, <i32 2048, i32 2048, i32 2048, i32 2048>
  %529 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %525, i32 %232) #9
  %530 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %526, i32 %232) #9
  %531 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %527, i32 %232) #9
  %532 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %528, i32 %232) #9
  %533 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %529, <4 x i32> %530) #9
  %534 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %531, <4 x i32> %532) #9
  %535 = shufflevector <8 x i16> %327, <8 x i16> %328, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %536 = shufflevector <8 x i16> %327, <8 x i16> %328, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %537 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %535, <8 x i16> %520) #9
  %538 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %536, <8 x i16> %520) #9
  %539 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %535, <8 x i16> %336) #9
  %540 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %536, <8 x i16> %336) #9
  %541 = add <4 x i32> %537, <i32 2048, i32 2048, i32 2048, i32 2048>
  %542 = add <4 x i32> %538, <i32 2048, i32 2048, i32 2048, i32 2048>
  %543 = add <4 x i32> %539, <i32 2048, i32 2048, i32 2048, i32 2048>
  %544 = add <4 x i32> %540, <i32 2048, i32 2048, i32 2048, i32 2048>
  %545 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %541, i32 %232) #9
  %546 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %542, i32 %232) #9
  %547 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %543, i32 %232) #9
  %548 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %544, i32 %232) #9
  %549 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %545, <4 x i32> %546) #9
  %550 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %547, <4 x i32> %548) #9
  %551 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %145, <8 x i16> %158) #9
  %552 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %145, <8 x i16> %158) #9
  %553 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %237, <8 x i16> %255) #9
  %554 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %237, <8 x i16> %255) #9
  %555 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %376, <8 x i16> %421) #9
  %556 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %376, <8 x i16> %421) #9
  %557 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %390, <8 x i16> %405) #9
  %558 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %390, <8 x i16> %405) #9
  %559 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %183, <8 x i16> %170) #9
  %560 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %183, <8 x i16> %170) #9
  %561 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %291, <8 x i16> %273) #9
  %562 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %291, <8 x i16> %273) #9
  %563 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %482, <8 x i16> %438) #9
  %564 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %482, <8 x i16> %438) #9
  %565 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %466, <8 x i16> %452) #9
  %566 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %466, <8 x i16> %452) #9
  %567 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %184, <8 x i16> %171) #9
  %568 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %184, <8 x i16> %171) #9
  %569 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %292, <8 x i16> %274) #9
  %570 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %292, <8 x i16> %274) #9
  %571 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %483, <8 x i16> %439) #9
  %572 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %483, <8 x i16> %439) #9
  %573 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %467, <8 x i16> %453) #9
  %574 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %467, <8 x i16> %453) #9
  %575 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %146, <8 x i16> %159) #9
  %576 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %146, <8 x i16> %159) #9
  %577 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %238, <8 x i16> %256) #9
  %578 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %238, <8 x i16> %256) #9
  %579 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %377, <8 x i16> %422) #9
  %580 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %377, <8 x i16> %422) #9
  %581 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %391, <8 x i16> %406) #9
  %582 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %391, <8 x i16> %406) #9
  %583 = shufflevector <8 x i16> %350, <8 x i16> %351, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %584 = shufflevector <8 x i16> %350, <8 x i16> %351, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %585 = bitcast <4 x i32> %111 to <8 x i16>
  %586 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %583, <8 x i16> %585) #9
  %587 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %584, <8 x i16> %585) #9
  %588 = bitcast <4 x i32> %93 to <8 x i16>
  %589 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %583, <8 x i16> %588) #9
  %590 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %584, <8 x i16> %588) #9
  %591 = add <4 x i32> %586, <i32 2048, i32 2048, i32 2048, i32 2048>
  %592 = add <4 x i32> %587, <i32 2048, i32 2048, i32 2048, i32 2048>
  %593 = add <4 x i32> %589, <i32 2048, i32 2048, i32 2048, i32 2048>
  %594 = add <4 x i32> %590, <i32 2048, i32 2048, i32 2048, i32 2048>
  %595 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %591, i32 %232) #9
  %596 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %592, i32 %232) #9
  %597 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %593, i32 %232) #9
  %598 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %594, i32 %232) #9
  %599 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %595, <4 x i32> %596) #9
  %600 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %597, <4 x i32> %598) #9
  %601 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %334, <8 x i16> %585) #9
  %602 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %335, <8 x i16> %585) #9
  %603 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %334, <8 x i16> %588) #9
  %604 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %335, <8 x i16> %588) #9
  %605 = add <4 x i32> %601, <i32 2048, i32 2048, i32 2048, i32 2048>
  %606 = add <4 x i32> %602, <i32 2048, i32 2048, i32 2048, i32 2048>
  %607 = add <4 x i32> %603, <i32 2048, i32 2048, i32 2048, i32 2048>
  %608 = add <4 x i32> %604, <i32 2048, i32 2048, i32 2048, i32 2048>
  %609 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %605, i32 %232) #9
  %610 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %606, i32 %232) #9
  %611 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %607, i32 %232) #9
  %612 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %608, i32 %232) #9
  %613 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %609, <4 x i32> %610) #9
  %614 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %611, <4 x i32> %612) #9
  %615 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %195, <8 x i16> %208) #9
  %616 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %195, <8 x i16> %208) #9
  %617 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %309, <8 x i16> %327) #9
  %618 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %309, <8 x i16> %327) #9
  %619 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %504, <8 x i16> %549) #9
  %620 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %504, <8 x i16> %549) #9
  %621 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %518, <8 x i16> %533) #9
  %622 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %518, <8 x i16> %533) #9
  %623 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %196, <8 x i16> %209) #9
  %624 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %196, <8 x i16> %209) #9
  %625 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %310, <8 x i16> %328) #9
  %626 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %310, <8 x i16> %328) #9
  %627 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %505, <8 x i16> %550) #9
  %628 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %505, <8 x i16> %550) #9
  %629 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %519, <8 x i16> %534) #9
  %630 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %519, <8 x i16> %534) #9
  %631 = shufflevector <8 x i16> %558, <8 x i16> %581, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %632 = shufflevector <8 x i16> %558, <8 x i16> %581, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %633 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %631, <8 x i16> %336) #9
  %634 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %632, <8 x i16> %336) #9
  %635 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %631, <8 x i16> %339) #9
  %636 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %632, <8 x i16> %339) #9
  %637 = add <4 x i32> %633, <i32 2048, i32 2048, i32 2048, i32 2048>
  %638 = add <4 x i32> %634, <i32 2048, i32 2048, i32 2048, i32 2048>
  %639 = add <4 x i32> %635, <i32 2048, i32 2048, i32 2048, i32 2048>
  %640 = add <4 x i32> %636, <i32 2048, i32 2048, i32 2048, i32 2048>
  %641 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %637, i32 %232) #9
  %642 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %638, i32 %232) #9
  %643 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %639, i32 %232) #9
  %644 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %640, i32 %232) #9
  %645 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %641, <4 x i32> %642) #9
  %646 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %643, <4 x i32> %644) #9
  %647 = shufflevector <8 x i16> %556, <8 x i16> %579, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %648 = shufflevector <8 x i16> %556, <8 x i16> %579, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %649 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %647, <8 x i16> %336) #9
  %650 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %648, <8 x i16> %336) #9
  %651 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %647, <8 x i16> %339) #9
  %652 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %648, <8 x i16> %339) #9
  %653 = add <4 x i32> %649, <i32 2048, i32 2048, i32 2048, i32 2048>
  %654 = add <4 x i32> %650, <i32 2048, i32 2048, i32 2048, i32 2048>
  %655 = add <4 x i32> %651, <i32 2048, i32 2048, i32 2048, i32 2048>
  %656 = add <4 x i32> %652, <i32 2048, i32 2048, i32 2048, i32 2048>
  %657 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %653, i32 %232) #9
  %658 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %654, i32 %232) #9
  %659 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %655, i32 %232) #9
  %660 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %656, i32 %232) #9
  %661 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %657, <4 x i32> %658) #9
  %662 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %659, <4 x i32> %660) #9
  %663 = shufflevector <8 x i16> %554, <8 x i16> %577, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %664 = shufflevector <8 x i16> %554, <8 x i16> %577, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %665 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %663, <8 x i16> %336) #9
  %666 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %664, <8 x i16> %336) #9
  %667 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %663, <8 x i16> %339) #9
  %668 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %664, <8 x i16> %339) #9
  %669 = add <4 x i32> %665, <i32 2048, i32 2048, i32 2048, i32 2048>
  %670 = add <4 x i32> %666, <i32 2048, i32 2048, i32 2048, i32 2048>
  %671 = add <4 x i32> %667, <i32 2048, i32 2048, i32 2048, i32 2048>
  %672 = add <4 x i32> %668, <i32 2048, i32 2048, i32 2048, i32 2048>
  %673 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %669, i32 %232) #9
  %674 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %670, i32 %232) #9
  %675 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %671, i32 %232) #9
  %676 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %672, i32 %232) #9
  %677 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %673, <4 x i32> %674) #9
  %678 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %675, <4 x i32> %676) #9
  %679 = shufflevector <8 x i16> %552, <8 x i16> %575, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %680 = shufflevector <8 x i16> %552, <8 x i16> %575, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %681 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %679, <8 x i16> %336) #9
  %682 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %680, <8 x i16> %336) #9
  %683 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %679, <8 x i16> %339) #9
  %684 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %680, <8 x i16> %339) #9
  %685 = add <4 x i32> %681, <i32 2048, i32 2048, i32 2048, i32 2048>
  %686 = add <4 x i32> %682, <i32 2048, i32 2048, i32 2048, i32 2048>
  %687 = add <4 x i32> %683, <i32 2048, i32 2048, i32 2048, i32 2048>
  %688 = add <4 x i32> %684, <i32 2048, i32 2048, i32 2048, i32 2048>
  %689 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %685, i32 %232) #9
  %690 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %686, i32 %232) #9
  %691 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %687, i32 %232) #9
  %692 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %688, i32 %232) #9
  %693 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %689, <4 x i32> %690) #9
  %694 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %691, <4 x i32> %692) #9
  %695 = shufflevector <8 x i16> %559, <8 x i16> %568, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %696 = shufflevector <8 x i16> %559, <8 x i16> %568, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %697 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %695, <8 x i16> %520) #9
  %698 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %696, <8 x i16> %520) #9
  %699 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %695, <8 x i16> %336) #9
  %700 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %696, <8 x i16> %336) #9
  %701 = add <4 x i32> %697, <i32 2048, i32 2048, i32 2048, i32 2048>
  %702 = add <4 x i32> %698, <i32 2048, i32 2048, i32 2048, i32 2048>
  %703 = add <4 x i32> %699, <i32 2048, i32 2048, i32 2048, i32 2048>
  %704 = add <4 x i32> %700, <i32 2048, i32 2048, i32 2048, i32 2048>
  %705 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %701, i32 %232) #9
  %706 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %702, i32 %232) #9
  %707 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %703, i32 %232) #9
  %708 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %704, i32 %232) #9
  %709 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %705, <4 x i32> %706) #9
  %710 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %707, <4 x i32> %708) #9
  %711 = shufflevector <8 x i16> %561, <8 x i16> %570, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %712 = shufflevector <8 x i16> %561, <8 x i16> %570, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %713 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %711, <8 x i16> %520) #9
  %714 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %712, <8 x i16> %520) #9
  %715 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %711, <8 x i16> %336) #9
  %716 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %712, <8 x i16> %336) #9
  %717 = add <4 x i32> %713, <i32 2048, i32 2048, i32 2048, i32 2048>
  %718 = add <4 x i32> %714, <i32 2048, i32 2048, i32 2048, i32 2048>
  %719 = add <4 x i32> %715, <i32 2048, i32 2048, i32 2048, i32 2048>
  %720 = add <4 x i32> %716, <i32 2048, i32 2048, i32 2048, i32 2048>
  %721 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %717, i32 %232) #9
  %722 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %718, i32 %232) #9
  %723 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %719, i32 %232) #9
  %724 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %720, i32 %232) #9
  %725 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %721, <4 x i32> %722) #9
  %726 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %723, <4 x i32> %724) #9
  %727 = shufflevector <8 x i16> %563, <8 x i16> %572, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %728 = shufflevector <8 x i16> %563, <8 x i16> %572, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %729 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %727, <8 x i16> %520) #9
  %730 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %728, <8 x i16> %520) #9
  %731 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %727, <8 x i16> %336) #9
  %732 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %728, <8 x i16> %336) #9
  %733 = add <4 x i32> %729, <i32 2048, i32 2048, i32 2048, i32 2048>
  %734 = add <4 x i32> %730, <i32 2048, i32 2048, i32 2048, i32 2048>
  %735 = add <4 x i32> %731, <i32 2048, i32 2048, i32 2048, i32 2048>
  %736 = add <4 x i32> %732, <i32 2048, i32 2048, i32 2048, i32 2048>
  %737 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %733, i32 %232) #9
  %738 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %734, i32 %232) #9
  %739 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %735, i32 %232) #9
  %740 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %736, i32 %232) #9
  %741 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %737, <4 x i32> %738) #9
  %742 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %739, <4 x i32> %740) #9
  %743 = shufflevector <8 x i16> %565, <8 x i16> %574, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %744 = shufflevector <8 x i16> %565, <8 x i16> %574, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %745 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %743, <8 x i16> %520) #9
  %746 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %744, <8 x i16> %520) #9
  %747 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %743, <8 x i16> %336) #9
  %748 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %744, <8 x i16> %336) #9
  %749 = add <4 x i32> %745, <i32 2048, i32 2048, i32 2048, i32 2048>
  %750 = add <4 x i32> %746, <i32 2048, i32 2048, i32 2048, i32 2048>
  %751 = add <4 x i32> %747, <i32 2048, i32 2048, i32 2048, i32 2048>
  %752 = add <4 x i32> %748, <i32 2048, i32 2048, i32 2048, i32 2048>
  %753 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %749, i32 %232) #9
  %754 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %750, i32 %232) #9
  %755 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %751, i32 %232) #9
  %756 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %752, i32 %232) #9
  %757 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %753, <4 x i32> %754) #9
  %758 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %755, <4 x i32> %756) #9
  %759 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %333, <8 x i16> %219) #9
  %760 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %333, <8 x i16> %219) #9
  %761 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %333, <8 x i16> %351) #9
  %762 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %333, <8 x i16> %351) #9
  %763 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %333, <8 x i16> %600) #9
  %764 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %333, <8 x i16> %600) #9
  %765 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %333, <8 x i16> %614) #9
  %766 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %333, <8 x i16> %614) #9
  %767 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %333, <8 x i16> %613) #9
  %768 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %333, <8 x i16> %613) #9
  %769 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %333, <8 x i16> %599) #9
  %770 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %333, <8 x i16> %599) #9
  %771 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %333, <8 x i16> %350) #9
  %772 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %333, <8 x i16> %350) #9
  %773 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %333, <8 x i16> %218) #9
  %774 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %333, <8 x i16> %218) #9
  %775 = shufflevector <8 x i16> %622, <8 x i16> %629, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %776 = shufflevector <8 x i16> %622, <8 x i16> %629, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %777 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %775, <8 x i16> %585) #9
  %778 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %776, <8 x i16> %585) #9
  %779 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %775, <8 x i16> %588) #9
  %780 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %776, <8 x i16> %588) #9
  %781 = add <4 x i32> %777, <i32 2048, i32 2048, i32 2048, i32 2048>
  %782 = add <4 x i32> %778, <i32 2048, i32 2048, i32 2048, i32 2048>
  %783 = add <4 x i32> %779, <i32 2048, i32 2048, i32 2048, i32 2048>
  %784 = add <4 x i32> %780, <i32 2048, i32 2048, i32 2048, i32 2048>
  %785 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %781, i32 %232) #9
  %786 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %782, i32 %232) #9
  %787 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %783, i32 %232) #9
  %788 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %784, i32 %232) #9
  %789 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %785, <4 x i32> %786) #9
  %790 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %787, <4 x i32> %788) #9
  %791 = shufflevector <8 x i16> %620, <8 x i16> %627, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %792 = shufflevector <8 x i16> %620, <8 x i16> %627, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %793 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %791, <8 x i16> %585) #9
  %794 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %792, <8 x i16> %585) #9
  %795 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %791, <8 x i16> %588) #9
  %796 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %792, <8 x i16> %588) #9
  %797 = add <4 x i32> %793, <i32 2048, i32 2048, i32 2048, i32 2048>
  %798 = add <4 x i32> %794, <i32 2048, i32 2048, i32 2048, i32 2048>
  %799 = add <4 x i32> %795, <i32 2048, i32 2048, i32 2048, i32 2048>
  %800 = add <4 x i32> %796, <i32 2048, i32 2048, i32 2048, i32 2048>
  %801 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %797, i32 %232) #9
  %802 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %798, i32 %232) #9
  %803 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %799, i32 %232) #9
  %804 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %800, i32 %232) #9
  %805 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %801, <4 x i32> %802) #9
  %806 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %803, <4 x i32> %804) #9
  %807 = shufflevector <8 x i16> %618, <8 x i16> %625, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %808 = shufflevector <8 x i16> %618, <8 x i16> %625, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %809 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %807, <8 x i16> %585) #9
  %810 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %808, <8 x i16> %585) #9
  %811 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %807, <8 x i16> %588) #9
  %812 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %808, <8 x i16> %588) #9
  %813 = add <4 x i32> %809, <i32 2048, i32 2048, i32 2048, i32 2048>
  %814 = add <4 x i32> %810, <i32 2048, i32 2048, i32 2048, i32 2048>
  %815 = add <4 x i32> %811, <i32 2048, i32 2048, i32 2048, i32 2048>
  %816 = add <4 x i32> %812, <i32 2048, i32 2048, i32 2048, i32 2048>
  %817 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %813, i32 %232) #9
  %818 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %814, i32 %232) #9
  %819 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %815, i32 %232) #9
  %820 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %816, i32 %232) #9
  %821 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %817, <4 x i32> %818) #9
  %822 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %819, <4 x i32> %820) #9
  %823 = shufflevector <8 x i16> %616, <8 x i16> %623, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %824 = shufflevector <8 x i16> %616, <8 x i16> %623, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %825 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %823, <8 x i16> %585) #9
  %826 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %824, <8 x i16> %585) #9
  %827 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %823, <8 x i16> %588) #9
  %828 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %824, <8 x i16> %588) #9
  %829 = add <4 x i32> %825, <i32 2048, i32 2048, i32 2048, i32 2048>
  %830 = add <4 x i32> %826, <i32 2048, i32 2048, i32 2048, i32 2048>
  %831 = add <4 x i32> %827, <i32 2048, i32 2048, i32 2048, i32 2048>
  %832 = add <4 x i32> %828, <i32 2048, i32 2048, i32 2048, i32 2048>
  %833 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %829, i32 %232) #9
  %834 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %830, i32 %232) #9
  %835 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %831, i32 %232) #9
  %836 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %832, i32 %232) #9
  %837 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %833, <4 x i32> %834) #9
  %838 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %835, <4 x i32> %836) #9
  %839 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %551, <8 x i16> %560) #9
  %840 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %551, <8 x i16> %560) #9
  %841 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %553, <8 x i16> %562) #9
  %842 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %553, <8 x i16> %562) #9
  %843 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %555, <8 x i16> %564) #9
  %844 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %555, <8 x i16> %564) #9
  %845 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %557, <8 x i16> %566) #9
  %846 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %557, <8 x i16> %566) #9
  %847 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %645, <8 x i16> %757) #9
  %848 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %645, <8 x i16> %757) #9
  %849 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %661, <8 x i16> %741) #9
  %850 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %661, <8 x i16> %741) #9
  %851 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %677, <8 x i16> %725) #9
  %852 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %677, <8 x i16> %725) #9
  %853 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %693, <8 x i16> %709) #9
  %854 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %693, <8 x i16> %709) #9
  %855 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %576, <8 x i16> %567) #9
  %856 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %576, <8 x i16> %567) #9
  %857 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %578, <8 x i16> %569) #9
  %858 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %578, <8 x i16> %569) #9
  %859 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %580, <8 x i16> %571) #9
  %860 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %580, <8 x i16> %571) #9
  %861 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %582, <8 x i16> %573) #9
  %862 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %582, <8 x i16> %573) #9
  %863 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %646, <8 x i16> %758) #9
  %864 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %646, <8 x i16> %758) #9
  %865 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %662, <8 x i16> %742) #9
  %866 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %662, <8 x i16> %742) #9
  %867 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %678, <8 x i16> %726) #9
  %868 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %678, <8 x i16> %726) #9
  %869 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %694, <8 x i16> %710) #9
  %870 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %694, <8 x i16> %710) #9
  %871 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %759, <8 x i16> %624) #9
  %872 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %759, <8 x i16> %624) #9
  %873 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %761, <8 x i16> %626) #9
  %874 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %761, <8 x i16> %626) #9
  %875 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %763, <8 x i16> %628) #9
  %876 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %763, <8 x i16> %628) #9
  %877 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %765, <8 x i16> %630) #9
  %878 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %765, <8 x i16> %630) #9
  %879 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %767, <8 x i16> %790) #9
  %880 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %767, <8 x i16> %790) #9
  %881 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %769, <8 x i16> %806) #9
  %882 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %769, <8 x i16> %806) #9
  %883 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %771, <8 x i16> %822) #9
  %884 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %771, <8 x i16> %822) #9
  %885 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %773, <8 x i16> %838) #9
  %886 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %773, <8 x i16> %838) #9
  %887 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %774, <8 x i16> %837) #9
  %888 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %774, <8 x i16> %837) #9
  %889 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %772, <8 x i16> %821) #9
  %890 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %772, <8 x i16> %821) #9
  %891 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %770, <8 x i16> %805) #9
  %892 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %770, <8 x i16> %805) #9
  %893 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %768, <8 x i16> %789) #9
  %894 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %768, <8 x i16> %789) #9
  %895 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %766, <8 x i16> %621) #9
  %896 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %766, <8 x i16> %621) #9
  %897 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %764, <8 x i16> %619) #9
  %898 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %764, <8 x i16> %619) #9
  %899 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %762, <8 x i16> %617) #9
  %900 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %762, <8 x i16> %617) #9
  %901 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %760, <8 x i16> %615) #9
  %902 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %760, <8 x i16> %615) #9
  %903 = shufflevector <8 x i16> %854, <8 x i16> %869, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %904 = shufflevector <8 x i16> %854, <8 x i16> %869, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %905 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %903, <8 x i16> %585) #9
  %906 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %904, <8 x i16> %585) #9
  %907 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %903, <8 x i16> %588) #9
  %908 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %904, <8 x i16> %588) #9
  %909 = add <4 x i32> %905, <i32 2048, i32 2048, i32 2048, i32 2048>
  %910 = add <4 x i32> %906, <i32 2048, i32 2048, i32 2048, i32 2048>
  %911 = add <4 x i32> %907, <i32 2048, i32 2048, i32 2048, i32 2048>
  %912 = add <4 x i32> %908, <i32 2048, i32 2048, i32 2048, i32 2048>
  %913 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %909, i32 %232) #9
  %914 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %910, i32 %232) #9
  %915 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %911, i32 %232) #9
  %916 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %912, i32 %232) #9
  %917 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %913, <4 x i32> %914) #9
  %918 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %915, <4 x i32> %916) #9
  %919 = shufflevector <8 x i16> %852, <8 x i16> %867, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %920 = shufflevector <8 x i16> %852, <8 x i16> %867, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %921 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %919, <8 x i16> %585) #9
  %922 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %920, <8 x i16> %585) #9
  %923 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %919, <8 x i16> %588) #9
  %924 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %920, <8 x i16> %588) #9
  %925 = add <4 x i32> %921, <i32 2048, i32 2048, i32 2048, i32 2048>
  %926 = add <4 x i32> %922, <i32 2048, i32 2048, i32 2048, i32 2048>
  %927 = add <4 x i32> %923, <i32 2048, i32 2048, i32 2048, i32 2048>
  %928 = add <4 x i32> %924, <i32 2048, i32 2048, i32 2048, i32 2048>
  %929 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %925, i32 %232) #9
  %930 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %926, i32 %232) #9
  %931 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %927, i32 %232) #9
  %932 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %928, i32 %232) #9
  %933 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %929, <4 x i32> %930) #9
  %934 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %931, <4 x i32> %932) #9
  %935 = shufflevector <8 x i16> %850, <8 x i16> %865, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %936 = shufflevector <8 x i16> %850, <8 x i16> %865, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %937 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %935, <8 x i16> %585) #9
  %938 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %936, <8 x i16> %585) #9
  %939 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %935, <8 x i16> %588) #9
  %940 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %936, <8 x i16> %588) #9
  %941 = add <4 x i32> %937, <i32 2048, i32 2048, i32 2048, i32 2048>
  %942 = add <4 x i32> %938, <i32 2048, i32 2048, i32 2048, i32 2048>
  %943 = add <4 x i32> %939, <i32 2048, i32 2048, i32 2048, i32 2048>
  %944 = add <4 x i32> %940, <i32 2048, i32 2048, i32 2048, i32 2048>
  %945 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %941, i32 %232) #9
  %946 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %942, i32 %232) #9
  %947 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %943, i32 %232) #9
  %948 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %944, i32 %232) #9
  %949 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %945, <4 x i32> %946) #9
  %950 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %947, <4 x i32> %948) #9
  %951 = shufflevector <8 x i16> %848, <8 x i16> %863, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %952 = shufflevector <8 x i16> %848, <8 x i16> %863, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %953 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %951, <8 x i16> %585) #9
  %954 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %952, <8 x i16> %585) #9
  %955 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %951, <8 x i16> %588) #9
  %956 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %952, <8 x i16> %588) #9
  %957 = add <4 x i32> %953, <i32 2048, i32 2048, i32 2048, i32 2048>
  %958 = add <4 x i32> %954, <i32 2048, i32 2048, i32 2048, i32 2048>
  %959 = add <4 x i32> %955, <i32 2048, i32 2048, i32 2048, i32 2048>
  %960 = add <4 x i32> %956, <i32 2048, i32 2048, i32 2048, i32 2048>
  %961 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %957, i32 %232) #9
  %962 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %958, i32 %232) #9
  %963 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %959, i32 %232) #9
  %964 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %960, i32 %232) #9
  %965 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %961, <4 x i32> %962) #9
  %966 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %963, <4 x i32> %964) #9
  %967 = shufflevector <8 x i16> %846, <8 x i16> %861, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %968 = shufflevector <8 x i16> %846, <8 x i16> %861, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %969 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %967, <8 x i16> %585) #9
  %970 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %968, <8 x i16> %585) #9
  %971 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %967, <8 x i16> %588) #9
  %972 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %968, <8 x i16> %588) #9
  %973 = add <4 x i32> %969, <i32 2048, i32 2048, i32 2048, i32 2048>
  %974 = add <4 x i32> %970, <i32 2048, i32 2048, i32 2048, i32 2048>
  %975 = add <4 x i32> %971, <i32 2048, i32 2048, i32 2048, i32 2048>
  %976 = add <4 x i32> %972, <i32 2048, i32 2048, i32 2048, i32 2048>
  %977 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %973, i32 %232) #9
  %978 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %974, i32 %232) #9
  %979 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %975, i32 %232) #9
  %980 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %976, i32 %232) #9
  %981 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %977, <4 x i32> %978) #9
  %982 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %979, <4 x i32> %980) #9
  %983 = shufflevector <8 x i16> %844, <8 x i16> %859, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %984 = shufflevector <8 x i16> %844, <8 x i16> %859, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %985 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %983, <8 x i16> %585) #9
  %986 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %984, <8 x i16> %585) #9
  %987 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %983, <8 x i16> %588) #9
  %988 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %984, <8 x i16> %588) #9
  %989 = add <4 x i32> %985, <i32 2048, i32 2048, i32 2048, i32 2048>
  %990 = add <4 x i32> %986, <i32 2048, i32 2048, i32 2048, i32 2048>
  %991 = add <4 x i32> %987, <i32 2048, i32 2048, i32 2048, i32 2048>
  %992 = add <4 x i32> %988, <i32 2048, i32 2048, i32 2048, i32 2048>
  %993 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %989, i32 %232) #9
  %994 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %990, i32 %232) #9
  %995 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %991, i32 %232) #9
  %996 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %992, i32 %232) #9
  %997 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %993, <4 x i32> %994) #9
  %998 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %995, <4 x i32> %996) #9
  %999 = shufflevector <8 x i16> %842, <8 x i16> %857, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1000 = shufflevector <8 x i16> %842, <8 x i16> %857, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1001 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %999, <8 x i16> %585) #9
  %1002 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1000, <8 x i16> %585) #9
  %1003 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %999, <8 x i16> %588) #9
  %1004 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1000, <8 x i16> %588) #9
  %1005 = add <4 x i32> %1001, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1006 = add <4 x i32> %1002, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1007 = add <4 x i32> %1003, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1008 = add <4 x i32> %1004, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1009 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1005, i32 %232) #9
  %1010 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1006, i32 %232) #9
  %1011 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1007, i32 %232) #9
  %1012 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1008, i32 %232) #9
  %1013 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1009, <4 x i32> %1010) #9
  %1014 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1011, <4 x i32> %1012) #9
  %1015 = shufflevector <8 x i16> %840, <8 x i16> %855, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1016 = shufflevector <8 x i16> %840, <8 x i16> %855, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1017 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1015, <8 x i16> %585) #9
  %1018 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1016, <8 x i16> %585) #9
  %1019 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1015, <8 x i16> %588) #9
  %1020 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1016, <8 x i16> %588) #9
  %1021 = add <4 x i32> %1017, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1022 = add <4 x i32> %1018, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1023 = add <4 x i32> %1019, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1024 = add <4 x i32> %1020, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1025 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1021, i32 %232) #9
  %1026 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1022, i32 %232) #9
  %1027 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1023, i32 %232) #9
  %1028 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1024, i32 %232) #9
  %1029 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1025, <4 x i32> %1026) #9
  %1030 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1027, <4 x i32> %1028) #9
  %1031 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %871, <8 x i16> %856) #9
  %1032 = bitcast <2 x i64>* %1 to <8 x i16>*
  store <8 x i16> %1031, <8 x i16>* %1032, align 16
  %1033 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %871, <8 x i16> %856) #9
  %1034 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 63
  %1035 = bitcast <2 x i64>* %1034 to <8 x i16>*
  store <8 x i16> %1033, <8 x i16>* %1035, align 16
  %1036 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %873, <8 x i16> %858) #9
  %1037 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 1
  %1038 = bitcast <2 x i64>* %1037 to <8 x i16>*
  store <8 x i16> %1036, <8 x i16>* %1038, align 16
  %1039 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %873, <8 x i16> %858) #9
  %1040 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 62
  %1041 = bitcast <2 x i64>* %1040 to <8 x i16>*
  store <8 x i16> %1039, <8 x i16>* %1041, align 16
  %1042 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %875, <8 x i16> %860) #9
  %1043 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 2
  %1044 = bitcast <2 x i64>* %1043 to <8 x i16>*
  store <8 x i16> %1042, <8 x i16>* %1044, align 16
  %1045 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %875, <8 x i16> %860) #9
  %1046 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 61
  %1047 = bitcast <2 x i64>* %1046 to <8 x i16>*
  store <8 x i16> %1045, <8 x i16>* %1047, align 16
  %1048 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %877, <8 x i16> %862) #9
  %1049 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 3
  %1050 = bitcast <2 x i64>* %1049 to <8 x i16>*
  store <8 x i16> %1048, <8 x i16>* %1050, align 16
  %1051 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %877, <8 x i16> %862) #9
  %1052 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 60
  %1053 = bitcast <2 x i64>* %1052 to <8 x i16>*
  store <8 x i16> %1051, <8 x i16>* %1053, align 16
  %1054 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %879, <8 x i16> %864) #9
  %1055 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 4
  %1056 = bitcast <2 x i64>* %1055 to <8 x i16>*
  store <8 x i16> %1054, <8 x i16>* %1056, align 16
  %1057 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %879, <8 x i16> %864) #9
  %1058 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 59
  %1059 = bitcast <2 x i64>* %1058 to <8 x i16>*
  store <8 x i16> %1057, <8 x i16>* %1059, align 16
  %1060 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %881, <8 x i16> %866) #9
  %1061 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 5
  %1062 = bitcast <2 x i64>* %1061 to <8 x i16>*
  store <8 x i16> %1060, <8 x i16>* %1062, align 16
  %1063 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %881, <8 x i16> %866) #9
  %1064 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 58
  %1065 = bitcast <2 x i64>* %1064 to <8 x i16>*
  store <8 x i16> %1063, <8 x i16>* %1065, align 16
  %1066 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %883, <8 x i16> %868) #9
  %1067 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 6
  %1068 = bitcast <2 x i64>* %1067 to <8 x i16>*
  store <8 x i16> %1066, <8 x i16>* %1068, align 16
  %1069 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %883, <8 x i16> %868) #9
  %1070 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 57
  %1071 = bitcast <2 x i64>* %1070 to <8 x i16>*
  store <8 x i16> %1069, <8 x i16>* %1071, align 16
  %1072 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %885, <8 x i16> %870) #9
  %1073 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 7
  %1074 = bitcast <2 x i64>* %1073 to <8 x i16>*
  store <8 x i16> %1072, <8 x i16>* %1074, align 16
  %1075 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %885, <8 x i16> %870) #9
  %1076 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 56
  %1077 = bitcast <2 x i64>* %1076 to <8 x i16>*
  store <8 x i16> %1075, <8 x i16>* %1077, align 16
  %1078 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %887, <8 x i16> %918) #9
  %1079 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 8
  %1080 = bitcast <2 x i64>* %1079 to <8 x i16>*
  store <8 x i16> %1078, <8 x i16>* %1080, align 16
  %1081 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %887, <8 x i16> %918) #9
  %1082 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 55
  %1083 = bitcast <2 x i64>* %1082 to <8 x i16>*
  store <8 x i16> %1081, <8 x i16>* %1083, align 16
  %1084 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %889, <8 x i16> %934) #9
  %1085 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 9
  %1086 = bitcast <2 x i64>* %1085 to <8 x i16>*
  store <8 x i16> %1084, <8 x i16>* %1086, align 16
  %1087 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %889, <8 x i16> %934) #9
  %1088 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 54
  %1089 = bitcast <2 x i64>* %1088 to <8 x i16>*
  store <8 x i16> %1087, <8 x i16>* %1089, align 16
  %1090 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %891, <8 x i16> %950) #9
  %1091 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 10
  %1092 = bitcast <2 x i64>* %1091 to <8 x i16>*
  store <8 x i16> %1090, <8 x i16>* %1092, align 16
  %1093 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %891, <8 x i16> %950) #9
  %1094 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 53
  %1095 = bitcast <2 x i64>* %1094 to <8 x i16>*
  store <8 x i16> %1093, <8 x i16>* %1095, align 16
  %1096 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %893, <8 x i16> %966) #9
  %1097 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 11
  %1098 = bitcast <2 x i64>* %1097 to <8 x i16>*
  store <8 x i16> %1096, <8 x i16>* %1098, align 16
  %1099 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %893, <8 x i16> %966) #9
  %1100 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 52
  %1101 = bitcast <2 x i64>* %1100 to <8 x i16>*
  store <8 x i16> %1099, <8 x i16>* %1101, align 16
  %1102 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %895, <8 x i16> %982) #9
  %1103 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 12
  %1104 = bitcast <2 x i64>* %1103 to <8 x i16>*
  store <8 x i16> %1102, <8 x i16>* %1104, align 16
  %1105 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %895, <8 x i16> %982) #9
  %1106 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 51
  %1107 = bitcast <2 x i64>* %1106 to <8 x i16>*
  store <8 x i16> %1105, <8 x i16>* %1107, align 16
  %1108 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %897, <8 x i16> %998) #9
  %1109 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 13
  %1110 = bitcast <2 x i64>* %1109 to <8 x i16>*
  store <8 x i16> %1108, <8 x i16>* %1110, align 16
  %1111 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %897, <8 x i16> %998) #9
  %1112 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 50
  %1113 = bitcast <2 x i64>* %1112 to <8 x i16>*
  store <8 x i16> %1111, <8 x i16>* %1113, align 16
  %1114 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %899, <8 x i16> %1014) #9
  %1115 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 14
  %1116 = bitcast <2 x i64>* %1115 to <8 x i16>*
  store <8 x i16> %1114, <8 x i16>* %1116, align 16
  %1117 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %899, <8 x i16> %1014) #9
  %1118 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 49
  %1119 = bitcast <2 x i64>* %1118 to <8 x i16>*
  store <8 x i16> %1117, <8 x i16>* %1119, align 16
  %1120 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %901, <8 x i16> %1030) #9
  %1121 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 15
  %1122 = bitcast <2 x i64>* %1121 to <8 x i16>*
  store <8 x i16> %1120, <8 x i16>* %1122, align 16
  %1123 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %901, <8 x i16> %1030) #9
  %1124 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 48
  %1125 = bitcast <2 x i64>* %1124 to <8 x i16>*
  store <8 x i16> %1123, <8 x i16>* %1125, align 16
  %1126 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %902, <8 x i16> %1029) #9
  %1127 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 16
  %1128 = bitcast <2 x i64>* %1127 to <8 x i16>*
  store <8 x i16> %1126, <8 x i16>* %1128, align 16
  %1129 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %902, <8 x i16> %1029) #9
  %1130 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 47
  %1131 = bitcast <2 x i64>* %1130 to <8 x i16>*
  store <8 x i16> %1129, <8 x i16>* %1131, align 16
  %1132 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %900, <8 x i16> %1013) #9
  %1133 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 17
  %1134 = bitcast <2 x i64>* %1133 to <8 x i16>*
  store <8 x i16> %1132, <8 x i16>* %1134, align 16
  %1135 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %900, <8 x i16> %1013) #9
  %1136 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 46
  %1137 = bitcast <2 x i64>* %1136 to <8 x i16>*
  store <8 x i16> %1135, <8 x i16>* %1137, align 16
  %1138 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %898, <8 x i16> %997) #9
  %1139 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 18
  %1140 = bitcast <2 x i64>* %1139 to <8 x i16>*
  store <8 x i16> %1138, <8 x i16>* %1140, align 16
  %1141 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %898, <8 x i16> %997) #9
  %1142 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 45
  %1143 = bitcast <2 x i64>* %1142 to <8 x i16>*
  store <8 x i16> %1141, <8 x i16>* %1143, align 16
  %1144 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %896, <8 x i16> %981) #9
  %1145 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 19
  %1146 = bitcast <2 x i64>* %1145 to <8 x i16>*
  store <8 x i16> %1144, <8 x i16>* %1146, align 16
  %1147 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %896, <8 x i16> %981) #9
  %1148 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 44
  %1149 = bitcast <2 x i64>* %1148 to <8 x i16>*
  store <8 x i16> %1147, <8 x i16>* %1149, align 16
  %1150 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %894, <8 x i16> %965) #9
  %1151 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 20
  %1152 = bitcast <2 x i64>* %1151 to <8 x i16>*
  store <8 x i16> %1150, <8 x i16>* %1152, align 16
  %1153 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %894, <8 x i16> %965) #9
  %1154 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 43
  %1155 = bitcast <2 x i64>* %1154 to <8 x i16>*
  store <8 x i16> %1153, <8 x i16>* %1155, align 16
  %1156 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %892, <8 x i16> %949) #9
  %1157 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 21
  %1158 = bitcast <2 x i64>* %1157 to <8 x i16>*
  store <8 x i16> %1156, <8 x i16>* %1158, align 16
  %1159 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %892, <8 x i16> %949) #9
  %1160 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 42
  %1161 = bitcast <2 x i64>* %1160 to <8 x i16>*
  store <8 x i16> %1159, <8 x i16>* %1161, align 16
  %1162 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %890, <8 x i16> %933) #9
  %1163 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 22
  %1164 = bitcast <2 x i64>* %1163 to <8 x i16>*
  store <8 x i16> %1162, <8 x i16>* %1164, align 16
  %1165 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %890, <8 x i16> %933) #9
  %1166 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 41
  %1167 = bitcast <2 x i64>* %1166 to <8 x i16>*
  store <8 x i16> %1165, <8 x i16>* %1167, align 16
  %1168 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %888, <8 x i16> %917) #9
  %1169 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 23
  %1170 = bitcast <2 x i64>* %1169 to <8 x i16>*
  store <8 x i16> %1168, <8 x i16>* %1170, align 16
  %1171 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %888, <8 x i16> %917) #9
  %1172 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 40
  %1173 = bitcast <2 x i64>* %1172 to <8 x i16>*
  store <8 x i16> %1171, <8 x i16>* %1173, align 16
  %1174 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %886, <8 x i16> %853) #9
  %1175 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 24
  %1176 = bitcast <2 x i64>* %1175 to <8 x i16>*
  store <8 x i16> %1174, <8 x i16>* %1176, align 16
  %1177 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %886, <8 x i16> %853) #9
  %1178 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 39
  %1179 = bitcast <2 x i64>* %1178 to <8 x i16>*
  store <8 x i16> %1177, <8 x i16>* %1179, align 16
  %1180 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %884, <8 x i16> %851) #9
  %1181 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 25
  %1182 = bitcast <2 x i64>* %1181 to <8 x i16>*
  store <8 x i16> %1180, <8 x i16>* %1182, align 16
  %1183 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %884, <8 x i16> %851) #9
  %1184 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 38
  %1185 = bitcast <2 x i64>* %1184 to <8 x i16>*
  store <8 x i16> %1183, <8 x i16>* %1185, align 16
  %1186 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %882, <8 x i16> %849) #9
  %1187 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 26
  %1188 = bitcast <2 x i64>* %1187 to <8 x i16>*
  store <8 x i16> %1186, <8 x i16>* %1188, align 16
  %1189 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %882, <8 x i16> %849) #9
  %1190 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 37
  %1191 = bitcast <2 x i64>* %1190 to <8 x i16>*
  store <8 x i16> %1189, <8 x i16>* %1191, align 16
  %1192 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %880, <8 x i16> %847) #9
  %1193 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 27
  %1194 = bitcast <2 x i64>* %1193 to <8 x i16>*
  store <8 x i16> %1192, <8 x i16>* %1194, align 16
  %1195 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %880, <8 x i16> %847) #9
  %1196 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 36
  %1197 = bitcast <2 x i64>* %1196 to <8 x i16>*
  store <8 x i16> %1195, <8 x i16>* %1197, align 16
  %1198 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %878, <8 x i16> %845) #9
  %1199 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 28
  %1200 = bitcast <2 x i64>* %1199 to <8 x i16>*
  store <8 x i16> %1198, <8 x i16>* %1200, align 16
  %1201 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %878, <8 x i16> %845) #9
  %1202 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 35
  %1203 = bitcast <2 x i64>* %1202 to <8 x i16>*
  store <8 x i16> %1201, <8 x i16>* %1203, align 16
  %1204 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %876, <8 x i16> %843) #9
  %1205 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 29
  %1206 = bitcast <2 x i64>* %1205 to <8 x i16>*
  store <8 x i16> %1204, <8 x i16>* %1206, align 16
  %1207 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %876, <8 x i16> %843) #9
  %1208 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 34
  %1209 = bitcast <2 x i64>* %1208 to <8 x i16>*
  store <8 x i16> %1207, <8 x i16>* %1209, align 16
  %1210 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %874, <8 x i16> %841) #9
  %1211 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 30
  %1212 = bitcast <2 x i64>* %1211 to <8 x i16>*
  store <8 x i16> %1210, <8 x i16>* %1212, align 16
  %1213 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %874, <8 x i16> %841) #9
  %1214 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 33
  %1215 = bitcast <2 x i64>* %1214 to <8 x i16>*
  store <8 x i16> %1213, <8 x i16>* %1215, align 16
  %1216 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %872, <8 x i16> %839) #9
  %1217 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 31
  %1218 = bitcast <2 x i64>* %1217 to <8 x i16>*
  store <8 x i16> %1216, <8 x i16>* %1218, align 16
  %1219 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %872, <8 x i16> %839) #9
  %1220 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 32
  %1221 = bitcast <2 x i64>* %1220 to <8 x i16>*
  store <8 x i16> %1219, <8 x i16>* %1221, align 16
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal void @idct64_low16_ssse3(<2 x i64>* nocapture readonly, <2 x i64>*, i8 signext) #0 {
  %4 = alloca [64 x <2 x i64>], align 16
  %5 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 32), align 16
  %6 = and i32 %5, 65535
  %7 = shl i32 %5, 16
  %8 = or i32 %6, %7
  %9 = insertelement <4 x i32> undef, i32 %8, i32 0
  %10 = shufflevector <4 x i32> %9, <4 x i32> undef, <4 x i32> zeroinitializer
  %11 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 16), align 16
  %12 = sub i32 0, %11
  %13 = and i32 %12, 65535
  %14 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 48), align 16
  %15 = shl i32 %14, 16
  %16 = or i32 %15, %13
  %17 = insertelement <4 x i32> undef, i32 %16, i32 0
  %18 = shufflevector <4 x i32> %17, <4 x i32> undef, <4 x i32> zeroinitializer
  %19 = and i32 %14, 65535
  %20 = shl i32 %11, 16
  %21 = or i32 %19, %20
  %22 = insertelement <4 x i32> undef, i32 %21, i32 0
  %23 = shufflevector <4 x i32> %22, <4 x i32> undef, <4 x i32> zeroinitializer
  %24 = sub i32 0, %14
  %25 = and i32 %24, 65535
  %26 = sub i32 0, %20
  %27 = or i32 %25, %26
  %28 = insertelement <4 x i32> undef, i32 %27, i32 0
  %29 = shufflevector <4 x i32> %28, <4 x i32> undef, <4 x i32> zeroinitializer
  %30 = sub i32 0, %5
  %31 = and i32 %30, 65535
  %32 = or i32 %31, %7
  %33 = insertelement <4 x i32> undef, i32 %32, i32 0
  %34 = shufflevector <4 x i32> %33, <4 x i32> undef, <4 x i32> zeroinitializer
  %35 = bitcast [64 x <2 x i64>]* %4 to i8*
  call void @llvm.lifetime.start.p0i8(i64 1024, i8* nonnull %35) #9
  %36 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 1
  %37 = bitcast <2 x i64>* %36 to i8*
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %37, i8 -86, i64 992, i1 false)
  %38 = load <2 x i64>, <2 x i64>* %0, align 16
  %39 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 0
  store <2 x i64> %38, <2 x i64>* %39, align 16
  %40 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 8
  %41 = load <2 x i64>, <2 x i64>* %40, align 16
  %42 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 4
  store <2 x i64> %41, <2 x i64>* %42, align 16
  %43 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 4
  %44 = load <2 x i64>, <2 x i64>* %43, align 16
  %45 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 8
  store <2 x i64> %44, <2 x i64>* %45, align 16
  %46 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 12
  %47 = load <2 x i64>, <2 x i64>* %46, align 16
  %48 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 12
  store <2 x i64> %47, <2 x i64>* %48, align 16
  %49 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 2
  %50 = load <2 x i64>, <2 x i64>* %49, align 16
  %51 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 16
  store <2 x i64> %50, <2 x i64>* %51, align 16
  %52 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 10
  %53 = load <2 x i64>, <2 x i64>* %52, align 16
  %54 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 20
  store <2 x i64> %53, <2 x i64>* %54, align 16
  %55 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 6
  %56 = load <2 x i64>, <2 x i64>* %55, align 16
  %57 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 24
  store <2 x i64> %56, <2 x i64>* %57, align 16
  %58 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 14
  %59 = load <2 x i64>, <2 x i64>* %58, align 16
  %60 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 28
  store <2 x i64> %59, <2 x i64>* %60, align 16
  %61 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 1
  %62 = bitcast <2 x i64>* %61 to <8 x i16>*
  %63 = load <8 x i16>, <8 x i16>* %62, align 16
  %64 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 32
  %65 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 9
  %66 = bitcast <2 x i64>* %65 to <8 x i16>*
  %67 = load <8 x i16>, <8 x i16>* %66, align 16
  %68 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 36
  %69 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 5
  %70 = load <2 x i64>, <2 x i64>* %69, align 16
  %71 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 40
  store <2 x i64> %70, <2 x i64>* %71, align 16
  %72 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 13
  %73 = load <2 x i64>, <2 x i64>* %72, align 16
  %74 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 44
  store <2 x i64> %73, <2 x i64>* %74, align 16
  %75 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 3
  %76 = load <2 x i64>, <2 x i64>* %75, align 16
  %77 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 48
  store <2 x i64> %76, <2 x i64>* %77, align 16
  %78 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 11
  %79 = load <2 x i64>, <2 x i64>* %78, align 16
  %80 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 52
  store <2 x i64> %79, <2 x i64>* %80, align 16
  %81 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 7
  %82 = bitcast <2 x i64>* %81 to <8 x i16>*
  %83 = load <8 x i16>, <8 x i16>* %82, align 16
  %84 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 56
  %85 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 15
  %86 = bitcast <2 x i64>* %85 to <8 x i16>*
  %87 = load <8 x i16>, <8 x i16>* %86, align 16
  %88 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 60
  %89 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 63), align 4
  %90 = trunc i32 %89 to i16
  %91 = shl i16 %90, 3
  %92 = insertelement <8 x i16> undef, i16 %91, i32 0
  %93 = shufflevector <8 x i16> %92, <8 x i16> undef, <8 x i32> zeroinitializer
  %94 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 1), align 4
  %95 = trunc i32 %94 to i16
  %96 = shl i16 %95, 3
  %97 = insertelement <8 x i16> undef, i16 %96, i32 0
  %98 = shufflevector <8 x i16> %97, <8 x i16> undef, <8 x i32> zeroinitializer
  %99 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %63, <8 x i16> %93) #9
  %100 = bitcast <2 x i64>* %64 to <8 x i16>*
  store <8 x i16> %99, <8 x i16>* %100, align 16
  %101 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %63, <8 x i16> %98) #9
  %102 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 63
  %103 = bitcast <2 x i64>* %102 to <8 x i16>*
  store <8 x i16> %101, <8 x i16>* %103, align 16
  %104 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 49), align 4
  %105 = trunc i32 %104 to i16
  %106 = shl i16 %105, 3
  %107 = sub i16 0, %106
  %108 = insertelement <8 x i16> undef, i16 %107, i32 0
  %109 = shufflevector <8 x i16> %108, <8 x i16> undef, <8 x i32> zeroinitializer
  %110 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 15), align 4
  %111 = trunc i32 %110 to i16
  %112 = shl i16 %111, 3
  %113 = insertelement <8 x i16> undef, i16 %112, i32 0
  %114 = shufflevector <8 x i16> %113, <8 x i16> undef, <8 x i32> zeroinitializer
  %115 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %87, <8 x i16> %109) #9
  %116 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 35
  %117 = bitcast <2 x i64>* %116 to <8 x i16>*
  store <8 x i16> %115, <8 x i16>* %117, align 16
  %118 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %87, <8 x i16> %114) #9
  %119 = bitcast <2 x i64>* %88 to <8 x i16>*
  store <8 x i16> %118, <8 x i16>* %119, align 16
  %120 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 55), align 4
  %121 = trunc i32 %120 to i16
  %122 = shl i16 %121, 3
  %123 = insertelement <8 x i16> undef, i16 %122, i32 0
  %124 = shufflevector <8 x i16> %123, <8 x i16> undef, <8 x i32> zeroinitializer
  %125 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 9), align 4
  %126 = trunc i32 %125 to i16
  %127 = shl i16 %126, 3
  %128 = insertelement <8 x i16> undef, i16 %127, i32 0
  %129 = shufflevector <8 x i16> %128, <8 x i16> undef, <8 x i32> zeroinitializer
  %130 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %67, <8 x i16> %124) #9
  %131 = bitcast <2 x i64>* %68 to <8 x i16>*
  store <8 x i16> %130, <8 x i16>* %131, align 16
  %132 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %67, <8 x i16> %129) #9
  %133 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 59
  %134 = bitcast <2 x i64>* %133 to <8 x i16>*
  store <8 x i16> %132, <8 x i16>* %134, align 16
  %135 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 57), align 4
  %136 = trunc i32 %135 to i16
  %137 = shl i16 %136, 3
  %138 = sub i16 0, %137
  %139 = insertelement <8 x i16> undef, i16 %138, i32 0
  %140 = shufflevector <8 x i16> %139, <8 x i16> undef, <8 x i32> zeroinitializer
  %141 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 7), align 4
  %142 = trunc i32 %141 to i16
  %143 = shl i16 %142, 3
  %144 = insertelement <8 x i16> undef, i16 %143, i32 0
  %145 = shufflevector <8 x i16> %144, <8 x i16> undef, <8 x i32> zeroinitializer
  %146 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %83, <8 x i16> %140) #9
  %147 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 39
  %148 = bitcast <2 x i64>* %147 to <8 x i16>*
  store <8 x i16> %146, <8 x i16>* %148, align 16
  %149 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %83, <8 x i16> %145) #9
  %150 = bitcast <2 x i64>* %84 to <8 x i16>*
  store <8 x i16> %149, <8 x i16>* %150, align 16
  %151 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 59), align 4
  %152 = trunc i32 %151 to i16
  %153 = shl i16 %152, 3
  %154 = insertelement <8 x i16> undef, i16 %153, i32 0
  %155 = shufflevector <8 x i16> %154, <8 x i16> undef, <8 x i32> zeroinitializer
  %156 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 5), align 4
  %157 = trunc i32 %156 to i16
  %158 = shl i16 %157, 3
  %159 = insertelement <8 x i16> undef, i16 %158, i32 0
  %160 = shufflevector <8 x i16> %159, <8 x i16> undef, <8 x i32> zeroinitializer
  %161 = bitcast <2 x i64> %70 to <8 x i16>
  %162 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %161, <8 x i16> %155) #9
  %163 = bitcast <2 x i64>* %71 to <8 x i16>*
  store <8 x i16> %162, <8 x i16>* %163, align 16
  %164 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %161, <8 x i16> %160) #9
  %165 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 55
  %166 = bitcast <2 x i64>* %165 to <8 x i16>*
  store <8 x i16> %164, <8 x i16>* %166, align 16
  %167 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 53), align 4
  %168 = trunc i32 %167 to i16
  %169 = shl i16 %168, 3
  %170 = sub i16 0, %169
  %171 = insertelement <8 x i16> undef, i16 %170, i32 0
  %172 = shufflevector <8 x i16> %171, <8 x i16> undef, <8 x i32> zeroinitializer
  %173 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 11), align 4
  %174 = trunc i32 %173 to i16
  %175 = shl i16 %174, 3
  %176 = insertelement <8 x i16> undef, i16 %175, i32 0
  %177 = shufflevector <8 x i16> %176, <8 x i16> undef, <8 x i32> zeroinitializer
  %178 = bitcast <2 x i64> %79 to <8 x i16>
  %179 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %178, <8 x i16> %172) #9
  %180 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 43
  %181 = bitcast <2 x i64>* %180 to <8 x i16>*
  store <8 x i16> %179, <8 x i16>* %181, align 16
  %182 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %178, <8 x i16> %177) #9
  %183 = bitcast <2 x i64>* %80 to <8 x i16>*
  store <8 x i16> %182, <8 x i16>* %183, align 16
  %184 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 51), align 4
  %185 = trunc i32 %184 to i16
  %186 = shl i16 %185, 3
  %187 = insertelement <8 x i16> undef, i16 %186, i32 0
  %188 = shufflevector <8 x i16> %187, <8 x i16> undef, <8 x i32> zeroinitializer
  %189 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 13), align 4
  %190 = trunc i32 %189 to i16
  %191 = shl i16 %190, 3
  %192 = insertelement <8 x i16> undef, i16 %191, i32 0
  %193 = shufflevector <8 x i16> %192, <8 x i16> undef, <8 x i32> zeroinitializer
  %194 = bitcast <2 x i64> %73 to <8 x i16>
  %195 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %194, <8 x i16> %188) #9
  %196 = bitcast <2 x i64>* %74 to <8 x i16>*
  store <8 x i16> %195, <8 x i16>* %196, align 16
  %197 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %194, <8 x i16> %193) #9
  %198 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 51
  %199 = bitcast <2 x i64>* %198 to <8 x i16>*
  store <8 x i16> %197, <8 x i16>* %199, align 16
  %200 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 61), align 4
  %201 = trunc i32 %200 to i16
  %202 = shl i16 %201, 3
  %203 = sub i16 0, %202
  %204 = insertelement <8 x i16> undef, i16 %203, i32 0
  %205 = shufflevector <8 x i16> %204, <8 x i16> undef, <8 x i32> zeroinitializer
  %206 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 3), align 4
  %207 = trunc i32 %206 to i16
  %208 = shl i16 %207, 3
  %209 = insertelement <8 x i16> undef, i16 %208, i32 0
  %210 = shufflevector <8 x i16> %209, <8 x i16> undef, <8 x i32> zeroinitializer
  %211 = bitcast <2 x i64> %76 to <8 x i16>
  %212 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %211, <8 x i16> %205) #9
  %213 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 47
  %214 = bitcast <2 x i64>* %213 to <8 x i16>*
  store <8 x i16> %212, <8 x i16>* %214, align 16
  %215 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %211, <8 x i16> %210) #9
  %216 = bitcast <2 x i64>* %77 to <8 x i16>*
  store <8 x i16> %215, <8 x i16>* %216, align 16
  %217 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 62), align 8
  %218 = trunc i32 %217 to i16
  %219 = shl i16 %218, 3
  %220 = insertelement <8 x i16> undef, i16 %219, i32 0
  %221 = shufflevector <8 x i16> %220, <8 x i16> undef, <8 x i32> zeroinitializer
  %222 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 2), align 8
  %223 = trunc i32 %222 to i16
  %224 = shl i16 %223, 3
  %225 = insertelement <8 x i16> undef, i16 %224, i32 0
  %226 = shufflevector <8 x i16> %225, <8 x i16> undef, <8 x i32> zeroinitializer
  %227 = bitcast <2 x i64> %50 to <8 x i16>
  %228 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %227, <8 x i16> %221) #9
  %229 = bitcast <2 x i64>* %51 to <8 x i16>*
  store <8 x i16> %228, <8 x i16>* %229, align 16
  %230 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %227, <8 x i16> %226) #9
  %231 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 31
  %232 = bitcast <2 x i64>* %231 to <8 x i16>*
  store <8 x i16> %230, <8 x i16>* %232, align 16
  %233 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 50), align 8
  %234 = trunc i32 %233 to i16
  %235 = shl i16 %234, 3
  %236 = sub i16 0, %235
  %237 = insertelement <8 x i16> undef, i16 %236, i32 0
  %238 = shufflevector <8 x i16> %237, <8 x i16> undef, <8 x i32> zeroinitializer
  %239 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 14), align 8
  %240 = trunc i32 %239 to i16
  %241 = shl i16 %240, 3
  %242 = insertelement <8 x i16> undef, i16 %241, i32 0
  %243 = shufflevector <8 x i16> %242, <8 x i16> undef, <8 x i32> zeroinitializer
  %244 = bitcast <2 x i64> %59 to <8 x i16>
  %245 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %244, <8 x i16> %238) #9
  %246 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 19
  %247 = bitcast <2 x i64>* %246 to <8 x i16>*
  store <8 x i16> %245, <8 x i16>* %247, align 16
  %248 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %244, <8 x i16> %243) #9
  %249 = bitcast <2 x i64>* %60 to <8 x i16>*
  store <8 x i16> %248, <8 x i16>* %249, align 16
  %250 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 54), align 8
  %251 = trunc i32 %250 to i16
  %252 = shl i16 %251, 3
  %253 = insertelement <8 x i16> undef, i16 %252, i32 0
  %254 = shufflevector <8 x i16> %253, <8 x i16> undef, <8 x i32> zeroinitializer
  %255 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 10), align 8
  %256 = trunc i32 %255 to i16
  %257 = shl i16 %256, 3
  %258 = insertelement <8 x i16> undef, i16 %257, i32 0
  %259 = shufflevector <8 x i16> %258, <8 x i16> undef, <8 x i32> zeroinitializer
  %260 = bitcast <2 x i64> %53 to <8 x i16>
  %261 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %260, <8 x i16> %254) #9
  %262 = bitcast <2 x i64>* %54 to <8 x i16>*
  store <8 x i16> %261, <8 x i16>* %262, align 16
  %263 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %260, <8 x i16> %259) #9
  %264 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 27
  %265 = bitcast <2 x i64>* %264 to <8 x i16>*
  store <8 x i16> %263, <8 x i16>* %265, align 16
  %266 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 58), align 8
  %267 = trunc i32 %266 to i16
  %268 = shl i16 %267, 3
  %269 = sub i16 0, %268
  %270 = insertelement <8 x i16> undef, i16 %269, i32 0
  %271 = shufflevector <8 x i16> %270, <8 x i16> undef, <8 x i32> zeroinitializer
  %272 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 6), align 8
  %273 = trunc i32 %272 to i16
  %274 = shl i16 %273, 3
  %275 = insertelement <8 x i16> undef, i16 %274, i32 0
  %276 = shufflevector <8 x i16> %275, <8 x i16> undef, <8 x i32> zeroinitializer
  %277 = bitcast <2 x i64> %56 to <8 x i16>
  %278 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %277, <8 x i16> %271) #9
  %279 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 23
  %280 = bitcast <2 x i64>* %279 to <8 x i16>*
  store <8 x i16> %278, <8 x i16>* %280, align 16
  %281 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %277, <8 x i16> %276) #9
  %282 = bitcast <2 x i64>* %57 to <8 x i16>*
  store <8 x i16> %281, <8 x i16>* %282, align 16
  %283 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 33
  %284 = bitcast <2 x i64>* %283 to <8 x i16>*
  store <8 x i16> %99, <8 x i16>* %284, align 16
  %285 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 34
  %286 = bitcast <2 x i64>* %285 to <8 x i16>*
  store <8 x i16> %115, <8 x i16>* %286, align 16
  %287 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 37
  %288 = bitcast <2 x i64>* %287 to <8 x i16>*
  store <8 x i16> %130, <8 x i16>* %288, align 16
  %289 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 38
  %290 = bitcast <2 x i64>* %289 to <8 x i16>*
  store <8 x i16> %146, <8 x i16>* %290, align 16
  %291 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 41
  %292 = bitcast <2 x i64>* %291 to <8 x i16>*
  store <8 x i16> %162, <8 x i16>* %292, align 16
  %293 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 42
  %294 = bitcast <2 x i64>* %293 to <8 x i16>*
  store <8 x i16> %179, <8 x i16>* %294, align 16
  %295 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 45
  %296 = bitcast <2 x i64>* %295 to <8 x i16>*
  store <8 x i16> %195, <8 x i16>* %296, align 16
  %297 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 46
  %298 = bitcast <2 x i64>* %297 to <8 x i16>*
  store <8 x i16> %212, <8 x i16>* %298, align 16
  %299 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 49
  %300 = bitcast <2 x i64>* %299 to <8 x i16>*
  store <8 x i16> %215, <8 x i16>* %300, align 16
  %301 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 50
  %302 = bitcast <2 x i64>* %301 to <8 x i16>*
  store <8 x i16> %197, <8 x i16>* %302, align 16
  %303 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 53
  %304 = bitcast <2 x i64>* %303 to <8 x i16>*
  store <8 x i16> %182, <8 x i16>* %304, align 16
  %305 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 54
  %306 = bitcast <2 x i64>* %305 to <8 x i16>*
  store <8 x i16> %164, <8 x i16>* %306, align 16
  %307 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 57
  %308 = bitcast <2 x i64>* %307 to <8 x i16>*
  store <8 x i16> %149, <8 x i16>* %308, align 16
  %309 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 58
  %310 = bitcast <2 x i64>* %309 to <8 x i16>*
  store <8 x i16> %132, <8 x i16>* %310, align 16
  %311 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 61
  %312 = bitcast <2 x i64>* %311 to <8 x i16>*
  store <8 x i16> %118, <8 x i16>* %312, align 16
  %313 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 62
  %314 = bitcast <2 x i64>* %313 to <8 x i16>*
  store <8 x i16> %101, <8 x i16>* %314, align 16
  %315 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 60), align 16
  %316 = trunc i32 %315 to i16
  %317 = shl i16 %316, 3
  %318 = insertelement <8 x i16> undef, i16 %317, i32 0
  %319 = shufflevector <8 x i16> %318, <8 x i16> undef, <8 x i32> zeroinitializer
  %320 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 4), align 16
  %321 = trunc i32 %320 to i16
  %322 = shl i16 %321, 3
  %323 = insertelement <8 x i16> undef, i16 %322, i32 0
  %324 = shufflevector <8 x i16> %323, <8 x i16> undef, <8 x i32> zeroinitializer
  %325 = bitcast <2 x i64> %44 to <8 x i16>
  %326 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %325, <8 x i16> %319) #9
  %327 = bitcast <2 x i64>* %45 to <8 x i16>*
  store <8 x i16> %326, <8 x i16>* %327, align 16
  %328 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %325, <8 x i16> %324) #9
  %329 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 15
  %330 = bitcast <2 x i64>* %329 to <8 x i16>*
  store <8 x i16> %328, <8 x i16>* %330, align 16
  %331 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 52), align 16
  %332 = trunc i32 %331 to i16
  %333 = shl i16 %332, 3
  %334 = sub i16 0, %333
  %335 = insertelement <8 x i16> undef, i16 %334, i32 0
  %336 = shufflevector <8 x i16> %335, <8 x i16> undef, <8 x i32> zeroinitializer
  %337 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 12), align 16
  %338 = trunc i32 %337 to i16
  %339 = shl i16 %338, 3
  %340 = insertelement <8 x i16> undef, i16 %339, i32 0
  %341 = shufflevector <8 x i16> %340, <8 x i16> undef, <8 x i32> zeroinitializer
  %342 = bitcast <2 x i64> %47 to <8 x i16>
  %343 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %342, <8 x i16> %336) #9
  %344 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 11
  %345 = bitcast <2 x i64>* %344 to <8 x i16>*
  store <8 x i16> %343, <8 x i16>* %345, align 16
  %346 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %342, <8 x i16> %341) #9
  %347 = bitcast <2 x i64>* %48 to <8 x i16>*
  store <8 x i16> %346, <8 x i16>* %347, align 16
  %348 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 17
  %349 = bitcast <2 x i64>* %348 to <8 x i16>*
  store <8 x i16> %228, <8 x i16>* %349, align 16
  %350 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 18
  %351 = bitcast <2 x i64>* %350 to <8 x i16>*
  store <8 x i16> %245, <8 x i16>* %351, align 16
  %352 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 21
  %353 = bitcast <2 x i64>* %352 to <8 x i16>*
  store <8 x i16> %261, <8 x i16>* %353, align 16
  %354 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 22
  %355 = bitcast <2 x i64>* %354 to <8 x i16>*
  store <8 x i16> %278, <8 x i16>* %355, align 16
  %356 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 25
  %357 = bitcast <2 x i64>* %356 to <8 x i16>*
  store <8 x i16> %281, <8 x i16>* %357, align 16
  %358 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 26
  %359 = bitcast <2 x i64>* %358 to <8 x i16>*
  store <8 x i16> %263, <8 x i16>* %359, align 16
  %360 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 29
  %361 = bitcast <2 x i64>* %360 to <8 x i16>*
  store <8 x i16> %248, <8 x i16>* %361, align 16
  %362 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 30
  %363 = bitcast <2 x i64>* %362 to <8 x i16>*
  store <8 x i16> %230, <8 x i16>* %363, align 16
  call fastcc void @idct64_stage4_high32_sse2(<2 x i64>* nonnull %39, <2 x i64> <i64 8796093024256, i64 8796093024256>, i8 signext %2)
  %364 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 56), align 16
  %365 = trunc i32 %364 to i16
  %366 = shl i16 %365, 3
  %367 = insertelement <8 x i16> undef, i16 %366, i32 0
  %368 = shufflevector <8 x i16> %367, <8 x i16> undef, <8 x i32> zeroinitializer
  %369 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 8), align 16
  %370 = trunc i32 %369 to i16
  %371 = shl i16 %370, 3
  %372 = insertelement <8 x i16> undef, i16 %371, i32 0
  %373 = shufflevector <8 x i16> %372, <8 x i16> undef, <8 x i32> zeroinitializer
  %374 = bitcast <2 x i64>* %42 to <8 x i16>*
  %375 = load <8 x i16>, <8 x i16>* %374, align 16
  %376 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %375, <8 x i16> %368) #9
  store <8 x i16> %376, <8 x i16>* %374, align 16
  %377 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %375, <8 x i16> %373) #9
  %378 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 7
  %379 = bitcast <2 x i64>* %378 to <8 x i16>*
  store <8 x i16> %377, <8 x i16>* %379, align 16
  %380 = load <2 x i64>, <2 x i64>* %45, align 16
  %381 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 9
  store <2 x i64> %380, <2 x i64>* %381, align 16
  %382 = load <2 x i64>, <2 x i64>* %344, align 16
  %383 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 10
  store <2 x i64> %382, <2 x i64>* %383, align 16
  %384 = load <2 x i64>, <2 x i64>* %48, align 16
  %385 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 13
  store <2 x i64> %384, <2 x i64>* %385, align 16
  %386 = load <2 x i64>, <2 x i64>* %329, align 16
  %387 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 14
  store <2 x i64> %386, <2 x i64>* %387, align 16
  %388 = sub i32 0, %369
  %389 = and i32 %388, 65535
  %390 = shl i32 %364, 16
  %391 = or i32 %389, %390
  %392 = insertelement <4 x i32> undef, i32 %391, i32 0
  %393 = shufflevector <4 x i32> %392, <4 x i32> undef, <4 x i32> zeroinitializer
  %394 = and i32 %364, 65535
  %395 = shl i32 %369, 16
  %396 = or i32 %395, %394
  %397 = insertelement <4 x i32> undef, i32 %396, i32 0
  %398 = shufflevector <4 x i32> %397, <4 x i32> undef, <4 x i32> zeroinitializer
  %399 = sub i32 0, %364
  %400 = and i32 %399, 65535
  %401 = sub i32 0, %395
  %402 = or i32 %400, %401
  %403 = insertelement <4 x i32> undef, i32 %402, i32 0
  %404 = shufflevector <4 x i32> %403, <4 x i32> undef, <4 x i32> zeroinitializer
  %405 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 40), align 16
  %406 = sub i32 0, %405
  %407 = and i32 %406, 65535
  %408 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 24), align 16
  %409 = shl i32 %408, 16
  %410 = or i32 %409, %407
  %411 = insertelement <4 x i32> undef, i32 %410, i32 0
  %412 = shufflevector <4 x i32> %411, <4 x i32> undef, <4 x i32> zeroinitializer
  %413 = and i32 %408, 65535
  %414 = shl i32 %405, 16
  %415 = or i32 %413, %414
  %416 = insertelement <4 x i32> undef, i32 %415, i32 0
  %417 = shufflevector <4 x i32> %416, <4 x i32> undef, <4 x i32> zeroinitializer
  %418 = sub i32 0, %408
  %419 = and i32 %418, 65535
  %420 = sub i32 0, %414
  %421 = or i32 %419, %420
  %422 = insertelement <4 x i32> undef, i32 %421, i32 0
  %423 = shufflevector <4 x i32> %422, <4 x i32> undef, <4 x i32> zeroinitializer
  %424 = load <8 x i16>, <8 x i16>* %349, align 16
  %425 = load <8 x i16>, <8 x i16>* %363, align 16
  %426 = shufflevector <8 x i16> %424, <8 x i16> %425, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %427 = shufflevector <8 x i16> %424, <8 x i16> %425, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %428 = bitcast <4 x i32> %393 to <8 x i16>
  %429 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %426, <8 x i16> %428) #9
  %430 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %427, <8 x i16> %428) #9
  %431 = bitcast <4 x i32> %398 to <8 x i16>
  %432 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %426, <8 x i16> %431) #9
  %433 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %427, <8 x i16> %431) #9
  %434 = add <4 x i32> %429, <i32 2048, i32 2048, i32 2048, i32 2048>
  %435 = add <4 x i32> %430, <i32 2048, i32 2048, i32 2048, i32 2048>
  %436 = add <4 x i32> %432, <i32 2048, i32 2048, i32 2048, i32 2048>
  %437 = add <4 x i32> %433, <i32 2048, i32 2048, i32 2048, i32 2048>
  %438 = sext i8 %2 to i32
  %439 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %434, i32 %438) #9
  %440 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %435, i32 %438) #9
  %441 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %436, i32 %438) #9
  %442 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %437, i32 %438) #9
  %443 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %439, <4 x i32> %440) #9
  store <8 x i16> %443, <8 x i16>* %349, align 16
  %444 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %441, <4 x i32> %442) #9
  store <8 x i16> %444, <8 x i16>* %363, align 16
  %445 = load <8 x i16>, <8 x i16>* %351, align 16
  %446 = load <8 x i16>, <8 x i16>* %361, align 16
  %447 = shufflevector <8 x i16> %445, <8 x i16> %446, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %448 = shufflevector <8 x i16> %445, <8 x i16> %446, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %449 = bitcast <4 x i32> %404 to <8 x i16>
  %450 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %447, <8 x i16> %449) #9
  %451 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %448, <8 x i16> %449) #9
  %452 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %447, <8 x i16> %428) #9
  %453 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %448, <8 x i16> %428) #9
  %454 = add <4 x i32> %450, <i32 2048, i32 2048, i32 2048, i32 2048>
  %455 = add <4 x i32> %451, <i32 2048, i32 2048, i32 2048, i32 2048>
  %456 = add <4 x i32> %452, <i32 2048, i32 2048, i32 2048, i32 2048>
  %457 = add <4 x i32> %453, <i32 2048, i32 2048, i32 2048, i32 2048>
  %458 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %454, i32 %438) #9
  %459 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %455, i32 %438) #9
  %460 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %456, i32 %438) #9
  %461 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %457, i32 %438) #9
  %462 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %458, <4 x i32> %459) #9
  store <8 x i16> %462, <8 x i16>* %351, align 16
  %463 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %460, <4 x i32> %461) #9
  store <8 x i16> %463, <8 x i16>* %361, align 16
  %464 = load <8 x i16>, <8 x i16>* %353, align 16
  %465 = load <8 x i16>, <8 x i16>* %359, align 16
  %466 = shufflevector <8 x i16> %464, <8 x i16> %465, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %467 = shufflevector <8 x i16> %464, <8 x i16> %465, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %468 = bitcast <4 x i32> %412 to <8 x i16>
  %469 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %466, <8 x i16> %468) #9
  %470 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %467, <8 x i16> %468) #9
  %471 = bitcast <4 x i32> %417 to <8 x i16>
  %472 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %466, <8 x i16> %471) #9
  %473 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %467, <8 x i16> %471) #9
  %474 = add <4 x i32> %469, <i32 2048, i32 2048, i32 2048, i32 2048>
  %475 = add <4 x i32> %470, <i32 2048, i32 2048, i32 2048, i32 2048>
  %476 = add <4 x i32> %472, <i32 2048, i32 2048, i32 2048, i32 2048>
  %477 = add <4 x i32> %473, <i32 2048, i32 2048, i32 2048, i32 2048>
  %478 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %474, i32 %438) #9
  %479 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %475, i32 %438) #9
  %480 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %476, i32 %438) #9
  %481 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %477, i32 %438) #9
  %482 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %478, <4 x i32> %479) #9
  store <8 x i16> %482, <8 x i16>* %353, align 16
  %483 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %480, <4 x i32> %481) #9
  store <8 x i16> %483, <8 x i16>* %359, align 16
  %484 = load <8 x i16>, <8 x i16>* %355, align 16
  %485 = load <8 x i16>, <8 x i16>* %357, align 16
  %486 = shufflevector <8 x i16> %484, <8 x i16> %485, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %487 = shufflevector <8 x i16> %484, <8 x i16> %485, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %488 = bitcast <4 x i32> %423 to <8 x i16>
  %489 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %486, <8 x i16> %488) #9
  %490 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %487, <8 x i16> %488) #9
  %491 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %486, <8 x i16> %468) #9
  %492 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %487, <8 x i16> %468) #9
  %493 = add <4 x i32> %489, <i32 2048, i32 2048, i32 2048, i32 2048>
  %494 = add <4 x i32> %490, <i32 2048, i32 2048, i32 2048, i32 2048>
  %495 = add <4 x i32> %491, <i32 2048, i32 2048, i32 2048, i32 2048>
  %496 = add <4 x i32> %492, <i32 2048, i32 2048, i32 2048, i32 2048>
  %497 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %493, i32 %438) #9
  %498 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %494, i32 %438) #9
  %499 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %495, i32 %438) #9
  %500 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %496, i32 %438) #9
  %501 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %497, <4 x i32> %498) #9
  store <8 x i16> %501, <8 x i16>* %355, align 16
  %502 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %499, <4 x i32> %500) #9
  store <8 x i16> %502, <8 x i16>* %357, align 16
  %503 = load <8 x i16>, <8 x i16>* %100, align 16
  %504 = load <8 x i16>, <8 x i16>* %117, align 16
  %505 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %503, <8 x i16> %504) #9
  store <8 x i16> %505, <8 x i16>* %100, align 16
  %506 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %503, <8 x i16> %504) #9
  store <8 x i16> %506, <8 x i16>* %117, align 16
  %507 = load <8 x i16>, <8 x i16>* %284, align 16
  %508 = load <8 x i16>, <8 x i16>* %286, align 16
  %509 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %507, <8 x i16> %508) #9
  store <8 x i16> %509, <8 x i16>* %284, align 16
  %510 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %507, <8 x i16> %508) #9
  store <8 x i16> %510, <8 x i16>* %286, align 16
  %511 = load <8 x i16>, <8 x i16>* %148, align 16
  %512 = load <8 x i16>, <8 x i16>* %131, align 16
  %513 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %511, <8 x i16> %512) #9
  store <8 x i16> %513, <8 x i16>* %131, align 16
  %514 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %511, <8 x i16> %512) #9
  store <8 x i16> %514, <8 x i16>* %148, align 16
  %515 = load <8 x i16>, <8 x i16>* %290, align 16
  %516 = load <8 x i16>, <8 x i16>* %288, align 16
  %517 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %515, <8 x i16> %516) #9
  store <8 x i16> %517, <8 x i16>* %288, align 16
  %518 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %515, <8 x i16> %516) #9
  store <8 x i16> %518, <8 x i16>* %290, align 16
  %519 = load <8 x i16>, <8 x i16>* %163, align 16
  %520 = load <8 x i16>, <8 x i16>* %181, align 16
  %521 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %519, <8 x i16> %520) #9
  store <8 x i16> %521, <8 x i16>* %163, align 16
  %522 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %519, <8 x i16> %520) #9
  store <8 x i16> %522, <8 x i16>* %181, align 16
  %523 = load <8 x i16>, <8 x i16>* %292, align 16
  %524 = load <8 x i16>, <8 x i16>* %294, align 16
  %525 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %523, <8 x i16> %524) #9
  store <8 x i16> %525, <8 x i16>* %292, align 16
  %526 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %523, <8 x i16> %524) #9
  store <8 x i16> %526, <8 x i16>* %294, align 16
  %527 = load <8 x i16>, <8 x i16>* %214, align 16
  %528 = load <8 x i16>, <8 x i16>* %196, align 16
  %529 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %527, <8 x i16> %528) #9
  store <8 x i16> %529, <8 x i16>* %196, align 16
  %530 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %527, <8 x i16> %528) #9
  store <8 x i16> %530, <8 x i16>* %214, align 16
  %531 = load <8 x i16>, <8 x i16>* %298, align 16
  %532 = load <8 x i16>, <8 x i16>* %296, align 16
  %533 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %531, <8 x i16> %532) #9
  store <8 x i16> %533, <8 x i16>* %296, align 16
  %534 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %531, <8 x i16> %532) #9
  store <8 x i16> %534, <8 x i16>* %298, align 16
  %535 = load <8 x i16>, <8 x i16>* %216, align 16
  %536 = load <8 x i16>, <8 x i16>* %199, align 16
  %537 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %535, <8 x i16> %536) #9
  store <8 x i16> %537, <8 x i16>* %216, align 16
  %538 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %535, <8 x i16> %536) #9
  store <8 x i16> %538, <8 x i16>* %199, align 16
  %539 = load <8 x i16>, <8 x i16>* %300, align 16
  %540 = load <8 x i16>, <8 x i16>* %302, align 16
  %541 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %539, <8 x i16> %540) #9
  store <8 x i16> %541, <8 x i16>* %300, align 16
  %542 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %539, <8 x i16> %540) #9
  store <8 x i16> %542, <8 x i16>* %302, align 16
  %543 = load <8 x i16>, <8 x i16>* %166, align 16
  %544 = load <8 x i16>, <8 x i16>* %183, align 16
  %545 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %543, <8 x i16> %544) #9
  store <8 x i16> %545, <8 x i16>* %183, align 16
  %546 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %543, <8 x i16> %544) #9
  store <8 x i16> %546, <8 x i16>* %166, align 16
  %547 = load <8 x i16>, <8 x i16>* %306, align 16
  %548 = load <8 x i16>, <8 x i16>* %304, align 16
  %549 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %547, <8 x i16> %548) #9
  store <8 x i16> %549, <8 x i16>* %304, align 16
  %550 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %547, <8 x i16> %548) #9
  store <8 x i16> %550, <8 x i16>* %306, align 16
  %551 = load <8 x i16>, <8 x i16>* %150, align 16
  %552 = load <8 x i16>, <8 x i16>* %134, align 16
  %553 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %551, <8 x i16> %552) #9
  store <8 x i16> %553, <8 x i16>* %150, align 16
  %554 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %551, <8 x i16> %552) #9
  store <8 x i16> %554, <8 x i16>* %134, align 16
  %555 = load <8 x i16>, <8 x i16>* %308, align 16
  %556 = load <8 x i16>, <8 x i16>* %310, align 16
  %557 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %555, <8 x i16> %556) #9
  store <8 x i16> %557, <8 x i16>* %308, align 16
  %558 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %555, <8 x i16> %556) #9
  store <8 x i16> %558, <8 x i16>* %310, align 16
  %559 = load <8 x i16>, <8 x i16>* %103, align 16
  %560 = load <8 x i16>, <8 x i16>* %119, align 16
  %561 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %559, <8 x i16> %560) #9
  store <8 x i16> %561, <8 x i16>* %119, align 16
  %562 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %559, <8 x i16> %560) #9
  store <8 x i16> %562, <8 x i16>* %103, align 16
  %563 = load <8 x i16>, <8 x i16>* %314, align 16
  %564 = load <8 x i16>, <8 x i16>* %312, align 16
  %565 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %563, <8 x i16> %564) #9
  store <8 x i16> %565, <8 x i16>* %312, align 16
  %566 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %563, <8 x i16> %564) #9
  store <8 x i16> %566, <8 x i16>* %314, align 16
  %567 = trunc i32 %5 to i16
  %568 = shl i16 %567, 3
  %569 = insertelement <8 x i16> undef, i16 %568, i32 0
  %570 = shufflevector <8 x i16> %569, <8 x i16> undef, <8 x i32> zeroinitializer
  %571 = bitcast [64 x <2 x i64>]* %4 to <8 x i16>*
  %572 = load <8 x i16>, <8 x i16>* %571, align 16
  %573 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %572, <8 x i16> %570) #9
  store <8 x i16> %573, <8 x i16>* %571, align 16
  %574 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 1
  %575 = bitcast <2 x i64>* %574 to <8 x i16>*
  store <8 x i16> %573, <8 x i16>* %575, align 16
  %576 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 5
  %577 = bitcast <2 x i64>* %576 to <8 x i16>*
  store <8 x i16> %376, <8 x i16>* %577, align 16
  %578 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 6
  %579 = bitcast <2 x i64>* %578 to <8 x i16>*
  store <8 x i16> %377, <8 x i16>* %579, align 16
  %580 = bitcast <2 x i64> %380 to <8 x i16>
  %581 = bitcast <2 x i64> %386 to <8 x i16>
  %582 = shufflevector <8 x i16> %580, <8 x i16> %581, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %583 = shufflevector <8 x i16> %580, <8 x i16> %581, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %584 = bitcast <4 x i32> %18 to <8 x i16>
  %585 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %582, <8 x i16> %584) #9
  %586 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %583, <8 x i16> %584) #9
  %587 = bitcast <4 x i32> %23 to <8 x i16>
  %588 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %582, <8 x i16> %587) #9
  %589 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %583, <8 x i16> %587) #9
  %590 = add <4 x i32> %585, <i32 2048, i32 2048, i32 2048, i32 2048>
  %591 = add <4 x i32> %586, <i32 2048, i32 2048, i32 2048, i32 2048>
  %592 = add <4 x i32> %588, <i32 2048, i32 2048, i32 2048, i32 2048>
  %593 = add <4 x i32> %589, <i32 2048, i32 2048, i32 2048, i32 2048>
  %594 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %590, i32 %438) #9
  %595 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %591, i32 %438) #9
  %596 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %592, i32 %438) #9
  %597 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %593, i32 %438) #9
  %598 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %594, <4 x i32> %595) #9
  %599 = bitcast <2 x i64>* %381 to <8 x i16>*
  store <8 x i16> %598, <8 x i16>* %599, align 16
  %600 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %596, <4 x i32> %597) #9
  %601 = bitcast <2 x i64>* %387 to <8 x i16>*
  store <8 x i16> %600, <8 x i16>* %601, align 16
  %602 = bitcast <2 x i64> %382 to <8 x i16>
  %603 = bitcast <2 x i64> %384 to <8 x i16>
  %604 = shufflevector <8 x i16> %602, <8 x i16> %603, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %605 = shufflevector <8 x i16> %602, <8 x i16> %603, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %606 = bitcast <4 x i32> %29 to <8 x i16>
  %607 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %604, <8 x i16> %606) #9
  %608 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %605, <8 x i16> %606) #9
  %609 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %604, <8 x i16> %584) #9
  %610 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %605, <8 x i16> %584) #9
  %611 = add <4 x i32> %607, <i32 2048, i32 2048, i32 2048, i32 2048>
  %612 = add <4 x i32> %608, <i32 2048, i32 2048, i32 2048, i32 2048>
  %613 = add <4 x i32> %609, <i32 2048, i32 2048, i32 2048, i32 2048>
  %614 = add <4 x i32> %610, <i32 2048, i32 2048, i32 2048, i32 2048>
  %615 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %611, i32 %438) #9
  %616 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %612, i32 %438) #9
  %617 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %613, i32 %438) #9
  %618 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %614, i32 %438) #9
  %619 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %615, <4 x i32> %616) #9
  %620 = bitcast <2 x i64>* %383 to <8 x i16>*
  store <8 x i16> %619, <8 x i16>* %620, align 16
  %621 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %617, <4 x i32> %618) #9
  %622 = bitcast <2 x i64>* %385 to <8 x i16>*
  store <8 x i16> %621, <8 x i16>* %622, align 16
  %623 = load <8 x i16>, <8 x i16>* %229, align 16
  %624 = load <8 x i16>, <8 x i16>* %247, align 16
  %625 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %623, <8 x i16> %624) #9
  store <8 x i16> %625, <8 x i16>* %229, align 16
  %626 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %623, <8 x i16> %624) #9
  store <8 x i16> %626, <8 x i16>* %247, align 16
  %627 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %443, <8 x i16> %462) #9
  store <8 x i16> %627, <8 x i16>* %349, align 16
  %628 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %443, <8 x i16> %462) #9
  store <8 x i16> %628, <8 x i16>* %351, align 16
  %629 = load <8 x i16>, <8 x i16>* %280, align 16
  %630 = load <8 x i16>, <8 x i16>* %262, align 16
  %631 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %629, <8 x i16> %630) #9
  store <8 x i16> %631, <8 x i16>* %262, align 16
  %632 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %629, <8 x i16> %630) #9
  store <8 x i16> %632, <8 x i16>* %280, align 16
  %633 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %501, <8 x i16> %482) #9
  store <8 x i16> %633, <8 x i16>* %353, align 16
  %634 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %501, <8 x i16> %482) #9
  store <8 x i16> %634, <8 x i16>* %355, align 16
  %635 = load <8 x i16>, <8 x i16>* %282, align 16
  %636 = load <8 x i16>, <8 x i16>* %265, align 16
  %637 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %635, <8 x i16> %636) #9
  store <8 x i16> %637, <8 x i16>* %282, align 16
  %638 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %635, <8 x i16> %636) #9
  store <8 x i16> %638, <8 x i16>* %265, align 16
  %639 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %502, <8 x i16> %483) #9
  store <8 x i16> %639, <8 x i16>* %357, align 16
  %640 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %502, <8 x i16> %483) #9
  store <8 x i16> %640, <8 x i16>* %359, align 16
  %641 = load <8 x i16>, <8 x i16>* %232, align 16
  %642 = load <8 x i16>, <8 x i16>* %249, align 16
  %643 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %641, <8 x i16> %642) #9
  store <8 x i16> %643, <8 x i16>* %249, align 16
  %644 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %641, <8 x i16> %642) #9
  store <8 x i16> %644, <8 x i16>* %232, align 16
  %645 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %444, <8 x i16> %463) #9
  store <8 x i16> %645, <8 x i16>* %361, align 16
  %646 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %444, <8 x i16> %463) #9
  store <8 x i16> %646, <8 x i16>* %363, align 16
  %647 = shufflevector <8 x i16> %510, <8 x i16> %565, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %648 = shufflevector <8 x i16> %510, <8 x i16> %565, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %649 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %647, <8 x i16> %428) #9
  %650 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %648, <8 x i16> %428) #9
  %651 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %647, <8 x i16> %431) #9
  %652 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %648, <8 x i16> %431) #9
  %653 = add <4 x i32> %649, <i32 2048, i32 2048, i32 2048, i32 2048>
  %654 = add <4 x i32> %650, <i32 2048, i32 2048, i32 2048, i32 2048>
  %655 = add <4 x i32> %651, <i32 2048, i32 2048, i32 2048, i32 2048>
  %656 = add <4 x i32> %652, <i32 2048, i32 2048, i32 2048, i32 2048>
  %657 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %653, i32 %438) #9
  %658 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %654, i32 %438) #9
  %659 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %655, i32 %438) #9
  %660 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %656, i32 %438) #9
  %661 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %657, <4 x i32> %658) #9
  store <8 x i16> %661, <8 x i16>* %286, align 16
  %662 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %659, <4 x i32> %660) #9
  store <8 x i16> %662, <8 x i16>* %312, align 16
  %663 = shufflevector <8 x i16> %506, <8 x i16> %561, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %664 = shufflevector <8 x i16> %506, <8 x i16> %561, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %665 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %663, <8 x i16> %428) #9
  %666 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %664, <8 x i16> %428) #9
  %667 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %663, <8 x i16> %431) #9
  %668 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %664, <8 x i16> %431) #9
  %669 = add <4 x i32> %665, <i32 2048, i32 2048, i32 2048, i32 2048>
  %670 = add <4 x i32> %666, <i32 2048, i32 2048, i32 2048, i32 2048>
  %671 = add <4 x i32> %667, <i32 2048, i32 2048, i32 2048, i32 2048>
  %672 = add <4 x i32> %668, <i32 2048, i32 2048, i32 2048, i32 2048>
  %673 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %669, i32 %438) #9
  %674 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %670, i32 %438) #9
  %675 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %671, i32 %438) #9
  %676 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %672, i32 %438) #9
  %677 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %673, <4 x i32> %674) #9
  store <8 x i16> %677, <8 x i16>* %117, align 16
  %678 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %675, <4 x i32> %676) #9
  store <8 x i16> %678, <8 x i16>* %119, align 16
  %679 = shufflevector <8 x i16> %513, <8 x i16> %554, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %680 = shufflevector <8 x i16> %513, <8 x i16> %554, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %681 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %679, <8 x i16> %449) #9
  %682 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %680, <8 x i16> %449) #9
  %683 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %679, <8 x i16> %428) #9
  %684 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %680, <8 x i16> %428) #9
  %685 = add <4 x i32> %681, <i32 2048, i32 2048, i32 2048, i32 2048>
  %686 = add <4 x i32> %682, <i32 2048, i32 2048, i32 2048, i32 2048>
  %687 = add <4 x i32> %683, <i32 2048, i32 2048, i32 2048, i32 2048>
  %688 = add <4 x i32> %684, <i32 2048, i32 2048, i32 2048, i32 2048>
  %689 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %685, i32 %438) #9
  %690 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %686, i32 %438) #9
  %691 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %687, i32 %438) #9
  %692 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %688, i32 %438) #9
  %693 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %689, <4 x i32> %690) #9
  store <8 x i16> %693, <8 x i16>* %131, align 16
  %694 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %691, <4 x i32> %692) #9
  store <8 x i16> %694, <8 x i16>* %134, align 16
  %695 = shufflevector <8 x i16> %517, <8 x i16> %558, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %696 = shufflevector <8 x i16> %517, <8 x i16> %558, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %697 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %695, <8 x i16> %449) #9
  %698 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %696, <8 x i16> %449) #9
  %699 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %695, <8 x i16> %428) #9
  %700 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %696, <8 x i16> %428) #9
  %701 = add <4 x i32> %697, <i32 2048, i32 2048, i32 2048, i32 2048>
  %702 = add <4 x i32> %698, <i32 2048, i32 2048, i32 2048, i32 2048>
  %703 = add <4 x i32> %699, <i32 2048, i32 2048, i32 2048, i32 2048>
  %704 = add <4 x i32> %700, <i32 2048, i32 2048, i32 2048, i32 2048>
  %705 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %701, i32 %438) #9
  %706 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %702, i32 %438) #9
  %707 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %703, i32 %438) #9
  %708 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %704, i32 %438) #9
  %709 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %705, <4 x i32> %706) #9
  store <8 x i16> %709, <8 x i16>* %288, align 16
  %710 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %707, <4 x i32> %708) #9
  store <8 x i16> %710, <8 x i16>* %310, align 16
  %711 = shufflevector <8 x i16> %526, <8 x i16> %549, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %712 = shufflevector <8 x i16> %526, <8 x i16> %549, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %713 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %711, <8 x i16> %468) #9
  %714 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %712, <8 x i16> %468) #9
  %715 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %711, <8 x i16> %471) #9
  %716 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %712, <8 x i16> %471) #9
  %717 = add <4 x i32> %713, <i32 2048, i32 2048, i32 2048, i32 2048>
  %718 = add <4 x i32> %714, <i32 2048, i32 2048, i32 2048, i32 2048>
  %719 = add <4 x i32> %715, <i32 2048, i32 2048, i32 2048, i32 2048>
  %720 = add <4 x i32> %716, <i32 2048, i32 2048, i32 2048, i32 2048>
  %721 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %717, i32 %438) #9
  %722 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %718, i32 %438) #9
  %723 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %719, i32 %438) #9
  %724 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %720, i32 %438) #9
  %725 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %721, <4 x i32> %722) #9
  store <8 x i16> %725, <8 x i16>* %294, align 16
  %726 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %723, <4 x i32> %724) #9
  store <8 x i16> %726, <8 x i16>* %304, align 16
  %727 = shufflevector <8 x i16> %522, <8 x i16> %545, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %728 = shufflevector <8 x i16> %522, <8 x i16> %545, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %729 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %727, <8 x i16> %468) #9
  %730 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %728, <8 x i16> %468) #9
  %731 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %727, <8 x i16> %471) #9
  %732 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %728, <8 x i16> %471) #9
  %733 = add <4 x i32> %729, <i32 2048, i32 2048, i32 2048, i32 2048>
  %734 = add <4 x i32> %730, <i32 2048, i32 2048, i32 2048, i32 2048>
  %735 = add <4 x i32> %731, <i32 2048, i32 2048, i32 2048, i32 2048>
  %736 = add <4 x i32> %732, <i32 2048, i32 2048, i32 2048, i32 2048>
  %737 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %733, i32 %438) #9
  %738 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %734, i32 %438) #9
  %739 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %735, i32 %438) #9
  %740 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %736, i32 %438) #9
  %741 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %737, <4 x i32> %738) #9
  store <8 x i16> %741, <8 x i16>* %181, align 16
  %742 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %739, <4 x i32> %740) #9
  store <8 x i16> %742, <8 x i16>* %183, align 16
  %743 = shufflevector <8 x i16> %529, <8 x i16> %538, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %744 = shufflevector <8 x i16> %529, <8 x i16> %538, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %745 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %743, <8 x i16> %488) #9
  %746 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %744, <8 x i16> %488) #9
  %747 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %743, <8 x i16> %468) #9
  %748 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %744, <8 x i16> %468) #9
  %749 = add <4 x i32> %745, <i32 2048, i32 2048, i32 2048, i32 2048>
  %750 = add <4 x i32> %746, <i32 2048, i32 2048, i32 2048, i32 2048>
  %751 = add <4 x i32> %747, <i32 2048, i32 2048, i32 2048, i32 2048>
  %752 = add <4 x i32> %748, <i32 2048, i32 2048, i32 2048, i32 2048>
  %753 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %749, i32 %438) #9
  %754 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %750, i32 %438) #9
  %755 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %751, i32 %438) #9
  %756 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %752, i32 %438) #9
  %757 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %753, <4 x i32> %754) #9
  store <8 x i16> %757, <8 x i16>* %196, align 16
  %758 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %755, <4 x i32> %756) #9
  store <8 x i16> %758, <8 x i16>* %199, align 16
  %759 = shufflevector <8 x i16> %533, <8 x i16> %542, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %760 = shufflevector <8 x i16> %533, <8 x i16> %542, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %761 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %759, <8 x i16> %488) #9
  %762 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %760, <8 x i16> %488) #9
  %763 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %759, <8 x i16> %468) #9
  %764 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %760, <8 x i16> %468) #9
  %765 = add <4 x i32> %761, <i32 2048, i32 2048, i32 2048, i32 2048>
  %766 = add <4 x i32> %762, <i32 2048, i32 2048, i32 2048, i32 2048>
  %767 = add <4 x i32> %763, <i32 2048, i32 2048, i32 2048, i32 2048>
  %768 = add <4 x i32> %764, <i32 2048, i32 2048, i32 2048, i32 2048>
  %769 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %765, i32 %438) #9
  %770 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %766, i32 %438) #9
  %771 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %767, i32 %438) #9
  %772 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %768, i32 %438) #9
  %773 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %769, <4 x i32> %770) #9
  store <8 x i16> %773, <8 x i16>* %296, align 16
  %774 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %771, <4 x i32> %772) #9
  store <8 x i16> %774, <8 x i16>* %302, align 16
  %775 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 3
  %776 = bitcast <2 x i64>* %775 to <8 x i16>*
  store <8 x i16> %573, <8 x i16>* %776, align 16
  %777 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 2
  %778 = bitcast <2 x i64>* %777 to <8 x i16>*
  store <8 x i16> %573, <8 x i16>* %778, align 16
  %779 = shufflevector <8 x i16> %376, <8 x i16> %377, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %780 = shufflevector <8 x i16> %376, <8 x i16> %377, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %781 = bitcast <4 x i32> %34 to <8 x i16>
  %782 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %779, <8 x i16> %781) #9
  %783 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %780, <8 x i16> %781) #9
  %784 = bitcast <4 x i32> %10 to <8 x i16>
  %785 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %779, <8 x i16> %784) #9
  %786 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %780, <8 x i16> %784) #9
  %787 = add <4 x i32> %782, <i32 2048, i32 2048, i32 2048, i32 2048>
  %788 = add <4 x i32> %783, <i32 2048, i32 2048, i32 2048, i32 2048>
  %789 = add <4 x i32> %785, <i32 2048, i32 2048, i32 2048, i32 2048>
  %790 = add <4 x i32> %786, <i32 2048, i32 2048, i32 2048, i32 2048>
  %791 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %787, i32 %438) #9
  %792 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %788, i32 %438) #9
  %793 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %789, i32 %438) #9
  %794 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %790, i32 %438) #9
  %795 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %791, <4 x i32> %792) #9
  store <8 x i16> %795, <8 x i16>* %577, align 16
  %796 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %793, <4 x i32> %794) #9
  store <8 x i16> %796, <8 x i16>* %579, align 16
  %797 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %580, <8 x i16> %602) #9
  store <8 x i16> %797, <8 x i16>* %327, align 16
  %798 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %580, <8 x i16> %602) #9
  store <8 x i16> %798, <8 x i16>* %345, align 16
  %799 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %598, <8 x i16> %619) #9
  store <8 x i16> %799, <8 x i16>* %599, align 16
  %800 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %598, <8 x i16> %619) #9
  store <8 x i16> %800, <8 x i16>* %620, align 16
  %801 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %581, <8 x i16> %603) #9
  store <8 x i16> %801, <8 x i16>* %347, align 16
  %802 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %581, <8 x i16> %603) #9
  store <8 x i16> %802, <8 x i16>* %330, align 16
  %803 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %600, <8 x i16> %621) #9
  store <8 x i16> %803, <8 x i16>* %622, align 16
  %804 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %600, <8 x i16> %621) #9
  store <8 x i16> %804, <8 x i16>* %601, align 16
  %805 = shufflevector <8 x i16> %628, <8 x i16> %645, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %806 = shufflevector <8 x i16> %628, <8 x i16> %645, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %807 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %805, <8 x i16> %584) #9
  %808 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %806, <8 x i16> %584) #9
  %809 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %805, <8 x i16> %587) #9
  %810 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %806, <8 x i16> %587) #9
  %811 = add <4 x i32> %807, <i32 2048, i32 2048, i32 2048, i32 2048>
  %812 = add <4 x i32> %808, <i32 2048, i32 2048, i32 2048, i32 2048>
  %813 = add <4 x i32> %809, <i32 2048, i32 2048, i32 2048, i32 2048>
  %814 = add <4 x i32> %810, <i32 2048, i32 2048, i32 2048, i32 2048>
  %815 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %811, i32 %438) #9
  %816 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %812, i32 %438) #9
  %817 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %813, i32 %438) #9
  %818 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %814, i32 %438) #9
  %819 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %815, <4 x i32> %816) #9
  store <8 x i16> %819, <8 x i16>* %351, align 16
  %820 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %817, <4 x i32> %818) #9
  store <8 x i16> %820, <8 x i16>* %361, align 16
  %821 = shufflevector <8 x i16> %626, <8 x i16> %643, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %822 = shufflevector <8 x i16> %626, <8 x i16> %643, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %823 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %821, <8 x i16> %584) #9
  %824 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %822, <8 x i16> %584) #9
  %825 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %821, <8 x i16> %587) #9
  %826 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %822, <8 x i16> %587) #9
  %827 = add <4 x i32> %823, <i32 2048, i32 2048, i32 2048, i32 2048>
  %828 = add <4 x i32> %824, <i32 2048, i32 2048, i32 2048, i32 2048>
  %829 = add <4 x i32> %825, <i32 2048, i32 2048, i32 2048, i32 2048>
  %830 = add <4 x i32> %826, <i32 2048, i32 2048, i32 2048, i32 2048>
  %831 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %827, i32 %438) #9
  %832 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %828, i32 %438) #9
  %833 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %829, i32 %438) #9
  %834 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %830, i32 %438) #9
  %835 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %831, <4 x i32> %832) #9
  store <8 x i16> %835, <8 x i16>* %247, align 16
  %836 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %833, <4 x i32> %834) #9
  store <8 x i16> %836, <8 x i16>* %249, align 16
  %837 = shufflevector <8 x i16> %631, <8 x i16> %638, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %838 = shufflevector <8 x i16> %631, <8 x i16> %638, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %839 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %837, <8 x i16> %606) #9
  %840 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %838, <8 x i16> %606) #9
  %841 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %837, <8 x i16> %584) #9
  %842 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %838, <8 x i16> %584) #9
  %843 = add <4 x i32> %839, <i32 2048, i32 2048, i32 2048, i32 2048>
  %844 = add <4 x i32> %840, <i32 2048, i32 2048, i32 2048, i32 2048>
  %845 = add <4 x i32> %841, <i32 2048, i32 2048, i32 2048, i32 2048>
  %846 = add <4 x i32> %842, <i32 2048, i32 2048, i32 2048, i32 2048>
  %847 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %843, i32 %438) #9
  %848 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %844, i32 %438) #9
  %849 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %845, i32 %438) #9
  %850 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %846, i32 %438) #9
  %851 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %847, <4 x i32> %848) #9
  store <8 x i16> %851, <8 x i16>* %262, align 16
  %852 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %849, <4 x i32> %850) #9
  store <8 x i16> %852, <8 x i16>* %265, align 16
  %853 = shufflevector <8 x i16> %633, <8 x i16> %640, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %854 = shufflevector <8 x i16> %633, <8 x i16> %640, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %855 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %853, <8 x i16> %606) #9
  %856 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %854, <8 x i16> %606) #9
  %857 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %853, <8 x i16> %584) #9
  %858 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %854, <8 x i16> %584) #9
  %859 = add <4 x i32> %855, <i32 2048, i32 2048, i32 2048, i32 2048>
  %860 = add <4 x i32> %856, <i32 2048, i32 2048, i32 2048, i32 2048>
  %861 = add <4 x i32> %857, <i32 2048, i32 2048, i32 2048, i32 2048>
  %862 = add <4 x i32> %858, <i32 2048, i32 2048, i32 2048, i32 2048>
  %863 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %859, i32 %438) #9
  %864 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %860, i32 %438) #9
  %865 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %861, i32 %438) #9
  %866 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %862, i32 %438) #9
  %867 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %863, <4 x i32> %864) #9
  store <8 x i16> %867, <8 x i16>* %353, align 16
  %868 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %865, <4 x i32> %866) #9
  store <8 x i16> %868, <8 x i16>* %359, align 16
  %869 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %505, <8 x i16> %514) #9
  store <8 x i16> %869, <8 x i16>* %100, align 16
  %870 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %505, <8 x i16> %514) #9
  store <8 x i16> %870, <8 x i16>* %148, align 16
  %871 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %509, <8 x i16> %518) #9
  store <8 x i16> %871, <8 x i16>* %284, align 16
  %872 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %509, <8 x i16> %518) #9
  store <8 x i16> %872, <8 x i16>* %290, align 16
  %873 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %661, <8 x i16> %709) #9
  store <8 x i16> %873, <8 x i16>* %286, align 16
  %874 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %661, <8 x i16> %709) #9
  store <8 x i16> %874, <8 x i16>* %288, align 16
  %875 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %677, <8 x i16> %693) #9
  store <8 x i16> %875, <8 x i16>* %117, align 16
  %876 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %677, <8 x i16> %693) #9
  store <8 x i16> %876, <8 x i16>* %131, align 16
  %877 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %530, <8 x i16> %521) #9
  store <8 x i16> %877, <8 x i16>* %163, align 16
  %878 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %530, <8 x i16> %521) #9
  store <8 x i16> %878, <8 x i16>* %214, align 16
  %879 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %534, <8 x i16> %525) #9
  store <8 x i16> %879, <8 x i16>* %292, align 16
  %880 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %534, <8 x i16> %525) #9
  store <8 x i16> %880, <8 x i16>* %298, align 16
  %881 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %773, <8 x i16> %725) #9
  store <8 x i16> %881, <8 x i16>* %294, align 16
  %882 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %773, <8 x i16> %725) #9
  store <8 x i16> %882, <8 x i16>* %296, align 16
  %883 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %757, <8 x i16> %741) #9
  store <8 x i16> %883, <8 x i16>* %181, align 16
  %884 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %757, <8 x i16> %741) #9
  store <8 x i16> %884, <8 x i16>* %196, align 16
  %885 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %537, <8 x i16> %546) #9
  store <8 x i16> %885, <8 x i16>* %216, align 16
  %886 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %537, <8 x i16> %546) #9
  store <8 x i16> %886, <8 x i16>* %166, align 16
  %887 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %541, <8 x i16> %550) #9
  store <8 x i16> %887, <8 x i16>* %300, align 16
  %888 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %541, <8 x i16> %550) #9
  store <8 x i16> %888, <8 x i16>* %306, align 16
  %889 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %774, <8 x i16> %726) #9
  store <8 x i16> %889, <8 x i16>* %302, align 16
  %890 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %774, <8 x i16> %726) #9
  store <8 x i16> %890, <8 x i16>* %304, align 16
  %891 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %758, <8 x i16> %742) #9
  store <8 x i16> %891, <8 x i16>* %199, align 16
  %892 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %758, <8 x i16> %742) #9
  store <8 x i16> %892, <8 x i16>* %183, align 16
  %893 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %562, <8 x i16> %553) #9
  store <8 x i16> %893, <8 x i16>* %150, align 16
  %894 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %562, <8 x i16> %553) #9
  store <8 x i16> %894, <8 x i16>* %103, align 16
  %895 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %566, <8 x i16> %557) #9
  store <8 x i16> %895, <8 x i16>* %308, align 16
  %896 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %566, <8 x i16> %557) #9
  store <8 x i16> %896, <8 x i16>* %314, align 16
  %897 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %662, <8 x i16> %710) #9
  store <8 x i16> %897, <8 x i16>* %310, align 16
  %898 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %662, <8 x i16> %710) #9
  store <8 x i16> %898, <8 x i16>* %312, align 16
  %899 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %678, <8 x i16> %694) #9
  store <8 x i16> %899, <8 x i16>* %134, align 16
  %900 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %678, <8 x i16> %694) #9
  store <8 x i16> %900, <8 x i16>* %119, align 16
  %901 = load <8 x i16>, <8 x i16>* %379, align 16
  %902 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %573, <8 x i16> %901) #9
  store <8 x i16> %902, <8 x i16>* %571, align 16
  %903 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %573, <8 x i16> %901) #9
  store <8 x i16> %903, <8 x i16>* %379, align 16
  %904 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %573, <8 x i16> %796) #9
  store <8 x i16> %904, <8 x i16>* %575, align 16
  %905 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %573, <8 x i16> %796) #9
  store <8 x i16> %905, <8 x i16>* %579, align 16
  %906 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %573, <8 x i16> %795) #9
  store <8 x i16> %906, <8 x i16>* %778, align 16
  %907 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %573, <8 x i16> %795) #9
  store <8 x i16> %907, <8 x i16>* %577, align 16
  %908 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %573, <8 x i16> %376) #9
  store <8 x i16> %908, <8 x i16>* %776, align 16
  %909 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %573, <8 x i16> %376) #9
  store <8 x i16> %909, <8 x i16>* %374, align 16
  %910 = shufflevector <8 x i16> %800, <8 x i16> %803, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %911 = shufflevector <8 x i16> %800, <8 x i16> %803, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %912 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %910, <8 x i16> %781) #9
  %913 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %911, <8 x i16> %781) #9
  %914 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %910, <8 x i16> %784) #9
  %915 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %911, <8 x i16> %784) #9
  %916 = add <4 x i32> %912, <i32 2048, i32 2048, i32 2048, i32 2048>
  %917 = add <4 x i32> %913, <i32 2048, i32 2048, i32 2048, i32 2048>
  %918 = add <4 x i32> %914, <i32 2048, i32 2048, i32 2048, i32 2048>
  %919 = add <4 x i32> %915, <i32 2048, i32 2048, i32 2048, i32 2048>
  %920 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %916, i32 %438) #9
  %921 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %917, i32 %438) #9
  %922 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %918, i32 %438) #9
  %923 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %919, i32 %438) #9
  %924 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %920, <4 x i32> %921) #9
  store <8 x i16> %924, <8 x i16>* %620, align 16
  %925 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %922, <4 x i32> %923) #9
  store <8 x i16> %925, <8 x i16>* %622, align 16
  %926 = shufflevector <8 x i16> %798, <8 x i16> %801, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %927 = shufflevector <8 x i16> %798, <8 x i16> %801, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %928 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %926, <8 x i16> %781) #9
  %929 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %927, <8 x i16> %781) #9
  %930 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %926, <8 x i16> %784) #9
  %931 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %927, <8 x i16> %784) #9
  %932 = add <4 x i32> %928, <i32 2048, i32 2048, i32 2048, i32 2048>
  %933 = add <4 x i32> %929, <i32 2048, i32 2048, i32 2048, i32 2048>
  %934 = add <4 x i32> %930, <i32 2048, i32 2048, i32 2048, i32 2048>
  %935 = add <4 x i32> %931, <i32 2048, i32 2048, i32 2048, i32 2048>
  %936 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %932, i32 %438) #9
  %937 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %933, i32 %438) #9
  %938 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %934, i32 %438) #9
  %939 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %935, i32 %438) #9
  %940 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %936, <4 x i32> %937) #9
  store <8 x i16> %940, <8 x i16>* %345, align 16
  %941 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %938, <4 x i32> %939) #9
  store <8 x i16> %941, <8 x i16>* %347, align 16
  %942 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %625, <8 x i16> %632) #9
  store <8 x i16> %942, <8 x i16>* %229, align 16
  %943 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %625, <8 x i16> %632) #9
  store <8 x i16> %943, <8 x i16>* %280, align 16
  %944 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %627, <8 x i16> %634) #9
  store <8 x i16> %944, <8 x i16>* %349, align 16
  %945 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %627, <8 x i16> %634) #9
  store <8 x i16> %945, <8 x i16>* %355, align 16
  %946 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %819, <8 x i16> %867) #9
  store <8 x i16> %946, <8 x i16>* %351, align 16
  %947 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %819, <8 x i16> %867) #9
  store <8 x i16> %947, <8 x i16>* %353, align 16
  %948 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %835, <8 x i16> %851) #9
  store <8 x i16> %948, <8 x i16>* %247, align 16
  %949 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %835, <8 x i16> %851) #9
  store <8 x i16> %949, <8 x i16>* %262, align 16
  %950 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %644, <8 x i16> %637) #9
  store <8 x i16> %950, <8 x i16>* %282, align 16
  %951 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %644, <8 x i16> %637) #9
  store <8 x i16> %951, <8 x i16>* %232, align 16
  %952 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %646, <8 x i16> %639) #9
  store <8 x i16> %952, <8 x i16>* %357, align 16
  %953 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %646, <8 x i16> %639) #9
  store <8 x i16> %953, <8 x i16>* %363, align 16
  %954 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %820, <8 x i16> %868) #9
  store <8 x i16> %954, <8 x i16>* %359, align 16
  %955 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %820, <8 x i16> %868) #9
  store <8 x i16> %955, <8 x i16>* %361, align 16
  %956 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %836, <8 x i16> %852) #9
  store <8 x i16> %956, <8 x i16>* %265, align 16
  %957 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %836, <8 x i16> %852) #9
  store <8 x i16> %957, <8 x i16>* %249, align 16
  %958 = shufflevector <8 x i16> %876, <8 x i16> %899, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %959 = shufflevector <8 x i16> %876, <8 x i16> %899, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %960 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %958, <8 x i16> %584) #9
  %961 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %959, <8 x i16> %584) #9
  %962 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %958, <8 x i16> %587) #9
  %963 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %959, <8 x i16> %587) #9
  %964 = add <4 x i32> %960, <i32 2048, i32 2048, i32 2048, i32 2048>
  %965 = add <4 x i32> %961, <i32 2048, i32 2048, i32 2048, i32 2048>
  %966 = add <4 x i32> %962, <i32 2048, i32 2048, i32 2048, i32 2048>
  %967 = add <4 x i32> %963, <i32 2048, i32 2048, i32 2048, i32 2048>
  %968 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %964, i32 %438) #9
  %969 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %965, i32 %438) #9
  %970 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %966, i32 %438) #9
  %971 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %967, i32 %438) #9
  %972 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %968, <4 x i32> %969) #9
  store <8 x i16> %972, <8 x i16>* %131, align 16
  %973 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %970, <4 x i32> %971) #9
  store <8 x i16> %973, <8 x i16>* %134, align 16
  %974 = shufflevector <8 x i16> %874, <8 x i16> %897, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %975 = shufflevector <8 x i16> %874, <8 x i16> %897, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %976 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %974, <8 x i16> %584) #9
  %977 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %975, <8 x i16> %584) #9
  %978 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %974, <8 x i16> %587) #9
  %979 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %975, <8 x i16> %587) #9
  %980 = add <4 x i32> %976, <i32 2048, i32 2048, i32 2048, i32 2048>
  %981 = add <4 x i32> %977, <i32 2048, i32 2048, i32 2048, i32 2048>
  %982 = add <4 x i32> %978, <i32 2048, i32 2048, i32 2048, i32 2048>
  %983 = add <4 x i32> %979, <i32 2048, i32 2048, i32 2048, i32 2048>
  %984 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %980, i32 %438) #9
  %985 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %981, i32 %438) #9
  %986 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %982, i32 %438) #9
  %987 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %983, i32 %438) #9
  %988 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %984, <4 x i32> %985) #9
  store <8 x i16> %988, <8 x i16>* %288, align 16
  %989 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %986, <4 x i32> %987) #9
  store <8 x i16> %989, <8 x i16>* %310, align 16
  %990 = shufflevector <8 x i16> %872, <8 x i16> %895, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %991 = shufflevector <8 x i16> %872, <8 x i16> %895, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %992 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %990, <8 x i16> %584) #9
  %993 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %991, <8 x i16> %584) #9
  %994 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %990, <8 x i16> %587) #9
  %995 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %991, <8 x i16> %587) #9
  %996 = add <4 x i32> %992, <i32 2048, i32 2048, i32 2048, i32 2048>
  %997 = add <4 x i32> %993, <i32 2048, i32 2048, i32 2048, i32 2048>
  %998 = add <4 x i32> %994, <i32 2048, i32 2048, i32 2048, i32 2048>
  %999 = add <4 x i32> %995, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1000 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %996, i32 %438) #9
  %1001 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %997, i32 %438) #9
  %1002 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %998, i32 %438) #9
  %1003 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %999, i32 %438) #9
  %1004 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1000, <4 x i32> %1001) #9
  store <8 x i16> %1004, <8 x i16>* %290, align 16
  %1005 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1002, <4 x i32> %1003) #9
  store <8 x i16> %1005, <8 x i16>* %308, align 16
  %1006 = shufflevector <8 x i16> %870, <8 x i16> %893, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1007 = shufflevector <8 x i16> %870, <8 x i16> %893, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1008 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1006, <8 x i16> %584) #9
  %1009 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1007, <8 x i16> %584) #9
  %1010 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1006, <8 x i16> %587) #9
  %1011 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1007, <8 x i16> %587) #9
  %1012 = add <4 x i32> %1008, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1013 = add <4 x i32> %1009, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1014 = add <4 x i32> %1010, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1015 = add <4 x i32> %1011, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1016 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1012, i32 %438) #9
  %1017 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1013, i32 %438) #9
  %1018 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1014, i32 %438) #9
  %1019 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1015, i32 %438) #9
  %1020 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1016, <4 x i32> %1017) #9
  store <8 x i16> %1020, <8 x i16>* %148, align 16
  %1021 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1018, <4 x i32> %1019) #9
  store <8 x i16> %1021, <8 x i16>* %150, align 16
  %1022 = shufflevector <8 x i16> %877, <8 x i16> %886, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1023 = shufflevector <8 x i16> %877, <8 x i16> %886, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1024 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1022, <8 x i16> %606) #9
  %1025 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1023, <8 x i16> %606) #9
  %1026 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1022, <8 x i16> %584) #9
  %1027 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1023, <8 x i16> %584) #9
  %1028 = add <4 x i32> %1024, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1029 = add <4 x i32> %1025, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1030 = add <4 x i32> %1026, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1031 = add <4 x i32> %1027, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1032 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1028, i32 %438) #9
  %1033 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1029, i32 %438) #9
  %1034 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1030, i32 %438) #9
  %1035 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1031, i32 %438) #9
  %1036 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1032, <4 x i32> %1033) #9
  store <8 x i16> %1036, <8 x i16>* %163, align 16
  %1037 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1034, <4 x i32> %1035) #9
  store <8 x i16> %1037, <8 x i16>* %166, align 16
  %1038 = shufflevector <8 x i16> %879, <8 x i16> %888, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1039 = shufflevector <8 x i16> %879, <8 x i16> %888, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1040 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1038, <8 x i16> %606) #9
  %1041 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1039, <8 x i16> %606) #9
  %1042 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1038, <8 x i16> %584) #9
  %1043 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1039, <8 x i16> %584) #9
  %1044 = add <4 x i32> %1040, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1045 = add <4 x i32> %1041, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1046 = add <4 x i32> %1042, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1047 = add <4 x i32> %1043, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1048 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1044, i32 %438) #9
  %1049 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1045, i32 %438) #9
  %1050 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1046, i32 %438) #9
  %1051 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1047, i32 %438) #9
  %1052 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1048, <4 x i32> %1049) #9
  store <8 x i16> %1052, <8 x i16>* %292, align 16
  %1053 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1050, <4 x i32> %1051) #9
  store <8 x i16> %1053, <8 x i16>* %306, align 16
  %1054 = shufflevector <8 x i16> %881, <8 x i16> %890, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1055 = shufflevector <8 x i16> %881, <8 x i16> %890, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1056 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1054, <8 x i16> %606) #9
  %1057 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1055, <8 x i16> %606) #9
  %1058 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1054, <8 x i16> %584) #9
  %1059 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1055, <8 x i16> %584) #9
  %1060 = add <4 x i32> %1056, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1061 = add <4 x i32> %1057, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1062 = add <4 x i32> %1058, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1063 = add <4 x i32> %1059, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1064 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1060, i32 %438) #9
  %1065 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1061, i32 %438) #9
  %1066 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1062, i32 %438) #9
  %1067 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1063, i32 %438) #9
  %1068 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1064, <4 x i32> %1065) #9
  store <8 x i16> %1068, <8 x i16>* %294, align 16
  %1069 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1066, <4 x i32> %1067) #9
  store <8 x i16> %1069, <8 x i16>* %304, align 16
  %1070 = shufflevector <8 x i16> %883, <8 x i16> %892, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1071 = shufflevector <8 x i16> %883, <8 x i16> %892, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1072 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1070, <8 x i16> %606) #9
  %1073 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1071, <8 x i16> %606) #9
  %1074 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1070, <8 x i16> %584) #9
  %1075 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1071, <8 x i16> %584) #9
  %1076 = add <4 x i32> %1072, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1077 = add <4 x i32> %1073, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1078 = add <4 x i32> %1074, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1079 = add <4 x i32> %1075, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1080 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1076, i32 %438) #9
  %1081 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1077, i32 %438) #9
  %1082 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1078, i32 %438) #9
  %1083 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1079, i32 %438) #9
  %1084 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1080, <4 x i32> %1081) #9
  store <8 x i16> %1084, <8 x i16>* %181, align 16
  %1085 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1082, <4 x i32> %1083) #9
  store <8 x i16> %1085, <8 x i16>* %183, align 16
  %1086 = load <8 x i16>, <8 x i16>* %571, align 16
  %1087 = load <8 x i16>, <8 x i16>* %330, align 16
  %1088 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1086, <8 x i16> %1087) #9
  store <8 x i16> %1088, <8 x i16>* %571, align 16
  %1089 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1086, <8 x i16> %1087) #9
  store <8 x i16> %1089, <8 x i16>* %330, align 16
  %1090 = load <8 x i16>, <8 x i16>* %575, align 16
  %1091 = load <8 x i16>, <8 x i16>* %601, align 16
  %1092 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1090, <8 x i16> %1091) #9
  store <8 x i16> %1092, <8 x i16>* %575, align 16
  %1093 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1090, <8 x i16> %1091) #9
  store <8 x i16> %1093, <8 x i16>* %601, align 16
  %1094 = load <8 x i16>, <8 x i16>* %778, align 16
  %1095 = load <8 x i16>, <8 x i16>* %622, align 16
  %1096 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1094, <8 x i16> %1095) #9
  store <8 x i16> %1096, <8 x i16>* %778, align 16
  %1097 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1094, <8 x i16> %1095) #9
  store <8 x i16> %1097, <8 x i16>* %622, align 16
  %1098 = load <8 x i16>, <8 x i16>* %776, align 16
  %1099 = load <8 x i16>, <8 x i16>* %347, align 16
  %1100 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1098, <8 x i16> %1099) #9
  store <8 x i16> %1100, <8 x i16>* %776, align 16
  %1101 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1098, <8 x i16> %1099) #9
  store <8 x i16> %1101, <8 x i16>* %347, align 16
  %1102 = load <8 x i16>, <8 x i16>* %374, align 16
  %1103 = load <8 x i16>, <8 x i16>* %345, align 16
  %1104 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1102, <8 x i16> %1103) #9
  store <8 x i16> %1104, <8 x i16>* %374, align 16
  %1105 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1102, <8 x i16> %1103) #9
  store <8 x i16> %1105, <8 x i16>* %345, align 16
  %1106 = load <8 x i16>, <8 x i16>* %577, align 16
  %1107 = load <8 x i16>, <8 x i16>* %620, align 16
  %1108 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1106, <8 x i16> %1107) #9
  store <8 x i16> %1108, <8 x i16>* %577, align 16
  %1109 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1106, <8 x i16> %1107) #9
  store <8 x i16> %1109, <8 x i16>* %620, align 16
  %1110 = load <8 x i16>, <8 x i16>* %579, align 16
  %1111 = load <8 x i16>, <8 x i16>* %599, align 16
  %1112 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1110, <8 x i16> %1111) #9
  store <8 x i16> %1112, <8 x i16>* %579, align 16
  %1113 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1110, <8 x i16> %1111) #9
  store <8 x i16> %1113, <8 x i16>* %599, align 16
  %1114 = load <8 x i16>, <8 x i16>* %379, align 16
  %1115 = load <8 x i16>, <8 x i16>* %327, align 16
  %1116 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1114, <8 x i16> %1115) #9
  store <8 x i16> %1116, <8 x i16>* %379, align 16
  %1117 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1114, <8 x i16> %1115) #9
  store <8 x i16> %1117, <8 x i16>* %327, align 16
  %1118 = shufflevector <8 x i16> %949, <8 x i16> %956, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1119 = shufflevector <8 x i16> %949, <8 x i16> %956, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1120 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1118, <8 x i16> %781) #9
  %1121 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1119, <8 x i16> %781) #9
  %1122 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1118, <8 x i16> %784) #9
  %1123 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1119, <8 x i16> %784) #9
  %1124 = add <4 x i32> %1120, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1125 = add <4 x i32> %1121, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1126 = add <4 x i32> %1122, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1127 = add <4 x i32> %1123, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1128 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1124, i32 %438) #9
  %1129 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1125, i32 %438) #9
  %1130 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1126, i32 %438) #9
  %1131 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1127, i32 %438) #9
  %1132 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1128, <4 x i32> %1129) #9
  store <8 x i16> %1132, <8 x i16>* %262, align 16
  %1133 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1130, <4 x i32> %1131) #9
  store <8 x i16> %1133, <8 x i16>* %265, align 16
  %1134 = shufflevector <8 x i16> %947, <8 x i16> %954, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1135 = shufflevector <8 x i16> %947, <8 x i16> %954, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1136 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1134, <8 x i16> %781) #9
  %1137 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1135, <8 x i16> %781) #9
  %1138 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1134, <8 x i16> %784) #9
  %1139 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1135, <8 x i16> %784) #9
  %1140 = add <4 x i32> %1136, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1141 = add <4 x i32> %1137, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1142 = add <4 x i32> %1138, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1143 = add <4 x i32> %1139, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1144 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1140, i32 %438) #9
  %1145 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1141, i32 %438) #9
  %1146 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1142, i32 %438) #9
  %1147 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1143, i32 %438) #9
  %1148 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1144, <4 x i32> %1145) #9
  store <8 x i16> %1148, <8 x i16>* %353, align 16
  %1149 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1146, <4 x i32> %1147) #9
  store <8 x i16> %1149, <8 x i16>* %359, align 16
  %1150 = shufflevector <8 x i16> %945, <8 x i16> %952, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1151 = shufflevector <8 x i16> %945, <8 x i16> %952, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1152 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1150, <8 x i16> %781) #9
  %1153 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1151, <8 x i16> %781) #9
  %1154 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1150, <8 x i16> %784) #9
  %1155 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1151, <8 x i16> %784) #9
  %1156 = add <4 x i32> %1152, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1157 = add <4 x i32> %1153, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1158 = add <4 x i32> %1154, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1159 = add <4 x i32> %1155, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1160 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1156, i32 %438) #9
  %1161 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1157, i32 %438) #9
  %1162 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1158, i32 %438) #9
  %1163 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1159, i32 %438) #9
  %1164 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1160, <4 x i32> %1161) #9
  store <8 x i16> %1164, <8 x i16>* %355, align 16
  %1165 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1162, <4 x i32> %1163) #9
  store <8 x i16> %1165, <8 x i16>* %357, align 16
  %1166 = shufflevector <8 x i16> %943, <8 x i16> %950, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1167 = shufflevector <8 x i16> %943, <8 x i16> %950, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1168 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1166, <8 x i16> %781) #9
  %1169 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1167, <8 x i16> %781) #9
  %1170 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1166, <8 x i16> %784) #9
  %1171 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1167, <8 x i16> %784) #9
  %1172 = add <4 x i32> %1168, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1173 = add <4 x i32> %1169, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1174 = add <4 x i32> %1170, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1175 = add <4 x i32> %1171, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1176 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1172, i32 %438) #9
  %1177 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1173, i32 %438) #9
  %1178 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1174, i32 %438) #9
  %1179 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1175, i32 %438) #9
  %1180 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1176, <4 x i32> %1177) #9
  store <8 x i16> %1180, <8 x i16>* %280, align 16
  %1181 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1178, <4 x i32> %1179) #9
  %1182 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %869, <8 x i16> %878) #9
  store <8 x i16> %1182, <8 x i16>* %100, align 16
  %1183 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %869, <8 x i16> %878) #9
  store <8 x i16> %1183, <8 x i16>* %214, align 16
  %1184 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %871, <8 x i16> %880) #9
  store <8 x i16> %1184, <8 x i16>* %284, align 16
  %1185 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %871, <8 x i16> %880) #9
  store <8 x i16> %1185, <8 x i16>* %298, align 16
  %1186 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %873, <8 x i16> %882) #9
  store <8 x i16> %1186, <8 x i16>* %286, align 16
  %1187 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %873, <8 x i16> %882) #9
  store <8 x i16> %1187, <8 x i16>* %296, align 16
  %1188 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %875, <8 x i16> %884) #9
  store <8 x i16> %1188, <8 x i16>* %117, align 16
  %1189 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %875, <8 x i16> %884) #9
  store <8 x i16> %1189, <8 x i16>* %196, align 16
  %1190 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %972, <8 x i16> %1084) #9
  store <8 x i16> %1190, <8 x i16>* %131, align 16
  %1191 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %972, <8 x i16> %1084) #9
  store <8 x i16> %1191, <8 x i16>* %181, align 16
  %1192 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %988, <8 x i16> %1068) #9
  store <8 x i16> %1192, <8 x i16>* %288, align 16
  %1193 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %988, <8 x i16> %1068) #9
  store <8 x i16> %1193, <8 x i16>* %294, align 16
  %1194 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1004, <8 x i16> %1052) #9
  store <8 x i16> %1194, <8 x i16>* %290, align 16
  %1195 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1004, <8 x i16> %1052) #9
  store <8 x i16> %1195, <8 x i16>* %292, align 16
  %1196 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1020, <8 x i16> %1036) #9
  store <8 x i16> %1196, <8 x i16>* %148, align 16
  %1197 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1020, <8 x i16> %1036) #9
  store <8 x i16> %1197, <8 x i16>* %163, align 16
  %1198 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %894, <8 x i16> %885) #9
  store <8 x i16> %1198, <8 x i16>* %216, align 16
  %1199 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %894, <8 x i16> %885) #9
  store <8 x i16> %1199, <8 x i16>* %103, align 16
  %1200 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %896, <8 x i16> %887) #9
  store <8 x i16> %1200, <8 x i16>* %300, align 16
  %1201 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %896, <8 x i16> %887) #9
  store <8 x i16> %1201, <8 x i16>* %314, align 16
  %1202 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %898, <8 x i16> %889) #9
  store <8 x i16> %1202, <8 x i16>* %302, align 16
  %1203 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %898, <8 x i16> %889) #9
  store <8 x i16> %1203, <8 x i16>* %312, align 16
  %1204 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %900, <8 x i16> %891) #9
  store <8 x i16> %1204, <8 x i16>* %199, align 16
  %1205 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %900, <8 x i16> %891) #9
  store <8 x i16> %1205, <8 x i16>* %119, align 16
  %1206 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %973, <8 x i16> %1085) #9
  store <8 x i16> %1206, <8 x i16>* %183, align 16
  %1207 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %973, <8 x i16> %1085) #9
  store <8 x i16> %1207, <8 x i16>* %134, align 16
  %1208 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %989, <8 x i16> %1069) #9
  store <8 x i16> %1208, <8 x i16>* %304, align 16
  %1209 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %989, <8 x i16> %1069) #9
  store <8 x i16> %1209, <8 x i16>* %310, align 16
  %1210 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1005, <8 x i16> %1053) #9
  store <8 x i16> %1210, <8 x i16>* %306, align 16
  %1211 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1005, <8 x i16> %1053) #9
  store <8 x i16> %1211, <8 x i16>* %308, align 16
  %1212 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1021, <8 x i16> %1037) #9
  %1213 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1021, <8 x i16> %1037) #9
  store <8 x i16> %1213, <8 x i16>* %150, align 16
  %1214 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1088, <8 x i16> %951) #9
  store <8 x i16> %1214, <8 x i16>* %571, align 16
  %1215 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1088, <8 x i16> %951) #9
  store <8 x i16> %1215, <8 x i16>* %232, align 16
  %1216 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1092, <8 x i16> %953) #9
  store <8 x i16> %1216, <8 x i16>* %575, align 16
  %1217 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1092, <8 x i16> %953) #9
  store <8 x i16> %1217, <8 x i16>* %363, align 16
  %1218 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1096, <8 x i16> %955) #9
  store <8 x i16> %1218, <8 x i16>* %778, align 16
  %1219 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1096, <8 x i16> %955) #9
  store <8 x i16> %1219, <8 x i16>* %361, align 16
  %1220 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1100, <8 x i16> %957) #9
  store <8 x i16> %1220, <8 x i16>* %776, align 16
  %1221 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1100, <8 x i16> %957) #9
  store <8 x i16> %1221, <8 x i16>* %249, align 16
  %1222 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1104, <8 x i16> %1133) #9
  store <8 x i16> %1222, <8 x i16>* %374, align 16
  %1223 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1104, <8 x i16> %1133) #9
  store <8 x i16> %1223, <8 x i16>* %265, align 16
  %1224 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1108, <8 x i16> %1149) #9
  store <8 x i16> %1224, <8 x i16>* %577, align 16
  %1225 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1108, <8 x i16> %1149) #9
  store <8 x i16> %1225, <8 x i16>* %359, align 16
  %1226 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1112, <8 x i16> %1165) #9
  store <8 x i16> %1226, <8 x i16>* %579, align 16
  %1227 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1112, <8 x i16> %1165) #9
  store <8 x i16> %1227, <8 x i16>* %357, align 16
  %1228 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1116, <8 x i16> %1181) #9
  store <8 x i16> %1228, <8 x i16>* %379, align 16
  %1229 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1116, <8 x i16> %1181) #9
  store <8 x i16> %1229, <8 x i16>* %282, align 16
  %1230 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1117, <8 x i16> %1180) #9
  store <8 x i16> %1230, <8 x i16>* %327, align 16
  %1231 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1117, <8 x i16> %1180) #9
  store <8 x i16> %1231, <8 x i16>* %280, align 16
  %1232 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1113, <8 x i16> %1164) #9
  store <8 x i16> %1232, <8 x i16>* %599, align 16
  %1233 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1113, <8 x i16> %1164) #9
  store <8 x i16> %1233, <8 x i16>* %355, align 16
  %1234 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1109, <8 x i16> %1148) #9
  store <8 x i16> %1234, <8 x i16>* %620, align 16
  %1235 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1109, <8 x i16> %1148) #9
  store <8 x i16> %1235, <8 x i16>* %353, align 16
  %1236 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1105, <8 x i16> %1132) #9
  store <8 x i16> %1236, <8 x i16>* %345, align 16
  %1237 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1105, <8 x i16> %1132) #9
  store <8 x i16> %1237, <8 x i16>* %262, align 16
  %1238 = load <8 x i16>, <8 x i16>* %247, align 16
  %1239 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1101, <8 x i16> %1238) #9
  store <8 x i16> %1239, <8 x i16>* %347, align 16
  %1240 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1101, <8 x i16> %1238) #9
  store <8 x i16> %1240, <8 x i16>* %247, align 16
  %1241 = load <8 x i16>, <8 x i16>* %351, align 16
  %1242 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1097, <8 x i16> %1241) #9
  store <8 x i16> %1242, <8 x i16>* %622, align 16
  %1243 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1097, <8 x i16> %1241) #9
  store <8 x i16> %1243, <8 x i16>* %351, align 16
  %1244 = load <8 x i16>, <8 x i16>* %349, align 16
  %1245 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1093, <8 x i16> %1244) #9
  store <8 x i16> %1245, <8 x i16>* %601, align 16
  %1246 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1093, <8 x i16> %1244) #9
  store <8 x i16> %1246, <8 x i16>* %349, align 16
  %1247 = load <8 x i16>, <8 x i16>* %229, align 16
  %1248 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1089, <8 x i16> %1247) #9
  store <8 x i16> %1248, <8 x i16>* %330, align 16
  %1249 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1089, <8 x i16> %1247) #9
  store <8 x i16> %1249, <8 x i16>* %229, align 16
  %1250 = shufflevector <8 x i16> %1197, <8 x i16> %1212, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1251 = shufflevector <8 x i16> %1197, <8 x i16> %1212, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1252 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1250, <8 x i16> %781) #9
  %1253 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1251, <8 x i16> %781) #9
  %1254 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1250, <8 x i16> %784) #9
  %1255 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1251, <8 x i16> %784) #9
  %1256 = add <4 x i32> %1252, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1257 = add <4 x i32> %1253, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1258 = add <4 x i32> %1254, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1259 = add <4 x i32> %1255, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1260 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1256, i32 %438) #9
  %1261 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1257, i32 %438) #9
  %1262 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1258, i32 %438) #9
  %1263 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1259, i32 %438) #9
  %1264 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1260, <4 x i32> %1261) #9
  store <8 x i16> %1264, <8 x i16>* %163, align 16
  %1265 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1262, <4 x i32> %1263) #9
  store <8 x i16> %1265, <8 x i16>* %166, align 16
  %1266 = shufflevector <8 x i16> %1195, <8 x i16> %1210, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1267 = shufflevector <8 x i16> %1195, <8 x i16> %1210, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1268 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1266, <8 x i16> %781) #9
  %1269 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1267, <8 x i16> %781) #9
  %1270 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1266, <8 x i16> %784) #9
  %1271 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1267, <8 x i16> %784) #9
  %1272 = add <4 x i32> %1268, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1273 = add <4 x i32> %1269, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1274 = add <4 x i32> %1270, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1275 = add <4 x i32> %1271, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1276 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1272, i32 %438) #9
  %1277 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1273, i32 %438) #9
  %1278 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1274, i32 %438) #9
  %1279 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1275, i32 %438) #9
  %1280 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1276, <4 x i32> %1277) #9
  store <8 x i16> %1280, <8 x i16>* %292, align 16
  %1281 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1278, <4 x i32> %1279) #9
  store <8 x i16> %1281, <8 x i16>* %306, align 16
  %1282 = shufflevector <8 x i16> %1193, <8 x i16> %1208, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1283 = shufflevector <8 x i16> %1193, <8 x i16> %1208, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1284 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1282, <8 x i16> %781) #9
  %1285 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1283, <8 x i16> %781) #9
  %1286 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1282, <8 x i16> %784) #9
  %1287 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1283, <8 x i16> %784) #9
  %1288 = add <4 x i32> %1284, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1289 = add <4 x i32> %1285, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1290 = add <4 x i32> %1286, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1291 = add <4 x i32> %1287, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1292 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1288, i32 %438) #9
  %1293 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1289, i32 %438) #9
  %1294 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1290, i32 %438) #9
  %1295 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1291, i32 %438) #9
  %1296 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1292, <4 x i32> %1293) #9
  store <8 x i16> %1296, <8 x i16>* %294, align 16
  %1297 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1294, <4 x i32> %1295) #9
  store <8 x i16> %1297, <8 x i16>* %304, align 16
  %1298 = shufflevector <8 x i16> %1191, <8 x i16> %1206, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1299 = shufflevector <8 x i16> %1191, <8 x i16> %1206, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1300 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1298, <8 x i16> %781) #9
  %1301 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1299, <8 x i16> %781) #9
  %1302 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1298, <8 x i16> %784) #9
  %1303 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1299, <8 x i16> %784) #9
  %1304 = add <4 x i32> %1300, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1305 = add <4 x i32> %1301, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1306 = add <4 x i32> %1302, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1307 = add <4 x i32> %1303, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1308 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1304, i32 %438) #9
  %1309 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1305, i32 %438) #9
  %1310 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1306, i32 %438) #9
  %1311 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1307, i32 %438) #9
  %1312 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1308, <4 x i32> %1309) #9
  store <8 x i16> %1312, <8 x i16>* %181, align 16
  %1313 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1310, <4 x i32> %1311) #9
  store <8 x i16> %1313, <8 x i16>* %183, align 16
  %1314 = shufflevector <8 x i16> %1189, <8 x i16> %1204, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1315 = shufflevector <8 x i16> %1189, <8 x i16> %1204, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1316 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1314, <8 x i16> %781) #9
  %1317 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1315, <8 x i16> %781) #9
  %1318 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1314, <8 x i16> %784) #9
  %1319 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1315, <8 x i16> %784) #9
  %1320 = add <4 x i32> %1316, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1321 = add <4 x i32> %1317, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1322 = add <4 x i32> %1318, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1323 = add <4 x i32> %1319, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1324 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1320, i32 %438) #9
  %1325 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1321, i32 %438) #9
  %1326 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1322, i32 %438) #9
  %1327 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1323, i32 %438) #9
  %1328 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1324, <4 x i32> %1325) #9
  store <8 x i16> %1328, <8 x i16>* %196, align 16
  %1329 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1326, <4 x i32> %1327) #9
  store <8 x i16> %1329, <8 x i16>* %199, align 16
  %1330 = shufflevector <8 x i16> %1187, <8 x i16> %1202, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1331 = shufflevector <8 x i16> %1187, <8 x i16> %1202, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1332 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1330, <8 x i16> %781) #9
  %1333 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1331, <8 x i16> %781) #9
  %1334 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1330, <8 x i16> %784) #9
  %1335 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1331, <8 x i16> %784) #9
  %1336 = add <4 x i32> %1332, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1337 = add <4 x i32> %1333, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1338 = add <4 x i32> %1334, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1339 = add <4 x i32> %1335, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1340 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1336, i32 %438) #9
  %1341 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1337, i32 %438) #9
  %1342 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1338, i32 %438) #9
  %1343 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1339, i32 %438) #9
  %1344 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1340, <4 x i32> %1341) #9
  store <8 x i16> %1344, <8 x i16>* %296, align 16
  %1345 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1342, <4 x i32> %1343) #9
  store <8 x i16> %1345, <8 x i16>* %302, align 16
  %1346 = shufflevector <8 x i16> %1185, <8 x i16> %1200, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1347 = shufflevector <8 x i16> %1185, <8 x i16> %1200, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1348 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1346, <8 x i16> %781) #9
  %1349 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1347, <8 x i16> %781) #9
  %1350 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1346, <8 x i16> %784) #9
  %1351 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1347, <8 x i16> %784) #9
  %1352 = add <4 x i32> %1348, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1353 = add <4 x i32> %1349, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1354 = add <4 x i32> %1350, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1355 = add <4 x i32> %1351, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1356 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1352, i32 %438) #9
  %1357 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1353, i32 %438) #9
  %1358 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1354, i32 %438) #9
  %1359 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1355, i32 %438) #9
  %1360 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1356, <4 x i32> %1357) #9
  store <8 x i16> %1360, <8 x i16>* %298, align 16
  %1361 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1358, <4 x i32> %1359) #9
  store <8 x i16> %1361, <8 x i16>* %300, align 16
  %1362 = shufflevector <8 x i16> %1183, <8 x i16> %1198, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1363 = shufflevector <8 x i16> %1183, <8 x i16> %1198, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1364 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1362, <8 x i16> %781) #9
  %1365 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1363, <8 x i16> %781) #9
  %1366 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1362, <8 x i16> %784) #9
  %1367 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1363, <8 x i16> %784) #9
  %1368 = add <4 x i32> %1364, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1369 = add <4 x i32> %1365, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1370 = add <4 x i32> %1366, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1371 = add <4 x i32> %1367, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1372 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1368, i32 %438) #9
  %1373 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1369, i32 %438) #9
  %1374 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1370, i32 %438) #9
  %1375 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1371, i32 %438) #9
  %1376 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1372, <4 x i32> %1373) #9
  store <8 x i16> %1376, <8 x i16>* %214, align 16
  %1377 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1374, <4 x i32> %1375) #9
  store <8 x i16> %1377, <8 x i16>* %216, align 16
  %1378 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1214, <8 x i16> %1199) #9
  %1379 = bitcast <2 x i64>* %1 to <8 x i16>*
  store <8 x i16> %1378, <8 x i16>* %1379, align 16
  %1380 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1214, <8 x i16> %1199) #9
  %1381 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 63
  %1382 = bitcast <2 x i64>* %1381 to <8 x i16>*
  store <8 x i16> %1380, <8 x i16>* %1382, align 16
  %1383 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1216, <8 x i16> %1201) #9
  %1384 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 1
  %1385 = bitcast <2 x i64>* %1384 to <8 x i16>*
  store <8 x i16> %1383, <8 x i16>* %1385, align 16
  %1386 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1216, <8 x i16> %1201) #9
  %1387 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 62
  %1388 = bitcast <2 x i64>* %1387 to <8 x i16>*
  store <8 x i16> %1386, <8 x i16>* %1388, align 16
  %1389 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1218, <8 x i16> %1203) #9
  %1390 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 2
  %1391 = bitcast <2 x i64>* %1390 to <8 x i16>*
  store <8 x i16> %1389, <8 x i16>* %1391, align 16
  %1392 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1218, <8 x i16> %1203) #9
  %1393 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 61
  %1394 = bitcast <2 x i64>* %1393 to <8 x i16>*
  store <8 x i16> %1392, <8 x i16>* %1394, align 16
  %1395 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1220, <8 x i16> %1205) #9
  %1396 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 3
  %1397 = bitcast <2 x i64>* %1396 to <8 x i16>*
  store <8 x i16> %1395, <8 x i16>* %1397, align 16
  %1398 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1220, <8 x i16> %1205) #9
  %1399 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 60
  %1400 = bitcast <2 x i64>* %1399 to <8 x i16>*
  store <8 x i16> %1398, <8 x i16>* %1400, align 16
  %1401 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1222, <8 x i16> %1207) #9
  %1402 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 4
  %1403 = bitcast <2 x i64>* %1402 to <8 x i16>*
  store <8 x i16> %1401, <8 x i16>* %1403, align 16
  %1404 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1222, <8 x i16> %1207) #9
  %1405 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 59
  %1406 = bitcast <2 x i64>* %1405 to <8 x i16>*
  store <8 x i16> %1404, <8 x i16>* %1406, align 16
  %1407 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1224, <8 x i16> %1209) #9
  %1408 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 5
  %1409 = bitcast <2 x i64>* %1408 to <8 x i16>*
  store <8 x i16> %1407, <8 x i16>* %1409, align 16
  %1410 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1224, <8 x i16> %1209) #9
  %1411 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 58
  %1412 = bitcast <2 x i64>* %1411 to <8 x i16>*
  store <8 x i16> %1410, <8 x i16>* %1412, align 16
  %1413 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1226, <8 x i16> %1211) #9
  %1414 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 6
  %1415 = bitcast <2 x i64>* %1414 to <8 x i16>*
  store <8 x i16> %1413, <8 x i16>* %1415, align 16
  %1416 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1226, <8 x i16> %1211) #9
  %1417 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 57
  %1418 = bitcast <2 x i64>* %1417 to <8 x i16>*
  store <8 x i16> %1416, <8 x i16>* %1418, align 16
  %1419 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1228, <8 x i16> %1213) #9
  %1420 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 7
  %1421 = bitcast <2 x i64>* %1420 to <8 x i16>*
  store <8 x i16> %1419, <8 x i16>* %1421, align 16
  %1422 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1228, <8 x i16> %1213) #9
  %1423 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 56
  %1424 = bitcast <2 x i64>* %1423 to <8 x i16>*
  store <8 x i16> %1422, <8 x i16>* %1424, align 16
  %1425 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1230, <8 x i16> %1265) #9
  %1426 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 8
  %1427 = bitcast <2 x i64>* %1426 to <8 x i16>*
  store <8 x i16> %1425, <8 x i16>* %1427, align 16
  %1428 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1230, <8 x i16> %1265) #9
  %1429 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 55
  %1430 = bitcast <2 x i64>* %1429 to <8 x i16>*
  store <8 x i16> %1428, <8 x i16>* %1430, align 16
  %1431 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1232, <8 x i16> %1281) #9
  %1432 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 9
  %1433 = bitcast <2 x i64>* %1432 to <8 x i16>*
  store <8 x i16> %1431, <8 x i16>* %1433, align 16
  %1434 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1232, <8 x i16> %1281) #9
  %1435 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 54
  %1436 = bitcast <2 x i64>* %1435 to <8 x i16>*
  store <8 x i16> %1434, <8 x i16>* %1436, align 16
  %1437 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1234, <8 x i16> %1297) #9
  %1438 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 10
  %1439 = bitcast <2 x i64>* %1438 to <8 x i16>*
  store <8 x i16> %1437, <8 x i16>* %1439, align 16
  %1440 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1234, <8 x i16> %1297) #9
  %1441 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 53
  %1442 = bitcast <2 x i64>* %1441 to <8 x i16>*
  store <8 x i16> %1440, <8 x i16>* %1442, align 16
  %1443 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1236, <8 x i16> %1313) #9
  %1444 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 11
  %1445 = bitcast <2 x i64>* %1444 to <8 x i16>*
  store <8 x i16> %1443, <8 x i16>* %1445, align 16
  %1446 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1236, <8 x i16> %1313) #9
  %1447 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 52
  %1448 = bitcast <2 x i64>* %1447 to <8 x i16>*
  store <8 x i16> %1446, <8 x i16>* %1448, align 16
  %1449 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1239, <8 x i16> %1329) #9
  %1450 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 12
  %1451 = bitcast <2 x i64>* %1450 to <8 x i16>*
  store <8 x i16> %1449, <8 x i16>* %1451, align 16
  %1452 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1239, <8 x i16> %1329) #9
  %1453 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 51
  %1454 = bitcast <2 x i64>* %1453 to <8 x i16>*
  store <8 x i16> %1452, <8 x i16>* %1454, align 16
  %1455 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1242, <8 x i16> %1345) #9
  %1456 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 13
  %1457 = bitcast <2 x i64>* %1456 to <8 x i16>*
  store <8 x i16> %1455, <8 x i16>* %1457, align 16
  %1458 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1242, <8 x i16> %1345) #9
  %1459 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 50
  %1460 = bitcast <2 x i64>* %1459 to <8 x i16>*
  store <8 x i16> %1458, <8 x i16>* %1460, align 16
  %1461 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1245, <8 x i16> %1361) #9
  %1462 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 14
  %1463 = bitcast <2 x i64>* %1462 to <8 x i16>*
  store <8 x i16> %1461, <8 x i16>* %1463, align 16
  %1464 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1245, <8 x i16> %1361) #9
  %1465 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 49
  %1466 = bitcast <2 x i64>* %1465 to <8 x i16>*
  store <8 x i16> %1464, <8 x i16>* %1466, align 16
  %1467 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1248, <8 x i16> %1377) #9
  %1468 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 15
  %1469 = bitcast <2 x i64>* %1468 to <8 x i16>*
  store <8 x i16> %1467, <8 x i16>* %1469, align 16
  %1470 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1248, <8 x i16> %1377) #9
  %1471 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 48
  %1472 = bitcast <2 x i64>* %1471 to <8 x i16>*
  store <8 x i16> %1470, <8 x i16>* %1472, align 16
  %1473 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1249, <8 x i16> %1376) #9
  %1474 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 16
  %1475 = bitcast <2 x i64>* %1474 to <8 x i16>*
  store <8 x i16> %1473, <8 x i16>* %1475, align 16
  %1476 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1249, <8 x i16> %1376) #9
  %1477 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 47
  %1478 = bitcast <2 x i64>* %1477 to <8 x i16>*
  store <8 x i16> %1476, <8 x i16>* %1478, align 16
  %1479 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1246, <8 x i16> %1360) #9
  %1480 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 17
  %1481 = bitcast <2 x i64>* %1480 to <8 x i16>*
  store <8 x i16> %1479, <8 x i16>* %1481, align 16
  %1482 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1246, <8 x i16> %1360) #9
  %1483 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 46
  %1484 = bitcast <2 x i64>* %1483 to <8 x i16>*
  store <8 x i16> %1482, <8 x i16>* %1484, align 16
  %1485 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1243, <8 x i16> %1344) #9
  %1486 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 18
  %1487 = bitcast <2 x i64>* %1486 to <8 x i16>*
  store <8 x i16> %1485, <8 x i16>* %1487, align 16
  %1488 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1243, <8 x i16> %1344) #9
  %1489 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 45
  %1490 = bitcast <2 x i64>* %1489 to <8 x i16>*
  store <8 x i16> %1488, <8 x i16>* %1490, align 16
  %1491 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1240, <8 x i16> %1328) #9
  %1492 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 19
  %1493 = bitcast <2 x i64>* %1492 to <8 x i16>*
  store <8 x i16> %1491, <8 x i16>* %1493, align 16
  %1494 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1240, <8 x i16> %1328) #9
  %1495 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 44
  %1496 = bitcast <2 x i64>* %1495 to <8 x i16>*
  store <8 x i16> %1494, <8 x i16>* %1496, align 16
  %1497 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1237, <8 x i16> %1312) #9
  %1498 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 20
  %1499 = bitcast <2 x i64>* %1498 to <8 x i16>*
  store <8 x i16> %1497, <8 x i16>* %1499, align 16
  %1500 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1237, <8 x i16> %1312) #9
  %1501 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 43
  %1502 = bitcast <2 x i64>* %1501 to <8 x i16>*
  store <8 x i16> %1500, <8 x i16>* %1502, align 16
  %1503 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1235, <8 x i16> %1296) #9
  %1504 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 21
  %1505 = bitcast <2 x i64>* %1504 to <8 x i16>*
  store <8 x i16> %1503, <8 x i16>* %1505, align 16
  %1506 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1235, <8 x i16> %1296) #9
  %1507 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 42
  %1508 = bitcast <2 x i64>* %1507 to <8 x i16>*
  store <8 x i16> %1506, <8 x i16>* %1508, align 16
  %1509 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1233, <8 x i16> %1280) #9
  %1510 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 22
  %1511 = bitcast <2 x i64>* %1510 to <8 x i16>*
  store <8 x i16> %1509, <8 x i16>* %1511, align 16
  %1512 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1233, <8 x i16> %1280) #9
  %1513 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 41
  %1514 = bitcast <2 x i64>* %1513 to <8 x i16>*
  store <8 x i16> %1512, <8 x i16>* %1514, align 16
  %1515 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1231, <8 x i16> %1264) #9
  %1516 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 23
  %1517 = bitcast <2 x i64>* %1516 to <8 x i16>*
  store <8 x i16> %1515, <8 x i16>* %1517, align 16
  %1518 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1231, <8 x i16> %1264) #9
  %1519 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 40
  %1520 = bitcast <2 x i64>* %1519 to <8 x i16>*
  store <8 x i16> %1518, <8 x i16>* %1520, align 16
  %1521 = load <8 x i16>, <8 x i16>* %148, align 16
  %1522 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1229, <8 x i16> %1521) #9
  %1523 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 24
  %1524 = bitcast <2 x i64>* %1523 to <8 x i16>*
  store <8 x i16> %1522, <8 x i16>* %1524, align 16
  %1525 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1229, <8 x i16> %1521) #9
  %1526 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 39
  %1527 = bitcast <2 x i64>* %1526 to <8 x i16>*
  store <8 x i16> %1525, <8 x i16>* %1527, align 16
  %1528 = load <8 x i16>, <8 x i16>* %290, align 16
  %1529 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1227, <8 x i16> %1528) #9
  %1530 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 25
  %1531 = bitcast <2 x i64>* %1530 to <8 x i16>*
  store <8 x i16> %1529, <8 x i16>* %1531, align 16
  %1532 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1227, <8 x i16> %1528) #9
  %1533 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 38
  %1534 = bitcast <2 x i64>* %1533 to <8 x i16>*
  store <8 x i16> %1532, <8 x i16>* %1534, align 16
  %1535 = load <8 x i16>, <8 x i16>* %288, align 16
  %1536 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1225, <8 x i16> %1535) #9
  %1537 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 26
  %1538 = bitcast <2 x i64>* %1537 to <8 x i16>*
  store <8 x i16> %1536, <8 x i16>* %1538, align 16
  %1539 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1225, <8 x i16> %1535) #9
  %1540 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 37
  %1541 = bitcast <2 x i64>* %1540 to <8 x i16>*
  store <8 x i16> %1539, <8 x i16>* %1541, align 16
  %1542 = load <8 x i16>, <8 x i16>* %131, align 16
  %1543 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1223, <8 x i16> %1542) #9
  %1544 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 27
  %1545 = bitcast <2 x i64>* %1544 to <8 x i16>*
  store <8 x i16> %1543, <8 x i16>* %1545, align 16
  %1546 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1223, <8 x i16> %1542) #9
  %1547 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 36
  %1548 = bitcast <2 x i64>* %1547 to <8 x i16>*
  store <8 x i16> %1546, <8 x i16>* %1548, align 16
  %1549 = load <8 x i16>, <8 x i16>* %117, align 16
  %1550 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1221, <8 x i16> %1549) #9
  %1551 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 28
  %1552 = bitcast <2 x i64>* %1551 to <8 x i16>*
  store <8 x i16> %1550, <8 x i16>* %1552, align 16
  %1553 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1221, <8 x i16> %1549) #9
  %1554 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 35
  %1555 = bitcast <2 x i64>* %1554 to <8 x i16>*
  store <8 x i16> %1553, <8 x i16>* %1555, align 16
  %1556 = load <8 x i16>, <8 x i16>* %361, align 16
  %1557 = load <8 x i16>, <8 x i16>* %286, align 16
  %1558 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1556, <8 x i16> %1557) #9
  %1559 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 29
  %1560 = bitcast <2 x i64>* %1559 to <8 x i16>*
  store <8 x i16> %1558, <8 x i16>* %1560, align 16
  %1561 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1556, <8 x i16> %1557) #9
  %1562 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 34
  %1563 = bitcast <2 x i64>* %1562 to <8 x i16>*
  store <8 x i16> %1561, <8 x i16>* %1563, align 16
  %1564 = load <8 x i16>, <8 x i16>* %363, align 16
  %1565 = load <8 x i16>, <8 x i16>* %284, align 16
  %1566 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1564, <8 x i16> %1565) #9
  %1567 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 30
  %1568 = bitcast <2 x i64>* %1567 to <8 x i16>*
  store <8 x i16> %1566, <8 x i16>* %1568, align 16
  %1569 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1564, <8 x i16> %1565) #9
  %1570 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 33
  %1571 = bitcast <2 x i64>* %1570 to <8 x i16>*
  store <8 x i16> %1569, <8 x i16>* %1571, align 16
  %1572 = load <8 x i16>, <8 x i16>* %232, align 16
  %1573 = load <8 x i16>, <8 x i16>* %100, align 16
  %1574 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1572, <8 x i16> %1573) #9
  %1575 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 31
  %1576 = bitcast <2 x i64>* %1575 to <8 x i16>*
  store <8 x i16> %1574, <8 x i16>* %1576, align 16
  %1577 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1572, <8 x i16> %1573) #9
  %1578 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 32
  %1579 = bitcast <2 x i64>* %1578 to <8 x i16>*
  store <8 x i16> %1577, <8 x i16>* %1579, align 16
  call void @llvm.lifetime.end.p0i8(i64 1024, i8* nonnull %35) #9
  ret void
}

attributes #0 = { nounwind ssp uwtable "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "min-legal-vector-width"="128" "no-frame-pointer-elim"="false" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "null-pointer-is-valid"="true" "stack-protector-buffer-size"="4" "target-cpu"="x86-64" "target-features"="+cx8,+fxsr,+mmx,+sse,+sse2,+sse3,+ssse3,+x87" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #1 = { nounwind ssp uwtable "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "min-legal-vector-width"="0" "no-frame-pointer-elim"="false" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "null-pointer-is-valid"="true" "stack-protector-buffer-size"="4" "target-cpu"="x86-64" "target-features"="+cx8,+fxsr,+mmx,+sse,+sse2,+sse3,+ssse3,+x87" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #2 = { argmemonly nounwind }
attributes #3 = { "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "no-frame-pointer-elim"="false" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "null-pointer-is-valid"="true" "stack-protector-buffer-size"="4" "target-cpu"="x86-64" "target-features"="+cx8,+fxsr,+mmx,+sse,+sse2,+sse3,+ssse3,+x87" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #4 = { nofree nounwind ssp uwtable "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "min-legal-vector-width"="128" "no-frame-pointer-elim"="false" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "null-pointer-is-valid"="true" "stack-protector-buffer-size"="4" "target-cpu"="x86-64" "target-features"="+cx8,+fxsr,+mmx,+sse,+sse2,+sse3,+ssse3,+x87" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #5 = { nounwind readnone }
attributes #6 = { nounwind readnone speculatable }
attributes #7 = { inlinehint nofree nounwind ssp uwtable "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "min-legal-vector-width"="128" "no-frame-pointer-elim"="false" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "null-pointer-is-valid"="true" "stack-protector-buffer-size"="4" "target-cpu"="x86-64" "target-features"="+cx8,+fxsr,+mmx,+sse,+sse2,+sse3,+ssse3,+x87" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #8 = { inlinehint nounwind ssp uwtable "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "min-legal-vector-width"="128" "no-frame-pointer-elim"="false" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "null-pointer-is-valid"="true" "stack-protector-buffer-size"="4" "target-cpu"="x86-64" "target-features"="+cx8,+fxsr,+mmx,+sse,+sse2,+sse3,+ssse3,+x87" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #9 = { nounwind }

!llvm.module.flags = !{!0, !1}

!0 = !{i32 1, !"wchar_size", i32 4}
!1 = !{i32 7, !"PIC Level", i32 2}
!2 = distinct !{!2, !3}
!3 = !{!"llvm.loop.unroll.disable"}
!4 = distinct !{!4, !3}
!5 = distinct !{!5, !3}
!6 = distinct !{!6, !3}
!7 = distinct !{!7, !3}
!8 = distinct !{!8, !3}
!9 = distinct !{!9, !3}
!10 = distinct !{!10, !3}
