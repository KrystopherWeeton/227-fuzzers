; ModuleID = '../../third_party/libwebp/src/dsp/yuv_sse41.c'
source_filename = "../../third_party/libwebp/src/dsp/yuv_sse41.c"
target datalayout = "e-m:e-i64:64-f80:128-n8:16:32:64-S128"
target triple = "x86_64-unknown-linux-gnu"

@WebPSamplers = external local_unnamed_addr global [0 x void (i8*, i8*, i8*, i8*, i32)*], align 8
@WebPConvertARGBToY = external local_unnamed_addr global void (i32*, i8*, i32)*, align 8
@WebPConvertARGBToUV = external local_unnamed_addr global void (i32*, i8*, i8*, i32, i32)*, align 8
@WebPConvertRGB24ToY = external local_unnamed_addr global void (i8*, i8*, i32)*, align 8
@WebPConvertBGR24ToY = external local_unnamed_addr global void (i8*, i8*, i32)*, align 8
@WebPConvertRGBA32ToUV = external local_unnamed_addr global void (i16*, i8*, i8*, i32)*, align 8

; Function Attrs: nounwind ssp uwtable
define hidden void @VP8YuvToRgb32_SSE41(i8* nocapture readonly, i8* nocapture readonly, i8* nocapture readonly, i8* nocapture) local_unnamed_addr #0 {
  %5 = bitcast i8* %0 to i64*
  %6 = load i64, i64* %5, align 1
  %7 = insertelement <2 x i64> undef, i64 %6, i32 0
  %8 = bitcast <2 x i64> %7 to <16 x i8>
  %9 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %8, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %10 = bitcast i8* %1 to i64*
  %11 = load i64, i64* %10, align 1
  %12 = insertelement <2 x i64> undef, i64 %11, i32 0
  %13 = bitcast <2 x i64> %12 to <16 x i8>
  %14 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %13, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %15 = bitcast i8* %2 to i64*
  %16 = load i64, i64* %15, align 1
  %17 = insertelement <2 x i64> undef, i64 %16, i32 0
  %18 = bitcast <2 x i64> %17 to <16 x i8>
  %19 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %18, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %20 = bitcast <16 x i8> %9 to <8 x i16>
  %21 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %20, <8 x i16> <i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077>) #5
  %22 = bitcast <16 x i8> %19 to <8 x i16>
  %23 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %22, <8 x i16> <i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149>) #5
  %24 = add <8 x i16> %21, <i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234>
  %25 = add <8 x i16> %24, %23
  %26 = bitcast <16 x i8> %14 to <8 x i16>
  %27 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %26, <8 x i16> <i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419>) #5
  %28 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %22, <8 x i16> <i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320>) #5
  %29 = add <8 x i16> %21, <i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708>
  %30 = sub <8 x i16> %29, %27
  %31 = sub <8 x i16> %30, %28
  %32 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %26, <8 x i16> <i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486>) #5
  %33 = tail call <8 x i16> @llvm.uadd.sat.v8i16(<8 x i16> %32, <8 x i16> %21) #5
  %34 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %33, <8 x i16> <i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685>) #5
  %35 = ashr <8 x i16> %25, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %36 = ashr <8 x i16> %31, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %37 = lshr <8 x i16> %34, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %38 = getelementptr inbounds i8, i8* %0, i64 8
  %39 = getelementptr inbounds i8, i8* %1, i64 8
  %40 = getelementptr inbounds i8, i8* %2, i64 8
  %41 = bitcast i8* %38 to i64*
  %42 = load i64, i64* %41, align 1
  %43 = insertelement <2 x i64> undef, i64 %42, i32 0
  %44 = bitcast <2 x i64> %43 to <16 x i8>
  %45 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %44, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %46 = bitcast i8* %39 to i64*
  %47 = load i64, i64* %46, align 1
  %48 = insertelement <2 x i64> undef, i64 %47, i32 0
  %49 = bitcast <2 x i64> %48 to <16 x i8>
  %50 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %49, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %51 = bitcast i8* %40 to i64*
  %52 = load i64, i64* %51, align 1
  %53 = insertelement <2 x i64> undef, i64 %52, i32 0
  %54 = bitcast <2 x i64> %53 to <16 x i8>
  %55 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %54, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %56 = bitcast <16 x i8> %45 to <8 x i16>
  %57 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %56, <8 x i16> <i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077>) #5
  %58 = bitcast <16 x i8> %55 to <8 x i16>
  %59 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %58, <8 x i16> <i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149>) #5
  %60 = add <8 x i16> %57, <i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234>
  %61 = add <8 x i16> %60, %59
  %62 = bitcast <16 x i8> %50 to <8 x i16>
  %63 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %62, <8 x i16> <i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419>) #5
  %64 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %58, <8 x i16> <i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320>) #5
  %65 = add <8 x i16> %57, <i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708>
  %66 = sub <8 x i16> %65, %63
  %67 = sub <8 x i16> %66, %64
  %68 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %62, <8 x i16> <i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486>) #5
  %69 = tail call <8 x i16> @llvm.uadd.sat.v8i16(<8 x i16> %68, <8 x i16> %57) #5
  %70 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %69, <8 x i16> <i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685>) #5
  %71 = ashr <8 x i16> %61, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %72 = ashr <8 x i16> %67, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %73 = lshr <8 x i16> %70, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %74 = getelementptr inbounds i8, i8* %0, i64 16
  %75 = getelementptr inbounds i8, i8* %1, i64 16
  %76 = getelementptr inbounds i8, i8* %2, i64 16
  %77 = bitcast i8* %74 to i64*
  %78 = load i64, i64* %77, align 1
  %79 = insertelement <2 x i64> undef, i64 %78, i32 0
  %80 = bitcast <2 x i64> %79 to <16 x i8>
  %81 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %80, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %82 = bitcast i8* %75 to i64*
  %83 = load i64, i64* %82, align 1
  %84 = insertelement <2 x i64> undef, i64 %83, i32 0
  %85 = bitcast <2 x i64> %84 to <16 x i8>
  %86 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %85, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %87 = bitcast i8* %76 to i64*
  %88 = load i64, i64* %87, align 1
  %89 = insertelement <2 x i64> undef, i64 %88, i32 0
  %90 = bitcast <2 x i64> %89 to <16 x i8>
  %91 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %90, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %92 = bitcast <16 x i8> %81 to <8 x i16>
  %93 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %92, <8 x i16> <i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077>) #5
  %94 = bitcast <16 x i8> %91 to <8 x i16>
  %95 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %94, <8 x i16> <i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149>) #5
  %96 = add <8 x i16> %93, <i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234>
  %97 = add <8 x i16> %96, %95
  %98 = bitcast <16 x i8> %86 to <8 x i16>
  %99 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %98, <8 x i16> <i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419>) #5
  %100 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %94, <8 x i16> <i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320>) #5
  %101 = add <8 x i16> %93, <i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708>
  %102 = sub <8 x i16> %101, %99
  %103 = sub <8 x i16> %102, %100
  %104 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %98, <8 x i16> <i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486>) #5
  %105 = tail call <8 x i16> @llvm.uadd.sat.v8i16(<8 x i16> %104, <8 x i16> %93) #5
  %106 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %105, <8 x i16> <i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685>) #5
  %107 = ashr <8 x i16> %97, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %108 = ashr <8 x i16> %103, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %109 = lshr <8 x i16> %106, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %110 = getelementptr inbounds i8, i8* %0, i64 24
  %111 = getelementptr inbounds i8, i8* %1, i64 24
  %112 = getelementptr inbounds i8, i8* %2, i64 24
  %113 = bitcast i8* %110 to i64*
  %114 = load i64, i64* %113, align 1
  %115 = insertelement <2 x i64> undef, i64 %114, i32 0
  %116 = bitcast <2 x i64> %115 to <16 x i8>
  %117 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %116, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %118 = bitcast i8* %111 to i64*
  %119 = load i64, i64* %118, align 1
  %120 = insertelement <2 x i64> undef, i64 %119, i32 0
  %121 = bitcast <2 x i64> %120 to <16 x i8>
  %122 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %121, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %123 = bitcast i8* %112 to i64*
  %124 = load i64, i64* %123, align 1
  %125 = insertelement <2 x i64> undef, i64 %124, i32 0
  %126 = bitcast <2 x i64> %125 to <16 x i8>
  %127 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %126, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %128 = bitcast <16 x i8> %117 to <8 x i16>
  %129 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %128, <8 x i16> <i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077>) #5
  %130 = bitcast <16 x i8> %127 to <8 x i16>
  %131 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %130, <8 x i16> <i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149>) #5
  %132 = add <8 x i16> %129, <i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234>
  %133 = add <8 x i16> %132, %131
  %134 = bitcast <16 x i8> %122 to <8 x i16>
  %135 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %134, <8 x i16> <i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419>) #5
  %136 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %130, <8 x i16> <i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320>) #5
  %137 = add <8 x i16> %129, <i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708>
  %138 = sub <8 x i16> %137, %135
  %139 = sub <8 x i16> %138, %136
  %140 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %134, <8 x i16> <i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486>) #5
  %141 = tail call <8 x i16> @llvm.uadd.sat.v8i16(<8 x i16> %140, <8 x i16> %129) #5
  %142 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %141, <8 x i16> <i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685>) #5
  %143 = ashr <8 x i16> %133, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %144 = ashr <8 x i16> %139, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %145 = lshr <8 x i16> %142, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %146 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %35, <8 x i16> %71) #5
  %147 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %107, <8 x i16> %143) #5
  %148 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %36, <8 x i16> %72) #5
  %149 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %108, <8 x i16> %144) #5
  %150 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %37, <8 x i16> %73) #5
  %151 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %109, <8 x i16> %145) #5
  %152 = shufflevector <16 x i8> %146, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 16, i32 1, i32 16, i32 16, i32 2, i32 16, i32 16, i32 3, i32 16, i32 16, i32 4, i32 16, i32 16, i32 5>
  %153 = shufflevector <16 x i8> %146, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 16, i32 16, i32 6, i32 16, i32 16, i32 7, i32 16, i32 16, i32 8, i32 16, i32 16, i32 9, i32 16, i32 16, i32 10, i32 16>
  %154 = shufflevector <16 x i8> %146, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 16, i32 11, i32 16, i32 16, i32 12, i32 16, i32 16, i32 13, i32 16, i32 16, i32 14, i32 16, i32 16, i32 15, i32 16, i32 16>
  %155 = shufflevector <16 x i8> %147, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 16, i32 1, i32 16, i32 16, i32 2, i32 16, i32 16, i32 3, i32 16, i32 16, i32 4, i32 16, i32 16, i32 5>
  %156 = shufflevector <16 x i8> %147, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 16, i32 16, i32 6, i32 16, i32 16, i32 7, i32 16, i32 16, i32 8, i32 16, i32 16, i32 9, i32 16, i32 16, i32 10, i32 16>
  %157 = shufflevector <16 x i8> %147, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 16, i32 11, i32 16, i32 16, i32 12, i32 16, i32 16, i32 13, i32 16, i32 16, i32 14, i32 16, i32 16, i32 15, i32 16, i32 16>
  %158 = shufflevector <16 x i8> %148, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 16, i32 0, i32 16, i32 16, i32 1, i32 16, i32 16, i32 2, i32 16, i32 16, i32 3, i32 16, i32 16, i32 4, i32 16, i32 16>
  %159 = shufflevector <16 x i8> %148, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 5, i32 16, i32 16, i32 6, i32 16, i32 16, i32 7, i32 16, i32 16, i32 8, i32 16, i32 16, i32 9, i32 16, i32 16, i32 10>
  %160 = shufflevector <16 x i8> %148, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 16, i32 16, i32 11, i32 16, i32 16, i32 12, i32 16, i32 16, i32 13, i32 16, i32 16, i32 14, i32 16, i32 16, i32 15, i32 16>
  %161 = shufflevector <16 x i8> %149, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 16, i32 0, i32 16, i32 16, i32 1, i32 16, i32 16, i32 2, i32 16, i32 16, i32 3, i32 16, i32 16, i32 4, i32 16, i32 16>
  %162 = shufflevector <16 x i8> %149, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 5, i32 16, i32 16, i32 6, i32 16, i32 16, i32 7, i32 16, i32 16, i32 8, i32 16, i32 16, i32 9, i32 16, i32 16, i32 10>
  %163 = shufflevector <16 x i8> %149, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 16, i32 16, i32 11, i32 16, i32 16, i32 12, i32 16, i32 16, i32 13, i32 16, i32 16, i32 14, i32 16, i32 16, i32 15, i32 16>
  %164 = shufflevector <16 x i8> %150, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 16, i32 16, i32 0, i32 16, i32 16, i32 1, i32 16, i32 16, i32 2, i32 16, i32 16, i32 3, i32 16, i32 16, i32 4, i32 16>
  %165 = shufflevector <16 x i8> %150, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 16, i32 5, i32 16, i32 16, i32 6, i32 16, i32 16, i32 7, i32 16, i32 16, i32 8, i32 16, i32 16, i32 9, i32 16, i32 16>
  %166 = shufflevector <16 x i8> %150, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 10, i32 16, i32 16, i32 11, i32 16, i32 16, i32 12, i32 16, i32 16, i32 13, i32 16, i32 16, i32 14, i32 16, i32 16, i32 15>
  %167 = shufflevector <16 x i8> %151, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 16, i32 16, i32 0, i32 16, i32 16, i32 1, i32 16, i32 16, i32 2, i32 16, i32 16, i32 3, i32 16, i32 16, i32 4, i32 16>
  %168 = shufflevector <16 x i8> %151, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 16, i32 5, i32 16, i32 16, i32 6, i32 16, i32 16, i32 7, i32 16, i32 16, i32 8, i32 16, i32 16, i32 9, i32 16, i32 16>
  %169 = shufflevector <16 x i8> %151, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 10, i32 16, i32 16, i32 11, i32 16, i32 16, i32 12, i32 16, i32 16, i32 13, i32 16, i32 16, i32 14, i32 16, i32 16, i32 15>
  %170 = or <16 x i8> %158, %152
  %171 = or <16 x i8> %159, %153
  %172 = or <16 x i8> %160, %154
  %173 = or <16 x i8> %161, %155
  %174 = or <16 x i8> %162, %156
  %175 = or <16 x i8> %163, %157
  %176 = or <16 x i8> %170, %164
  %177 = or <16 x i8> %171, %165
  %178 = or <16 x i8> %172, %166
  %179 = or <16 x i8> %173, %167
  %180 = or <16 x i8> %174, %168
  %181 = or <16 x i8> %175, %169
  %182 = bitcast i8* %3 to <16 x i8>*
  store <16 x i8> %176, <16 x i8>* %182, align 1
  %183 = getelementptr inbounds i8, i8* %3, i64 16
  %184 = bitcast i8* %183 to <16 x i8>*
  store <16 x i8> %177, <16 x i8>* %184, align 1
  %185 = getelementptr inbounds i8, i8* %3, i64 32
  %186 = bitcast i8* %185 to <16 x i8>*
  store <16 x i8> %178, <16 x i8>* %186, align 1
  %187 = getelementptr inbounds i8, i8* %3, i64 48
  %188 = bitcast i8* %187 to <16 x i8>*
  store <16 x i8> %179, <16 x i8>* %188, align 1
  %189 = getelementptr inbounds i8, i8* %3, i64 64
  %190 = bitcast i8* %189 to <16 x i8>*
  store <16 x i8> %180, <16 x i8>* %190, align 1
  %191 = getelementptr inbounds i8, i8* %3, i64 80
  %192 = bitcast i8* %191 to <16 x i8>*
  store <16 x i8> %181, <16 x i8>* %192, align 1
  ret void
}

; Function Attrs: nounwind ssp uwtable
define hidden void @VP8YuvToBgr32_SSE41(i8* nocapture readonly, i8* nocapture readonly, i8* nocapture readonly, i8* nocapture) local_unnamed_addr #0 {
  %5 = bitcast i8* %0 to i64*
  %6 = load i64, i64* %5, align 1
  %7 = insertelement <2 x i64> undef, i64 %6, i32 0
  %8 = bitcast <2 x i64> %7 to <16 x i8>
  %9 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %8, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %10 = bitcast i8* %1 to i64*
  %11 = load i64, i64* %10, align 1
  %12 = insertelement <2 x i64> undef, i64 %11, i32 0
  %13 = bitcast <2 x i64> %12 to <16 x i8>
  %14 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %13, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %15 = bitcast i8* %2 to i64*
  %16 = load i64, i64* %15, align 1
  %17 = insertelement <2 x i64> undef, i64 %16, i32 0
  %18 = bitcast <2 x i64> %17 to <16 x i8>
  %19 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %18, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %20 = bitcast <16 x i8> %9 to <8 x i16>
  %21 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %20, <8 x i16> <i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077>) #5
  %22 = bitcast <16 x i8> %19 to <8 x i16>
  %23 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %22, <8 x i16> <i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149>) #5
  %24 = add <8 x i16> %21, <i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234>
  %25 = add <8 x i16> %24, %23
  %26 = bitcast <16 x i8> %14 to <8 x i16>
  %27 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %26, <8 x i16> <i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419>) #5
  %28 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %22, <8 x i16> <i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320>) #5
  %29 = add <8 x i16> %21, <i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708>
  %30 = sub <8 x i16> %29, %27
  %31 = sub <8 x i16> %30, %28
  %32 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %26, <8 x i16> <i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486>) #5
  %33 = tail call <8 x i16> @llvm.uadd.sat.v8i16(<8 x i16> %32, <8 x i16> %21) #5
  %34 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %33, <8 x i16> <i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685>) #5
  %35 = ashr <8 x i16> %25, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %36 = ashr <8 x i16> %31, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %37 = lshr <8 x i16> %34, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %38 = getelementptr inbounds i8, i8* %0, i64 8
  %39 = getelementptr inbounds i8, i8* %1, i64 8
  %40 = getelementptr inbounds i8, i8* %2, i64 8
  %41 = bitcast i8* %38 to i64*
  %42 = load i64, i64* %41, align 1
  %43 = insertelement <2 x i64> undef, i64 %42, i32 0
  %44 = bitcast <2 x i64> %43 to <16 x i8>
  %45 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %44, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %46 = bitcast i8* %39 to i64*
  %47 = load i64, i64* %46, align 1
  %48 = insertelement <2 x i64> undef, i64 %47, i32 0
  %49 = bitcast <2 x i64> %48 to <16 x i8>
  %50 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %49, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %51 = bitcast i8* %40 to i64*
  %52 = load i64, i64* %51, align 1
  %53 = insertelement <2 x i64> undef, i64 %52, i32 0
  %54 = bitcast <2 x i64> %53 to <16 x i8>
  %55 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %54, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %56 = bitcast <16 x i8> %45 to <8 x i16>
  %57 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %56, <8 x i16> <i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077>) #5
  %58 = bitcast <16 x i8> %55 to <8 x i16>
  %59 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %58, <8 x i16> <i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149>) #5
  %60 = add <8 x i16> %57, <i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234>
  %61 = add <8 x i16> %60, %59
  %62 = bitcast <16 x i8> %50 to <8 x i16>
  %63 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %62, <8 x i16> <i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419>) #5
  %64 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %58, <8 x i16> <i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320>) #5
  %65 = add <8 x i16> %57, <i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708>
  %66 = sub <8 x i16> %65, %63
  %67 = sub <8 x i16> %66, %64
  %68 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %62, <8 x i16> <i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486>) #5
  %69 = tail call <8 x i16> @llvm.uadd.sat.v8i16(<8 x i16> %68, <8 x i16> %57) #5
  %70 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %69, <8 x i16> <i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685>) #5
  %71 = ashr <8 x i16> %61, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %72 = ashr <8 x i16> %67, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %73 = lshr <8 x i16> %70, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %74 = getelementptr inbounds i8, i8* %0, i64 16
  %75 = getelementptr inbounds i8, i8* %1, i64 16
  %76 = getelementptr inbounds i8, i8* %2, i64 16
  %77 = bitcast i8* %74 to i64*
  %78 = load i64, i64* %77, align 1
  %79 = insertelement <2 x i64> undef, i64 %78, i32 0
  %80 = bitcast <2 x i64> %79 to <16 x i8>
  %81 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %80, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %82 = bitcast i8* %75 to i64*
  %83 = load i64, i64* %82, align 1
  %84 = insertelement <2 x i64> undef, i64 %83, i32 0
  %85 = bitcast <2 x i64> %84 to <16 x i8>
  %86 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %85, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %87 = bitcast i8* %76 to i64*
  %88 = load i64, i64* %87, align 1
  %89 = insertelement <2 x i64> undef, i64 %88, i32 0
  %90 = bitcast <2 x i64> %89 to <16 x i8>
  %91 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %90, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %92 = bitcast <16 x i8> %81 to <8 x i16>
  %93 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %92, <8 x i16> <i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077>) #5
  %94 = bitcast <16 x i8> %91 to <8 x i16>
  %95 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %94, <8 x i16> <i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149>) #5
  %96 = add <8 x i16> %93, <i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234>
  %97 = add <8 x i16> %96, %95
  %98 = bitcast <16 x i8> %86 to <8 x i16>
  %99 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %98, <8 x i16> <i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419>) #5
  %100 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %94, <8 x i16> <i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320>) #5
  %101 = add <8 x i16> %93, <i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708>
  %102 = sub <8 x i16> %101, %99
  %103 = sub <8 x i16> %102, %100
  %104 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %98, <8 x i16> <i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486>) #5
  %105 = tail call <8 x i16> @llvm.uadd.sat.v8i16(<8 x i16> %104, <8 x i16> %93) #5
  %106 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %105, <8 x i16> <i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685>) #5
  %107 = ashr <8 x i16> %97, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %108 = ashr <8 x i16> %103, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %109 = lshr <8 x i16> %106, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %110 = getelementptr inbounds i8, i8* %0, i64 24
  %111 = getelementptr inbounds i8, i8* %1, i64 24
  %112 = getelementptr inbounds i8, i8* %2, i64 24
  %113 = bitcast i8* %110 to i64*
  %114 = load i64, i64* %113, align 1
  %115 = insertelement <2 x i64> undef, i64 %114, i32 0
  %116 = bitcast <2 x i64> %115 to <16 x i8>
  %117 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %116, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %118 = bitcast i8* %111 to i64*
  %119 = load i64, i64* %118, align 1
  %120 = insertelement <2 x i64> undef, i64 %119, i32 0
  %121 = bitcast <2 x i64> %120 to <16 x i8>
  %122 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %121, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %123 = bitcast i8* %112 to i64*
  %124 = load i64, i64* %123, align 1
  %125 = insertelement <2 x i64> undef, i64 %124, i32 0
  %126 = bitcast <2 x i64> %125 to <16 x i8>
  %127 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %126, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %128 = bitcast <16 x i8> %117 to <8 x i16>
  %129 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %128, <8 x i16> <i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077>) #5
  %130 = bitcast <16 x i8> %127 to <8 x i16>
  %131 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %130, <8 x i16> <i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149>) #5
  %132 = add <8 x i16> %129, <i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234>
  %133 = add <8 x i16> %132, %131
  %134 = bitcast <16 x i8> %122 to <8 x i16>
  %135 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %134, <8 x i16> <i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419>) #5
  %136 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %130, <8 x i16> <i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320>) #5
  %137 = add <8 x i16> %129, <i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708>
  %138 = sub <8 x i16> %137, %135
  %139 = sub <8 x i16> %138, %136
  %140 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %134, <8 x i16> <i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486>) #5
  %141 = tail call <8 x i16> @llvm.uadd.sat.v8i16(<8 x i16> %140, <8 x i16> %129) #5
  %142 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %141, <8 x i16> <i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685>) #5
  %143 = ashr <8 x i16> %133, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %144 = ashr <8 x i16> %139, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %145 = lshr <8 x i16> %142, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %146 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %37, <8 x i16> %73) #5
  %147 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %109, <8 x i16> %145) #5
  %148 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %36, <8 x i16> %72) #5
  %149 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %108, <8 x i16> %144) #5
  %150 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %35, <8 x i16> %71) #5
  %151 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %107, <8 x i16> %143) #5
  %152 = shufflevector <16 x i8> %146, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 16, i32 1, i32 16, i32 16, i32 2, i32 16, i32 16, i32 3, i32 16, i32 16, i32 4, i32 16, i32 16, i32 5>
  %153 = shufflevector <16 x i8> %146, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 16, i32 16, i32 6, i32 16, i32 16, i32 7, i32 16, i32 16, i32 8, i32 16, i32 16, i32 9, i32 16, i32 16, i32 10, i32 16>
  %154 = shufflevector <16 x i8> %146, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 16, i32 11, i32 16, i32 16, i32 12, i32 16, i32 16, i32 13, i32 16, i32 16, i32 14, i32 16, i32 16, i32 15, i32 16, i32 16>
  %155 = shufflevector <16 x i8> %147, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 16, i32 1, i32 16, i32 16, i32 2, i32 16, i32 16, i32 3, i32 16, i32 16, i32 4, i32 16, i32 16, i32 5>
  %156 = shufflevector <16 x i8> %147, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 16, i32 16, i32 6, i32 16, i32 16, i32 7, i32 16, i32 16, i32 8, i32 16, i32 16, i32 9, i32 16, i32 16, i32 10, i32 16>
  %157 = shufflevector <16 x i8> %147, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 16, i32 11, i32 16, i32 16, i32 12, i32 16, i32 16, i32 13, i32 16, i32 16, i32 14, i32 16, i32 16, i32 15, i32 16, i32 16>
  %158 = shufflevector <16 x i8> %148, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 16, i32 0, i32 16, i32 16, i32 1, i32 16, i32 16, i32 2, i32 16, i32 16, i32 3, i32 16, i32 16, i32 4, i32 16, i32 16>
  %159 = shufflevector <16 x i8> %148, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 5, i32 16, i32 16, i32 6, i32 16, i32 16, i32 7, i32 16, i32 16, i32 8, i32 16, i32 16, i32 9, i32 16, i32 16, i32 10>
  %160 = shufflevector <16 x i8> %148, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 16, i32 16, i32 11, i32 16, i32 16, i32 12, i32 16, i32 16, i32 13, i32 16, i32 16, i32 14, i32 16, i32 16, i32 15, i32 16>
  %161 = shufflevector <16 x i8> %149, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 16, i32 0, i32 16, i32 16, i32 1, i32 16, i32 16, i32 2, i32 16, i32 16, i32 3, i32 16, i32 16, i32 4, i32 16, i32 16>
  %162 = shufflevector <16 x i8> %149, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 5, i32 16, i32 16, i32 6, i32 16, i32 16, i32 7, i32 16, i32 16, i32 8, i32 16, i32 16, i32 9, i32 16, i32 16, i32 10>
  %163 = shufflevector <16 x i8> %149, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 16, i32 16, i32 11, i32 16, i32 16, i32 12, i32 16, i32 16, i32 13, i32 16, i32 16, i32 14, i32 16, i32 16, i32 15, i32 16>
  %164 = shufflevector <16 x i8> %150, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 16, i32 16, i32 0, i32 16, i32 16, i32 1, i32 16, i32 16, i32 2, i32 16, i32 16, i32 3, i32 16, i32 16, i32 4, i32 16>
  %165 = shufflevector <16 x i8> %150, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 16, i32 5, i32 16, i32 16, i32 6, i32 16, i32 16, i32 7, i32 16, i32 16, i32 8, i32 16, i32 16, i32 9, i32 16, i32 16>
  %166 = shufflevector <16 x i8> %150, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 10, i32 16, i32 16, i32 11, i32 16, i32 16, i32 12, i32 16, i32 16, i32 13, i32 16, i32 16, i32 14, i32 16, i32 16, i32 15>
  %167 = shufflevector <16 x i8> %151, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 16, i32 16, i32 0, i32 16, i32 16, i32 1, i32 16, i32 16, i32 2, i32 16, i32 16, i32 3, i32 16, i32 16, i32 4, i32 16>
  %168 = shufflevector <16 x i8> %151, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 16, i32 5, i32 16, i32 16, i32 6, i32 16, i32 16, i32 7, i32 16, i32 16, i32 8, i32 16, i32 16, i32 9, i32 16, i32 16>
  %169 = shufflevector <16 x i8> %151, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 10, i32 16, i32 16, i32 11, i32 16, i32 16, i32 12, i32 16, i32 16, i32 13, i32 16, i32 16, i32 14, i32 16, i32 16, i32 15>
  %170 = or <16 x i8> %158, %152
  %171 = or <16 x i8> %159, %153
  %172 = or <16 x i8> %160, %154
  %173 = or <16 x i8> %161, %155
  %174 = or <16 x i8> %162, %156
  %175 = or <16 x i8> %163, %157
  %176 = or <16 x i8> %170, %164
  %177 = or <16 x i8> %171, %165
  %178 = or <16 x i8> %172, %166
  %179 = or <16 x i8> %173, %167
  %180 = or <16 x i8> %174, %168
  %181 = or <16 x i8> %175, %169
  %182 = bitcast i8* %3 to <16 x i8>*
  store <16 x i8> %176, <16 x i8>* %182, align 1
  %183 = getelementptr inbounds i8, i8* %3, i64 16
  %184 = bitcast i8* %183 to <16 x i8>*
  store <16 x i8> %177, <16 x i8>* %184, align 1
  %185 = getelementptr inbounds i8, i8* %3, i64 32
  %186 = bitcast i8* %185 to <16 x i8>*
  store <16 x i8> %178, <16 x i8>* %186, align 1
  %187 = getelementptr inbounds i8, i8* %3, i64 48
  %188 = bitcast i8* %187 to <16 x i8>*
  store <16 x i8> %179, <16 x i8>* %188, align 1
  %189 = getelementptr inbounds i8, i8* %3, i64 64
  %190 = bitcast i8* %189 to <16 x i8>*
  store <16 x i8> %180, <16 x i8>* %190, align 1
  %191 = getelementptr inbounds i8, i8* %3, i64 80
  %192 = bitcast i8* %191 to <16 x i8>*
  store <16 x i8> %181, <16 x i8>* %192, align 1
  ret void
}

; Function Attrs: nofree norecurse nounwind ssp uwtable writeonly
define hidden void @WebPInitSamplersSSE41() local_unnamed_addr #1 {
  store void (i8*, i8*, i8*, i8*, i32)* @YuvToRgbRow_SSE41, void (i8*, i8*, i8*, i8*, i32)** getelementptr inbounds ([0 x void (i8*, i8*, i8*, i8*, i32)*], [0 x void (i8*, i8*, i8*, i8*, i32)*]* @WebPSamplers, i64 0, i64 0), align 8
  store void (i8*, i8*, i8*, i8*, i32)* @YuvToBgrRow_SSE41, void (i8*, i8*, i8*, i8*, i32)** getelementptr inbounds ([0 x void (i8*, i8*, i8*, i8*, i32)*], [0 x void (i8*, i8*, i8*, i8*, i32)*]* @WebPSamplers, i64 0, i64 2), align 8
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal void @YuvToRgbRow_SSE41(i8* nocapture readonly, i8* nocapture readonly, i8* nocapture readonly, i8* nocapture, i32) #0 {
  %6 = icmp slt i32 %4, 32
  br i1 %6, label %9, label %16

7:                                                ; preds = %16
  %8 = and i32 %4, -32
  br label %9

9:                                                ; preds = %7, %5
  %10 = phi i8* [ %1, %5 ], [ %219, %7 ]
  %11 = phi i8* [ %2, %5 ], [ %220, %7 ]
  %12 = phi i8* [ %3, %5 ], [ %221, %7 ]
  %13 = phi i8* [ %0, %5 ], [ %218, %7 ]
  %14 = phi i32 [ 0, %5 ], [ %8, %7 ]
  %15 = icmp slt i32 %14, %4
  br i1 %15, label %224, label %281

16:                                               ; preds = %5, %16
  %17 = phi i32 [ %222, %16 ], [ 32, %5 ]
  %18 = phi i8* [ %218, %16 ], [ %0, %5 ]
  %19 = phi i8* [ %221, %16 ], [ %3, %5 ]
  %20 = phi i8* [ %220, %16 ], [ %2, %5 ]
  %21 = phi i8* [ %219, %16 ], [ %1, %5 ]
  %22 = bitcast i8* %18 to i64*
  %23 = load i64, i64* %22, align 1
  %24 = insertelement <2 x i64> undef, i64 %23, i32 0
  %25 = bitcast <2 x i64> %24 to <16 x i8>
  %26 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %25, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %27 = bitcast i8* %21 to i32*
  %28 = load i32, i32* %27, align 4
  %29 = insertelement <4 x i32> undef, i32 %28, i32 0
  %30 = bitcast <4 x i32> %29 to <16 x i8>
  %31 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %30, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %32 = bitcast <16 x i8> %31 to <8 x i16>
  %33 = shufflevector <8 x i16> %32, <8 x i16> undef, <8 x i32> <i32 0, i32 0, i32 1, i32 1, i32 2, i32 2, i32 3, i32 3>
  %34 = bitcast i8* %20 to i32*
  %35 = load i32, i32* %34, align 4
  %36 = insertelement <4 x i32> undef, i32 %35, i32 0
  %37 = bitcast <4 x i32> %36 to <16 x i8>
  %38 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %37, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %39 = bitcast <16 x i8> %38 to <8 x i16>
  %40 = shufflevector <8 x i16> %39, <8 x i16> undef, <8 x i32> <i32 0, i32 0, i32 1, i32 1, i32 2, i32 2, i32 3, i32 3>
  %41 = bitcast <16 x i8> %26 to <8 x i16>
  %42 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %41, <8 x i16> <i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077>) #5
  %43 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %40, <8 x i16> <i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149>) #5
  %44 = add <8 x i16> %42, <i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234>
  %45 = add <8 x i16> %44, %43
  %46 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %33, <8 x i16> <i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419>) #5
  %47 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %40, <8 x i16> <i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320>) #5
  %48 = add <8 x i16> %42, <i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708>
  %49 = sub <8 x i16> %48, %46
  %50 = sub <8 x i16> %49, %47
  %51 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %33, <8 x i16> <i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486>) #5
  %52 = tail call <8 x i16> @llvm.uadd.sat.v8i16(<8 x i16> %51, <8 x i16> %42) #5
  %53 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %52, <8 x i16> <i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685>) #5
  %54 = ashr <8 x i16> %45, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %55 = ashr <8 x i16> %50, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %56 = lshr <8 x i16> %53, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %57 = getelementptr inbounds i8, i8* %18, i64 8
  %58 = getelementptr inbounds i8, i8* %21, i64 4
  %59 = getelementptr inbounds i8, i8* %20, i64 4
  %60 = bitcast i8* %57 to i64*
  %61 = load i64, i64* %60, align 1
  %62 = insertelement <2 x i64> undef, i64 %61, i32 0
  %63 = bitcast <2 x i64> %62 to <16 x i8>
  %64 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %63, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %65 = bitcast i8* %58 to i32*
  %66 = load i32, i32* %65, align 4
  %67 = insertelement <4 x i32> undef, i32 %66, i32 0
  %68 = bitcast <4 x i32> %67 to <16 x i8>
  %69 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %68, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %70 = bitcast <16 x i8> %69 to <8 x i16>
  %71 = shufflevector <8 x i16> %70, <8 x i16> undef, <8 x i32> <i32 0, i32 0, i32 1, i32 1, i32 2, i32 2, i32 3, i32 3>
  %72 = bitcast i8* %59 to i32*
  %73 = load i32, i32* %72, align 4
  %74 = insertelement <4 x i32> undef, i32 %73, i32 0
  %75 = bitcast <4 x i32> %74 to <16 x i8>
  %76 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %75, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %77 = bitcast <16 x i8> %76 to <8 x i16>
  %78 = shufflevector <8 x i16> %77, <8 x i16> undef, <8 x i32> <i32 0, i32 0, i32 1, i32 1, i32 2, i32 2, i32 3, i32 3>
  %79 = bitcast <16 x i8> %64 to <8 x i16>
  %80 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %79, <8 x i16> <i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077>) #5
  %81 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %78, <8 x i16> <i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149>) #5
  %82 = add <8 x i16> %80, <i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234>
  %83 = add <8 x i16> %82, %81
  %84 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %71, <8 x i16> <i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419>) #5
  %85 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %78, <8 x i16> <i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320>) #5
  %86 = add <8 x i16> %80, <i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708>
  %87 = sub <8 x i16> %86, %84
  %88 = sub <8 x i16> %87, %85
  %89 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %71, <8 x i16> <i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486>) #5
  %90 = tail call <8 x i16> @llvm.uadd.sat.v8i16(<8 x i16> %89, <8 x i16> %80) #5
  %91 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %90, <8 x i16> <i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685>) #5
  %92 = ashr <8 x i16> %83, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %93 = ashr <8 x i16> %88, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %94 = lshr <8 x i16> %91, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %95 = getelementptr inbounds i8, i8* %18, i64 16
  %96 = getelementptr inbounds i8, i8* %21, i64 8
  %97 = getelementptr inbounds i8, i8* %20, i64 8
  %98 = bitcast i8* %95 to i64*
  %99 = load i64, i64* %98, align 1
  %100 = insertelement <2 x i64> undef, i64 %99, i32 0
  %101 = bitcast <2 x i64> %100 to <16 x i8>
  %102 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %101, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %103 = bitcast i8* %96 to i32*
  %104 = load i32, i32* %103, align 4
  %105 = insertelement <4 x i32> undef, i32 %104, i32 0
  %106 = bitcast <4 x i32> %105 to <16 x i8>
  %107 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %106, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %108 = bitcast <16 x i8> %107 to <8 x i16>
  %109 = shufflevector <8 x i16> %108, <8 x i16> undef, <8 x i32> <i32 0, i32 0, i32 1, i32 1, i32 2, i32 2, i32 3, i32 3>
  %110 = bitcast i8* %97 to i32*
  %111 = load i32, i32* %110, align 4
  %112 = insertelement <4 x i32> undef, i32 %111, i32 0
  %113 = bitcast <4 x i32> %112 to <16 x i8>
  %114 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %113, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %115 = bitcast <16 x i8> %114 to <8 x i16>
  %116 = shufflevector <8 x i16> %115, <8 x i16> undef, <8 x i32> <i32 0, i32 0, i32 1, i32 1, i32 2, i32 2, i32 3, i32 3>
  %117 = bitcast <16 x i8> %102 to <8 x i16>
  %118 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %117, <8 x i16> <i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077>) #5
  %119 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %116, <8 x i16> <i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149>) #5
  %120 = add <8 x i16> %118, <i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234>
  %121 = add <8 x i16> %120, %119
  %122 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %109, <8 x i16> <i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419>) #5
  %123 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %116, <8 x i16> <i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320>) #5
  %124 = add <8 x i16> %118, <i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708>
  %125 = sub <8 x i16> %124, %122
  %126 = sub <8 x i16> %125, %123
  %127 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %109, <8 x i16> <i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486>) #5
  %128 = tail call <8 x i16> @llvm.uadd.sat.v8i16(<8 x i16> %127, <8 x i16> %118) #5
  %129 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %128, <8 x i16> <i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685>) #5
  %130 = ashr <8 x i16> %121, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %131 = ashr <8 x i16> %126, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %132 = lshr <8 x i16> %129, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %133 = getelementptr inbounds i8, i8* %18, i64 24
  %134 = getelementptr inbounds i8, i8* %21, i64 12
  %135 = getelementptr inbounds i8, i8* %20, i64 12
  %136 = bitcast i8* %133 to i64*
  %137 = load i64, i64* %136, align 1
  %138 = insertelement <2 x i64> undef, i64 %137, i32 0
  %139 = bitcast <2 x i64> %138 to <16 x i8>
  %140 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %139, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %141 = bitcast i8* %134 to i32*
  %142 = load i32, i32* %141, align 4
  %143 = insertelement <4 x i32> undef, i32 %142, i32 0
  %144 = bitcast <4 x i32> %143 to <16 x i8>
  %145 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %144, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %146 = bitcast <16 x i8> %145 to <8 x i16>
  %147 = shufflevector <8 x i16> %146, <8 x i16> undef, <8 x i32> <i32 0, i32 0, i32 1, i32 1, i32 2, i32 2, i32 3, i32 3>
  %148 = bitcast i8* %135 to i32*
  %149 = load i32, i32* %148, align 4
  %150 = insertelement <4 x i32> undef, i32 %149, i32 0
  %151 = bitcast <4 x i32> %150 to <16 x i8>
  %152 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %151, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %153 = bitcast <16 x i8> %152 to <8 x i16>
  %154 = shufflevector <8 x i16> %153, <8 x i16> undef, <8 x i32> <i32 0, i32 0, i32 1, i32 1, i32 2, i32 2, i32 3, i32 3>
  %155 = bitcast <16 x i8> %140 to <8 x i16>
  %156 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %155, <8 x i16> <i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077>) #5
  %157 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %154, <8 x i16> <i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149>) #5
  %158 = add <8 x i16> %156, <i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234>
  %159 = add <8 x i16> %158, %157
  %160 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %147, <8 x i16> <i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419>) #5
  %161 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %154, <8 x i16> <i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320>) #5
  %162 = add <8 x i16> %156, <i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708>
  %163 = sub <8 x i16> %162, %160
  %164 = sub <8 x i16> %163, %161
  %165 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %147, <8 x i16> <i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486>) #5
  %166 = tail call <8 x i16> @llvm.uadd.sat.v8i16(<8 x i16> %165, <8 x i16> %156) #5
  %167 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %166, <8 x i16> <i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685>) #5
  %168 = ashr <8 x i16> %159, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %169 = ashr <8 x i16> %164, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %170 = lshr <8 x i16> %167, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %171 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %54, <8 x i16> %92) #5
  %172 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %130, <8 x i16> %168) #5
  %173 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %55, <8 x i16> %93) #5
  %174 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %131, <8 x i16> %169) #5
  %175 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %56, <8 x i16> %94) #5
  %176 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %132, <8 x i16> %170) #5
  %177 = shufflevector <16 x i8> %171, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 16, i32 1, i32 16, i32 16, i32 2, i32 16, i32 16, i32 3, i32 16, i32 16, i32 4, i32 16, i32 16, i32 5>
  %178 = shufflevector <16 x i8> %171, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 16, i32 16, i32 6, i32 16, i32 16, i32 7, i32 16, i32 16, i32 8, i32 16, i32 16, i32 9, i32 16, i32 16, i32 10, i32 16>
  %179 = shufflevector <16 x i8> %171, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 16, i32 11, i32 16, i32 16, i32 12, i32 16, i32 16, i32 13, i32 16, i32 16, i32 14, i32 16, i32 16, i32 15, i32 16, i32 16>
  %180 = shufflevector <16 x i8> %172, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 16, i32 1, i32 16, i32 16, i32 2, i32 16, i32 16, i32 3, i32 16, i32 16, i32 4, i32 16, i32 16, i32 5>
  %181 = shufflevector <16 x i8> %172, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 16, i32 16, i32 6, i32 16, i32 16, i32 7, i32 16, i32 16, i32 8, i32 16, i32 16, i32 9, i32 16, i32 16, i32 10, i32 16>
  %182 = shufflevector <16 x i8> %172, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 16, i32 11, i32 16, i32 16, i32 12, i32 16, i32 16, i32 13, i32 16, i32 16, i32 14, i32 16, i32 16, i32 15, i32 16, i32 16>
  %183 = shufflevector <16 x i8> %173, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 16, i32 0, i32 16, i32 16, i32 1, i32 16, i32 16, i32 2, i32 16, i32 16, i32 3, i32 16, i32 16, i32 4, i32 16, i32 16>
  %184 = shufflevector <16 x i8> %173, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 5, i32 16, i32 16, i32 6, i32 16, i32 16, i32 7, i32 16, i32 16, i32 8, i32 16, i32 16, i32 9, i32 16, i32 16, i32 10>
  %185 = shufflevector <16 x i8> %173, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 16, i32 16, i32 11, i32 16, i32 16, i32 12, i32 16, i32 16, i32 13, i32 16, i32 16, i32 14, i32 16, i32 16, i32 15, i32 16>
  %186 = shufflevector <16 x i8> %174, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 16, i32 0, i32 16, i32 16, i32 1, i32 16, i32 16, i32 2, i32 16, i32 16, i32 3, i32 16, i32 16, i32 4, i32 16, i32 16>
  %187 = shufflevector <16 x i8> %174, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 5, i32 16, i32 16, i32 6, i32 16, i32 16, i32 7, i32 16, i32 16, i32 8, i32 16, i32 16, i32 9, i32 16, i32 16, i32 10>
  %188 = shufflevector <16 x i8> %174, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 16, i32 16, i32 11, i32 16, i32 16, i32 12, i32 16, i32 16, i32 13, i32 16, i32 16, i32 14, i32 16, i32 16, i32 15, i32 16>
  %189 = shufflevector <16 x i8> %175, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 16, i32 16, i32 0, i32 16, i32 16, i32 1, i32 16, i32 16, i32 2, i32 16, i32 16, i32 3, i32 16, i32 16, i32 4, i32 16>
  %190 = shufflevector <16 x i8> %175, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 16, i32 5, i32 16, i32 16, i32 6, i32 16, i32 16, i32 7, i32 16, i32 16, i32 8, i32 16, i32 16, i32 9, i32 16, i32 16>
  %191 = shufflevector <16 x i8> %175, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 10, i32 16, i32 16, i32 11, i32 16, i32 16, i32 12, i32 16, i32 16, i32 13, i32 16, i32 16, i32 14, i32 16, i32 16, i32 15>
  %192 = shufflevector <16 x i8> %176, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 16, i32 16, i32 0, i32 16, i32 16, i32 1, i32 16, i32 16, i32 2, i32 16, i32 16, i32 3, i32 16, i32 16, i32 4, i32 16>
  %193 = shufflevector <16 x i8> %176, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 16, i32 5, i32 16, i32 16, i32 6, i32 16, i32 16, i32 7, i32 16, i32 16, i32 8, i32 16, i32 16, i32 9, i32 16, i32 16>
  %194 = shufflevector <16 x i8> %176, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 10, i32 16, i32 16, i32 11, i32 16, i32 16, i32 12, i32 16, i32 16, i32 13, i32 16, i32 16, i32 14, i32 16, i32 16, i32 15>
  %195 = or <16 x i8> %183, %177
  %196 = or <16 x i8> %184, %178
  %197 = or <16 x i8> %185, %179
  %198 = or <16 x i8> %186, %180
  %199 = or <16 x i8> %187, %181
  %200 = or <16 x i8> %188, %182
  %201 = or <16 x i8> %195, %189
  %202 = or <16 x i8> %196, %190
  %203 = or <16 x i8> %197, %191
  %204 = or <16 x i8> %198, %192
  %205 = or <16 x i8> %199, %193
  %206 = or <16 x i8> %200, %194
  %207 = bitcast i8* %19 to <16 x i8>*
  store <16 x i8> %201, <16 x i8>* %207, align 1
  %208 = getelementptr inbounds i8, i8* %19, i64 16
  %209 = bitcast i8* %208 to <16 x i8>*
  store <16 x i8> %202, <16 x i8>* %209, align 1
  %210 = getelementptr inbounds i8, i8* %19, i64 32
  %211 = bitcast i8* %210 to <16 x i8>*
  store <16 x i8> %203, <16 x i8>* %211, align 1
  %212 = getelementptr inbounds i8, i8* %19, i64 48
  %213 = bitcast i8* %212 to <16 x i8>*
  store <16 x i8> %204, <16 x i8>* %213, align 1
  %214 = getelementptr inbounds i8, i8* %19, i64 64
  %215 = bitcast i8* %214 to <16 x i8>*
  store <16 x i8> %205, <16 x i8>* %215, align 1
  %216 = getelementptr inbounds i8, i8* %19, i64 80
  %217 = bitcast i8* %216 to <16 x i8>*
  store <16 x i8> %206, <16 x i8>* %217, align 1
  %218 = getelementptr inbounds i8, i8* %18, i64 32
  %219 = getelementptr inbounds i8, i8* %21, i64 16
  %220 = getelementptr inbounds i8, i8* %20, i64 16
  %221 = getelementptr inbounds i8, i8* %19, i64 96
  %222 = add nuw nsw i32 %17, 32
  %223 = icmp sgt i32 %222, %4
  br i1 %223, label %7, label %16

224:                                              ; preds = %9, %224
  %225 = phi i32 [ %279, %224 ], [ %14, %9 ]
  %226 = phi i8* [ %274, %224 ], [ %13, %9 ]
  %227 = phi i8* [ %273, %224 ], [ %12, %9 ]
  %228 = phi i8* [ %278, %224 ], [ %11, %9 ]
  %229 = phi i8* [ %277, %224 ], [ %10, %9 ]
  %230 = load i8, i8* %226, align 1
  %231 = zext i8 %230 to i32
  %232 = load i8, i8* %229, align 1
  %233 = zext i8 %232 to i32
  %234 = load i8, i8* %228, align 1
  %235 = zext i8 %234 to i32
  %236 = mul nuw nsw i32 %231, 19077
  %237 = lshr i32 %236, 8
  %238 = mul nuw nsw i32 %235, 26149
  %239 = lshr i32 %238, 8
  %240 = add nsw i32 %237, -14234
  %241 = add nsw i32 %240, %239
  %242 = icmp ult i32 %241, 16384
  %243 = lshr i32 %241, 6
  %244 = icmp slt i32 %241, 0
  %245 = select i1 %244, i32 0, i32 255
  %246 = select i1 %242, i32 %243, i32 %245
  %247 = trunc i32 %246 to i8
  store i8 %247, i8* %227, align 1
  %248 = mul nuw nsw i32 %233, 6419
  %249 = lshr i32 %248, 8
  %250 = mul nuw nsw i32 %235, 13320
  %251 = lshr i32 %250, 8
  %252 = add nuw nsw i32 %237, 8708
  %253 = sub nuw nsw i32 %252, %249
  %254 = sub nsw i32 %253, %251
  %255 = icmp ult i32 %254, 16384
  %256 = lshr i32 %254, 6
  %257 = icmp slt i32 %254, 0
  %258 = select i1 %257, i32 0, i32 255
  %259 = select i1 %255, i32 %256, i32 %258
  %260 = trunc i32 %259 to i8
  %261 = getelementptr inbounds i8, i8* %227, i64 1
  store i8 %260, i8* %261, align 1
  %262 = mul nuw nsw i32 %233, 33050
  %263 = lshr i32 %262, 8
  %264 = add nsw i32 %237, -17685
  %265 = add nsw i32 %264, %263
  %266 = icmp ult i32 %265, 16384
  %267 = lshr i32 %265, 6
  %268 = icmp slt i32 %265, 0
  %269 = select i1 %268, i32 0, i32 255
  %270 = select i1 %266, i32 %267, i32 %269
  %271 = trunc i32 %270 to i8
  %272 = getelementptr inbounds i8, i8* %227, i64 2
  store i8 %271, i8* %272, align 1
  %273 = getelementptr inbounds i8, i8* %227, i64 3
  %274 = getelementptr inbounds i8, i8* %226, i64 1
  %275 = and i32 %225, 1
  %276 = zext i32 %275 to i64
  %277 = getelementptr inbounds i8, i8* %229, i64 %276
  %278 = getelementptr inbounds i8, i8* %228, i64 %276
  %279 = add nuw nsw i32 %225, 1
  %280 = icmp eq i32 %279, %4
  br i1 %280, label %281, label %224

281:                                              ; preds = %224, %9
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal void @YuvToBgrRow_SSE41(i8* nocapture readonly, i8* nocapture readonly, i8* nocapture readonly, i8* nocapture, i32) #0 {
  %6 = icmp slt i32 %4, 32
  br i1 %6, label %9, label %16

7:                                                ; preds = %16
  %8 = and i32 %4, -32
  br label %9

9:                                                ; preds = %7, %5
  %10 = phi i8* [ %1, %5 ], [ %219, %7 ]
  %11 = phi i8* [ %2, %5 ], [ %220, %7 ]
  %12 = phi i8* [ %3, %5 ], [ %221, %7 ]
  %13 = phi i8* [ %0, %5 ], [ %218, %7 ]
  %14 = phi i32 [ 0, %5 ], [ %8, %7 ]
  %15 = icmp slt i32 %14, %4
  br i1 %15, label %224, label %281

16:                                               ; preds = %5, %16
  %17 = phi i32 [ %222, %16 ], [ 32, %5 ]
  %18 = phi i8* [ %218, %16 ], [ %0, %5 ]
  %19 = phi i8* [ %221, %16 ], [ %3, %5 ]
  %20 = phi i8* [ %220, %16 ], [ %2, %5 ]
  %21 = phi i8* [ %219, %16 ], [ %1, %5 ]
  %22 = bitcast i8* %18 to i64*
  %23 = load i64, i64* %22, align 1
  %24 = insertelement <2 x i64> undef, i64 %23, i32 0
  %25 = bitcast <2 x i64> %24 to <16 x i8>
  %26 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %25, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %27 = bitcast i8* %21 to i32*
  %28 = load i32, i32* %27, align 4
  %29 = insertelement <4 x i32> undef, i32 %28, i32 0
  %30 = bitcast <4 x i32> %29 to <16 x i8>
  %31 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %30, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %32 = bitcast <16 x i8> %31 to <8 x i16>
  %33 = shufflevector <8 x i16> %32, <8 x i16> undef, <8 x i32> <i32 0, i32 0, i32 1, i32 1, i32 2, i32 2, i32 3, i32 3>
  %34 = bitcast i8* %20 to i32*
  %35 = load i32, i32* %34, align 4
  %36 = insertelement <4 x i32> undef, i32 %35, i32 0
  %37 = bitcast <4 x i32> %36 to <16 x i8>
  %38 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %37, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %39 = bitcast <16 x i8> %38 to <8 x i16>
  %40 = shufflevector <8 x i16> %39, <8 x i16> undef, <8 x i32> <i32 0, i32 0, i32 1, i32 1, i32 2, i32 2, i32 3, i32 3>
  %41 = bitcast <16 x i8> %26 to <8 x i16>
  %42 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %41, <8 x i16> <i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077>) #5
  %43 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %40, <8 x i16> <i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149>) #5
  %44 = add <8 x i16> %42, <i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234>
  %45 = add <8 x i16> %44, %43
  %46 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %33, <8 x i16> <i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419>) #5
  %47 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %40, <8 x i16> <i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320>) #5
  %48 = add <8 x i16> %42, <i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708>
  %49 = sub <8 x i16> %48, %46
  %50 = sub <8 x i16> %49, %47
  %51 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %33, <8 x i16> <i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486>) #5
  %52 = tail call <8 x i16> @llvm.uadd.sat.v8i16(<8 x i16> %51, <8 x i16> %42) #5
  %53 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %52, <8 x i16> <i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685>) #5
  %54 = ashr <8 x i16> %45, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %55 = ashr <8 x i16> %50, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %56 = lshr <8 x i16> %53, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %57 = getelementptr inbounds i8, i8* %18, i64 8
  %58 = getelementptr inbounds i8, i8* %21, i64 4
  %59 = getelementptr inbounds i8, i8* %20, i64 4
  %60 = bitcast i8* %57 to i64*
  %61 = load i64, i64* %60, align 1
  %62 = insertelement <2 x i64> undef, i64 %61, i32 0
  %63 = bitcast <2 x i64> %62 to <16 x i8>
  %64 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %63, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %65 = bitcast i8* %58 to i32*
  %66 = load i32, i32* %65, align 4
  %67 = insertelement <4 x i32> undef, i32 %66, i32 0
  %68 = bitcast <4 x i32> %67 to <16 x i8>
  %69 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %68, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %70 = bitcast <16 x i8> %69 to <8 x i16>
  %71 = shufflevector <8 x i16> %70, <8 x i16> undef, <8 x i32> <i32 0, i32 0, i32 1, i32 1, i32 2, i32 2, i32 3, i32 3>
  %72 = bitcast i8* %59 to i32*
  %73 = load i32, i32* %72, align 4
  %74 = insertelement <4 x i32> undef, i32 %73, i32 0
  %75 = bitcast <4 x i32> %74 to <16 x i8>
  %76 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %75, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %77 = bitcast <16 x i8> %76 to <8 x i16>
  %78 = shufflevector <8 x i16> %77, <8 x i16> undef, <8 x i32> <i32 0, i32 0, i32 1, i32 1, i32 2, i32 2, i32 3, i32 3>
  %79 = bitcast <16 x i8> %64 to <8 x i16>
  %80 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %79, <8 x i16> <i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077>) #5
  %81 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %78, <8 x i16> <i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149>) #5
  %82 = add <8 x i16> %80, <i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234>
  %83 = add <8 x i16> %82, %81
  %84 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %71, <8 x i16> <i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419>) #5
  %85 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %78, <8 x i16> <i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320>) #5
  %86 = add <8 x i16> %80, <i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708>
  %87 = sub <8 x i16> %86, %84
  %88 = sub <8 x i16> %87, %85
  %89 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %71, <8 x i16> <i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486>) #5
  %90 = tail call <8 x i16> @llvm.uadd.sat.v8i16(<8 x i16> %89, <8 x i16> %80) #5
  %91 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %90, <8 x i16> <i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685>) #5
  %92 = ashr <8 x i16> %83, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %93 = ashr <8 x i16> %88, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %94 = lshr <8 x i16> %91, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %95 = getelementptr inbounds i8, i8* %18, i64 16
  %96 = getelementptr inbounds i8, i8* %21, i64 8
  %97 = getelementptr inbounds i8, i8* %20, i64 8
  %98 = bitcast i8* %95 to i64*
  %99 = load i64, i64* %98, align 1
  %100 = insertelement <2 x i64> undef, i64 %99, i32 0
  %101 = bitcast <2 x i64> %100 to <16 x i8>
  %102 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %101, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %103 = bitcast i8* %96 to i32*
  %104 = load i32, i32* %103, align 4
  %105 = insertelement <4 x i32> undef, i32 %104, i32 0
  %106 = bitcast <4 x i32> %105 to <16 x i8>
  %107 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %106, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %108 = bitcast <16 x i8> %107 to <8 x i16>
  %109 = shufflevector <8 x i16> %108, <8 x i16> undef, <8 x i32> <i32 0, i32 0, i32 1, i32 1, i32 2, i32 2, i32 3, i32 3>
  %110 = bitcast i8* %97 to i32*
  %111 = load i32, i32* %110, align 4
  %112 = insertelement <4 x i32> undef, i32 %111, i32 0
  %113 = bitcast <4 x i32> %112 to <16 x i8>
  %114 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %113, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %115 = bitcast <16 x i8> %114 to <8 x i16>
  %116 = shufflevector <8 x i16> %115, <8 x i16> undef, <8 x i32> <i32 0, i32 0, i32 1, i32 1, i32 2, i32 2, i32 3, i32 3>
  %117 = bitcast <16 x i8> %102 to <8 x i16>
  %118 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %117, <8 x i16> <i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077>) #5
  %119 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %116, <8 x i16> <i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149>) #5
  %120 = add <8 x i16> %118, <i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234>
  %121 = add <8 x i16> %120, %119
  %122 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %109, <8 x i16> <i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419>) #5
  %123 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %116, <8 x i16> <i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320>) #5
  %124 = add <8 x i16> %118, <i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708>
  %125 = sub <8 x i16> %124, %122
  %126 = sub <8 x i16> %125, %123
  %127 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %109, <8 x i16> <i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486>) #5
  %128 = tail call <8 x i16> @llvm.uadd.sat.v8i16(<8 x i16> %127, <8 x i16> %118) #5
  %129 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %128, <8 x i16> <i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685>) #5
  %130 = ashr <8 x i16> %121, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %131 = ashr <8 x i16> %126, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %132 = lshr <8 x i16> %129, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %133 = getelementptr inbounds i8, i8* %18, i64 24
  %134 = getelementptr inbounds i8, i8* %21, i64 12
  %135 = getelementptr inbounds i8, i8* %20, i64 12
  %136 = bitcast i8* %133 to i64*
  %137 = load i64, i64* %136, align 1
  %138 = insertelement <2 x i64> undef, i64 %137, i32 0
  %139 = bitcast <2 x i64> %138 to <16 x i8>
  %140 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %139, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %141 = bitcast i8* %134 to i32*
  %142 = load i32, i32* %141, align 4
  %143 = insertelement <4 x i32> undef, i32 %142, i32 0
  %144 = bitcast <4 x i32> %143 to <16 x i8>
  %145 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %144, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %146 = bitcast <16 x i8> %145 to <8 x i16>
  %147 = shufflevector <8 x i16> %146, <8 x i16> undef, <8 x i32> <i32 0, i32 0, i32 1, i32 1, i32 2, i32 2, i32 3, i32 3>
  %148 = bitcast i8* %135 to i32*
  %149 = load i32, i32* %148, align 4
  %150 = insertelement <4 x i32> undef, i32 %149, i32 0
  %151 = bitcast <4 x i32> %150 to <16 x i8>
  %152 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %151, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %153 = bitcast <16 x i8> %152 to <8 x i16>
  %154 = shufflevector <8 x i16> %153, <8 x i16> undef, <8 x i32> <i32 0, i32 0, i32 1, i32 1, i32 2, i32 2, i32 3, i32 3>
  %155 = bitcast <16 x i8> %140 to <8 x i16>
  %156 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %155, <8 x i16> <i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077>) #5
  %157 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %154, <8 x i16> <i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149>) #5
  %158 = add <8 x i16> %156, <i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234>
  %159 = add <8 x i16> %158, %157
  %160 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %147, <8 x i16> <i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419>) #5
  %161 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %154, <8 x i16> <i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320>) #5
  %162 = add <8 x i16> %156, <i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708>
  %163 = sub <8 x i16> %162, %160
  %164 = sub <8 x i16> %163, %161
  %165 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %147, <8 x i16> <i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486>) #5
  %166 = tail call <8 x i16> @llvm.uadd.sat.v8i16(<8 x i16> %165, <8 x i16> %156) #5
  %167 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %166, <8 x i16> <i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685>) #5
  %168 = ashr <8 x i16> %159, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %169 = ashr <8 x i16> %164, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %170 = lshr <8 x i16> %167, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %171 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %56, <8 x i16> %94) #5
  %172 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %132, <8 x i16> %170) #5
  %173 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %55, <8 x i16> %93) #5
  %174 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %131, <8 x i16> %169) #5
  %175 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %54, <8 x i16> %92) #5
  %176 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %130, <8 x i16> %168) #5
  %177 = shufflevector <16 x i8> %171, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 16, i32 1, i32 16, i32 16, i32 2, i32 16, i32 16, i32 3, i32 16, i32 16, i32 4, i32 16, i32 16, i32 5>
  %178 = shufflevector <16 x i8> %171, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 16, i32 16, i32 6, i32 16, i32 16, i32 7, i32 16, i32 16, i32 8, i32 16, i32 16, i32 9, i32 16, i32 16, i32 10, i32 16>
  %179 = shufflevector <16 x i8> %171, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 16, i32 11, i32 16, i32 16, i32 12, i32 16, i32 16, i32 13, i32 16, i32 16, i32 14, i32 16, i32 16, i32 15, i32 16, i32 16>
  %180 = shufflevector <16 x i8> %172, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 16, i32 1, i32 16, i32 16, i32 2, i32 16, i32 16, i32 3, i32 16, i32 16, i32 4, i32 16, i32 16, i32 5>
  %181 = shufflevector <16 x i8> %172, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 16, i32 16, i32 6, i32 16, i32 16, i32 7, i32 16, i32 16, i32 8, i32 16, i32 16, i32 9, i32 16, i32 16, i32 10, i32 16>
  %182 = shufflevector <16 x i8> %172, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 16, i32 11, i32 16, i32 16, i32 12, i32 16, i32 16, i32 13, i32 16, i32 16, i32 14, i32 16, i32 16, i32 15, i32 16, i32 16>
  %183 = shufflevector <16 x i8> %173, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 16, i32 0, i32 16, i32 16, i32 1, i32 16, i32 16, i32 2, i32 16, i32 16, i32 3, i32 16, i32 16, i32 4, i32 16, i32 16>
  %184 = shufflevector <16 x i8> %173, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 5, i32 16, i32 16, i32 6, i32 16, i32 16, i32 7, i32 16, i32 16, i32 8, i32 16, i32 16, i32 9, i32 16, i32 16, i32 10>
  %185 = shufflevector <16 x i8> %173, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 16, i32 16, i32 11, i32 16, i32 16, i32 12, i32 16, i32 16, i32 13, i32 16, i32 16, i32 14, i32 16, i32 16, i32 15, i32 16>
  %186 = shufflevector <16 x i8> %174, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 16, i32 0, i32 16, i32 16, i32 1, i32 16, i32 16, i32 2, i32 16, i32 16, i32 3, i32 16, i32 16, i32 4, i32 16, i32 16>
  %187 = shufflevector <16 x i8> %174, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 5, i32 16, i32 16, i32 6, i32 16, i32 16, i32 7, i32 16, i32 16, i32 8, i32 16, i32 16, i32 9, i32 16, i32 16, i32 10>
  %188 = shufflevector <16 x i8> %174, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 16, i32 16, i32 11, i32 16, i32 16, i32 12, i32 16, i32 16, i32 13, i32 16, i32 16, i32 14, i32 16, i32 16, i32 15, i32 16>
  %189 = shufflevector <16 x i8> %175, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 16, i32 16, i32 0, i32 16, i32 16, i32 1, i32 16, i32 16, i32 2, i32 16, i32 16, i32 3, i32 16, i32 16, i32 4, i32 16>
  %190 = shufflevector <16 x i8> %175, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 16, i32 5, i32 16, i32 16, i32 6, i32 16, i32 16, i32 7, i32 16, i32 16, i32 8, i32 16, i32 16, i32 9, i32 16, i32 16>
  %191 = shufflevector <16 x i8> %175, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 10, i32 16, i32 16, i32 11, i32 16, i32 16, i32 12, i32 16, i32 16, i32 13, i32 16, i32 16, i32 14, i32 16, i32 16, i32 15>
  %192 = shufflevector <16 x i8> %176, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 16, i32 16, i32 0, i32 16, i32 16, i32 1, i32 16, i32 16, i32 2, i32 16, i32 16, i32 3, i32 16, i32 16, i32 4, i32 16>
  %193 = shufflevector <16 x i8> %176, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 16, i32 5, i32 16, i32 16, i32 6, i32 16, i32 16, i32 7, i32 16, i32 16, i32 8, i32 16, i32 16, i32 9, i32 16, i32 16>
  %194 = shufflevector <16 x i8> %176, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 10, i32 16, i32 16, i32 11, i32 16, i32 16, i32 12, i32 16, i32 16, i32 13, i32 16, i32 16, i32 14, i32 16, i32 16, i32 15>
  %195 = or <16 x i8> %183, %177
  %196 = or <16 x i8> %184, %178
  %197 = or <16 x i8> %185, %179
  %198 = or <16 x i8> %186, %180
  %199 = or <16 x i8> %187, %181
  %200 = or <16 x i8> %188, %182
  %201 = or <16 x i8> %195, %189
  %202 = or <16 x i8> %196, %190
  %203 = or <16 x i8> %197, %191
  %204 = or <16 x i8> %198, %192
  %205 = or <16 x i8> %199, %193
  %206 = or <16 x i8> %200, %194
  %207 = bitcast i8* %19 to <16 x i8>*
  store <16 x i8> %201, <16 x i8>* %207, align 1
  %208 = getelementptr inbounds i8, i8* %19, i64 16
  %209 = bitcast i8* %208 to <16 x i8>*
  store <16 x i8> %202, <16 x i8>* %209, align 1
  %210 = getelementptr inbounds i8, i8* %19, i64 32
  %211 = bitcast i8* %210 to <16 x i8>*
  store <16 x i8> %203, <16 x i8>* %211, align 1
  %212 = getelementptr inbounds i8, i8* %19, i64 48
  %213 = bitcast i8* %212 to <16 x i8>*
  store <16 x i8> %204, <16 x i8>* %213, align 1
  %214 = getelementptr inbounds i8, i8* %19, i64 64
  %215 = bitcast i8* %214 to <16 x i8>*
  store <16 x i8> %205, <16 x i8>* %215, align 1
  %216 = getelementptr inbounds i8, i8* %19, i64 80
  %217 = bitcast i8* %216 to <16 x i8>*
  store <16 x i8> %206, <16 x i8>* %217, align 1
  %218 = getelementptr inbounds i8, i8* %18, i64 32
  %219 = getelementptr inbounds i8, i8* %21, i64 16
  %220 = getelementptr inbounds i8, i8* %20, i64 16
  %221 = getelementptr inbounds i8, i8* %19, i64 96
  %222 = add nuw nsw i32 %17, 32
  %223 = icmp sgt i32 %222, %4
  br i1 %223, label %7, label %16

224:                                              ; preds = %9, %224
  %225 = phi i32 [ %279, %224 ], [ %14, %9 ]
  %226 = phi i8* [ %274, %224 ], [ %13, %9 ]
  %227 = phi i8* [ %273, %224 ], [ %12, %9 ]
  %228 = phi i8* [ %278, %224 ], [ %11, %9 ]
  %229 = phi i8* [ %277, %224 ], [ %10, %9 ]
  %230 = load i8, i8* %226, align 1
  %231 = zext i8 %230 to i32
  %232 = load i8, i8* %229, align 1
  %233 = zext i8 %232 to i32
  %234 = load i8, i8* %228, align 1
  %235 = zext i8 %234 to i32
  %236 = mul nuw nsw i32 %231, 19077
  %237 = lshr i32 %236, 8
  %238 = mul nuw nsw i32 %233, 33050
  %239 = lshr i32 %238, 8
  %240 = add nsw i32 %237, -17685
  %241 = add nsw i32 %240, %239
  %242 = icmp ult i32 %241, 16384
  %243 = lshr i32 %241, 6
  %244 = icmp slt i32 %241, 0
  %245 = select i1 %244, i32 0, i32 255
  %246 = select i1 %242, i32 %243, i32 %245
  %247 = trunc i32 %246 to i8
  store i8 %247, i8* %227, align 1
  %248 = mul nuw nsw i32 %233, 6419
  %249 = lshr i32 %248, 8
  %250 = mul nuw nsw i32 %235, 13320
  %251 = lshr i32 %250, 8
  %252 = add nuw nsw i32 %237, 8708
  %253 = sub nuw nsw i32 %252, %249
  %254 = sub nsw i32 %253, %251
  %255 = icmp ult i32 %254, 16384
  %256 = lshr i32 %254, 6
  %257 = icmp slt i32 %254, 0
  %258 = select i1 %257, i32 0, i32 255
  %259 = select i1 %255, i32 %256, i32 %258
  %260 = trunc i32 %259 to i8
  %261 = getelementptr inbounds i8, i8* %227, i64 1
  store i8 %260, i8* %261, align 1
  %262 = mul nuw nsw i32 %235, 26149
  %263 = lshr i32 %262, 8
  %264 = add nsw i32 %237, -14234
  %265 = add nsw i32 %264, %263
  %266 = icmp ult i32 %265, 16384
  %267 = lshr i32 %265, 6
  %268 = icmp slt i32 %265, 0
  %269 = select i1 %268, i32 0, i32 255
  %270 = select i1 %266, i32 %267, i32 %269
  %271 = trunc i32 %270 to i8
  %272 = getelementptr inbounds i8, i8* %227, i64 2
  store i8 %271, i8* %272, align 1
  %273 = getelementptr inbounds i8, i8* %227, i64 3
  %274 = getelementptr inbounds i8, i8* %226, i64 1
  %275 = and i32 %225, 1
  %276 = zext i32 %275 to i64
  %277 = getelementptr inbounds i8, i8* %229, i64 %276
  %278 = getelementptr inbounds i8, i8* %228, i64 %276
  %279 = add nuw nsw i32 %225, 1
  %280 = icmp eq i32 %279, %4
  br i1 %280, label %281, label %224

281:                                              ; preds = %224, %9
  ret void
}

; Function Attrs: nofree norecurse nounwind ssp uwtable writeonly
define hidden void @WebPInitConvertARGBToYUVSSE41() local_unnamed_addr #1 {
  store void (i32*, i8*, i32)* @ConvertARGBToY_SSE41, void (i32*, i8*, i32)** @WebPConvertARGBToY, align 8
  store void (i32*, i8*, i8*, i32, i32)* @ConvertARGBToUV_SSE41, void (i32*, i8*, i8*, i32, i32)** @WebPConvertARGBToUV, align 8
  store void (i8*, i8*, i32)* @ConvertRGB24ToY_SSE41, void (i8*, i8*, i32)** @WebPConvertRGB24ToY, align 8
  store void (i8*, i8*, i32)* @ConvertBGR24ToY_SSE41, void (i8*, i8*, i32)** @WebPConvertBGR24ToY, align 8
  store void (i16*, i8*, i8*, i32)* @ConvertRGBA32ToUV_SSE41, void (i16*, i8*, i8*, i32)** @WebPConvertRGBA32ToUV, align 8
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal void @ConvertARGBToY_SSE41(i32* nocapture readonly, i8* nocapture, i32) #0 {
  %4 = and i32 %2, -16
  %5 = icmp sgt i32 %4, 0
  br i1 %5, label %6, label %10

6:                                                ; preds = %3
  %7 = sext i32 %4 to i64
  br label %58

8:                                                ; preds = %58
  %9 = trunc i64 %139 to i32
  br label %10

10:                                               ; preds = %8, %3
  %11 = phi i32 [ 0, %3 ], [ %9, %8 ]
  %12 = icmp slt i32 %11, %2
  br i1 %12, label %13, label %161

13:                                               ; preds = %10
  %14 = zext i32 %11 to i64
  %15 = zext i32 %2 to i64
  %16 = sub nsw i64 %15, %14
  %17 = icmp ult i64 %16, 4
  br i1 %17, label %18, label %20

18:                                               ; preds = %56, %20, %13
  %19 = phi i64 [ %14, %20 ], [ %14, %13 ], [ %32, %56 ]
  br label %141

20:                                               ; preds = %13
  %21 = getelementptr i8, i8* %1, i64 %14
  %22 = getelementptr i8, i8* %1, i64 %15
  %23 = getelementptr i32, i32* %0, i64 %14
  %24 = bitcast i32* %23 to i8*
  %25 = getelementptr i32, i32* %0, i64 %15
  %26 = bitcast i32* %25 to i8*
  %27 = icmp ult i8* %21, %26
  %28 = icmp ugt i8* %22, %24
  %29 = and i1 %27, %28
  br i1 %29, label %18, label %30

30:                                               ; preds = %20
  %31 = and i64 %16, -4
  %32 = add nsw i64 %31, %14
  br label %33

33:                                               ; preds = %33, %30
  %34 = phi i64 [ 0, %30 ], [ %54, %33 ]
  %35 = add i64 %34, %14
  %36 = getelementptr inbounds i32, i32* %0, i64 %35
  %37 = bitcast i32* %36 to <4 x i32>*
  %38 = load <4 x i32>, <4 x i32>* %37, align 4, !alias.scope !2
  %39 = lshr <4 x i32> %38, <i32 16, i32 16, i32 16, i32 16>
  %40 = and <4 x i32> %39, <i32 255, i32 255, i32 255, i32 255>
  %41 = lshr <4 x i32> %38, <i32 8, i32 8, i32 8, i32 8>
  %42 = and <4 x i32> %41, <i32 255, i32 255, i32 255, i32 255>
  %43 = and <4 x i32> %38, <i32 255, i32 255, i32 255, i32 255>
  %44 = mul nuw nsw <4 x i32> %40, <i32 16839, i32 16839, i32 16839, i32 16839>
  %45 = mul nuw nsw <4 x i32> %42, <i32 33059, i32 33059, i32 33059, i32 33059>
  %46 = mul nuw nsw <4 x i32> %43, <i32 6420, i32 6420, i32 6420, i32 6420>
  %47 = add nuw nsw <4 x i32> %46, <i32 1081344, i32 1081344, i32 1081344, i32 1081344>
  %48 = add nuw nsw <4 x i32> %47, %44
  %49 = add nuw nsw <4 x i32> %48, %45
  %50 = lshr <4 x i32> %49, <i32 16, i32 16, i32 16, i32 16>
  %51 = trunc <4 x i32> %50 to <4 x i8>
  %52 = getelementptr inbounds i8, i8* %1, i64 %35
  %53 = bitcast i8* %52 to <4 x i8>*
  store <4 x i8> %51, <4 x i8>* %53, align 1, !alias.scope !5, !noalias !2
  %54 = add i64 %34, 4
  %55 = icmp eq i64 %54, %31
  br i1 %55, label %56, label %33, !llvm.loop !7

56:                                               ; preds = %33
  %57 = icmp eq i64 %16, %31
  br i1 %57, label %161, label %18

58:                                               ; preds = %6, %58
  %59 = phi i64 [ 0, %6 ], [ %139, %58 ]
  %60 = getelementptr inbounds i32, i32* %0, i64 %59
  %61 = bitcast i32* %60 to <16 x i8>*
  %62 = load <16 x i8>, <16 x i8>* %61, align 1
  %63 = getelementptr inbounds i32, i32* %60, i64 4
  %64 = bitcast i32* %63 to <16 x i8>*
  %65 = load <16 x i8>, <16 x i8>* %64, align 1
  %66 = getelementptr inbounds i32, i32* %60, i64 8
  %67 = bitcast i32* %66 to <16 x i8>*
  %68 = load <16 x i8>, <16 x i8>* %67, align 1
  %69 = getelementptr inbounds i32, i32* %60, i64 12
  %70 = bitcast i32* %69 to <16 x i8>*
  %71 = load <16 x i8>, <16 x i8>* %70, align 1
  %72 = shufflevector <16 x i8> %62, <16 x i8> undef, <16 x i32> <i32 0, i32 4, i32 8, i32 12, i32 1, i32 5, i32 9, i32 13, i32 2, i32 6, i32 10, i32 14, i32 3, i32 7, i32 11, i32 15>
  %73 = shufflevector <16 x i8> %65, <16 x i8> undef, <16 x i32> <i32 0, i32 4, i32 8, i32 12, i32 1, i32 5, i32 9, i32 13, i32 2, i32 6, i32 10, i32 14, i32 3, i32 7, i32 11, i32 15>
  %74 = shufflevector <16 x i8> %68, <16 x i8> undef, <16 x i32> <i32 0, i32 4, i32 8, i32 12, i32 1, i32 5, i32 9, i32 13, i32 2, i32 6, i32 10, i32 14, i32 3, i32 7, i32 11, i32 15>
  %75 = shufflevector <16 x i8> %71, <16 x i8> undef, <16 x i32> <i32 0, i32 4, i32 8, i32 12, i32 1, i32 5, i32 9, i32 13, i32 2, i32 6, i32 10, i32 14, i32 3, i32 7, i32 11, i32 15>
  %76 = bitcast <16 x i8> %72 to <4 x i32>
  %77 = bitcast <16 x i8> %73 to <4 x i32>
  %78 = shufflevector <4 x i32> %76, <4 x i32> %77, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %79 = bitcast <4 x i32> %78 to <2 x i64>
  %80 = shufflevector <4 x i32> %76, <4 x i32> %77, <4 x i32> <i32 2, i32 6, i32 undef, i32 undef>
  %81 = bitcast <4 x i32> %80 to <2 x i64>
  %82 = bitcast <16 x i8> %74 to <4 x i32>
  %83 = bitcast <16 x i8> %75 to <4 x i32>
  %84 = shufflevector <4 x i32> %82, <4 x i32> %83, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %85 = bitcast <4 x i32> %84 to <2 x i64>
  %86 = shufflevector <4 x i32> %82, <4 x i32> %83, <4 x i32> <i32 2, i32 6, i32 undef, i32 undef>
  %87 = bitcast <4 x i32> %86 to <2 x i64>
  %88 = shufflevector <2 x i64> %79, <2 x i64> %85, <2 x i32> <i32 0, i32 2>
  %89 = shufflevector <2 x i64> %79, <2 x i64> %85, <2 x i32> <i32 1, i32 3>
  %90 = shufflevector <2 x i64> %81, <2 x i64> %87, <2 x i32> <i32 0, i32 2>
  %91 = bitcast <2 x i64> %90 to <16 x i8>
  %92 = shufflevector <16 x i8> %91, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %93 = bitcast <16 x i8> %92 to <8 x i16>
  %94 = shufflevector <16 x i8> %91, <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %95 = bitcast <16 x i8> %94 to <8 x i16>
  %96 = bitcast <2 x i64> %89 to <16 x i8>
  %97 = shufflevector <16 x i8> %96, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %98 = bitcast <16 x i8> %97 to <8 x i16>
  %99 = shufflevector <16 x i8> %96, <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %100 = bitcast <16 x i8> %99 to <8 x i16>
  %101 = bitcast <2 x i64> %88 to <16 x i8>
  %102 = shufflevector <16 x i8> %101, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %103 = bitcast <16 x i8> %102 to <8 x i16>
  %104 = shufflevector <16 x i8> %101, <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %105 = bitcast <16 x i8> %104 to <8 x i16>
  %106 = shufflevector <8 x i16> %93, <8 x i16> %98, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %107 = shufflevector <8 x i16> %93, <8 x i16> %98, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %108 = shufflevector <8 x i16> %98, <8 x i16> %103, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %109 = shufflevector <8 x i16> %98, <8 x i16> %103, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %110 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %106, <8 x i16> <i16 16839, i16 16675, i16 16839, i16 16675, i16 16839, i16 16675, i16 16839, i16 16675>) #5
  %111 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %107, <8 x i16> <i16 16839, i16 16675, i16 16839, i16 16675, i16 16839, i16 16675, i16 16839, i16 16675>) #5
  %112 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %108, <8 x i16> <i16 16384, i16 6420, i16 16384, i16 6420, i16 16384, i16 6420, i16 16384, i16 6420>) #5
  %113 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %109, <8 x i16> <i16 16384, i16 6420, i16 16384, i16 6420, i16 16384, i16 6420, i16 16384, i16 6420>) #5
  %114 = add <4 x i32> %110, <i32 1081344, i32 1081344, i32 1081344, i32 1081344>
  %115 = add <4 x i32> %114, %112
  %116 = add <4 x i32> %111, <i32 1081344, i32 1081344, i32 1081344, i32 1081344>
  %117 = add <4 x i32> %116, %113
  %118 = ashr <4 x i32> %115, <i32 16, i32 16, i32 16, i32 16>
  %119 = ashr <4 x i32> %117, <i32 16, i32 16, i32 16, i32 16>
  %120 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %118, <4 x i32> %119) #5
  %121 = shufflevector <8 x i16> %95, <8 x i16> %100, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %122 = shufflevector <8 x i16> %95, <8 x i16> %100, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %123 = shufflevector <8 x i16> %100, <8 x i16> %105, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %124 = shufflevector <8 x i16> %100, <8 x i16> %105, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %125 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %121, <8 x i16> <i16 16839, i16 16675, i16 16839, i16 16675, i16 16839, i16 16675, i16 16839, i16 16675>) #5
  %126 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %122, <8 x i16> <i16 16839, i16 16675, i16 16839, i16 16675, i16 16839, i16 16675, i16 16839, i16 16675>) #5
  %127 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %123, <8 x i16> <i16 16384, i16 6420, i16 16384, i16 6420, i16 16384, i16 6420, i16 16384, i16 6420>) #5
  %128 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %124, <8 x i16> <i16 16384, i16 6420, i16 16384, i16 6420, i16 16384, i16 6420, i16 16384, i16 6420>) #5
  %129 = add <4 x i32> %125, <i32 1081344, i32 1081344, i32 1081344, i32 1081344>
  %130 = add <4 x i32> %129, %127
  %131 = add <4 x i32> %126, <i32 1081344, i32 1081344, i32 1081344, i32 1081344>
  %132 = add <4 x i32> %131, %128
  %133 = ashr <4 x i32> %130, <i32 16, i32 16, i32 16, i32 16>
  %134 = ashr <4 x i32> %132, <i32 16, i32 16, i32 16, i32 16>
  %135 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %133, <4 x i32> %134) #5
  %136 = getelementptr inbounds i8, i8* %1, i64 %59
  %137 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %120, <8 x i16> %135) #5
  %138 = bitcast i8* %136 to <16 x i8>*
  store <16 x i8> %137, <16 x i8>* %138, align 1
  %139 = add nuw nsw i64 %59, 16
  %140 = icmp slt i64 %139, %7
  br i1 %140, label %58, label %8

141:                                              ; preds = %18, %141
  %142 = phi i64 [ %159, %141 ], [ %19, %18 ]
  %143 = getelementptr inbounds i32, i32* %0, i64 %142
  %144 = load i32, i32* %143, align 4
  %145 = lshr i32 %144, 16
  %146 = and i32 %145, 255
  %147 = lshr i32 %144, 8
  %148 = and i32 %147, 255
  %149 = and i32 %144, 255
  %150 = mul nuw nsw i32 %146, 16839
  %151 = mul nuw nsw i32 %148, 33059
  %152 = mul nuw nsw i32 %149, 6420
  %153 = add nuw nsw i32 %152, 1081344
  %154 = add nuw nsw i32 %153, %150
  %155 = add nuw nsw i32 %154, %151
  %156 = lshr i32 %155, 16
  %157 = trunc i32 %156 to i8
  %158 = getelementptr inbounds i8, i8* %1, i64 %142
  store i8 %157, i8* %158, align 1
  %159 = add nuw nsw i64 %142, 1
  %160 = icmp eq i64 %159, %15
  br i1 %160, label %161, label %141, !llvm.loop !9

161:                                              ; preds = %141, %56, %10
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal void @ConvertARGBToUV_SSE41(i32*, i8*, i8*, i32, i32) #0 {
  %6 = and i32 %3, -32
  %7 = icmp sgt i32 %6, 0
  br i1 %7, label %8, label %199

8:                                                ; preds = %5
  %9 = icmp eq i32 %4, 0
  %10 = sext i32 %6 to i64
  br label %11

11:                                               ; preds = %8, %189
  %12 = phi i64 [ 0, %8 ], [ %193, %189 ]
  %13 = phi i8* [ %1, %8 ], [ %194, %189 ]
  %14 = phi i8* [ %2, %8 ], [ %195, %189 ]
  %15 = getelementptr inbounds i32, i32* %0, i64 %12
  %16 = bitcast i32* %15 to <16 x i8>*
  %17 = load <16 x i8>, <16 x i8>* %16, align 1
  %18 = getelementptr inbounds i32, i32* %15, i64 4
  %19 = bitcast i32* %18 to <16 x i8>*
  %20 = load <16 x i8>, <16 x i8>* %19, align 1
  %21 = getelementptr inbounds i32, i32* %15, i64 8
  %22 = bitcast i32* %21 to <16 x i8>*
  %23 = load <16 x i8>, <16 x i8>* %22, align 1
  %24 = getelementptr inbounds i32, i32* %15, i64 12
  %25 = bitcast i32* %24 to <16 x i8>*
  %26 = load <16 x i8>, <16 x i8>* %25, align 1
  %27 = shufflevector <16 x i8> %17, <16 x i8> undef, <16 x i32> <i32 0, i32 4, i32 8, i32 12, i32 1, i32 5, i32 9, i32 13, i32 2, i32 6, i32 10, i32 14, i32 3, i32 7, i32 11, i32 15>
  %28 = shufflevector <16 x i8> %20, <16 x i8> undef, <16 x i32> <i32 0, i32 4, i32 8, i32 12, i32 1, i32 5, i32 9, i32 13, i32 2, i32 6, i32 10, i32 14, i32 3, i32 7, i32 11, i32 15>
  %29 = shufflevector <16 x i8> %23, <16 x i8> undef, <16 x i32> <i32 0, i32 4, i32 8, i32 12, i32 1, i32 5, i32 9, i32 13, i32 2, i32 6, i32 10, i32 14, i32 3, i32 7, i32 11, i32 15>
  %30 = shufflevector <16 x i8> %26, <16 x i8> undef, <16 x i32> <i32 0, i32 4, i32 8, i32 12, i32 1, i32 5, i32 9, i32 13, i32 2, i32 6, i32 10, i32 14, i32 3, i32 7, i32 11, i32 15>
  %31 = bitcast <16 x i8> %27 to <4 x i32>
  %32 = bitcast <16 x i8> %28 to <4 x i32>
  %33 = shufflevector <4 x i32> %31, <4 x i32> %32, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %34 = bitcast <4 x i32> %33 to <2 x i64>
  %35 = shufflevector <4 x i32> %31, <4 x i32> %32, <4 x i32> <i32 2, i32 6, i32 undef, i32 undef>
  %36 = bitcast <4 x i32> %35 to <2 x i64>
  %37 = bitcast <16 x i8> %29 to <4 x i32>
  %38 = bitcast <16 x i8> %30 to <4 x i32>
  %39 = shufflevector <4 x i32> %37, <4 x i32> %38, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %40 = bitcast <4 x i32> %39 to <2 x i64>
  %41 = shufflevector <4 x i32> %37, <4 x i32> %38, <4 x i32> <i32 2, i32 6, i32 undef, i32 undef>
  %42 = bitcast <4 x i32> %41 to <2 x i64>
  %43 = shufflevector <2 x i64> %34, <2 x i64> %40, <2 x i32> <i32 0, i32 2>
  %44 = shufflevector <2 x i64> %34, <2 x i64> %40, <2 x i32> <i32 1, i32 3>
  %45 = shufflevector <2 x i64> %36, <2 x i64> %42, <2 x i32> <i32 0, i32 2>
  %46 = bitcast <2 x i64> %45 to <16 x i8>
  %47 = shufflevector <16 x i8> %46, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %48 = bitcast <16 x i8> %47 to <8 x i16>
  %49 = shufflevector <16 x i8> %46, <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %50 = bitcast <16 x i8> %49 to <8 x i16>
  %51 = bitcast <2 x i64> %44 to <16 x i8>
  %52 = shufflevector <16 x i8> %51, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %53 = bitcast <16 x i8> %52 to <8 x i16>
  %54 = shufflevector <16 x i8> %51, <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %55 = bitcast <16 x i8> %54 to <8 x i16>
  %56 = bitcast <2 x i64> %43 to <16 x i8>
  %57 = shufflevector <16 x i8> %56, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %58 = bitcast <16 x i8> %57 to <8 x i16>
  %59 = shufflevector <16 x i8> %56, <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %60 = bitcast <16 x i8> %59 to <8 x i16>
  %61 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %48, <8 x i16> <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>) #5
  %62 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %50, <8 x i16> <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>) #5
  %63 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %61, <4 x i32> %62) #5
  %64 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %53, <8 x i16> <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>) #5
  %65 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %55, <8 x i16> <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>) #5
  %66 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %64, <4 x i32> %65) #5
  %67 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %58, <8 x i16> <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>) #5
  %68 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %60, <8 x i16> <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>) #5
  %69 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %67, <4 x i32> %68) #5
  %70 = shufflevector <8 x i16> %63, <8 x i16> %66, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %71 = shufflevector <8 x i16> %63, <8 x i16> %66, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %72 = shufflevector <8 x i16> %66, <8 x i16> %69, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %73 = shufflevector <8 x i16> %66, <8 x i16> %69, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %74 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %70, <8 x i16> <i16 -9719, i16 -19081, i16 -9719, i16 -19081, i16 -9719, i16 -19081, i16 -9719, i16 -19081>) #5
  %75 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %71, <8 x i16> <i16 -9719, i16 -19081, i16 -9719, i16 -19081, i16 -9719, i16 -19081, i16 -9719, i16 -19081>) #5
  %76 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %72, <8 x i16> <i16 0, i16 28800, i16 0, i16 28800, i16 0, i16 28800, i16 0, i16 28800>) #5
  %77 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %73, <8 x i16> <i16 0, i16 28800, i16 0, i16 28800, i16 0, i16 28800, i16 0, i16 28800>) #5
  %78 = add <4 x i32> %74, <i32 33685504, i32 33685504, i32 33685504, i32 33685504>
  %79 = add <4 x i32> %78, %76
  %80 = add <4 x i32> %75, <i32 33685504, i32 33685504, i32 33685504, i32 33685504>
  %81 = add <4 x i32> %80, %77
  %82 = ashr <4 x i32> %79, <i32 18, i32 18, i32 18, i32 18>
  %83 = ashr <4 x i32> %81, <i32 18, i32 18, i32 18, i32 18>
  %84 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %82, <4 x i32> %83) #5
  %85 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %70, <8 x i16> <i16 28800, i16 0, i16 28800, i16 0, i16 28800, i16 0, i16 28800, i16 0>) #5
  %86 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %71, <8 x i16> <i16 28800, i16 0, i16 28800, i16 0, i16 28800, i16 0, i16 28800, i16 0>) #5
  %87 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %72, <8 x i16> <i16 -24116, i16 -4684, i16 -24116, i16 -4684, i16 -24116, i16 -4684, i16 -24116, i16 -4684>) #5
  %88 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %73, <8 x i16> <i16 -24116, i16 -4684, i16 -24116, i16 -4684, i16 -24116, i16 -4684, i16 -24116, i16 -4684>) #5
  %89 = add <4 x i32> %85, <i32 33685504, i32 33685504, i32 33685504, i32 33685504>
  %90 = add <4 x i32> %89, %87
  %91 = add <4 x i32> %86, <i32 33685504, i32 33685504, i32 33685504, i32 33685504>
  %92 = add <4 x i32> %91, %88
  %93 = ashr <4 x i32> %90, <i32 18, i32 18, i32 18, i32 18>
  %94 = ashr <4 x i32> %92, <i32 18, i32 18, i32 18, i32 18>
  %95 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %93, <4 x i32> %94) #5
  %96 = or i64 %12, 16
  %97 = getelementptr inbounds i32, i32* %0, i64 %96
  %98 = bitcast i32* %97 to <16 x i8>*
  %99 = load <16 x i8>, <16 x i8>* %98, align 1
  %100 = getelementptr inbounds i32, i32* %97, i64 4
  %101 = bitcast i32* %100 to <16 x i8>*
  %102 = load <16 x i8>, <16 x i8>* %101, align 1
  %103 = getelementptr inbounds i32, i32* %97, i64 8
  %104 = bitcast i32* %103 to <16 x i8>*
  %105 = load <16 x i8>, <16 x i8>* %104, align 1
  %106 = getelementptr inbounds i32, i32* %97, i64 12
  %107 = bitcast i32* %106 to <16 x i8>*
  %108 = load <16 x i8>, <16 x i8>* %107, align 1
  %109 = shufflevector <16 x i8> %99, <16 x i8> undef, <16 x i32> <i32 0, i32 4, i32 8, i32 12, i32 1, i32 5, i32 9, i32 13, i32 2, i32 6, i32 10, i32 14, i32 3, i32 7, i32 11, i32 15>
  %110 = shufflevector <16 x i8> %102, <16 x i8> undef, <16 x i32> <i32 0, i32 4, i32 8, i32 12, i32 1, i32 5, i32 9, i32 13, i32 2, i32 6, i32 10, i32 14, i32 3, i32 7, i32 11, i32 15>
  %111 = shufflevector <16 x i8> %105, <16 x i8> undef, <16 x i32> <i32 0, i32 4, i32 8, i32 12, i32 1, i32 5, i32 9, i32 13, i32 2, i32 6, i32 10, i32 14, i32 3, i32 7, i32 11, i32 15>
  %112 = shufflevector <16 x i8> %108, <16 x i8> undef, <16 x i32> <i32 0, i32 4, i32 8, i32 12, i32 1, i32 5, i32 9, i32 13, i32 2, i32 6, i32 10, i32 14, i32 3, i32 7, i32 11, i32 15>
  %113 = bitcast <16 x i8> %109 to <4 x i32>
  %114 = bitcast <16 x i8> %110 to <4 x i32>
  %115 = shufflevector <4 x i32> %113, <4 x i32> %114, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %116 = bitcast <4 x i32> %115 to <2 x i64>
  %117 = shufflevector <4 x i32> %113, <4 x i32> %114, <4 x i32> <i32 2, i32 6, i32 undef, i32 undef>
  %118 = bitcast <4 x i32> %117 to <2 x i64>
  %119 = bitcast <16 x i8> %111 to <4 x i32>
  %120 = bitcast <16 x i8> %112 to <4 x i32>
  %121 = shufflevector <4 x i32> %119, <4 x i32> %120, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %122 = bitcast <4 x i32> %121 to <2 x i64>
  %123 = shufflevector <4 x i32> %119, <4 x i32> %120, <4 x i32> <i32 2, i32 6, i32 undef, i32 undef>
  %124 = bitcast <4 x i32> %123 to <2 x i64>
  %125 = shufflevector <2 x i64> %116, <2 x i64> %122, <2 x i32> <i32 0, i32 2>
  %126 = shufflevector <2 x i64> %116, <2 x i64> %122, <2 x i32> <i32 1, i32 3>
  %127 = shufflevector <2 x i64> %118, <2 x i64> %124, <2 x i32> <i32 0, i32 2>
  %128 = bitcast <2 x i64> %127 to <16 x i8>
  %129 = shufflevector <16 x i8> %128, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %130 = bitcast <16 x i8> %129 to <8 x i16>
  %131 = shufflevector <16 x i8> %128, <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %132 = bitcast <16 x i8> %131 to <8 x i16>
  %133 = bitcast <2 x i64> %126 to <16 x i8>
  %134 = shufflevector <16 x i8> %133, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %135 = bitcast <16 x i8> %134 to <8 x i16>
  %136 = shufflevector <16 x i8> %133, <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %137 = bitcast <16 x i8> %136 to <8 x i16>
  %138 = bitcast <2 x i64> %125 to <16 x i8>
  %139 = shufflevector <16 x i8> %138, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %140 = bitcast <16 x i8> %139 to <8 x i16>
  %141 = shufflevector <16 x i8> %138, <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %142 = bitcast <16 x i8> %141 to <8 x i16>
  %143 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %130, <8 x i16> <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>) #5
  %144 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %132, <8 x i16> <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>) #5
  %145 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %143, <4 x i32> %144) #5
  %146 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %135, <8 x i16> <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>) #5
  %147 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %137, <8 x i16> <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>) #5
  %148 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %146, <4 x i32> %147) #5
  %149 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %140, <8 x i16> <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>) #5
  %150 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %142, <8 x i16> <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>) #5
  %151 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %149, <4 x i32> %150) #5
  %152 = shufflevector <8 x i16> %145, <8 x i16> %148, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %153 = shufflevector <8 x i16> %145, <8 x i16> %148, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %154 = shufflevector <8 x i16> %148, <8 x i16> %151, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %155 = shufflevector <8 x i16> %148, <8 x i16> %151, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %156 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %152, <8 x i16> <i16 -9719, i16 -19081, i16 -9719, i16 -19081, i16 -9719, i16 -19081, i16 -9719, i16 -19081>) #5
  %157 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %153, <8 x i16> <i16 -9719, i16 -19081, i16 -9719, i16 -19081, i16 -9719, i16 -19081, i16 -9719, i16 -19081>) #5
  %158 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %154, <8 x i16> <i16 0, i16 28800, i16 0, i16 28800, i16 0, i16 28800, i16 0, i16 28800>) #5
  %159 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %155, <8 x i16> <i16 0, i16 28800, i16 0, i16 28800, i16 0, i16 28800, i16 0, i16 28800>) #5
  %160 = add <4 x i32> %156, <i32 33685504, i32 33685504, i32 33685504, i32 33685504>
  %161 = add <4 x i32> %160, %158
  %162 = add <4 x i32> %157, <i32 33685504, i32 33685504, i32 33685504, i32 33685504>
  %163 = add <4 x i32> %162, %159
  %164 = ashr <4 x i32> %161, <i32 18, i32 18, i32 18, i32 18>
  %165 = ashr <4 x i32> %163, <i32 18, i32 18, i32 18, i32 18>
  %166 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %164, <4 x i32> %165) #5
  %167 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %152, <8 x i16> <i16 28800, i16 0, i16 28800, i16 0, i16 28800, i16 0, i16 28800, i16 0>) #5
  %168 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %153, <8 x i16> <i16 28800, i16 0, i16 28800, i16 0, i16 28800, i16 0, i16 28800, i16 0>) #5
  %169 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %154, <8 x i16> <i16 -24116, i16 -4684, i16 -24116, i16 -4684, i16 -24116, i16 -4684, i16 -24116, i16 -4684>) #5
  %170 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %155, <8 x i16> <i16 -24116, i16 -4684, i16 -24116, i16 -4684, i16 -24116, i16 -4684, i16 -24116, i16 -4684>) #5
  %171 = add <4 x i32> %167, <i32 33685504, i32 33685504, i32 33685504, i32 33685504>
  %172 = add <4 x i32> %171, %169
  %173 = add <4 x i32> %168, <i32 33685504, i32 33685504, i32 33685504, i32 33685504>
  %174 = add <4 x i32> %173, %170
  %175 = ashr <4 x i32> %172, <i32 18, i32 18, i32 18, i32 18>
  %176 = ashr <4 x i32> %174, <i32 18, i32 18, i32 18, i32 18>
  %177 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %175, <4 x i32> %176) #5
  %178 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %84, <8 x i16> %166) #5
  %179 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %95, <8 x i16> %177) #5
  %180 = bitcast i8* %13 to <16 x i8>*
  br i1 %9, label %183, label %181

181:                                              ; preds = %11
  %182 = bitcast i8* %14 to <16 x i8>*
  br label %189

183:                                              ; preds = %11
  %184 = load <16 x i8>, <16 x i8>* %180, align 1
  %185 = bitcast i8* %14 to <16 x i8>*
  %186 = load <16 x i8>, <16 x i8>* %185, align 1
  %187 = tail call <16 x i8> @llvm.x86.sse2.pavg.b(<16 x i8> %178, <16 x i8> %184) #5
  %188 = tail call <16 x i8> @llvm.x86.sse2.pavg.b(<16 x i8> %179, <16 x i8> %186) #5
  br label %189

189:                                              ; preds = %181, %183
  %190 = phi <16 x i8>* [ %182, %181 ], [ %185, %183 ]
  %191 = phi <16 x i8> [ %178, %181 ], [ %187, %183 ]
  %192 = phi <16 x i8> [ %179, %181 ], [ %188, %183 ]
  store <16 x i8> %191, <16 x i8>* %180, align 1
  store <16 x i8> %192, <16 x i8>* %190, align 1
  %193 = add nuw nsw i64 %12, 32
  %194 = getelementptr inbounds i8, i8* %13, i64 16
  %195 = getelementptr inbounds i8, i8* %14, i64 16
  %196 = icmp slt i64 %193, %10
  br i1 %196, label %11, label %197

197:                                              ; preds = %189
  %198 = trunc i64 %193 to i32
  br label %199

199:                                              ; preds = %197, %5
  %200 = phi i32 [ 0, %5 ], [ %198, %197 ]
  %201 = phi i8* [ %2, %5 ], [ %195, %197 ]
  %202 = phi i8* [ %1, %5 ], [ %194, %197 ]
  %203 = icmp slt i32 %200, %3
  br i1 %203, label %204, label %208

204:                                              ; preds = %199
  %205 = zext i32 %200 to i64
  %206 = getelementptr inbounds i32, i32* %0, i64 %205
  %207 = sub nsw i32 %3, %200
  tail call void @WebPConvertARGBToUV_C(i32* %206, i8* %202, i8* %201, i32 %207, i32 %4) #5
  br label %208

208:                                              ; preds = %204, %199
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal void @ConvertRGB24ToY_SSE41(i8* nocapture readonly, i8* nocapture, i32) #0 {
  %4 = and i32 %2, -32
  %5 = icmp sgt i32 %4, 0
  br i1 %5, label %44, label %8

6:                                                ; preds = %44
  %7 = trunc i64 %187 to i32
  br label %8

8:                                                ; preds = %6, %3
  %9 = phi i32 [ 0, %3 ], [ %7, %6 ]
  %10 = phi i8* [ %0, %3 ], [ %189, %6 ]
  %11 = icmp slt i32 %9, %2
  br i1 %11, label %12, label %233

12:                                               ; preds = %8
  %13 = sext i32 %9 to i64
  %14 = sext i32 %2 to i64
  %15 = sub nsw i64 %14, %13
  %16 = xor i64 %13, -1
  %17 = and i64 %15, 1
  %18 = icmp eq i64 %17, 0
  br i1 %18, label %39, label %19

19:                                               ; preds = %12
  %20 = load i8, i8* %10, align 1
  %21 = zext i8 %20 to i32
  %22 = getelementptr inbounds i8, i8* %10, i64 1
  %23 = load i8, i8* %22, align 1
  %24 = zext i8 %23 to i32
  %25 = getelementptr inbounds i8, i8* %10, i64 2
  %26 = load i8, i8* %25, align 1
  %27 = zext i8 %26 to i32
  %28 = mul nuw nsw i32 %21, 16839
  %29 = mul nuw nsw i32 %24, 33059
  %30 = mul nuw nsw i32 %27, 6420
  %31 = add nuw nsw i32 %28, 1081344
  %32 = add nuw nsw i32 %31, %29
  %33 = add nuw nsw i32 %32, %30
  %34 = lshr i32 %33, 16
  %35 = trunc i32 %34 to i8
  %36 = getelementptr inbounds i8, i8* %1, i64 %13
  store i8 %35, i8* %36, align 1
  %37 = add nsw i64 %13, 1
  %38 = getelementptr inbounds i8, i8* %10, i64 3
  br label %39

39:                                               ; preds = %12, %19
  %40 = phi i64 [ %13, %12 ], [ %37, %19 ]
  %41 = phi i8* [ %10, %12 ], [ %38, %19 ]
  %42 = sub nsw i64 0, %14
  %43 = icmp eq i64 %16, %42
  br i1 %43, label %233, label %191

44:                                               ; preds = %3, %44
  %45 = phi i8* [ %189, %44 ], [ %0, %3 ]
  %46 = phi i64 [ %187, %44 ], [ 0, %3 ]
  %47 = bitcast i8* %45 to <16 x i8>*
  %48 = load <16 x i8>, <16 x i8>* %47, align 1
  %49 = getelementptr inbounds i8, i8* %45, i64 16
  %50 = bitcast i8* %49 to <16 x i8>*
  %51 = load <16 x i8>, <16 x i8>* %50, align 1
  %52 = getelementptr inbounds i8, i8* %45, i64 32
  %53 = bitcast i8* %52 to <16 x i8>*
  %54 = load <16 x i8>, <16 x i8>* %53, align 1
  %55 = getelementptr inbounds i8, i8* %45, i64 48
  %56 = bitcast i8* %55 to <16 x i8>*
  %57 = load <16 x i8>, <16 x i8>* %56, align 1
  %58 = getelementptr inbounds i8, i8* %45, i64 64
  %59 = bitcast i8* %58 to <16 x i8>*
  %60 = load <16 x i8>, <16 x i8>* %59, align 1
  %61 = getelementptr inbounds i8, i8* %45, i64 80
  %62 = bitcast i8* %61 to <16 x i8>*
  %63 = load <16 x i8>, <16 x i8>* %62, align 1
  %64 = shufflevector <16 x i8> %48, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 3, i32 6, i32 9, i32 12, i32 15, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16>
  %65 = shufflevector <16 x i8> %51, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 2, i32 5, i32 8, i32 11, i32 14, i32 16, i32 16, i32 16, i32 16, i32 16>
  %66 = shufflevector <16 x i8> %54, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 1, i32 4, i32 7, i32 10, i32 13>
  %67 = shufflevector <16 x i8> %57, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 3, i32 6, i32 9, i32 12, i32 15, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16>
  %68 = shufflevector <16 x i8> %60, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 2, i32 5, i32 8, i32 11, i32 14, i32 16, i32 16, i32 16, i32 16, i32 16>
  %69 = shufflevector <16 x i8> %63, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 1, i32 4, i32 7, i32 10, i32 13>
  %70 = or <16 x i8> %65, %64
  %71 = or <16 x i8> %68, %67
  %72 = or <16 x i8> %70, %66
  %73 = or <16 x i8> %71, %69
  %74 = shufflevector <16 x i8> %48, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 1, i32 4, i32 7, i32 10, i32 13, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16>
  %75 = shufflevector <16 x i8> %51, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 16, i32 16, i32 16, i32 16, i32 16, i32 0, i32 3, i32 6, i32 9, i32 12, i32 15, i32 16, i32 16, i32 16, i32 16, i32 16>
  %76 = shufflevector <16 x i8> %54, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 2, i32 5, i32 8, i32 11, i32 14>
  %77 = shufflevector <16 x i8> %57, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 1, i32 4, i32 7, i32 10, i32 13, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16>
  %78 = shufflevector <16 x i8> %60, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 16, i32 16, i32 16, i32 16, i32 16, i32 0, i32 3, i32 6, i32 9, i32 12, i32 15, i32 16, i32 16, i32 16, i32 16, i32 16>
  %79 = shufflevector <16 x i8> %63, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 2, i32 5, i32 8, i32 11, i32 14>
  %80 = or <16 x i8> %75, %74
  %81 = or <16 x i8> %78, %77
  %82 = or <16 x i8> %80, %76
  %83 = or <16 x i8> %81, %79
  %84 = shufflevector <16 x i8> %48, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 5, i32 8, i32 11, i32 14, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16>
  %85 = shufflevector <16 x i8> %51, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 16, i32 16, i32 16, i32 16, i32 16, i32 1, i32 4, i32 7, i32 10, i32 13, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16>
  %86 = shufflevector <16 x i8> %54, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 0, i32 3, i32 6, i32 9, i32 12, i32 15>
  %87 = shufflevector <16 x i8> %57, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 5, i32 8, i32 11, i32 14, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16>
  %88 = shufflevector <16 x i8> %60, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 16, i32 16, i32 16, i32 16, i32 16, i32 1, i32 4, i32 7, i32 10, i32 13, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16>
  %89 = shufflevector <16 x i8> %63, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 0, i32 3, i32 6, i32 9, i32 12, i32 15>
  %90 = or <16 x i8> %85, %84
  %91 = or <16 x i8> %88, %87
  %92 = or <16 x i8> %90, %86
  %93 = or <16 x i8> %91, %89
  %94 = shl i64 %46, 32
  %95 = ashr exact i64 %94, 32
  %96 = shufflevector <16 x i8> %72, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %97 = shufflevector <16 x i8> %82, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %98 = shufflevector <16 x i8> %92, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %99 = bitcast <16 x i8> %96 to <8 x i16>
  %100 = bitcast <16 x i8> %97 to <8 x i16>
  %101 = shufflevector <8 x i16> %99, <8 x i16> %100, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %102 = shufflevector <8 x i16> %99, <8 x i16> %100, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %103 = bitcast <16 x i8> %98 to <8 x i16>
  %104 = shufflevector <8 x i16> %100, <8 x i16> %103, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %105 = shufflevector <8 x i16> %100, <8 x i16> %103, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %106 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %101, <8 x i16> <i16 16839, i16 16675, i16 16839, i16 16675, i16 16839, i16 16675, i16 16839, i16 16675>) #5
  %107 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %102, <8 x i16> <i16 16839, i16 16675, i16 16839, i16 16675, i16 16839, i16 16675, i16 16839, i16 16675>) #5
  %108 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %104, <8 x i16> <i16 16384, i16 6420, i16 16384, i16 6420, i16 16384, i16 6420, i16 16384, i16 6420>) #5
  %109 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %105, <8 x i16> <i16 16384, i16 6420, i16 16384, i16 6420, i16 16384, i16 6420, i16 16384, i16 6420>) #5
  %110 = add <4 x i32> %106, <i32 1081344, i32 1081344, i32 1081344, i32 1081344>
  %111 = add <4 x i32> %110, %108
  %112 = add <4 x i32> %107, <i32 1081344, i32 1081344, i32 1081344, i32 1081344>
  %113 = add <4 x i32> %112, %109
  %114 = ashr <4 x i32> %111, <i32 16, i32 16, i32 16, i32 16>
  %115 = ashr <4 x i32> %113, <i32 16, i32 16, i32 16, i32 16>
  %116 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %114, <4 x i32> %115) #5
  %117 = shufflevector <16 x i8> %72, <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %118 = shufflevector <16 x i8> %82, <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %119 = shufflevector <16 x i8> %92, <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %120 = bitcast <16 x i8> %117 to <8 x i16>
  %121 = bitcast <16 x i8> %118 to <8 x i16>
  %122 = shufflevector <8 x i16> %120, <8 x i16> %121, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %123 = shufflevector <8 x i16> %120, <8 x i16> %121, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %124 = bitcast <16 x i8> %119 to <8 x i16>
  %125 = shufflevector <8 x i16> %121, <8 x i16> %124, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %126 = shufflevector <8 x i16> %121, <8 x i16> %124, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %127 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %122, <8 x i16> <i16 16839, i16 16675, i16 16839, i16 16675, i16 16839, i16 16675, i16 16839, i16 16675>) #5
  %128 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %123, <8 x i16> <i16 16839, i16 16675, i16 16839, i16 16675, i16 16839, i16 16675, i16 16839, i16 16675>) #5
  %129 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %125, <8 x i16> <i16 16384, i16 6420, i16 16384, i16 6420, i16 16384, i16 6420, i16 16384, i16 6420>) #5
  %130 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %126, <8 x i16> <i16 16384, i16 6420, i16 16384, i16 6420, i16 16384, i16 6420, i16 16384, i16 6420>) #5
  %131 = add <4 x i32> %127, <i32 1081344, i32 1081344, i32 1081344, i32 1081344>
  %132 = add <4 x i32> %131, %129
  %133 = add <4 x i32> %128, <i32 1081344, i32 1081344, i32 1081344, i32 1081344>
  %134 = add <4 x i32> %133, %130
  %135 = ashr <4 x i32> %132, <i32 16, i32 16, i32 16, i32 16>
  %136 = ashr <4 x i32> %134, <i32 16, i32 16, i32 16, i32 16>
  %137 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %135, <4 x i32> %136) #5
  %138 = getelementptr inbounds i8, i8* %1, i64 %95
  %139 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %116, <8 x i16> %137) #5
  %140 = bitcast i8* %138 to <16 x i8>*
  store <16 x i8> %139, <16 x i8>* %140, align 1
  %141 = add nsw i64 %95, 16
  %142 = shufflevector <16 x i8> %73, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %143 = shufflevector <16 x i8> %83, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %144 = shufflevector <16 x i8> %93, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %145 = bitcast <16 x i8> %142 to <8 x i16>
  %146 = bitcast <16 x i8> %143 to <8 x i16>
  %147 = shufflevector <8 x i16> %145, <8 x i16> %146, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %148 = shufflevector <8 x i16> %145, <8 x i16> %146, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %149 = bitcast <16 x i8> %144 to <8 x i16>
  %150 = shufflevector <8 x i16> %146, <8 x i16> %149, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %151 = shufflevector <8 x i16> %146, <8 x i16> %149, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %152 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %147, <8 x i16> <i16 16839, i16 16675, i16 16839, i16 16675, i16 16839, i16 16675, i16 16839, i16 16675>) #5
  %153 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %148, <8 x i16> <i16 16839, i16 16675, i16 16839, i16 16675, i16 16839, i16 16675, i16 16839, i16 16675>) #5
  %154 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %150, <8 x i16> <i16 16384, i16 6420, i16 16384, i16 6420, i16 16384, i16 6420, i16 16384, i16 6420>) #5
  %155 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %151, <8 x i16> <i16 16384, i16 6420, i16 16384, i16 6420, i16 16384, i16 6420, i16 16384, i16 6420>) #5
  %156 = add <4 x i32> %152, <i32 1081344, i32 1081344, i32 1081344, i32 1081344>
  %157 = add <4 x i32> %156, %154
  %158 = add <4 x i32> %153, <i32 1081344, i32 1081344, i32 1081344, i32 1081344>
  %159 = add <4 x i32> %158, %155
  %160 = ashr <4 x i32> %157, <i32 16, i32 16, i32 16, i32 16>
  %161 = ashr <4 x i32> %159, <i32 16, i32 16, i32 16, i32 16>
  %162 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %160, <4 x i32> %161) #5
  %163 = shufflevector <16 x i8> %73, <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %164 = shufflevector <16 x i8> %83, <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %165 = shufflevector <16 x i8> %93, <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %166 = bitcast <16 x i8> %163 to <8 x i16>
  %167 = bitcast <16 x i8> %164 to <8 x i16>
  %168 = shufflevector <8 x i16> %166, <8 x i16> %167, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %169 = shufflevector <8 x i16> %166, <8 x i16> %167, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %170 = bitcast <16 x i8> %165 to <8 x i16>
  %171 = shufflevector <8 x i16> %167, <8 x i16> %170, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %172 = shufflevector <8 x i16> %167, <8 x i16> %170, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %173 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %168, <8 x i16> <i16 16839, i16 16675, i16 16839, i16 16675, i16 16839, i16 16675, i16 16839, i16 16675>) #5
  %174 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %169, <8 x i16> <i16 16839, i16 16675, i16 16839, i16 16675, i16 16839, i16 16675, i16 16839, i16 16675>) #5
  %175 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %171, <8 x i16> <i16 16384, i16 6420, i16 16384, i16 6420, i16 16384, i16 6420, i16 16384, i16 6420>) #5
  %176 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %172, <8 x i16> <i16 16384, i16 6420, i16 16384, i16 6420, i16 16384, i16 6420, i16 16384, i16 6420>) #5
  %177 = add <4 x i32> %173, <i32 1081344, i32 1081344, i32 1081344, i32 1081344>
  %178 = add <4 x i32> %177, %175
  %179 = add <4 x i32> %174, <i32 1081344, i32 1081344, i32 1081344, i32 1081344>
  %180 = add <4 x i32> %179, %176
  %181 = ashr <4 x i32> %178, <i32 16, i32 16, i32 16, i32 16>
  %182 = ashr <4 x i32> %180, <i32 16, i32 16, i32 16, i32 16>
  %183 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %181, <4 x i32> %182) #5
  %184 = getelementptr inbounds i8, i8* %1, i64 %141
  %185 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %162, <8 x i16> %183) #5
  %186 = bitcast i8* %184 to <16 x i8>*
  store <16 x i8> %185, <16 x i8>* %186, align 1
  %187 = add nsw i64 %95, 32
  %188 = trunc i64 %187 to i32
  %189 = getelementptr inbounds i8, i8* %45, i64 96
  %190 = icmp sgt i32 %4, %188
  br i1 %190, label %44, label %6

191:                                              ; preds = %39, %191
  %192 = phi i64 [ %230, %191 ], [ %40, %39 ]
  %193 = phi i8* [ %231, %191 ], [ %41, %39 ]
  %194 = load i8, i8* %193, align 1
  %195 = zext i8 %194 to i32
  %196 = getelementptr inbounds i8, i8* %193, i64 1
  %197 = load i8, i8* %196, align 1
  %198 = zext i8 %197 to i32
  %199 = getelementptr inbounds i8, i8* %193, i64 2
  %200 = load i8, i8* %199, align 1
  %201 = zext i8 %200 to i32
  %202 = mul nuw nsw i32 %195, 16839
  %203 = mul nuw nsw i32 %198, 33059
  %204 = mul nuw nsw i32 %201, 6420
  %205 = add nuw nsw i32 %202, 1081344
  %206 = add nuw nsw i32 %205, %203
  %207 = add nuw nsw i32 %206, %204
  %208 = lshr i32 %207, 16
  %209 = trunc i32 %208 to i8
  %210 = getelementptr inbounds i8, i8* %1, i64 %192
  store i8 %209, i8* %210, align 1
  %211 = add nsw i64 %192, 1
  %212 = getelementptr inbounds i8, i8* %193, i64 3
  %213 = load i8, i8* %212, align 1
  %214 = zext i8 %213 to i32
  %215 = getelementptr inbounds i8, i8* %193, i64 4
  %216 = load i8, i8* %215, align 1
  %217 = zext i8 %216 to i32
  %218 = getelementptr inbounds i8, i8* %193, i64 5
  %219 = load i8, i8* %218, align 1
  %220 = zext i8 %219 to i32
  %221 = mul nuw nsw i32 %214, 16839
  %222 = mul nuw nsw i32 %217, 33059
  %223 = mul nuw nsw i32 %220, 6420
  %224 = add nuw nsw i32 %221, 1081344
  %225 = add nuw nsw i32 %224, %222
  %226 = add nuw nsw i32 %225, %223
  %227 = lshr i32 %226, 16
  %228 = trunc i32 %227 to i8
  %229 = getelementptr inbounds i8, i8* %1, i64 %211
  store i8 %228, i8* %229, align 1
  %230 = add nsw i64 %192, 2
  %231 = getelementptr inbounds i8, i8* %193, i64 6
  %232 = icmp eq i64 %230, %14
  br i1 %232, label %233, label %191

233:                                              ; preds = %39, %191, %8
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal void @ConvertBGR24ToY_SSE41(i8* nocapture readonly, i8* nocapture, i32) #0 {
  %4 = and i32 %2, -32
  %5 = icmp sgt i32 %4, 0
  br i1 %5, label %44, label %8

6:                                                ; preds = %44
  %7 = trunc i64 %187 to i32
  br label %8

8:                                                ; preds = %6, %3
  %9 = phi i32 [ 0, %3 ], [ %7, %6 ]
  %10 = phi i8* [ %0, %3 ], [ %189, %6 ]
  %11 = icmp slt i32 %9, %2
  br i1 %11, label %12, label %233

12:                                               ; preds = %8
  %13 = sext i32 %9 to i64
  %14 = sext i32 %2 to i64
  %15 = sub nsw i64 %14, %13
  %16 = xor i64 %13, -1
  %17 = and i64 %15, 1
  %18 = icmp eq i64 %17, 0
  br i1 %18, label %39, label %19

19:                                               ; preds = %12
  %20 = getelementptr inbounds i8, i8* %10, i64 2
  %21 = load i8, i8* %20, align 1
  %22 = zext i8 %21 to i32
  %23 = getelementptr inbounds i8, i8* %10, i64 1
  %24 = load i8, i8* %23, align 1
  %25 = zext i8 %24 to i32
  %26 = load i8, i8* %10, align 1
  %27 = zext i8 %26 to i32
  %28 = mul nuw nsw i32 %22, 16839
  %29 = mul nuw nsw i32 %25, 33059
  %30 = mul nuw nsw i32 %27, 6420
  %31 = add nuw nsw i32 %28, 1081344
  %32 = add nuw nsw i32 %31, %29
  %33 = add nuw nsw i32 %32, %30
  %34 = lshr i32 %33, 16
  %35 = trunc i32 %34 to i8
  %36 = getelementptr inbounds i8, i8* %1, i64 %13
  store i8 %35, i8* %36, align 1
  %37 = add nsw i64 %13, 1
  %38 = getelementptr inbounds i8, i8* %10, i64 3
  br label %39

39:                                               ; preds = %12, %19
  %40 = phi i64 [ %13, %12 ], [ %37, %19 ]
  %41 = phi i8* [ %10, %12 ], [ %38, %19 ]
  %42 = sub nsw i64 0, %14
  %43 = icmp eq i64 %16, %42
  br i1 %43, label %233, label %191

44:                                               ; preds = %3, %44
  %45 = phi i8* [ %189, %44 ], [ %0, %3 ]
  %46 = phi i64 [ %187, %44 ], [ 0, %3 ]
  %47 = bitcast i8* %45 to <16 x i8>*
  %48 = load <16 x i8>, <16 x i8>* %47, align 1
  %49 = getelementptr inbounds i8, i8* %45, i64 16
  %50 = bitcast i8* %49 to <16 x i8>*
  %51 = load <16 x i8>, <16 x i8>* %50, align 1
  %52 = getelementptr inbounds i8, i8* %45, i64 32
  %53 = bitcast i8* %52 to <16 x i8>*
  %54 = load <16 x i8>, <16 x i8>* %53, align 1
  %55 = getelementptr inbounds i8, i8* %45, i64 48
  %56 = bitcast i8* %55 to <16 x i8>*
  %57 = load <16 x i8>, <16 x i8>* %56, align 1
  %58 = getelementptr inbounds i8, i8* %45, i64 64
  %59 = bitcast i8* %58 to <16 x i8>*
  %60 = load <16 x i8>, <16 x i8>* %59, align 1
  %61 = getelementptr inbounds i8, i8* %45, i64 80
  %62 = bitcast i8* %61 to <16 x i8>*
  %63 = load <16 x i8>, <16 x i8>* %62, align 1
  %64 = shufflevector <16 x i8> %48, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 3, i32 6, i32 9, i32 12, i32 15, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16>
  %65 = shufflevector <16 x i8> %51, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 2, i32 5, i32 8, i32 11, i32 14, i32 16, i32 16, i32 16, i32 16, i32 16>
  %66 = shufflevector <16 x i8> %54, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 1, i32 4, i32 7, i32 10, i32 13>
  %67 = shufflevector <16 x i8> %57, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 3, i32 6, i32 9, i32 12, i32 15, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16>
  %68 = shufflevector <16 x i8> %60, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 2, i32 5, i32 8, i32 11, i32 14, i32 16, i32 16, i32 16, i32 16, i32 16>
  %69 = shufflevector <16 x i8> %63, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 1, i32 4, i32 7, i32 10, i32 13>
  %70 = or <16 x i8> %65, %64
  %71 = or <16 x i8> %68, %67
  %72 = or <16 x i8> %70, %66
  %73 = or <16 x i8> %71, %69
  %74 = shufflevector <16 x i8> %48, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 1, i32 4, i32 7, i32 10, i32 13, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16>
  %75 = shufflevector <16 x i8> %51, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 16, i32 16, i32 16, i32 16, i32 16, i32 0, i32 3, i32 6, i32 9, i32 12, i32 15, i32 16, i32 16, i32 16, i32 16, i32 16>
  %76 = shufflevector <16 x i8> %54, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 2, i32 5, i32 8, i32 11, i32 14>
  %77 = shufflevector <16 x i8> %57, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 1, i32 4, i32 7, i32 10, i32 13, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16>
  %78 = shufflevector <16 x i8> %60, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 16, i32 16, i32 16, i32 16, i32 16, i32 0, i32 3, i32 6, i32 9, i32 12, i32 15, i32 16, i32 16, i32 16, i32 16, i32 16>
  %79 = shufflevector <16 x i8> %63, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 2, i32 5, i32 8, i32 11, i32 14>
  %80 = or <16 x i8> %75, %74
  %81 = or <16 x i8> %78, %77
  %82 = or <16 x i8> %80, %76
  %83 = or <16 x i8> %81, %79
  %84 = shufflevector <16 x i8> %48, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 5, i32 8, i32 11, i32 14, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16>
  %85 = shufflevector <16 x i8> %51, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 16, i32 16, i32 16, i32 16, i32 16, i32 1, i32 4, i32 7, i32 10, i32 13, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16>
  %86 = shufflevector <16 x i8> %54, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 0, i32 3, i32 6, i32 9, i32 12, i32 15>
  %87 = shufflevector <16 x i8> %57, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 5, i32 8, i32 11, i32 14, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16>
  %88 = shufflevector <16 x i8> %60, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 16, i32 16, i32 16, i32 16, i32 16, i32 1, i32 4, i32 7, i32 10, i32 13, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16>
  %89 = shufflevector <16 x i8> %63, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 0, i32 3, i32 6, i32 9, i32 12, i32 15>
  %90 = or <16 x i8> %85, %84
  %91 = or <16 x i8> %88, %87
  %92 = or <16 x i8> %90, %86
  %93 = or <16 x i8> %91, %89
  %94 = shl i64 %46, 32
  %95 = ashr exact i64 %94, 32
  %96 = shufflevector <16 x i8> %72, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %97 = shufflevector <16 x i8> %82, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %98 = shufflevector <16 x i8> %92, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %99 = bitcast <16 x i8> %98 to <8 x i16>
  %100 = bitcast <16 x i8> %97 to <8 x i16>
  %101 = shufflevector <8 x i16> %99, <8 x i16> %100, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %102 = shufflevector <8 x i16> %99, <8 x i16> %100, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %103 = bitcast <16 x i8> %96 to <8 x i16>
  %104 = shufflevector <8 x i16> %100, <8 x i16> %103, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %105 = shufflevector <8 x i16> %100, <8 x i16> %103, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %106 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %101, <8 x i16> <i16 16839, i16 16675, i16 16839, i16 16675, i16 16839, i16 16675, i16 16839, i16 16675>) #5
  %107 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %102, <8 x i16> <i16 16839, i16 16675, i16 16839, i16 16675, i16 16839, i16 16675, i16 16839, i16 16675>) #5
  %108 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %104, <8 x i16> <i16 16384, i16 6420, i16 16384, i16 6420, i16 16384, i16 6420, i16 16384, i16 6420>) #5
  %109 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %105, <8 x i16> <i16 16384, i16 6420, i16 16384, i16 6420, i16 16384, i16 6420, i16 16384, i16 6420>) #5
  %110 = add <4 x i32> %106, <i32 1081344, i32 1081344, i32 1081344, i32 1081344>
  %111 = add <4 x i32> %110, %108
  %112 = add <4 x i32> %107, <i32 1081344, i32 1081344, i32 1081344, i32 1081344>
  %113 = add <4 x i32> %112, %109
  %114 = ashr <4 x i32> %111, <i32 16, i32 16, i32 16, i32 16>
  %115 = ashr <4 x i32> %113, <i32 16, i32 16, i32 16, i32 16>
  %116 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %114, <4 x i32> %115) #5
  %117 = shufflevector <16 x i8> %72, <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %118 = shufflevector <16 x i8> %82, <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %119 = shufflevector <16 x i8> %92, <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %120 = bitcast <16 x i8> %119 to <8 x i16>
  %121 = bitcast <16 x i8> %118 to <8 x i16>
  %122 = shufflevector <8 x i16> %120, <8 x i16> %121, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %123 = shufflevector <8 x i16> %120, <8 x i16> %121, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %124 = bitcast <16 x i8> %117 to <8 x i16>
  %125 = shufflevector <8 x i16> %121, <8 x i16> %124, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %126 = shufflevector <8 x i16> %121, <8 x i16> %124, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %127 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %122, <8 x i16> <i16 16839, i16 16675, i16 16839, i16 16675, i16 16839, i16 16675, i16 16839, i16 16675>) #5
  %128 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %123, <8 x i16> <i16 16839, i16 16675, i16 16839, i16 16675, i16 16839, i16 16675, i16 16839, i16 16675>) #5
  %129 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %125, <8 x i16> <i16 16384, i16 6420, i16 16384, i16 6420, i16 16384, i16 6420, i16 16384, i16 6420>) #5
  %130 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %126, <8 x i16> <i16 16384, i16 6420, i16 16384, i16 6420, i16 16384, i16 6420, i16 16384, i16 6420>) #5
  %131 = add <4 x i32> %127, <i32 1081344, i32 1081344, i32 1081344, i32 1081344>
  %132 = add <4 x i32> %131, %129
  %133 = add <4 x i32> %128, <i32 1081344, i32 1081344, i32 1081344, i32 1081344>
  %134 = add <4 x i32> %133, %130
  %135 = ashr <4 x i32> %132, <i32 16, i32 16, i32 16, i32 16>
  %136 = ashr <4 x i32> %134, <i32 16, i32 16, i32 16, i32 16>
  %137 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %135, <4 x i32> %136) #5
  %138 = getelementptr inbounds i8, i8* %1, i64 %95
  %139 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %116, <8 x i16> %137) #5
  %140 = bitcast i8* %138 to <16 x i8>*
  store <16 x i8> %139, <16 x i8>* %140, align 1
  %141 = add nsw i64 %95, 16
  %142 = shufflevector <16 x i8> %73, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %143 = shufflevector <16 x i8> %83, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %144 = shufflevector <16 x i8> %93, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %145 = bitcast <16 x i8> %144 to <8 x i16>
  %146 = bitcast <16 x i8> %143 to <8 x i16>
  %147 = shufflevector <8 x i16> %145, <8 x i16> %146, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %148 = shufflevector <8 x i16> %145, <8 x i16> %146, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %149 = bitcast <16 x i8> %142 to <8 x i16>
  %150 = shufflevector <8 x i16> %146, <8 x i16> %149, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %151 = shufflevector <8 x i16> %146, <8 x i16> %149, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %152 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %147, <8 x i16> <i16 16839, i16 16675, i16 16839, i16 16675, i16 16839, i16 16675, i16 16839, i16 16675>) #5
  %153 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %148, <8 x i16> <i16 16839, i16 16675, i16 16839, i16 16675, i16 16839, i16 16675, i16 16839, i16 16675>) #5
  %154 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %150, <8 x i16> <i16 16384, i16 6420, i16 16384, i16 6420, i16 16384, i16 6420, i16 16384, i16 6420>) #5
  %155 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %151, <8 x i16> <i16 16384, i16 6420, i16 16384, i16 6420, i16 16384, i16 6420, i16 16384, i16 6420>) #5
  %156 = add <4 x i32> %152, <i32 1081344, i32 1081344, i32 1081344, i32 1081344>
  %157 = add <4 x i32> %156, %154
  %158 = add <4 x i32> %153, <i32 1081344, i32 1081344, i32 1081344, i32 1081344>
  %159 = add <4 x i32> %158, %155
  %160 = ashr <4 x i32> %157, <i32 16, i32 16, i32 16, i32 16>
  %161 = ashr <4 x i32> %159, <i32 16, i32 16, i32 16, i32 16>
  %162 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %160, <4 x i32> %161) #5
  %163 = shufflevector <16 x i8> %73, <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %164 = shufflevector <16 x i8> %83, <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %165 = shufflevector <16 x i8> %93, <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %166 = bitcast <16 x i8> %165 to <8 x i16>
  %167 = bitcast <16 x i8> %164 to <8 x i16>
  %168 = shufflevector <8 x i16> %166, <8 x i16> %167, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %169 = shufflevector <8 x i16> %166, <8 x i16> %167, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %170 = bitcast <16 x i8> %163 to <8 x i16>
  %171 = shufflevector <8 x i16> %167, <8 x i16> %170, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %172 = shufflevector <8 x i16> %167, <8 x i16> %170, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %173 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %168, <8 x i16> <i16 16839, i16 16675, i16 16839, i16 16675, i16 16839, i16 16675, i16 16839, i16 16675>) #5
  %174 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %169, <8 x i16> <i16 16839, i16 16675, i16 16839, i16 16675, i16 16839, i16 16675, i16 16839, i16 16675>) #5
  %175 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %171, <8 x i16> <i16 16384, i16 6420, i16 16384, i16 6420, i16 16384, i16 6420, i16 16384, i16 6420>) #5
  %176 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %172, <8 x i16> <i16 16384, i16 6420, i16 16384, i16 6420, i16 16384, i16 6420, i16 16384, i16 6420>) #5
  %177 = add <4 x i32> %173, <i32 1081344, i32 1081344, i32 1081344, i32 1081344>
  %178 = add <4 x i32> %177, %175
  %179 = add <4 x i32> %174, <i32 1081344, i32 1081344, i32 1081344, i32 1081344>
  %180 = add <4 x i32> %179, %176
  %181 = ashr <4 x i32> %178, <i32 16, i32 16, i32 16, i32 16>
  %182 = ashr <4 x i32> %180, <i32 16, i32 16, i32 16, i32 16>
  %183 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %181, <4 x i32> %182) #5
  %184 = getelementptr inbounds i8, i8* %1, i64 %141
  %185 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %162, <8 x i16> %183) #5
  %186 = bitcast i8* %184 to <16 x i8>*
  store <16 x i8> %185, <16 x i8>* %186, align 1
  %187 = add nsw i64 %95, 32
  %188 = trunc i64 %187 to i32
  %189 = getelementptr inbounds i8, i8* %45, i64 96
  %190 = icmp sgt i32 %4, %188
  br i1 %190, label %44, label %6

191:                                              ; preds = %39, %191
  %192 = phi i64 [ %230, %191 ], [ %40, %39 ]
  %193 = phi i8* [ %231, %191 ], [ %41, %39 ]
  %194 = getelementptr inbounds i8, i8* %193, i64 2
  %195 = load i8, i8* %194, align 1
  %196 = zext i8 %195 to i32
  %197 = getelementptr inbounds i8, i8* %193, i64 1
  %198 = load i8, i8* %197, align 1
  %199 = zext i8 %198 to i32
  %200 = load i8, i8* %193, align 1
  %201 = zext i8 %200 to i32
  %202 = mul nuw nsw i32 %196, 16839
  %203 = mul nuw nsw i32 %199, 33059
  %204 = mul nuw nsw i32 %201, 6420
  %205 = add nuw nsw i32 %202, 1081344
  %206 = add nuw nsw i32 %205, %203
  %207 = add nuw nsw i32 %206, %204
  %208 = lshr i32 %207, 16
  %209 = trunc i32 %208 to i8
  %210 = getelementptr inbounds i8, i8* %1, i64 %192
  store i8 %209, i8* %210, align 1
  %211 = add nsw i64 %192, 1
  %212 = getelementptr inbounds i8, i8* %193, i64 3
  %213 = getelementptr inbounds i8, i8* %193, i64 5
  %214 = load i8, i8* %213, align 1
  %215 = zext i8 %214 to i32
  %216 = getelementptr inbounds i8, i8* %193, i64 4
  %217 = load i8, i8* %216, align 1
  %218 = zext i8 %217 to i32
  %219 = load i8, i8* %212, align 1
  %220 = zext i8 %219 to i32
  %221 = mul nuw nsw i32 %215, 16839
  %222 = mul nuw nsw i32 %218, 33059
  %223 = mul nuw nsw i32 %220, 6420
  %224 = add nuw nsw i32 %221, 1081344
  %225 = add nuw nsw i32 %224, %222
  %226 = add nuw nsw i32 %225, %223
  %227 = lshr i32 %226, 16
  %228 = trunc i32 %227 to i8
  %229 = getelementptr inbounds i8, i8* %1, i64 %211
  store i8 %228, i8* %229, align 1
  %230 = add nsw i64 %192, 2
  %231 = getelementptr inbounds i8, i8* %193, i64 6
  %232 = icmp eq i64 %230, %14
  br i1 %232, label %233, label %191

233:                                              ; preds = %39, %191, %8
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal void @ConvertRGBA32ToUV_SSE41(i16*, i8*, i8*, i32) #0 {
  %5 = and i32 %3, -16
  %6 = shl nsw i32 %5, 2
  %7 = sext i32 %6 to i64
  %8 = getelementptr inbounds i16, i16* %0, i64 %7
  %9 = icmp sgt i32 %5, 0
  br i1 %9, label %10, label %141

10:                                               ; preds = %4, %10
  %11 = phi i16* [ %139, %10 ], [ %0, %4 ]
  %12 = phi i8* [ %138, %10 ], [ %2, %4 ]
  %13 = phi i8* [ %137, %10 ], [ %1, %4 ]
  %14 = bitcast i16* %11 to <16 x i8>*
  %15 = load <16 x i8>, <16 x i8>* %14, align 1
  %16 = getelementptr inbounds i16, i16* %11, i64 8
  %17 = bitcast i16* %16 to <16 x i8>*
  %18 = load <16 x i8>, <16 x i8>* %17, align 1
  %19 = getelementptr inbounds i16, i16* %11, i64 16
  %20 = bitcast i16* %19 to <16 x i8>*
  %21 = load <16 x i8>, <16 x i8>* %20, align 1
  %22 = getelementptr inbounds i16, i16* %11, i64 24
  %23 = bitcast i16* %22 to <16 x i8>*
  %24 = load <16 x i8>, <16 x i8>* %23, align 1
  %25 = shufflevector <16 x i8> %15, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 1, i32 8, i32 9, i32 2, i32 3, i32 10, i32 11, i32 4, i32 5, i32 12, i32 13, i32 16, i32 16, i32 16, i32 16>
  %26 = shufflevector <16 x i8> %18, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 1, i32 8, i32 9, i32 2, i32 3, i32 10, i32 11, i32 16, i32 16, i32 16, i32 16, i32 4, i32 5, i32 12, i32 13>
  %27 = shufflevector <16 x i8> %21, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 1, i32 8, i32 9, i32 2, i32 3, i32 10, i32 11, i32 4, i32 5, i32 12, i32 13, i32 16, i32 16, i32 16, i32 16>
  %28 = shufflevector <16 x i8> %24, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 1, i32 8, i32 9, i32 2, i32 3, i32 10, i32 11, i32 16, i32 16, i32 16, i32 16, i32 4, i32 5, i32 12, i32 13>
  %29 = bitcast <16 x i8> %25 to <4 x i32>
  %30 = bitcast <16 x i8> %26 to <4 x i32>
  %31 = shufflevector <4 x i32> %29, <4 x i32> %30, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %32 = bitcast <4 x i32> %31 to <2 x i64>
  %33 = or <16 x i8> %26, %25
  %34 = bitcast <16 x i8> %33 to <2 x i64>
  %35 = bitcast <16 x i8> %27 to <4 x i32>
  %36 = bitcast <16 x i8> %28 to <4 x i32>
  %37 = shufflevector <4 x i32> %35, <4 x i32> %36, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %38 = bitcast <4 x i32> %37 to <2 x i64>
  %39 = or <16 x i8> %28, %27
  %40 = bitcast <16 x i8> %39 to <2 x i64>
  %41 = shufflevector <2 x i64> %32, <2 x i64> %38, <2 x i32> <i32 0, i32 2>
  %42 = shufflevector <2 x i64> %32, <2 x i64> %38, <2 x i32> <i32 1, i32 3>
  %43 = shufflevector <2 x i64> %34, <2 x i64> %40, <2 x i32> <i32 1, i32 3>
  %44 = bitcast <2 x i64> %41 to <8 x i16>
  %45 = bitcast <2 x i64> %42 to <8 x i16>
  %46 = shufflevector <8 x i16> %44, <8 x i16> %45, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %47 = shufflevector <8 x i16> %44, <8 x i16> %45, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %48 = bitcast <2 x i64> %43 to <8 x i16>
  %49 = shufflevector <8 x i16> %45, <8 x i16> %48, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %50 = shufflevector <8 x i16> %45, <8 x i16> %48, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %51 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %46, <8 x i16> <i16 -9719, i16 -19081, i16 -9719, i16 -19081, i16 -9719, i16 -19081, i16 -9719, i16 -19081>) #5
  %52 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %47, <8 x i16> <i16 -9719, i16 -19081, i16 -9719, i16 -19081, i16 -9719, i16 -19081, i16 -9719, i16 -19081>) #5
  %53 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %49, <8 x i16> <i16 0, i16 28800, i16 0, i16 28800, i16 0, i16 28800, i16 0, i16 28800>) #5
  %54 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %50, <8 x i16> <i16 0, i16 28800, i16 0, i16 28800, i16 0, i16 28800, i16 0, i16 28800>) #5
  %55 = add <4 x i32> %51, <i32 33685504, i32 33685504, i32 33685504, i32 33685504>
  %56 = add <4 x i32> %55, %53
  %57 = add <4 x i32> %52, <i32 33685504, i32 33685504, i32 33685504, i32 33685504>
  %58 = add <4 x i32> %57, %54
  %59 = ashr <4 x i32> %56, <i32 18, i32 18, i32 18, i32 18>
  %60 = ashr <4 x i32> %58, <i32 18, i32 18, i32 18, i32 18>
  %61 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %59, <4 x i32> %60) #5
  %62 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %46, <8 x i16> <i16 28800, i16 0, i16 28800, i16 0, i16 28800, i16 0, i16 28800, i16 0>) #5
  %63 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %47, <8 x i16> <i16 28800, i16 0, i16 28800, i16 0, i16 28800, i16 0, i16 28800, i16 0>) #5
  %64 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %49, <8 x i16> <i16 -24116, i16 -4684, i16 -24116, i16 -4684, i16 -24116, i16 -4684, i16 -24116, i16 -4684>) #5
  %65 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %50, <8 x i16> <i16 -24116, i16 -4684, i16 -24116, i16 -4684, i16 -24116, i16 -4684, i16 -24116, i16 -4684>) #5
  %66 = add <4 x i32> %62, <i32 33685504, i32 33685504, i32 33685504, i32 33685504>
  %67 = add <4 x i32> %66, %64
  %68 = add <4 x i32> %63, <i32 33685504, i32 33685504, i32 33685504, i32 33685504>
  %69 = add <4 x i32> %68, %65
  %70 = ashr <4 x i32> %67, <i32 18, i32 18, i32 18, i32 18>
  %71 = ashr <4 x i32> %69, <i32 18, i32 18, i32 18, i32 18>
  %72 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %70, <4 x i32> %71) #5
  %73 = getelementptr inbounds i16, i16* %11, i64 32
  %74 = bitcast i16* %73 to <16 x i8>*
  %75 = load <16 x i8>, <16 x i8>* %74, align 1
  %76 = getelementptr inbounds i16, i16* %11, i64 40
  %77 = bitcast i16* %76 to <16 x i8>*
  %78 = load <16 x i8>, <16 x i8>* %77, align 1
  %79 = getelementptr inbounds i16, i16* %11, i64 48
  %80 = bitcast i16* %79 to <16 x i8>*
  %81 = load <16 x i8>, <16 x i8>* %80, align 1
  %82 = getelementptr inbounds i16, i16* %11, i64 56
  %83 = bitcast i16* %82 to <16 x i8>*
  %84 = load <16 x i8>, <16 x i8>* %83, align 1
  %85 = shufflevector <16 x i8> %75, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 1, i32 8, i32 9, i32 2, i32 3, i32 10, i32 11, i32 4, i32 5, i32 12, i32 13, i32 16, i32 16, i32 16, i32 16>
  %86 = shufflevector <16 x i8> %78, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 1, i32 8, i32 9, i32 2, i32 3, i32 10, i32 11, i32 16, i32 16, i32 16, i32 16, i32 4, i32 5, i32 12, i32 13>
  %87 = shufflevector <16 x i8> %81, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 1, i32 8, i32 9, i32 2, i32 3, i32 10, i32 11, i32 4, i32 5, i32 12, i32 13, i32 16, i32 16, i32 16, i32 16>
  %88 = shufflevector <16 x i8> %84, <16 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 1, i32 8, i32 9, i32 2, i32 3, i32 10, i32 11, i32 16, i32 16, i32 16, i32 16, i32 4, i32 5, i32 12, i32 13>
  %89 = bitcast <16 x i8> %85 to <4 x i32>
  %90 = bitcast <16 x i8> %86 to <4 x i32>
  %91 = shufflevector <4 x i32> %89, <4 x i32> %90, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %92 = bitcast <4 x i32> %91 to <2 x i64>
  %93 = or <16 x i8> %86, %85
  %94 = bitcast <16 x i8> %93 to <2 x i64>
  %95 = bitcast <16 x i8> %87 to <4 x i32>
  %96 = bitcast <16 x i8> %88 to <4 x i32>
  %97 = shufflevector <4 x i32> %95, <4 x i32> %96, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %98 = bitcast <4 x i32> %97 to <2 x i64>
  %99 = or <16 x i8> %88, %87
  %100 = bitcast <16 x i8> %99 to <2 x i64>
  %101 = shufflevector <2 x i64> %92, <2 x i64> %98, <2 x i32> <i32 0, i32 2>
  %102 = shufflevector <2 x i64> %92, <2 x i64> %98, <2 x i32> <i32 1, i32 3>
  %103 = shufflevector <2 x i64> %94, <2 x i64> %100, <2 x i32> <i32 1, i32 3>
  %104 = bitcast <2 x i64> %101 to <8 x i16>
  %105 = bitcast <2 x i64> %102 to <8 x i16>
  %106 = shufflevector <8 x i16> %104, <8 x i16> %105, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %107 = shufflevector <8 x i16> %104, <8 x i16> %105, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %108 = bitcast <2 x i64> %103 to <8 x i16>
  %109 = shufflevector <8 x i16> %105, <8 x i16> %108, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %110 = shufflevector <8 x i16> %105, <8 x i16> %108, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %111 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %106, <8 x i16> <i16 -9719, i16 -19081, i16 -9719, i16 -19081, i16 -9719, i16 -19081, i16 -9719, i16 -19081>) #5
  %112 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %107, <8 x i16> <i16 -9719, i16 -19081, i16 -9719, i16 -19081, i16 -9719, i16 -19081, i16 -9719, i16 -19081>) #5
  %113 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %109, <8 x i16> <i16 0, i16 28800, i16 0, i16 28800, i16 0, i16 28800, i16 0, i16 28800>) #5
  %114 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %110, <8 x i16> <i16 0, i16 28800, i16 0, i16 28800, i16 0, i16 28800, i16 0, i16 28800>) #5
  %115 = add <4 x i32> %111, <i32 33685504, i32 33685504, i32 33685504, i32 33685504>
  %116 = add <4 x i32> %115, %113
  %117 = add <4 x i32> %112, <i32 33685504, i32 33685504, i32 33685504, i32 33685504>
  %118 = add <4 x i32> %117, %114
  %119 = ashr <4 x i32> %116, <i32 18, i32 18, i32 18, i32 18>
  %120 = ashr <4 x i32> %118, <i32 18, i32 18, i32 18, i32 18>
  %121 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %119, <4 x i32> %120) #5
  %122 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %106, <8 x i16> <i16 28800, i16 0, i16 28800, i16 0, i16 28800, i16 0, i16 28800, i16 0>) #5
  %123 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %107, <8 x i16> <i16 28800, i16 0, i16 28800, i16 0, i16 28800, i16 0, i16 28800, i16 0>) #5
  %124 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %109, <8 x i16> <i16 -24116, i16 -4684, i16 -24116, i16 -4684, i16 -24116, i16 -4684, i16 -24116, i16 -4684>) #5
  %125 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %110, <8 x i16> <i16 -24116, i16 -4684, i16 -24116, i16 -4684, i16 -24116, i16 -4684, i16 -24116, i16 -4684>) #5
  %126 = add <4 x i32> %122, <i32 33685504, i32 33685504, i32 33685504, i32 33685504>
  %127 = add <4 x i32> %126, %124
  %128 = add <4 x i32> %123, <i32 33685504, i32 33685504, i32 33685504, i32 33685504>
  %129 = add <4 x i32> %128, %125
  %130 = ashr <4 x i32> %127, <i32 18, i32 18, i32 18, i32 18>
  %131 = ashr <4 x i32> %129, <i32 18, i32 18, i32 18, i32 18>
  %132 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %130, <4 x i32> %131) #5
  %133 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %61, <8 x i16> %121) #5
  %134 = bitcast i8* %13 to <16 x i8>*
  store <16 x i8> %133, <16 x i8>* %134, align 1
  %135 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %72, <8 x i16> %132) #5
  %136 = bitcast i8* %12 to <16 x i8>*
  store <16 x i8> %135, <16 x i8>* %136, align 1
  %137 = getelementptr inbounds i8, i8* %13, i64 16
  %138 = getelementptr inbounds i8, i8* %12, i64 16
  %139 = getelementptr inbounds i16, i16* %11, i64 64
  %140 = icmp ult i16* %139, %8
  br i1 %140, label %10, label %141

141:                                              ; preds = %10, %4
  %142 = phi i8* [ %1, %4 ], [ %137, %10 ]
  %143 = phi i8* [ %2, %4 ], [ %138, %10 ]
  %144 = phi i16* [ %0, %4 ], [ %139, %10 ]
  %145 = icmp slt i32 %5, %3
  br i1 %145, label %146, label %148

146:                                              ; preds = %141
  %147 = sub nsw i32 %3, %5
  tail call void @WebPConvertRGBA32ToUV_C(i16* %144, i8* %142, i8* %143, i32 %147) #5
  br label %148

148:                                              ; preds = %146, %141
  ret void
}

; Function Attrs: nounwind readnone
declare <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16>, <8 x i16>) #2

; Function Attrs: nounwind readnone speculatable
declare <8 x i16> @llvm.uadd.sat.v8i16(<8 x i16>, <8 x i16>) #3

; Function Attrs: nounwind readnone speculatable
declare <8 x i16> @llvm.usub.sat.v8i16(<8 x i16>, <8 x i16>) #3

; Function Attrs: nounwind readnone
declare <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16>, <8 x i16>) #2

; Function Attrs: nounwind readnone
declare <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16>, <8 x i16>) #2

; Function Attrs: nounwind readnone
declare <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32>, <4 x i32>) #2

declare void @WebPConvertARGBToUV_C(i32*, i8*, i8*, i32, i32) local_unnamed_addr #4

; Function Attrs: nounwind readnone
declare <16 x i8> @llvm.x86.sse2.pavg.b(<16 x i8>, <16 x i8>) #2

declare void @WebPConvertRGBA32ToUV_C(i16*, i8*, i8*, i32) local_unnamed_addr #4

attributes #0 = { nounwind ssp uwtable "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "min-legal-vector-width"="128" "no-frame-pointer-elim"="false" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "null-pointer-is-valid"="true" "stack-protector-buffer-size"="4" "target-cpu"="x86-64" "target-features"="+cx8,+fxsr,+mmx,+sse,+sse2,+sse3,+sse4.1,+ssse3,+x87" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #1 = { nofree norecurse nounwind ssp uwtable writeonly "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "min-legal-vector-width"="0" "no-frame-pointer-elim"="false" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "null-pointer-is-valid"="true" "stack-protector-buffer-size"="4" "target-cpu"="x86-64" "target-features"="+cx8,+fxsr,+mmx,+sse,+sse2,+sse3,+sse4.1,+ssse3,+x87" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #2 = { nounwind readnone }
attributes #3 = { nounwind readnone speculatable }
attributes #4 = { "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "no-frame-pointer-elim"="false" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "null-pointer-is-valid"="true" "stack-protector-buffer-size"="4" "target-cpu"="x86-64" "target-features"="+cx8,+fxsr,+mmx,+sse,+sse2,+sse3,+sse4.1,+ssse3,+x87" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #5 = { nounwind }

!llvm.module.flags = !{!0, !1}

!0 = !{i32 1, !"wchar_size", i32 4}
!1 = !{i32 7, !"PIC Level", i32 2}
!2 = !{!3}
!3 = distinct !{!3, !4}
!4 = distinct !{!4, !"LVerDomain"}
!5 = !{!6}
!6 = distinct !{!6, !4}
!7 = distinct !{!7, !8}
!8 = !{!"llvm.loop.isvectorized", i32 1}
!9 = distinct !{!9, !8}
