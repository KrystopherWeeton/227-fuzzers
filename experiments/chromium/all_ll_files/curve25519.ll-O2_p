; ModuleID = '../../third_party/boringssl/src/crypto/curve25519/curve25519.c'
source_filename = "../../third_party/boringssl/src/crypto/curve25519/curve25519.c"
target datalayout = "e-m:e-i64:64-f80:128-n8:16:32:64-S128"
target triple = "x86_64-unknown-linux-gnu"

%struct.fe = type { [5 x i64] }
%struct.ge_precomp = type { %struct.fe_loose, %struct.fe_loose, %struct.fe_loose }
%struct.fe_loose = type { [5 x i64] }
%struct.ge_p2 = type { %struct.fe, %struct.fe, %struct.fe }
%struct.ge_p3 = type { %struct.fe, %struct.fe, %struct.fe, %struct.fe }
%struct.ge_cached = type { %struct.fe_loose, %struct.fe_loose, %struct.fe_loose, %struct.fe_loose }
%struct.ge_p1p1 = type { %struct.fe_loose, %struct.fe_loose, %struct.fe_loose, %struct.fe_loose }
%struct.sha512_state_st = type { [8 x i64], i64, i64, [128 x i8], i32, i32 }
%union.anon = type { [4 x i64] }

@d = internal constant %struct.fe { [5 x i64] [i64 929955233495203, i64 466365720129213, i64 1662059464998953, i64 2033849074728123, i64 1442794654840575] }, align 8
@sqrtm1 = internal constant %struct.fe { [5 x i64] [i64 1718705420411056, i64 234908883556509, i64 2233514472574048, i64 2117202627021982, i64 765476049583133] }, align 8
@d2 = internal constant %struct.fe { [5 x i64] [i64 1859910466990425, i64 932731440258426, i64 1072319116312658, i64 1815898335770999, i64 633789495995903] }, align 8
@k25519SmallPrecomp = internal constant [960 x i8] c"\1A\D5%\8F`-V\C9\B2\A7%\95`\C7,i\5C\DC\D6\FD1\E2\A4\C0\FESn\CD\D36i!Xfffffffffffffffffffffffffffffff\02\A2\ED\F4\8Fk\0B>\EB5\1A\D5~\DBx\00\96\8A\A0\B4\CF`K\D4\D5\F9-\BF\88\BD\22b\13S\E4\82W\FA\1E\8F\06+\90\BA\08\B6\10TO|\1B&\ED\DAk\DD%\D0N\EAB\BB%\03\A2\FB\CCag\06p\1A\C4x:\FF2b\DD,\ABP\19;\F2\9B}\B8\FDO)\9C\A7\91\BA\0EF^Q\FE\1D\BF\E5\E5\9B\95\0Dg\F8\D1\B5Z\A1\93,\C3\DE\0E\97\85-\7F\EA\AB>G0\18$\E8\B7`\AEG\80\FC\E5#\E7\C2\C9\85\E6\98\A0)N\E1\849-\95,\F3E<\FF\AF'Lk\A6\F5K\11\BD\BA[\9E\C4\A4Q\1E\BE\D0\90:\9C\C2&\B6\1E\F1\95}\C8mR\E6\99,_\9A\96\0Ch)\FD\E2\FB\E6\BC\EC1\08\EC\E6\B0S`\C3\8C\BE\C1\B3\8A\8F\E4\88+U\E5dn\9B\D0\AF{d*5%\10R\C5\9EX\1196EQ\B89\93\FC\9Dj\BEX\CB\A4\0FQ<8\05\CA\ABCc\0E\F3\8BA\A6\F8\9BSp\80S\86^\8F\E3\C3\0D\18\C8K4\1F\D8\1D\BC\F2m4:\BE\DF\D9\F6\F3\89\A1\E1\94\9F]L]\E9\A1I\92\EF\0ES\81\89X\87\A67\F1\DDb`cZ\9D\1B\8C\C6}R\EAp\09j\E12\F3s!\1F\07{|\9BI\D8\C0\F3%ro\9D\ED1g66T@\92q\E6\11(\11\AD\932\85{>\B7;I\13\1C\07\B0.\93\AA\FD\FD(G=\8D\D2\DA\C7D\D6z\DB&}\1D\B8\E1\DE\9Dz}\17~\1C7\04\8D-|^\188\1E\AF\C7\1B3H1\00Y\F6\F2\CA\0F'\1Bc\12~\02\1DI\C0]y\87\EF^z/\1FfU\D8\09\D9a8h\B0\07\A3\FC\CC\85\10\7FLee\B3\FA\FA\A5So\DBtLVF\03\E2\D5z)\1C\C6\02\BCY\F2\04uc\C0\84/`\1Cgv\FDc\86\F3\FA\BF\DC\D2-\90\91\BD3\A9\E5f\0C\DAB'\CA\F4f\C2\EC\92\14W\06c\D0M\15\06\EBiXOw\C5\8B\C7\F0\8E\EDd\A0\B3<fq\C6-\DA\0A\0D\FEp'd\F8'\FA\F6_0\A5\0Dl\DA\F2b^xG\D3f\00\1C\FDV\1F]?o\F4L\D8\FD\0E'\C9\5C+\BC\C0\A4\E7#)\02\9F1\D6\E9\D7\96\F4\E0^\0B\0E\13\EE<\09\ED\F2=v\91\C3\A4\97\AE\D4\87\D0]\F6\18G\1F\1Dg\F2\CFc\A0\91'\F8\93Eu#?\D1\F1\AD#\DDd\93\96Ap\7F\F7\F5\A9\89\A24\B0\8D\1B\AE\19\15IX#m\87\15O\81v\FB#\B5\EA\CF\ACT\8DNB/\EB\0Fc\DBh7\A8\CF\8B\AB\F5\A4n\96*\B2\D6\BE\9E\BD\0D\B4B\A9\CF\01\83\8A\17Gv\C4\C6\83\04\95\0B\FC\11\C9b\B8\0Cv\84\D9\B97\FA\FC|\C2mX>\B3\04\BB\8C\8FH\BC\91'\CC\F9\B7\22\19\83.\09\B5r\D9T\1CM\A1\EA\0B\F1\C6\08rF\87zn\80V\0A\8A\C0\DD\11k\D6\DDG\DF\10\D9\D8\EA|\B0\8F\03\00.\C1\8FD\A8\D30\06\89\A2\F94\AD\DC\03\85\EDQ\A7\82\9C\E7]R\93\0C2\9A[\E1\AA\CA\B8\02m:\D4\B1:\F0_\BE\B5\0D\10k82\ACv\80\BD\CA\94qz\F2\C95*\DE\9FBI\18\01\AB\BC\EF|d?X=\92Y\DB\13\DBXn\0A\E0\B7\91J\08 \D6.<E\C9\8B\17y\E7\C7\90\99:\18%", align 16
@X25519.kZeros = internal constant [32 x i8] zeroinitializer, align 16
@fe_isnonzero.zero = internal constant [32 x i8] zeroinitializer, align 16
@Bi = internal constant [8 x %struct.ge_precomp] [%struct.ge_precomp { %struct.fe_loose { [5 x i64] [i64 1288382639258501, i64 245678601348599, i64 269427782077623, i64 1462984067271730, i64 137412439391563] }, %struct.fe_loose { [5 x i64] [i64 62697248952638, i64 204681361388450, i64 631292143396476, i64 338455783676468, i64 1213667448819585] }, %struct.fe_loose { [5 x i64] [i64 301289933810280, i64 1259582250014073, i64 1422107436869536, i64 796239922652654, i64 1953934009299142] } }, %struct.ge_precomp { %struct.fe_loose { [5 x i64] [i64 1601611775252272, i64 1720807796594148, i64 1132070835939856, i64 1260455018889551, i64 2147779492816911] }, %struct.fe_loose { [5 x i64] [i64 316559037616741, i64 2177824224946892, i64 1459442586438991, i64 1461528397712656, i64 751590696113597] }, %struct.fe_loose { [5 x i64] [i64 1850748884277385, i64 1200145853858453, i64 1068094770532492, i64 672251375690438, i64 1586055907191707] } }, %struct.ge_precomp { %struct.fe_loose { [5 x i64] [i64 769950342298419, i64 132954430919746, i64 844085933195555, i64 974092374476333, i64 726076285546016] }, %struct.fe_loose { [5 x i64] [i64 425251763115706, i64 608463272472562, i64 442562545713235, i64 837766094556764, i64 374555092627893] }, %struct.fe_loose { [5 x i64] [i64 1086255230780037, i64 274979815921559, i64 1960002765731872, i64 929474102396301, i64 1190409889297339] } }, %struct.ge_precomp { %struct.fe_loose { [5 x i64] [i64 665000864555967, i64 2065379846933859, i64 370231110385876, i64 350988370788628, i64 1233371373142985] }, %struct.fe_loose { [5 x i64] [i64 2019367628972465, i64 676711900706637, i64 110710997811333, i64 1108646842542025, i64 517791959672113] }, %struct.fe_loose { [5 x i64] [i64 965130719900578, i64 247011430587952, i64 526356006571389, i64 91986625355052, i64 2157223321444601] } }, %struct.ge_precomp { %struct.fe_loose { [5 x i64] [i64 1802695059465007, i64 1664899123557221, i64 593559490740857, i64 2160434469266659, i64 927570450755031] }, %struct.fe_loose { [5 x i64] [i64 1725674970513508, i64 1933645953859181, i64 1542344539275782, i64 1767788773573747, i64 1297447965928905] }, %struct.fe_loose { [5 x i64] [i64 1381809363726107, i64 1430341051343062, i64 2061843536018959, i64 1551778050872521, i64 2036394857967624] } }, %struct.ge_precomp { %struct.fe_loose { [5 x i64] [i64 1970894096313054, i64 528066325833207, i64 1619374932191227, i64 2207306624415883, i64 1169170329061080] }, %struct.fe_loose { [5 x i64] [i64 2070390218572616, i64 1458919061857835, i64 624171843017421, i64 1055332792707765, i64 433987520732508] }, %struct.fe_loose { [5 x i64] [i64 893653801273833, i64 1168026499324677, i64 1242553501121234, i64 1306366254304474, i64 1086752658510815] } }, %struct.ge_precomp { %struct.fe_loose { [5 x i64] [i64 213454002618221, i64 939771523987438, i64 1159882208056014, i64 317388369627517, i64 621213314200687] }, %struct.fe_loose { [5 x i64] [i64 1971678598905747, i64 338026507889165, i64 762398079972271, i64 655096486107477, i64 42299032696322] }, %struct.fe_loose { [5 x i64] [i64 177130678690680, i64 1754759263300204, i64 1864311296286618, i64 1180675631479880, i64 1292726903152791] } }, %struct.ge_precomp { %struct.fe_loose { [5 x i64] [i64 1913163449625248, i64 460779200291993, i64 2193883288642314, i64 1008900146920800, i64 1721983679009502] }, %struct.fe_loose { [5 x i64] [i64 1070401523076875, i64 1272492007800961, i64 1910153608563310, i64 2075579521696771, i64 1191169788841221] }, %struct.fe_loose { [5 x i64] [i64 692896803108118, i64 500174642072499, i64 2068223309439677, i64 1162190621851337, i64 1426986007309901] } }], align 16

; Function Attrs: nounwind ssp uwtable
define hidden void @x25519_ge_tobytes(i8*, %struct.ge_p2* nocapture readonly) local_unnamed_addr #0 {
  %3 = alloca [32 x i8], align 16
  %4 = alloca %struct.fe_loose, align 8
  %5 = alloca %struct.fe, align 8
  %6 = alloca %struct.fe, align 8
  %7 = alloca %struct.fe, align 8
  %8 = bitcast %struct.fe* %5 to i8*
  call void @llvm.lifetime.start.p0i8(i64 40, i8* nonnull %8) #4
  %9 = getelementptr inbounds %struct.fe, %struct.fe* %5, i64 0, i32 0, i64 0
  %10 = bitcast %struct.fe* %6 to i8*
  %11 = bitcast %struct.fe* %5 to i8*
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %11, i8 -86, i64 40, i1 false)
  call void @llvm.lifetime.start.p0i8(i64 40, i8* nonnull %10) #4
  %12 = getelementptr inbounds %struct.fe, %struct.fe* %6, i64 0, i32 0, i64 0
  %13 = bitcast %struct.fe* %7 to i8*
  %14 = bitcast %struct.fe* %6 to i8*
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %14, i8 -86, i64 40, i1 false)
  call void @llvm.lifetime.start.p0i8(i64 40, i8* nonnull %13) #4
  %15 = getelementptr inbounds %struct.fe, %struct.fe* %7, i64 0, i32 0, i64 0
  %16 = getelementptr inbounds %struct.ge_p2, %struct.ge_p2* %1, i64 0, i32 2
  %17 = bitcast %struct.fe_loose* %4 to i8*
  %18 = bitcast %struct.fe* %7 to i8*
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %18, i8 -86, i64 40, i1 false)
  call void @llvm.lifetime.start.p0i8(i64 40, i8* nonnull %17) #4
  %19 = bitcast %struct.fe* %16 to i8*
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 8 %17, i8* align 1 %19, i64 40, i1 false) #4
  call fastcc void @fe_loose_invert(%struct.fe* nonnull %5, %struct.fe_loose* nonnull %4) #4
  call void @llvm.lifetime.end.p0i8(i64 40, i8* nonnull %17) #4
  %20 = getelementptr inbounds %struct.ge_p2, %struct.ge_p2* %1, i64 0, i32 0, i32 0, i64 0
  call fastcc void @fe_mul_impl(i64* nonnull %12, i64* %20, i64* nonnull %9) #4
  %21 = getelementptr inbounds %struct.ge_p2, %struct.ge_p2* %1, i64 0, i32 1, i32 0, i64 0
  call fastcc void @fe_mul_impl(i64* nonnull %15, i64* %21, i64* nonnull %9) #4
  call fastcc void @fe_tobytes(i8* %0, %struct.fe* nonnull %7)
  %22 = getelementptr inbounds [32 x i8], [32 x i8]* %3, i64 0, i64 0
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %22) #4
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %22, i8 -86, i64 32, i1 false) #4
  call fastcc void @fe_tobytes(i8* nonnull %22, %struct.fe* nonnull %6) #4
  %23 = load i8, i8* %22, align 16
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %22) #4
  %24 = getelementptr inbounds i8, i8* %0, i64 31
  %25 = load i8, i8* %24, align 1
  %26 = shl i8 %23, 7
  %27 = xor i8 %25, %26
  store i8 %27, i8* %24, align 1
  call void @llvm.lifetime.end.p0i8(i64 40, i8* nonnull %13) #4
  call void @llvm.lifetime.end.p0i8(i64 40, i8* nonnull %10) #4
  call void @llvm.lifetime.end.p0i8(i64 40, i8* nonnull %8) #4
  ret void
}

; Function Attrs: argmemonly nounwind
declare void @llvm.lifetime.start.p0i8(i64 immarg, i8* nocapture) #1

; Function Attrs: argmemonly nounwind
declare void @llvm.memset.p0i8.i64(i8* nocapture writeonly, i8, i64, i1 immarg) #1

; Function Attrs: nounwind ssp uwtable
define internal fastcc void @fe_tobytes(i8*, %struct.fe* nocapture readonly) unnamed_addr #0 {
  %3 = getelementptr inbounds %struct.fe, %struct.fe* %1, i64 0, i32 0, i64 0
  %4 = load i64, i64* %3, align 8
  %5 = add i64 %4, -2251799813685229
  %6 = lshr i64 %5, 51
  %7 = and i64 %5, 2251799813685247
  %8 = sub nsw i64 0, %6
  %9 = getelementptr inbounds %struct.fe, %struct.fe* %1, i64 0, i32 0, i64 1
  %10 = load i64, i64* %9, align 8
  %11 = and i64 %8, 255
  %12 = sub i64 %10, %11
  %13 = add i64 %12, -2251799813685247
  %14 = lshr i64 %13, 51
  %15 = and i64 %13, 2251799813685247
  %16 = sub nsw i64 0, %14
  %17 = getelementptr inbounds %struct.fe, %struct.fe* %1, i64 0, i32 0, i64 2
  %18 = load i64, i64* %17, align 8
  %19 = and i64 %16, 255
  %20 = sub i64 %18, %19
  %21 = add i64 %20, -2251799813685247
  %22 = lshr i64 %21, 51
  %23 = and i64 %21, 2251799813685247
  %24 = sub nsw i64 0, %22
  %25 = getelementptr inbounds %struct.fe, %struct.fe* %1, i64 0, i32 0, i64 3
  %26 = load i64, i64* %25, align 8
  %27 = and i64 %24, 255
  %28 = sub i64 %26, %27
  %29 = add i64 %28, -2251799813685247
  %30 = lshr i64 %29, 51
  %31 = and i64 %29, 2251799813685247
  %32 = sub nsw i64 0, %30
  %33 = getelementptr inbounds %struct.fe, %struct.fe* %1, i64 0, i32 0, i64 4
  %34 = load i64, i64* %33, align 8
  %35 = and i64 %32, 255
  %36 = sub i64 %34, %35
  %37 = add i64 %36, -2251799813685247
  %38 = and i64 %37, 574208952489738240
  %39 = icmp ne i64 %38, 0
  %40 = sext i1 %39 to i64
  %41 = tail call i64 asm "", "=r,0,~{dirflag},~{fpsr},~{flags}"(i64 %40) #5, !srcloc !2
  %42 = and i64 %41, 2251799813685229
  %43 = add nuw nsw i64 %42, %7
  %44 = lshr i64 %43, 51
  %45 = and i64 %41, 2251799813685247
  %46 = add nuw nsw i64 %45, %15
  %47 = add nuw nsw i64 %46, %44
  %48 = lshr i64 %47, 51
  %49 = add nuw nsw i64 %23, %45
  %50 = add nuw nsw i64 %49, %48
  %51 = lshr i64 %50, 51
  %52 = add nuw nsw i64 %51, %45
  %53 = add nuw nsw i64 %52, %31
  %54 = lshr i64 %53, 51
  %55 = add i64 %37, %41
  %56 = add i64 %55, %54
  %57 = shl i64 %56, 4
  %58 = shl nuw nsw i64 %53, 1
  %59 = shl nuw nsw i64 %50, 6
  %60 = shl nuw nsw i64 %47, 3
  %61 = lshr i64 %43, 8
  %62 = trunc i64 %43 to i8
  %63 = lshr i64 %43, 16
  %64 = trunc i64 %61 to i8
  %65 = lshr i64 %43, 24
  %66 = trunc i64 %63 to i8
  %67 = lshr i64 %43, 32
  %68 = trunc i64 %65 to i8
  %69 = lshr i64 %43, 40
  %70 = trunc i64 %67 to i8
  %71 = lshr i64 %43, 48
  %72 = and i64 %71, 7
  %73 = trunc i64 %69 to i8
  %74 = or i64 %60, %72
  %75 = lshr i64 %47, 5
  %76 = trunc i64 %74 to i8
  %77 = lshr i64 %47, 13
  %78 = trunc i64 %75 to i8
  %79 = lshr i64 %47, 21
  %80 = trunc i64 %77 to i8
  %81 = lshr i64 %47, 29
  %82 = trunc i64 %79 to i8
  %83 = lshr i64 %47, 37
  %84 = trunc i64 %81 to i8
  %85 = lshr i64 %47, 45
  %86 = and i64 %85, 63
  %87 = trunc i64 %83 to i8
  %88 = or i64 %59, %86
  %89 = lshr i64 %50, 2
  %90 = trunc i64 %88 to i8
  %91 = lshr i64 %50, 10
  %92 = trunc i64 %89 to i8
  %93 = lshr i64 %50, 18
  %94 = trunc i64 %91 to i8
  %95 = lshr i64 %50, 26
  %96 = trunc i64 %93 to i8
  %97 = lshr i64 %50, 34
  %98 = trunc i64 %95 to i8
  %99 = lshr i64 %50, 42
  %100 = trunc i64 %97 to i8
  %101 = lshr i64 %50, 50
  %102 = and i64 %101, 1
  %103 = trunc i64 %99 to i8
  %104 = or i64 %58, %102
  %105 = lshr i64 %53, 7
  %106 = trunc i64 %104 to i8
  %107 = lshr i64 %53, 15
  %108 = trunc i64 %105 to i8
  %109 = lshr i64 %53, 23
  %110 = trunc i64 %107 to i8
  %111 = lshr i64 %53, 31
  %112 = trunc i64 %109 to i8
  %113 = lshr i64 %53, 39
  %114 = trunc i64 %111 to i8
  %115 = lshr i64 %53, 47
  %116 = and i64 %115, 15
  %117 = trunc i64 %113 to i8
  %118 = or i64 %57, %116
  %119 = lshr i64 %56, 4
  %120 = trunc i64 %118 to i8
  %121 = lshr i64 %56, 12
  %122 = trunc i64 %119 to i8
  %123 = lshr i64 %56, 20
  %124 = trunc i64 %121 to i8
  %125 = lshr i64 %56, 28
  %126 = trunc i64 %123 to i8
  %127 = lshr i64 %56, 36
  %128 = trunc i64 %125 to i8
  %129 = lshr i64 %56, 44
  %130 = trunc i64 %129 to i8
  %131 = and i8 %130, 127
  %132 = trunc i64 %127 to i8
  store i8 %62, i8* %0, align 1
  %133 = getelementptr inbounds i8, i8* %0, i64 1
  store i8 %64, i8* %133, align 1
  %134 = getelementptr inbounds i8, i8* %0, i64 2
  store i8 %66, i8* %134, align 1
  %135 = getelementptr inbounds i8, i8* %0, i64 3
  store i8 %68, i8* %135, align 1
  %136 = getelementptr inbounds i8, i8* %0, i64 4
  store i8 %70, i8* %136, align 1
  %137 = getelementptr inbounds i8, i8* %0, i64 5
  store i8 %73, i8* %137, align 1
  %138 = getelementptr inbounds i8, i8* %0, i64 6
  store i8 %76, i8* %138, align 1
  %139 = getelementptr inbounds i8, i8* %0, i64 7
  store i8 %78, i8* %139, align 1
  %140 = getelementptr inbounds i8, i8* %0, i64 8
  store i8 %80, i8* %140, align 1
  %141 = getelementptr inbounds i8, i8* %0, i64 9
  store i8 %82, i8* %141, align 1
  %142 = getelementptr inbounds i8, i8* %0, i64 10
  store i8 %84, i8* %142, align 1
  %143 = getelementptr inbounds i8, i8* %0, i64 11
  store i8 %87, i8* %143, align 1
  %144 = getelementptr inbounds i8, i8* %0, i64 12
  store i8 %90, i8* %144, align 1
  %145 = getelementptr inbounds i8, i8* %0, i64 13
  store i8 %92, i8* %145, align 1
  %146 = getelementptr inbounds i8, i8* %0, i64 14
  store i8 %94, i8* %146, align 1
  %147 = getelementptr inbounds i8, i8* %0, i64 15
  store i8 %96, i8* %147, align 1
  %148 = getelementptr inbounds i8, i8* %0, i64 16
  store i8 %98, i8* %148, align 1
  %149 = getelementptr inbounds i8, i8* %0, i64 17
  store i8 %100, i8* %149, align 1
  %150 = getelementptr inbounds i8, i8* %0, i64 18
  store i8 %103, i8* %150, align 1
  %151 = getelementptr inbounds i8, i8* %0, i64 19
  store i8 %106, i8* %151, align 1
  %152 = getelementptr inbounds i8, i8* %0, i64 20
  store i8 %108, i8* %152, align 1
  %153 = getelementptr inbounds i8, i8* %0, i64 21
  store i8 %110, i8* %153, align 1
  %154 = getelementptr inbounds i8, i8* %0, i64 22
  store i8 %112, i8* %154, align 1
  %155 = getelementptr inbounds i8, i8* %0, i64 23
  store i8 %114, i8* %155, align 1
  %156 = getelementptr inbounds i8, i8* %0, i64 24
  store i8 %117, i8* %156, align 1
  %157 = getelementptr inbounds i8, i8* %0, i64 25
  store i8 %120, i8* %157, align 1
  %158 = getelementptr inbounds i8, i8* %0, i64 26
  store i8 %122, i8* %158, align 1
  %159 = getelementptr inbounds i8, i8* %0, i64 27
  store i8 %124, i8* %159, align 1
  %160 = getelementptr inbounds i8, i8* %0, i64 28
  store i8 %126, i8* %160, align 1
  %161 = getelementptr inbounds i8, i8* %0, i64 29
  store i8 %128, i8* %161, align 1
  %162 = getelementptr inbounds i8, i8* %0, i64 30
  store i8 %132, i8* %162, align 1
  %163 = getelementptr inbounds i8, i8* %0, i64 31
  store i8 %131, i8* %163, align 1
  ret void
}

; Function Attrs: argmemonly nounwind
declare void @llvm.lifetime.end.p0i8(i64 immarg, i8* nocapture) #1

; Function Attrs: nounwind ssp uwtable
define hidden i32 @x25519_ge_frombytes_vartime(%struct.ge_p3*, i8* nocapture readonly) local_unnamed_addr #0 {
  %3 = alloca %struct.fe, align 8
  %4 = alloca %struct.fe, align 8
  %5 = alloca %struct.fe, align 8
  %6 = alloca %struct.fe, align 8
  %7 = alloca %struct.fe, align 8
  %8 = alloca [32 x i8], align 16
  %9 = alloca %struct.fe, align 8
  %10 = alloca %struct.fe_loose, align 8
  %11 = alloca %struct.fe, align 8
  %12 = alloca %struct.fe, align 8
  %13 = bitcast %struct.fe* %9 to i8*
  call void @llvm.lifetime.start.p0i8(i64 40, i8* nonnull %13) #4
  %14 = getelementptr inbounds %struct.fe, %struct.fe* %9, i64 0, i32 0, i64 0
  %15 = getelementptr inbounds %struct.fe, %struct.fe* %9, i64 0, i32 0, i64 1
  %16 = getelementptr inbounds %struct.fe, %struct.fe* %9, i64 0, i32 0, i64 2
  %17 = getelementptr inbounds %struct.fe, %struct.fe* %9, i64 0, i32 0, i64 3
  %18 = getelementptr inbounds %struct.fe, %struct.fe* %9, i64 0, i32 0, i64 4
  %19 = bitcast %struct.fe_loose* %10 to i8*
  %20 = bitcast %struct.fe* %9 to i8*
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %20, i8 -86, i64 40, i1 false)
  call void @llvm.lifetime.start.p0i8(i64 40, i8* nonnull %19) #4
  %21 = getelementptr inbounds %struct.fe_loose, %struct.fe_loose* %10, i64 0, i32 0, i64 0
  %22 = getelementptr inbounds %struct.fe_loose, %struct.fe_loose* %10, i64 0, i32 0, i64 1
  %23 = getelementptr inbounds %struct.fe_loose, %struct.fe_loose* %10, i64 0, i32 0, i64 2
  %24 = getelementptr inbounds %struct.fe_loose, %struct.fe_loose* %10, i64 0, i32 0, i64 3
  %25 = getelementptr inbounds %struct.fe_loose, %struct.fe_loose* %10, i64 0, i32 0, i64 4
  %26 = bitcast %struct.fe* %11 to i8*
  %27 = bitcast %struct.fe_loose* %10 to i8*
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %27, i8 -86, i64 40, i1 false)
  call void @llvm.lifetime.start.p0i8(i64 40, i8* nonnull %26) #4
  %28 = getelementptr inbounds %struct.fe, %struct.fe* %11, i64 0, i32 0, i64 0
  %29 = getelementptr inbounds %struct.fe, %struct.fe* %11, i64 0, i32 0, i64 1
  %30 = getelementptr inbounds %struct.fe, %struct.fe* %11, i64 0, i32 0, i64 2
  %31 = getelementptr inbounds %struct.fe, %struct.fe* %11, i64 0, i32 0, i64 3
  %32 = getelementptr inbounds %struct.fe, %struct.fe* %11, i64 0, i32 0, i64 4
  %33 = bitcast %struct.fe* %12 to i8*
  %34 = bitcast %struct.fe* %11 to i8*
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %34, i8 -86, i64 40, i1 false)
  call void @llvm.lifetime.start.p0i8(i64 40, i8* nonnull %33) #4
  %35 = getelementptr inbounds %struct.fe, %struct.fe* %12, i64 0, i32 0, i64 0
  %36 = getelementptr inbounds %struct.fe, %struct.fe* %12, i64 0, i32 0, i64 1
  %37 = getelementptr inbounds %struct.fe, %struct.fe* %12, i64 0, i32 0, i64 2
  %38 = getelementptr inbounds %struct.fe, %struct.fe* %12, i64 0, i32 0, i64 3
  %39 = getelementptr inbounds %struct.fe, %struct.fe* %12, i64 0, i32 0, i64 4
  %40 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %0, i64 0, i32 1
  %41 = getelementptr inbounds [32 x i8], [32 x i8]* %8, i64 0, i64 0
  %42 = bitcast %struct.fe* %12 to i8*
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %42, i8 -86, i64 40, i1 false)
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %41) #4
  %43 = getelementptr inbounds [32 x i8], [32 x i8]* %8, i64 0, i64 31
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 16 %41, i8* align 1 %1, i64 32, i1 false) #4
  %44 = load i8, i8* %43, align 1
  %45 = and i8 %44, 127
  store i8 %45, i8* %43, align 1
  call fastcc void @fe_frombytes_strict(%struct.fe* %40, i8* nonnull %41) #4
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %41) #4
  %46 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %0, i64 0, i32 2, i32 0, i64 1
  %47 = bitcast i64* %46 to i8*
  call void @llvm.memset.p0i8.i64(i8* align 1 %47, i8 0, i64 32, i1 false) #4
  %48 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %0, i64 0, i32 2, i32 0, i64 0
  store i64 1, i64* %48, align 8
  %49 = getelementptr inbounds %struct.fe, %struct.fe* %40, i64 0, i32 0, i64 0
  %50 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %0, i64 0, i32 1, i32 0, i64 4
  %51 = load i64, i64* %50, align 8
  %52 = mul i64 %51, 19
  %53 = mul i64 %51, 38
  %54 = shl i64 %51, 1
  %55 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %0, i64 0, i32 1, i32 0, i64 3
  %56 = load i64, i64* %55, align 8
  %57 = mul i64 %56, 19
  %58 = mul i64 %56, 38
  %59 = shl i64 %56, 1
  %60 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %0, i64 0, i32 1, i32 0, i64 2
  %61 = load i64, i64* %60, align 8
  %62 = shl i64 %61, 1
  %63 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %0, i64 0, i32 1, i32 0, i64 1
  %64 = load i64, i64* %63, align 8
  %65 = shl i64 %64, 1
  %66 = zext i64 %51 to i128
  %67 = zext i64 %52 to i128
  %68 = mul nuw i128 %67, %66
  %69 = zext i64 %56 to i128
  %70 = zext i64 %53 to i128
  %71 = mul nuw i128 %69, %70
  %72 = zext i64 %57 to i128
  %73 = mul nuw i128 %72, %69
  %74 = zext i64 %61 to i128
  %75 = mul nuw i128 %74, %70
  %76 = zext i64 %58 to i128
  %77 = mul nuw i128 %74, %76
  %78 = mul nuw i128 %74, %74
  %79 = zext i64 %64 to i128
  %80 = mul nuw i128 %79, %70
  %81 = zext i64 %59 to i128
  %82 = mul nuw i128 %79, %81
  %83 = zext i64 %62 to i128
  %84 = mul nuw i128 %79, %83
  %85 = mul nuw i128 %79, %79
  %86 = load i64, i64* %49, align 8
  %87 = zext i64 %86 to i128
  %88 = zext i64 %54 to i128
  %89 = mul nuw i128 %87, %88
  %90 = mul nuw i128 %87, %81
  %91 = mul nuw i128 %87, %83
  %92 = zext i64 %65 to i128
  %93 = mul nuw i128 %87, %92
  %94 = mul nuw i128 %87, %87
  %95 = add i128 %80, %77
  %96 = add i128 %95, %94
  %97 = lshr i128 %96, 51
  %98 = trunc i128 %96 to i64
  %99 = and i64 %98, 2251799813685247
  %100 = add i128 %82, %78
  %101 = add i128 %100, %89
  %102 = add i128 %84, %68
  %103 = add i128 %102, %90
  %104 = add i128 %85, %71
  %105 = add i128 %104, %91
  %106 = add i128 %75, %73
  %107 = add i128 %106, %93
  %108 = and i128 %97, 18446744073709551615
  %109 = add i128 %107, %108
  %110 = lshr i128 %109, 51
  %111 = trunc i128 %109 to i64
  %112 = and i64 %111, 2251799813685247
  %113 = and i128 %110, 18446744073709551615
  %114 = add i128 %105, %113
  %115 = lshr i128 %114, 51
  %116 = trunc i128 %114 to i64
  %117 = and i64 %116, 2251799813685247
  %118 = and i128 %115, 18446744073709551615
  %119 = add i128 %103, %118
  %120 = lshr i128 %119, 51
  %121 = trunc i128 %119 to i64
  %122 = and i64 %121, 2251799813685247
  %123 = and i128 %120, 18446744073709551615
  %124 = add i128 %101, %123
  %125 = lshr i128 %124, 51
  %126 = trunc i128 %125 to i64
  %127 = trunc i128 %124 to i64
  %128 = and i64 %127, 2251799813685247
  %129 = mul i64 %126, 19
  %130 = add i64 %129, %99
  %131 = lshr i64 %130, 51
  %132 = and i64 %130, 2251799813685247
  %133 = add nuw nsw i64 %131, %112
  %134 = lshr i64 %133, 51
  %135 = and i64 %133, 2251799813685247
  %136 = add nuw nsw i64 %134, %117
  store i64 %132, i64* %28, align 8
  store i64 %135, i64* %29, align 8
  store i64 %136, i64* %30, align 8
  store i64 %122, i64* %31, align 8
  store i64 %128, i64* %32, align 8
  call fastcc void @fe_mul_impl(i64* nonnull %35, i64* nonnull %28, i64* getelementptr inbounds (%struct.fe, %struct.fe* @d, i64 0, i32 0, i64 0)) #4
  %137 = load i64, i64* %48, align 8
  %138 = sub i64 4503599627370458, %137
  %139 = add i64 %138, %132
  %140 = add nuw nsw i64 %135, 4503599627370494
  %141 = load i64, i64* %46, align 8
  %142 = sub i64 %140, %141
  %143 = add nuw nsw i64 %136, 4503599627370494
  %144 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %0, i64 0, i32 2, i32 0, i64 2
  %145 = load i64, i64* %144, align 8
  %146 = sub i64 %143, %145
  %147 = add nuw nsw i64 %122, 4503599627370494
  %148 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %0, i64 0, i32 2, i32 0, i64 3
  %149 = load i64, i64* %148, align 8
  %150 = sub i64 %147, %149
  %151 = add nuw nsw i64 %128, 4503599627370494
  %152 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %0, i64 0, i32 2, i32 0, i64 4
  %153 = load i64, i64* %152, align 8
  %154 = sub i64 %151, %153
  %155 = lshr i64 %139, 51
  %156 = add i64 %155, %142
  %157 = lshr i64 %156, 51
  %158 = add i64 %157, %146
  %159 = lshr i64 %158, 51
  %160 = add i64 %159, %150
  %161 = lshr i64 %160, 51
  %162 = add i64 %161, %154
  %163 = and i64 %139, 2251799813685247
  %164 = lshr i64 %162, 51
  %165 = mul nuw nsw i64 %164, 19
  %166 = add nuw nsw i64 %165, %163
  %167 = lshr i64 %166, 51
  %168 = and i64 %156, 2251799813685247
  %169 = add nuw nsw i64 %167, %168
  %170 = and i64 %166, 2251799813685247
  %171 = and i64 %169, 2251799813685247
  %172 = lshr i64 %169, 51
  %173 = and i64 %158, 2251799813685247
  %174 = add nuw nsw i64 %172, %173
  %175 = and i64 %160, 2251799813685247
  %176 = and i64 %162, 2251799813685247
  store i64 %170, i64* %14, align 8
  store i64 %171, i64* %15, align 8
  store i64 %174, i64* %16, align 8
  store i64 %175, i64* %17, align 8
  store i64 %176, i64* %18, align 8
  %177 = load i64, i64* %35, align 8
  %178 = add i64 %177, %137
  %179 = load i64, i64* %36, align 8
  %180 = add i64 %179, %141
  %181 = load i64, i64* %37, align 8
  %182 = add i64 %181, %145
  %183 = load i64, i64* %38, align 8
  %184 = add i64 %183, %149
  %185 = load i64, i64* %39, align 8
  %186 = add i64 %185, %153
  store i64 %178, i64* %21, align 8
  store i64 %180, i64* %22, align 8
  store i64 %182, i64* %23, align 8
  store i64 %184, i64* %24, align 8
  store i64 %186, i64* %25, align 8
  %187 = mul i64 %186, 19
  %188 = mul i64 %186, 38
  %189 = shl i64 %186, 1
  %190 = mul i64 %184, 19
  %191 = mul i64 %184, 38
  %192 = shl i64 %184, 1
  %193 = shl i64 %182, 1
  %194 = shl i64 %180, 1
  %195 = zext i64 %186 to i128
  %196 = zext i64 %187 to i128
  %197 = mul nuw i128 %196, %195
  %198 = zext i64 %184 to i128
  %199 = zext i64 %188 to i128
  %200 = mul nuw i128 %199, %198
  %201 = zext i64 %190 to i128
  %202 = mul nuw i128 %201, %198
  %203 = zext i64 %182 to i128
  %204 = mul nuw i128 %203, %199
  %205 = zext i64 %191 to i128
  %206 = mul nuw i128 %203, %205
  %207 = mul nuw i128 %203, %203
  %208 = zext i64 %180 to i128
  %209 = mul nuw i128 %208, %199
  %210 = zext i64 %192 to i128
  %211 = mul nuw i128 %208, %210
  %212 = zext i64 %193 to i128
  %213 = mul nuw i128 %208, %212
  %214 = mul nuw i128 %208, %208
  %215 = zext i64 %178 to i128
  %216 = zext i64 %189 to i128
  %217 = mul nuw i128 %216, %215
  %218 = mul nuw i128 %210, %215
  %219 = mul nuw i128 %212, %215
  %220 = zext i64 %194 to i128
  %221 = mul nuw i128 %220, %215
  %222 = mul nuw i128 %215, %215
  %223 = add i128 %206, %222
  %224 = add i128 %223, %209
  %225 = lshr i128 %224, 51
  %226 = trunc i128 %224 to i64
  %227 = and i64 %226, 2251799813685247
  %228 = add i128 %214, %200
  %229 = add i128 %228, %219
  %230 = add i128 %204, %202
  %231 = add i128 %230, %221
  %232 = and i128 %225, 18446744073709551615
  %233 = add i128 %231, %232
  %234 = lshr i128 %233, 51
  %235 = trunc i128 %233 to i64
  %236 = and i64 %235, 2251799813685247
  %237 = and i128 %234, 18446744073709551615
  %238 = add i128 %229, %237
  %239 = lshr i128 %238, 51
  %240 = trunc i128 %238 to i64
  %241 = and i64 %240, 2251799813685247
  %242 = and i128 %239, 18446744073709551615
  %243 = add i128 %197, %218
  %244 = add i128 %243, %213
  %245 = add i128 %244, %242
  %246 = lshr i128 %245, 51
  %247 = and i128 %246, 18446744073709551615
  %248 = add i128 %207, %217
  %249 = add i128 %248, %211
  %250 = add i128 %249, %247
  %251 = lshr i128 %250, 51
  %252 = trunc i128 %251 to i64
  %253 = insertelement <2 x i128> undef, i128 %245, i32 0
  %254 = insertelement <2 x i128> %253, i128 %250, i32 1
  %255 = trunc <2 x i128> %254 to <2 x i64>
  %256 = and <2 x i64> %255, <i64 2251799813685247, i64 2251799813685247>
  %257 = mul i64 %252, 19
  %258 = add i64 %257, %227
  %259 = lshr i64 %258, 51
  %260 = and i64 %258, 2251799813685247
  %261 = add nuw nsw i64 %259, %236
  %262 = lshr i64 %261, 51
  %263 = and i64 %261, 2251799813685247
  %264 = add nuw nsw i64 %262, %241
  store i64 %260, i64* %28, align 8
  store i64 %263, i64* %29, align 8
  store i64 %264, i64* %30, align 8
  %265 = bitcast i64* %31 to <2 x i64>*
  store <2 x i64> %256, <2 x i64>* %265, align 8
  call fastcc void @fe_mul_impl(i64* nonnull %28, i64* nonnull %28, i64* nonnull %21) #4
  %266 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %0, i64 0, i32 0, i32 0, i64 0
  %267 = load i64, i64* %32, align 8
  %268 = mul i64 %267, 19
  %269 = mul i64 %267, 38
  %270 = shl i64 %267, 1
  %271 = load i64, i64* %31, align 8
  %272 = mul i64 %271, 19
  %273 = mul i64 %271, 38
  %274 = shl i64 %271, 1
  %275 = load i64, i64* %30, align 8
  %276 = shl i64 %275, 1
  %277 = load i64, i64* %29, align 8
  %278 = shl i64 %277, 1
  %279 = zext i64 %267 to i128
  %280 = zext i64 %268 to i128
  %281 = mul nuw i128 %280, %279
  %282 = zext i64 %271 to i128
  %283 = zext i64 %269 to i128
  %284 = mul nuw i128 %282, %283
  %285 = zext i64 %272 to i128
  %286 = mul nuw i128 %285, %282
  %287 = zext i64 %275 to i128
  %288 = mul nuw i128 %287, %283
  %289 = zext i64 %273 to i128
  %290 = mul nuw i128 %287, %289
  %291 = mul nuw i128 %287, %287
  %292 = zext i64 %277 to i128
  %293 = mul nuw i128 %292, %283
  %294 = zext i64 %274 to i128
  %295 = mul nuw i128 %292, %294
  %296 = zext i64 %276 to i128
  %297 = mul nuw i128 %292, %296
  %298 = mul nuw i128 %292, %292
  %299 = load i64, i64* %28, align 8
  %300 = zext i64 %299 to i128
  %301 = zext i64 %270 to i128
  %302 = mul nuw i128 %300, %301
  %303 = mul nuw i128 %300, %294
  %304 = mul nuw i128 %300, %296
  %305 = zext i64 %278 to i128
  %306 = mul nuw i128 %300, %305
  %307 = mul nuw i128 %300, %300
  %308 = add i128 %293, %290
  %309 = add i128 %308, %307
  %310 = lshr i128 %309, 51
  %311 = trunc i128 %309 to i64
  %312 = and i64 %311, 2251799813685247
  %313 = add i128 %295, %291
  %314 = add i128 %313, %302
  %315 = add i128 %297, %281
  %316 = add i128 %315, %303
  %317 = add i128 %298, %284
  %318 = add i128 %317, %304
  %319 = add i128 %288, %286
  %320 = add i128 %319, %306
  %321 = and i128 %310, 18446744073709551615
  %322 = add i128 %320, %321
  %323 = lshr i128 %322, 51
  %324 = trunc i128 %322 to i64
  %325 = and i64 %324, 2251799813685247
  %326 = and i128 %323, 18446744073709551615
  %327 = add i128 %318, %326
  %328 = lshr i128 %327, 51
  %329 = trunc i128 %327 to i64
  %330 = and i64 %329, 2251799813685247
  %331 = and i128 %328, 18446744073709551615
  %332 = add i128 %316, %331
  %333 = lshr i128 %332, 51
  %334 = and i128 %333, 18446744073709551615
  %335 = add i128 %314, %334
  %336 = lshr i128 %335, 51
  %337 = trunc i128 %336 to i64
  %338 = insertelement <2 x i128> undef, i128 %332, i32 0
  %339 = insertelement <2 x i128> %338, i128 %335, i32 1
  %340 = trunc <2 x i128> %339 to <2 x i64>
  %341 = and <2 x i64> %340, <i64 2251799813685247, i64 2251799813685247>
  %342 = mul i64 %337, 19
  %343 = add i64 %342, %312
  %344 = lshr i64 %343, 51
  %345 = and i64 %343, 2251799813685247
  %346 = add nuw nsw i64 %344, %325
  %347 = lshr i64 %346, 51
  %348 = and i64 %346, 2251799813685247
  %349 = add nuw nsw i64 %347, %330
  store i64 %345, i64* %266, align 8
  %350 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %0, i64 0, i32 0, i32 0, i64 1
  store i64 %348, i64* %350, align 8
  %351 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %0, i64 0, i32 0, i32 0, i64 2
  store i64 %349, i64* %351, align 8
  %352 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %0, i64 0, i32 0, i32 0, i64 3
  %353 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %0, i64 0, i32 0, i32 0, i64 4
  %354 = bitcast i64* %352 to <2 x i64>*
  store <2 x i64> %341, <2 x i64>* %354, align 8
  call fastcc void @fe_mul_impl(i64* %266, i64* %266, i64* nonnull %21) #4
  call fastcc void @fe_mul_impl(i64* %266, i64* %266, i64* nonnull %14) #4
  %355 = bitcast %struct.fe* %3 to i8*
  call void @llvm.lifetime.start.p0i8(i64 40, i8* nonnull %355) #4
  %356 = getelementptr inbounds %struct.fe, %struct.fe* %3, i64 0, i32 0, i64 0
  %357 = getelementptr inbounds %struct.fe, %struct.fe* %3, i64 0, i32 0, i64 1
  %358 = getelementptr inbounds %struct.fe, %struct.fe* %3, i64 0, i32 0, i64 2
  %359 = getelementptr inbounds %struct.fe, %struct.fe* %3, i64 0, i32 0, i64 3
  %360 = getelementptr inbounds %struct.fe, %struct.fe* %3, i64 0, i32 0, i64 4
  %361 = bitcast %struct.fe* %4 to i8*
  %362 = getelementptr inbounds %struct.fe, %struct.fe* %3, i64 0, i32 0, i64 4
  store i64 -6148914691236517206, i64* %362, align 8
  call void @llvm.lifetime.start.p0i8(i64 40, i8* nonnull %361) #4
  %363 = getelementptr inbounds %struct.fe, %struct.fe* %4, i64 0, i32 0, i64 0
  %364 = getelementptr inbounds %struct.fe, %struct.fe* %4, i64 0, i32 0, i64 1
  %365 = getelementptr inbounds %struct.fe, %struct.fe* %4, i64 0, i32 0, i64 2
  %366 = getelementptr inbounds %struct.fe, %struct.fe* %4, i64 0, i32 0, i64 3
  %367 = getelementptr inbounds %struct.fe, %struct.fe* %4, i64 0, i32 0, i64 4
  %368 = bitcast %struct.fe* %5 to i8*
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %361, i8 -86, i64 40, i1 false) #4
  call void @llvm.lifetime.start.p0i8(i64 40, i8* nonnull %368) #4
  %369 = getelementptr inbounds %struct.fe, %struct.fe* %5, i64 0, i32 0, i64 1
  %370 = getelementptr inbounds %struct.fe, %struct.fe* %5, i64 0, i32 0, i64 3
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %368, i8 -86, i64 40, i1 false) #4
  %371 = load i64, i64* %353, align 8
  %372 = mul i64 %371, 19
  %373 = mul i64 %371, 38
  %374 = shl i64 %371, 1
  %375 = load i64, i64* %352, align 8
  %376 = mul i64 %375, 19
  %377 = mul i64 %375, 38
  %378 = shl i64 %375, 1
  %379 = load i64, i64* %351, align 8
  %380 = shl i64 %379, 1
  %381 = load i64, i64* %350, align 8
  %382 = shl i64 %381, 1
  %383 = zext i64 %371 to i128
  %384 = zext i64 %372 to i128
  %385 = mul nuw i128 %384, %383
  %386 = zext i64 %375 to i128
  %387 = zext i64 %373 to i128
  %388 = mul nuw i128 %386, %387
  %389 = zext i64 %376 to i128
  %390 = mul nuw i128 %389, %386
  %391 = zext i64 %379 to i128
  %392 = mul nuw i128 %391, %387
  %393 = zext i64 %377 to i128
  %394 = mul nuw i128 %391, %393
  %395 = mul nuw i128 %391, %391
  %396 = zext i64 %381 to i128
  %397 = mul nuw i128 %396, %387
  %398 = zext i64 %378 to i128
  %399 = mul nuw i128 %396, %398
  %400 = zext i64 %380 to i128
  %401 = mul nuw i128 %396, %400
  %402 = mul nuw i128 %396, %396
  %403 = load i64, i64* %266, align 8
  %404 = zext i64 %403 to i128
  %405 = zext i64 %374 to i128
  %406 = mul nuw i128 %404, %405
  %407 = mul nuw i128 %404, %398
  %408 = mul nuw i128 %404, %400
  %409 = zext i64 %382 to i128
  %410 = mul nuw i128 %404, %409
  %411 = mul nuw i128 %404, %404
  %412 = add i128 %397, %394
  %413 = add i128 %412, %411
  %414 = lshr i128 %413, 51
  %415 = trunc i128 %413 to i64
  %416 = and i64 %415, 2251799813685247
  %417 = add i128 %399, %395
  %418 = add i128 %417, %406
  %419 = add i128 %401, %385
  %420 = add i128 %419, %407
  %421 = add i128 %402, %388
  %422 = add i128 %421, %408
  %423 = add i128 %392, %390
  %424 = add i128 %423, %410
  %425 = and i128 %414, 18446744073709551615
  %426 = add i128 %424, %425
  %427 = lshr i128 %426, 51
  %428 = trunc i128 %426 to i64
  %429 = and i64 %428, 2251799813685247
  %430 = and i128 %427, 18446744073709551615
  %431 = add i128 %422, %430
  %432 = lshr i128 %431, 51
  %433 = trunc i128 %431 to i64
  %434 = and i64 %433, 2251799813685247
  %435 = and i128 %432, 18446744073709551615
  %436 = add i128 %420, %435
  %437 = lshr i128 %436, 51
  %438 = trunc i128 %436 to i64
  %439 = and i64 %438, 2251799813685247
  %440 = and i128 %437, 18446744073709551615
  %441 = add i128 %418, %440
  %442 = lshr i128 %441, 51
  %443 = trunc i128 %442 to i64
  %444 = trunc i128 %441 to i64
  %445 = and i64 %444, 2251799813685247
  %446 = mul i64 %443, 19
  %447 = add i64 %446, %416
  %448 = lshr i64 %447, 51
  %449 = and i64 %447, 2251799813685247
  %450 = add nuw nsw i64 %448, %429
  %451 = lshr i64 %450, 51
  %452 = and i64 %450, 2251799813685247
  %453 = add nuw nsw i64 %451, %434
  store i64 %449, i64* %356, align 8
  store i64 %452, i64* %357, align 8
  store i64 %453, i64* %358, align 8
  store i64 %439, i64* %359, align 8
  store i64 %445, i64* %360, align 8
  %454 = mul nuw nsw i64 %445, 19
  %455 = mul nuw nsw i64 %445, 38
  %456 = shl nuw nsw i64 %445, 1
  %457 = mul nuw nsw i64 %439, 19
  %458 = mul nuw nsw i64 %439, 38
  %459 = shl nuw nsw i64 %439, 1
  %460 = shl nuw nsw i64 %453, 1
  %461 = shl nuw nsw i64 %452, 1
  %462 = zext i64 %445 to i128
  %463 = zext i64 %454 to i128
  %464 = mul nuw nsw i128 %463, %462
  %465 = zext i64 %439 to i128
  %466 = zext i64 %455 to i128
  %467 = mul nuw nsw i128 %466, %465
  %468 = zext i64 %457 to i128
  %469 = mul nuw nsw i128 %468, %465
  %470 = zext i64 %453 to i128
  %471 = mul nuw nsw i128 %470, %466
  %472 = zext i64 %458 to i128
  %473 = mul nuw nsw i128 %470, %472
  %474 = mul nuw nsw i128 %470, %470
  %475 = zext i64 %452 to i128
  %476 = mul nuw nsw i128 %475, %466
  %477 = zext i64 %459 to i128
  %478 = mul nuw nsw i128 %475, %477
  %479 = zext i64 %460 to i128
  %480 = mul nuw nsw i128 %479, %475
  %481 = mul nuw nsw i128 %475, %475
  %482 = zext i64 %449 to i128
  %483 = zext i64 %456 to i128
  %484 = mul nuw nsw i128 %482, %483
  %485 = mul nuw nsw i128 %482, %477
  %486 = mul nuw nsw i128 %479, %482
  %487 = zext i64 %461 to i128
  %488 = mul nuw nsw i128 %487, %482
  %489 = mul nuw nsw i128 %482, %482
  %490 = add nuw nsw i128 %476, %489
  %491 = add nuw nsw i128 %490, %473
  %492 = lshr i128 %491, 51
  %493 = trunc i128 %491 to i64
  %494 = and i64 %493, 2251799813685247
  %495 = add nuw nsw i128 %481, %467
  %496 = add nuw nsw i128 %495, %486
  %497 = add nuw nsw i128 %471, %469
  %498 = add nuw nsw i128 %497, %488
  %499 = and i128 %492, 18446744073709551615
  %500 = add nuw nsw i128 %498, %499
  %501 = lshr i128 %500, 51
  %502 = trunc i128 %500 to i64
  %503 = and i64 %502, 2251799813685247
  %504 = and i128 %501, 18446744073709551615
  %505 = add nuw nsw i128 %496, %504
  %506 = lshr i128 %505, 51
  %507 = trunc i128 %505 to i64
  %508 = and i64 %507, 2251799813685247
  %509 = and i128 %506, 18446744073709551615
  %510 = add nuw nsw i128 %485, %464
  %511 = add nuw nsw i128 %510, %480
  %512 = add nuw nsw i128 %511, %509
  %513 = lshr i128 %512, 51
  %514 = trunc i128 %512 to i64
  %515 = and i64 %514, 2251799813685247
  %516 = and i128 %513, 18446744073709551615
  %517 = add nuw nsw i128 %478, %484
  %518 = add nuw nsw i128 %517, %474
  %519 = add nuw nsw i128 %518, %516
  %520 = lshr i128 %519, 51
  %521 = trunc i128 %520 to i64
  %522 = mul i64 %521, 19
  %523 = add nuw nsw i64 %522, %494
  %524 = lshr i64 %523, 51
  %525 = and i64 %523, 2251799813685247
  %526 = add nuw nsw i64 %524, %503
  %527 = lshr i64 %526, 51
  %528 = and i64 %526, 2251799813685247
  %529 = add nuw nsw i64 %527, %508
  store i64 %525, i64* %363, align 8
  store i64 %528, i64* %364, align 8
  store i64 %529, i64* %365, align 8
  store i64 %515, i64* %366, align 8
  %530 = trunc i128 %442 to i51
  %531 = mul i51 %530, 19
  %532 = trunc i128 %413 to i51
  %533 = add i51 %531, %532
  %534 = trunc i128 %441 to i51
  %535 = mul i51 %533, %534
  %536 = trunc i64 %448 to i51
  %537 = trunc i128 %426 to i51
  %538 = add i51 %536, %537
  %539 = trunc i128 %436 to i51
  %540 = mul i51 %538, %539
  %541 = add i51 %540, %535
  %542 = shl i51 %541, 1
  %543 = trunc i64 %451 to i51
  %544 = trunc i128 %431 to i51
  %545 = add i51 %543, %544
  %546 = mul i51 %545, %545
  %547 = add i51 %542, %546
  %548 = trunc i128 %513 to i51
  %549 = add i51 %547, %548
  %550 = mul i51 %549, 19
  %551 = mul i51 %550, %549
  %552 = mul i51 %534, 19
  %553 = mul i51 %552, %534
  %554 = mul i51 %545, %538
  %555 = mul i51 %533, %539
  %556 = trunc i128 %506 to i51
  %557 = add i51 %554, %555
  %558 = shl i51 %557, 1
  %559 = add i51 %558, %553
  %560 = add i51 %559, %556
  %561 = trunc i64 %523 to i51
  %562 = mul i51 %560, %561
  %563 = mul i51 %539, 38
  %564 = mul i51 %563, %534
  %565 = shl i51 %533, 1
  %566 = mul i51 %565, %545
  %567 = mul i51 %538, %538
  %568 = trunc i128 %501 to i51
  %569 = trunc i64 %527 to i51
  %570 = add i51 %567, %564
  %571 = add i51 %570, %566
  %572 = add i51 %571, %568
  %573 = add i51 %572, %569
  %574 = trunc i64 %526 to i51
  %575 = mul i51 %573, %574
  %576 = zext i51 %549 to i128
  %577 = zext i51 %560 to i128
  %578 = mul nuw nsw i128 %576, 38
  %579 = mul nuw nsw i128 %578, %577
  %580 = zext i64 %529 to i128
  %581 = shl nuw nsw i64 %529, 1
  %582 = zext i64 %581 to i128
  %583 = zext i64 %525 to i128
  %584 = mul nuw nsw i128 %582, %583
  %585 = zext i64 %528 to i128
  %586 = mul nuw nsw i128 %585, %585
  %587 = mul nuw nsw i128 %578, %580
  %588 = mul nuw nsw i128 %577, 19
  %589 = mul nuw nsw i128 %588, %577
  %590 = add nuw nsw i128 %587, %589
  %591 = shl nuw nsw i64 %528, 1
  %592 = zext i64 %591 to i128
  %593 = mul nuw nsw i128 %592, %583
  %594 = add nuw nsw i128 %590, %593
  %595 = mul nuw nsw i128 %585, %576
  %596 = mul nuw nsw i128 %580, %577
  %597 = add nuw nsw i128 %596, %595
  %598 = mul nuw nsw i128 %597, 38
  %599 = mul nuw nsw i128 %583, %583
  %600 = add nuw nsw i128 %598, %599
  %601 = lshr i128 %600, 51
  %602 = and i128 %601, 18446744073709551615
  %603 = add nuw nsw i128 %594, %602
  %604 = lshr i128 %603, 51
  %605 = and i128 %604, 18446744073709551615
  %606 = add nuw nsw i128 %586, %579
  %607 = add nuw nsw i128 %606, %584
  %608 = add nuw nsw i128 %607, %605
  %609 = lshr i128 %608, 51
  %610 = trunc i128 %609 to i51
  %611 = add i51 %575, %562
  %612 = shl i51 %611, 1
  %613 = add i51 %612, %551
  %614 = add i51 %613, %610
  %615 = mul i51 %549, %561
  %616 = mul i51 %560, %574
  %617 = add i51 %616, %615
  %618 = shl i51 %617, 1
  %619 = mul i51 %573, %573
  %620 = add i51 %618, %619
  %621 = mul nuw nsw i128 %576, 19
  %622 = mul nuw nsw i128 %621, %576
  %623 = shl nuw nsw i128 %577, 1
  %624 = mul nuw nsw i128 %623, %583
  %625 = add nuw nsw i128 %624, %622
  %626 = mul nuw nsw i128 %582, %585
  %627 = add nuw nsw i128 %625, %626
  %628 = and i128 %609, 18446744073709551615
  %629 = add nuw nsw i128 %627, %628
  %630 = lshr i128 %629, 51
  %631 = trunc i128 %630 to i51
  %632 = add i51 %620, %631
  %633 = mul nuw nsw i128 %583, %576
  %634 = mul nuw nsw i128 %585, %577
  %635 = add nuw nsw i128 %634, %633
  %636 = shl nuw nsw i128 %635, 1
  %637 = mul nuw nsw i128 %580, %580
  %638 = add nuw nsw i128 %636, %637
  %639 = and i128 %630, 18446744073709551615
  %640 = add nuw nsw i128 %638, %639
  %641 = lshr i128 %640, 51
  %642 = trunc i128 %641 to i51
  %643 = mul i51 %642, 19
  %644 = mul i51 %573, %560
  %645 = mul i51 %549, %574
  %646 = add i51 %644, %645
  %647 = mul i51 %646, 38
  %648 = mul i64 %525, %525
  %649 = trunc i64 %648 to i51
  %650 = add i51 %647, %649
  %651 = add i51 %643, %650
  %652 = mul i51 %549, 38
  %653 = mul i51 %652, %573
  %654 = mul i51 %560, 19
  %655 = mul i51 %654, %560
  %656 = trunc i128 %601 to i51
  %657 = shl nuw nsw i64 %525, 1
  %658 = mul i64 %657, %528
  %659 = trunc i64 %658 to i51
  %660 = add i51 %655, %659
  %661 = add i51 %660, %653
  %662 = add i51 %661, %656
  %663 = trunc i128 %641 to i64
  %664 = mul i64 %663, 19
  %665 = zext i51 %650 to i64
  %666 = add nuw nsw i64 %664, %665
  %667 = lshr i64 %666, 51
  %668 = trunc i64 %667 to i51
  %669 = add i51 %662, %668
  %670 = zext i51 %662 to i64
  %671 = add nuw nsw i64 %667, %670
  %672 = mul i51 %652, %560
  %673 = shl i51 %561, 1
  %674 = mul i51 %673, %573
  %675 = trunc i128 %604 to i51
  %676 = mul i64 %528, %528
  %677 = trunc i64 %676 to i51
  %678 = add i51 %672, %677
  %679 = add i51 %678, %674
  %680 = add i51 %679, %675
  %681 = zext i51 %680 to i64
  %682 = zext i51 %614 to i64
  %683 = zext i51 %632 to i64
  %684 = zext i51 %651 to i64
  %685 = zext i51 %669 to i64
  %686 = lshr i64 %671, 51
  %687 = add nuw nsw i64 %686, %681
  store i64 %683, i64* %367, align 8
  store i64 %682, i64* %366, align 8
  store i64 %687, i64* %365, align 8
  store i64 %685, i64* %364, align 8
  store i64 %684, i64* %363, align 8
  call fastcc void @fe_mul_impl(i64* nonnull %363, i64* %266, i64* nonnull %363) #4
  call fastcc void @fe_mul_impl(i64* nonnull %356, i64* nonnull %356, i64* nonnull %363) #4
  %688 = load i64, i64* %360, align 8
  %689 = mul i64 %688, 19
  %690 = mul i64 %688, 38
  %691 = shl i64 %688, 1
  %692 = load i64, i64* %359, align 8
  %693 = mul i64 %692, 19
  %694 = mul i64 %692, 38
  %695 = shl i64 %692, 1
  %696 = load i64, i64* %358, align 8
  %697 = shl i64 %696, 1
  %698 = load i64, i64* %357, align 8
  %699 = shl i64 %698, 1
  %700 = zext i64 %688 to i128
  %701 = zext i64 %689 to i128
  %702 = mul nuw i128 %701, %700
  %703 = zext i64 %692 to i128
  %704 = zext i64 %690 to i128
  %705 = mul nuw i128 %703, %704
  %706 = zext i64 %693 to i128
  %707 = mul nuw i128 %706, %703
  %708 = zext i64 %696 to i128
  %709 = mul nuw i128 %708, %704
  %710 = zext i64 %694 to i128
  %711 = mul nuw i128 %708, %710
  %712 = mul nuw i128 %708, %708
  %713 = zext i64 %698 to i128
  %714 = mul nuw i128 %713, %704
  %715 = zext i64 %695 to i128
  %716 = mul nuw i128 %713, %715
  %717 = zext i64 %697 to i128
  %718 = mul nuw i128 %713, %717
  %719 = mul nuw i128 %713, %713
  %720 = load i64, i64* %356, align 8
  %721 = zext i64 %720 to i128
  %722 = zext i64 %691 to i128
  %723 = mul nuw i128 %721, %722
  %724 = mul nuw i128 %721, %715
  %725 = mul nuw i128 %721, %717
  %726 = zext i64 %699 to i128
  %727 = mul nuw i128 %721, %726
  %728 = mul nuw i128 %721, %721
  %729 = add i128 %714, %711
  %730 = add i128 %729, %728
  %731 = lshr i128 %730, 51
  %732 = trunc i128 %730 to i64
  %733 = and i64 %732, 2251799813685247
  %734 = add i128 %716, %712
  %735 = add i128 %734, %723
  %736 = add i128 %718, %702
  %737 = add i128 %736, %724
  %738 = add i128 %719, %705
  %739 = add i128 %738, %725
  %740 = add i128 %709, %707
  %741 = add i128 %740, %727
  %742 = and i128 %731, 18446744073709551615
  %743 = add i128 %741, %742
  %744 = lshr i128 %743, 51
  %745 = trunc i128 %743 to i64
  %746 = and i64 %745, 2251799813685247
  %747 = and i128 %744, 18446744073709551615
  %748 = add i128 %739, %747
  %749 = lshr i128 %748, 51
  %750 = trunc i128 %748 to i64
  %751 = and i64 %750, 2251799813685247
  %752 = and i128 %749, 18446744073709551615
  %753 = add i128 %737, %752
  %754 = lshr i128 %753, 51
  %755 = and i128 %754, 18446744073709551615
  %756 = add i128 %735, %755
  %757 = lshr i128 %756, 51
  %758 = trunc i128 %757 to i64
  %759 = insertelement <2 x i128> undef, i128 %753, i32 0
  %760 = insertelement <2 x i128> %759, i128 %756, i32 1
  %761 = trunc <2 x i128> %760 to <2 x i64>
  %762 = and <2 x i64> %761, <i64 2251799813685247, i64 2251799813685247>
  %763 = mul i64 %758, 19
  %764 = add i64 %763, %733
  %765 = lshr i64 %764, 51
  %766 = and i64 %764, 2251799813685247
  %767 = add nuw nsw i64 %765, %746
  %768 = lshr i64 %767, 51
  %769 = and i64 %767, 2251799813685247
  %770 = add nuw nsw i64 %768, %751
  store i64 %766, i64* %356, align 8
  store i64 %769, i64* %357, align 8
  store i64 %770, i64* %358, align 8
  %771 = bitcast i64* %359 to <2 x i64>*
  store <2 x i64> %762, <2 x i64>* %771, align 8
  call fastcc void @fe_mul_impl(i64* nonnull %356, i64* nonnull %363, i64* nonnull %356) #4
  %772 = load i64, i64* %360, align 8
  %773 = mul i64 %772, 19
  %774 = mul i64 %772, 38
  %775 = shl i64 %772, 1
  %776 = load i64, i64* %359, align 8
  %777 = mul i64 %776, 19
  %778 = mul i64 %776, 38
  %779 = shl i64 %776, 1
  %780 = load i64, i64* %358, align 8
  %781 = shl i64 %780, 1
  %782 = load i64, i64* %357, align 8
  %783 = shl i64 %782, 1
  %784 = zext i64 %772 to i128
  %785 = zext i64 %773 to i128
  %786 = mul nuw i128 %785, %784
  %787 = zext i64 %776 to i128
  %788 = zext i64 %774 to i128
  %789 = mul nuw i128 %787, %788
  %790 = zext i64 %777 to i128
  %791 = mul nuw i128 %790, %787
  %792 = zext i64 %780 to i128
  %793 = mul nuw i128 %792, %788
  %794 = zext i64 %778 to i128
  %795 = mul nuw i128 %792, %794
  %796 = mul nuw i128 %792, %792
  %797 = zext i64 %782 to i128
  %798 = mul nuw i128 %797, %788
  %799 = zext i64 %779 to i128
  %800 = mul nuw i128 %797, %799
  %801 = zext i64 %781 to i128
  %802 = mul nuw i128 %797, %801
  %803 = mul nuw i128 %797, %797
  %804 = load i64, i64* %356, align 8
  %805 = zext i64 %804 to i128
  %806 = zext i64 %775 to i128
  %807 = mul nuw i128 %805, %806
  %808 = mul nuw i128 %805, %799
  %809 = mul nuw i128 %805, %801
  %810 = zext i64 %783 to i128
  %811 = mul nuw i128 %805, %810
  %812 = mul nuw i128 %805, %805
  %813 = add i128 %798, %795
  %814 = add i128 %813, %812
  %815 = lshr i128 %814, 51
  %816 = trunc i128 %814 to i64
  %817 = and i64 %816, 2251799813685247
  %818 = add i128 %800, %796
  %819 = add i128 %818, %807
  %820 = add i128 %802, %786
  %821 = add i128 %820, %808
  %822 = add i128 %803, %789
  %823 = add i128 %822, %809
  %824 = add i128 %793, %791
  %825 = add i128 %824, %811
  %826 = and i128 %815, 18446744073709551615
  %827 = add i128 %825, %826
  %828 = lshr i128 %827, 51
  %829 = trunc i128 %827 to i64
  %830 = and i64 %829, 2251799813685247
  %831 = and i128 %828, 18446744073709551615
  %832 = add i128 %823, %831
  %833 = lshr i128 %832, 51
  %834 = trunc i128 %832 to i64
  %835 = and i64 %834, 2251799813685247
  %836 = and i128 %833, 18446744073709551615
  %837 = add i128 %821, %836
  %838 = lshr i128 %837, 51
  %839 = trunc i128 %837 to i64
  %840 = and i64 %839, 2251799813685247
  %841 = and i128 %838, 18446744073709551615
  %842 = add i128 %819, %841
  %843 = lshr i128 %842, 51
  %844 = trunc i128 %843 to i64
  %845 = trunc i128 %842 to i64
  %846 = and i64 %845, 2251799813685247
  %847 = mul i64 %844, 19
  %848 = add i64 %847, %817
  %849 = lshr i64 %848, 51
  %850 = and i64 %848, 2251799813685247
  %851 = add nuw nsw i64 %849, %830
  %852 = lshr i64 %851, 51
  %853 = and i64 %851, 2251799813685247
  %854 = add nuw nsw i64 %852, %835
  store i64 %850, i64* %363, align 8
  store i64 %853, i64* %364, align 8
  store i64 %854, i64* %365, align 8
  store i64 %840, i64* %366, align 8
  store i64 %846, i64* %367, align 8
  br label %855

855:                                              ; preds = %855, %2
  %856 = phi i64 [ %850, %2 ], [ %935, %855 ]
  %857 = phi i64 [ %853, %2 ], [ %938, %855 ]
  %858 = phi i64 [ %854, %2 ], [ %939, %855 ]
  %859 = phi i64 [ %840, %2 ], [ %925, %855 ]
  %860 = phi i64 [ %846, %2 ], [ %931, %855 ]
  %861 = phi i32 [ 1, %2 ], [ %940, %855 ]
  %862 = mul nuw nsw i64 %860, 19
  %863 = mul nuw nsw i64 %860, 38
  %864 = shl nuw nsw i64 %860, 1
  %865 = mul nuw nsw i64 %859, 19
  %866 = mul nuw nsw i64 %859, 38
  %867 = shl nuw nsw i64 %859, 1
  %868 = shl nsw i64 %858, 1
  %869 = shl nuw nsw i64 %857, 1
  %870 = zext i64 %860 to i128
  %871 = zext i64 %862 to i128
  %872 = mul nuw nsw i128 %871, %870
  %873 = zext i64 %859 to i128
  %874 = zext i64 %863 to i128
  %875 = mul nuw nsw i128 %874, %873
  %876 = zext i64 %865 to i128
  %877 = mul nuw nsw i128 %876, %873
  %878 = zext i64 %858 to i128
  %879 = mul nuw nsw i128 %874, %878
  %880 = zext i64 %866 to i128
  %881 = mul nuw nsw i128 %880, %878
  %882 = mul nuw i128 %878, %878
  %883 = zext i64 %857 to i128
  %884 = mul nuw nsw i128 %874, %883
  %885 = zext i64 %867 to i128
  %886 = mul nuw nsw i128 %885, %883
  %887 = zext i64 %868 to i128
  %888 = mul nuw nsw i128 %887, %883
  %889 = mul nuw nsw i128 %883, %883
  %890 = zext i64 %856 to i128
  %891 = zext i64 %864 to i128
  %892 = mul nuw nsw i128 %891, %890
  %893 = mul nuw nsw i128 %885, %890
  %894 = mul nuw nsw i128 %887, %890
  %895 = zext i64 %869 to i128
  %896 = mul nuw nsw i128 %895, %890
  %897 = mul nuw nsw i128 %890, %890
  %898 = add nuw nsw i128 %881, %897
  %899 = add nuw nsw i128 %898, %884
  %900 = lshr i128 %899, 51
  %901 = trunc i128 %899 to i64
  %902 = and i64 %901, 2251799813685247
  %903 = add i128 %886, %882
  %904 = add i128 %903, %892
  %905 = and i128 %900, 18446744073709551615
  %906 = add nuw nsw i128 %877, %896
  %907 = add nuw nsw i128 %906, %879
  %908 = add nuw nsw i128 %907, %905
  %909 = lshr i128 %908, 51
  %910 = trunc i128 %908 to i64
  %911 = and i64 %910, 2251799813685247
  %912 = and i128 %909, 18446744073709551615
  %913 = add nuw nsw i128 %894, %889
  %914 = add nuw nsw i128 %913, %875
  %915 = add nuw nsw i128 %914, %912
  %916 = lshr i128 %915, 51
  %917 = trunc i128 %915 to i64
  %918 = and i64 %917, 2251799813685247
  %919 = and i128 %916, 18446744073709551615
  %920 = add nuw nsw i128 %893, %888
  %921 = add nuw nsw i128 %920, %872
  %922 = add nuw nsw i128 %921, %919
  %923 = lshr i128 %922, 51
  %924 = trunc i128 %922 to i64
  %925 = and i64 %924, 2251799813685247
  %926 = and i128 %923, 18446744073709551615
  %927 = add i128 %904, %926
  %928 = lshr i128 %927, 51
  %929 = trunc i128 %928 to i64
  %930 = trunc i128 %927 to i64
  %931 = and i64 %930, 2251799813685247
  %932 = mul i64 %929, 19
  %933 = add i64 %932, %902
  %934 = lshr i64 %933, 51
  %935 = and i64 %933, 2251799813685247
  %936 = add nuw nsw i64 %934, %911
  %937 = lshr i64 %936, 51
  %938 = and i64 %936, 2251799813685247
  %939 = add nuw nsw i64 %937, %918
  %940 = add nuw nsw i32 %861, 1
  %941 = icmp eq i32 %940, 5
  br i1 %941, label %942, label %855

942:                                              ; preds = %855
  %943 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %0, i64 0, i32 0
  %944 = getelementptr inbounds %struct.fe, %struct.fe* %5, i64 0, i32 0, i64 0
  %945 = getelementptr inbounds %struct.fe, %struct.fe* %5, i64 0, i32 0, i64 2
  %946 = getelementptr inbounds %struct.fe, %struct.fe* %5, i64 0, i32 0, i64 4
  store i64 %931, i64* %367, align 8
  store i64 %925, i64* %366, align 8
  store i64 %939, i64* %365, align 8
  store i64 %938, i64* %364, align 8
  store i64 %935, i64* %363, align 8
  call fastcc void @fe_mul_impl(i64* nonnull %356, i64* nonnull %363, i64* nonnull %356) #4
  %947 = load i64, i64* %360, align 8
  %948 = mul i64 %947, 19
  %949 = mul i64 %947, 38
  %950 = shl i64 %947, 1
  %951 = load i64, i64* %359, align 8
  %952 = mul i64 %951, 19
  %953 = mul i64 %951, 38
  %954 = shl i64 %951, 1
  %955 = load i64, i64* %358, align 8
  %956 = shl i64 %955, 1
  %957 = load i64, i64* %357, align 8
  %958 = shl i64 %957, 1
  %959 = zext i64 %947 to i128
  %960 = zext i64 %948 to i128
  %961 = mul nuw i128 %960, %959
  %962 = zext i64 %951 to i128
  %963 = zext i64 %949 to i128
  %964 = mul nuw i128 %962, %963
  %965 = zext i64 %952 to i128
  %966 = mul nuw i128 %965, %962
  %967 = zext i64 %955 to i128
  %968 = mul nuw i128 %967, %963
  %969 = zext i64 %953 to i128
  %970 = mul nuw i128 %967, %969
  %971 = mul nuw i128 %967, %967
  %972 = zext i64 %957 to i128
  %973 = mul nuw i128 %972, %963
  %974 = zext i64 %954 to i128
  %975 = mul nuw i128 %972, %974
  %976 = zext i64 %956 to i128
  %977 = mul nuw i128 %972, %976
  %978 = mul nuw i128 %972, %972
  %979 = load i64, i64* %356, align 8
  %980 = zext i64 %979 to i128
  %981 = zext i64 %950 to i128
  %982 = mul nuw i128 %980, %981
  %983 = mul nuw i128 %980, %974
  %984 = mul nuw i128 %980, %976
  %985 = zext i64 %958 to i128
  %986 = mul nuw i128 %980, %985
  %987 = mul nuw i128 %980, %980
  %988 = add i128 %973, %970
  %989 = add i128 %988, %987
  %990 = lshr i128 %989, 51
  %991 = trunc i128 %989 to i64
  %992 = and i64 %991, 2251799813685247
  %993 = add i128 %975, %971
  %994 = add i128 %993, %982
  %995 = add i128 %977, %961
  %996 = add i128 %995, %983
  %997 = add i128 %978, %964
  %998 = add i128 %997, %984
  %999 = add i128 %968, %966
  %1000 = add i128 %999, %986
  %1001 = and i128 %990, 18446744073709551615
  %1002 = add i128 %1000, %1001
  %1003 = lshr i128 %1002, 51
  %1004 = trunc i128 %1002 to i64
  %1005 = and i64 %1004, 2251799813685247
  %1006 = and i128 %1003, 18446744073709551615
  %1007 = add i128 %998, %1006
  %1008 = lshr i128 %1007, 51
  %1009 = trunc i128 %1007 to i64
  %1010 = and i64 %1009, 2251799813685247
  %1011 = and i128 %1008, 18446744073709551615
  %1012 = add i128 %996, %1011
  %1013 = lshr i128 %1012, 51
  %1014 = trunc i128 %1012 to i64
  %1015 = and i64 %1014, 2251799813685247
  %1016 = and i128 %1013, 18446744073709551615
  %1017 = add i128 %994, %1016
  %1018 = lshr i128 %1017, 51
  %1019 = trunc i128 %1018 to i64
  %1020 = trunc i128 %1017 to i64
  %1021 = and i64 %1020, 2251799813685247
  %1022 = mul i64 %1019, 19
  %1023 = add i64 %1022, %992
  %1024 = lshr i64 %1023, 51
  %1025 = and i64 %1023, 2251799813685247
  %1026 = add nuw nsw i64 %1024, %1005
  %1027 = lshr i64 %1026, 51
  %1028 = and i64 %1026, 2251799813685247
  %1029 = add nuw nsw i64 %1027, %1010
  store i64 %1025, i64* %363, align 8
  store i64 %1028, i64* %364, align 8
  store i64 %1029, i64* %365, align 8
  store i64 %1015, i64* %366, align 8
  store i64 %1021, i64* %367, align 8
  br label %1030

1030:                                             ; preds = %1030, %942
  %1031 = phi i64 [ %1025, %942 ], [ %1110, %1030 ]
  %1032 = phi i64 [ %1028, %942 ], [ %1113, %1030 ]
  %1033 = phi i64 [ %1029, %942 ], [ %1114, %1030 ]
  %1034 = phi i64 [ %1015, %942 ], [ %1100, %1030 ]
  %1035 = phi i64 [ %1021, %942 ], [ %1106, %1030 ]
  %1036 = phi i32 [ 1, %942 ], [ %1115, %1030 ]
  %1037 = mul nuw nsw i64 %1035, 19
  %1038 = mul nuw nsw i64 %1035, 38
  %1039 = shl nuw nsw i64 %1035, 1
  %1040 = mul nuw nsw i64 %1034, 19
  %1041 = mul nuw nsw i64 %1034, 38
  %1042 = shl nuw nsw i64 %1034, 1
  %1043 = shl nsw i64 %1033, 1
  %1044 = shl nuw nsw i64 %1032, 1
  %1045 = zext i64 %1035 to i128
  %1046 = zext i64 %1037 to i128
  %1047 = mul nuw nsw i128 %1046, %1045
  %1048 = zext i64 %1034 to i128
  %1049 = zext i64 %1038 to i128
  %1050 = mul nuw nsw i128 %1049, %1048
  %1051 = zext i64 %1040 to i128
  %1052 = mul nuw nsw i128 %1051, %1048
  %1053 = zext i64 %1033 to i128
  %1054 = mul nuw nsw i128 %1049, %1053
  %1055 = zext i64 %1041 to i128
  %1056 = mul nuw nsw i128 %1055, %1053
  %1057 = mul nuw i128 %1053, %1053
  %1058 = zext i64 %1032 to i128
  %1059 = mul nuw nsw i128 %1049, %1058
  %1060 = zext i64 %1042 to i128
  %1061 = mul nuw nsw i128 %1060, %1058
  %1062 = zext i64 %1043 to i128
  %1063 = mul nuw nsw i128 %1062, %1058
  %1064 = mul nuw nsw i128 %1058, %1058
  %1065 = zext i64 %1031 to i128
  %1066 = zext i64 %1039 to i128
  %1067 = mul nuw nsw i128 %1066, %1065
  %1068 = mul nuw nsw i128 %1060, %1065
  %1069 = mul nuw nsw i128 %1062, %1065
  %1070 = zext i64 %1044 to i128
  %1071 = mul nuw nsw i128 %1070, %1065
  %1072 = mul nuw nsw i128 %1065, %1065
  %1073 = add nuw nsw i128 %1056, %1072
  %1074 = add nuw nsw i128 %1073, %1059
  %1075 = lshr i128 %1074, 51
  %1076 = trunc i128 %1074 to i64
  %1077 = and i64 %1076, 2251799813685247
  %1078 = add i128 %1061, %1057
  %1079 = add i128 %1078, %1067
  %1080 = and i128 %1075, 18446744073709551615
  %1081 = add nuw nsw i128 %1052, %1071
  %1082 = add nuw nsw i128 %1081, %1054
  %1083 = add nuw nsw i128 %1082, %1080
  %1084 = lshr i128 %1083, 51
  %1085 = trunc i128 %1083 to i64
  %1086 = and i64 %1085, 2251799813685247
  %1087 = and i128 %1084, 18446744073709551615
  %1088 = add nuw nsw i128 %1069, %1064
  %1089 = add nuw nsw i128 %1088, %1050
  %1090 = add nuw nsw i128 %1089, %1087
  %1091 = lshr i128 %1090, 51
  %1092 = trunc i128 %1090 to i64
  %1093 = and i64 %1092, 2251799813685247
  %1094 = and i128 %1091, 18446744073709551615
  %1095 = add nuw nsw i128 %1068, %1063
  %1096 = add nuw nsw i128 %1095, %1047
  %1097 = add nuw nsw i128 %1096, %1094
  %1098 = lshr i128 %1097, 51
  %1099 = trunc i128 %1097 to i64
  %1100 = and i64 %1099, 2251799813685247
  %1101 = and i128 %1098, 18446744073709551615
  %1102 = add i128 %1079, %1101
  %1103 = lshr i128 %1102, 51
  %1104 = trunc i128 %1103 to i64
  %1105 = trunc i128 %1102 to i64
  %1106 = and i64 %1105, 2251799813685247
  %1107 = mul i64 %1104, 19
  %1108 = add i64 %1107, %1077
  %1109 = lshr i64 %1108, 51
  %1110 = and i64 %1108, 2251799813685247
  %1111 = add nuw nsw i64 %1109, %1086
  %1112 = lshr i64 %1111, 51
  %1113 = and i64 %1111, 2251799813685247
  %1114 = add nuw nsw i64 %1112, %1093
  %1115 = add nuw nsw i32 %1036, 1
  %1116 = icmp eq i32 %1115, 10
  br i1 %1116, label %1117, label %1030

1117:                                             ; preds = %1030
  store i64 %1106, i64* %367, align 8
  store i64 %1100, i64* %366, align 8
  store i64 %1114, i64* %365, align 8
  store i64 %1113, i64* %364, align 8
  store i64 %1110, i64* %363, align 8
  call fastcc void @fe_mul_impl(i64* nonnull %363, i64* nonnull %363, i64* nonnull %356) #4
  %1118 = load i64, i64* %367, align 8
  %1119 = mul i64 %1118, 19
  %1120 = mul i64 %1118, 38
  %1121 = shl i64 %1118, 1
  %1122 = load i64, i64* %366, align 8
  %1123 = mul i64 %1122, 19
  %1124 = mul i64 %1122, 38
  %1125 = shl i64 %1122, 1
  %1126 = load i64, i64* %365, align 8
  %1127 = shl i64 %1126, 1
  %1128 = load i64, i64* %364, align 8
  %1129 = shl i64 %1128, 1
  %1130 = zext i64 %1118 to i128
  %1131 = zext i64 %1119 to i128
  %1132 = mul nuw i128 %1131, %1130
  %1133 = zext i64 %1122 to i128
  %1134 = zext i64 %1120 to i128
  %1135 = mul nuw i128 %1133, %1134
  %1136 = zext i64 %1123 to i128
  %1137 = mul nuw i128 %1136, %1133
  %1138 = zext i64 %1126 to i128
  %1139 = mul nuw i128 %1138, %1134
  %1140 = zext i64 %1124 to i128
  %1141 = mul nuw i128 %1138, %1140
  %1142 = mul nuw i128 %1138, %1138
  %1143 = zext i64 %1128 to i128
  %1144 = mul nuw i128 %1143, %1134
  %1145 = zext i64 %1125 to i128
  %1146 = mul nuw i128 %1143, %1145
  %1147 = zext i64 %1127 to i128
  %1148 = mul nuw i128 %1143, %1147
  %1149 = mul nuw i128 %1143, %1143
  %1150 = load i64, i64* %363, align 8
  %1151 = zext i64 %1150 to i128
  %1152 = zext i64 %1121 to i128
  %1153 = mul nuw i128 %1151, %1152
  %1154 = mul nuw i128 %1151, %1145
  %1155 = mul nuw i128 %1151, %1147
  %1156 = zext i64 %1129 to i128
  %1157 = mul nuw i128 %1151, %1156
  %1158 = mul nuw i128 %1151, %1151
  %1159 = add i128 %1144, %1141
  %1160 = add i128 %1159, %1158
  %1161 = lshr i128 %1160, 51
  %1162 = trunc i128 %1160 to i64
  %1163 = and i64 %1162, 2251799813685247
  %1164 = add i128 %1146, %1142
  %1165 = add i128 %1164, %1153
  %1166 = add i128 %1148, %1132
  %1167 = add i128 %1166, %1154
  %1168 = add i128 %1149, %1135
  %1169 = add i128 %1168, %1155
  %1170 = add i128 %1139, %1137
  %1171 = add i128 %1170, %1157
  %1172 = and i128 %1161, 18446744073709551615
  %1173 = add i128 %1171, %1172
  %1174 = lshr i128 %1173, 51
  %1175 = trunc i128 %1173 to i64
  %1176 = and i64 %1175, 2251799813685247
  %1177 = and i128 %1174, 18446744073709551615
  %1178 = add i128 %1169, %1177
  %1179 = lshr i128 %1178, 51
  %1180 = trunc i128 %1178 to i64
  %1181 = and i64 %1180, 2251799813685247
  %1182 = and i128 %1179, 18446744073709551615
  %1183 = add i128 %1167, %1182
  %1184 = lshr i128 %1183, 51
  %1185 = trunc i128 %1183 to i64
  %1186 = and i64 %1185, 2251799813685247
  %1187 = and i128 %1184, 18446744073709551615
  %1188 = add i128 %1165, %1187
  %1189 = lshr i128 %1188, 51
  %1190 = trunc i128 %1189 to i64
  %1191 = trunc i128 %1188 to i64
  %1192 = and i64 %1191, 2251799813685247
  %1193 = mul i64 %1190, 19
  %1194 = add i64 %1193, %1163
  %1195 = lshr i64 %1194, 51
  %1196 = and i64 %1194, 2251799813685247
  %1197 = add nuw nsw i64 %1195, %1176
  %1198 = lshr i64 %1197, 51
  %1199 = and i64 %1197, 2251799813685247
  %1200 = add nuw nsw i64 %1198, %1181
  store i64 %1196, i64* %944, align 8
  store i64 %1199, i64* %369, align 8
  store i64 %1200, i64* %945, align 8
  store i64 %1186, i64* %370, align 8
  store i64 %1192, i64* %946, align 8
  br label %1201

1201:                                             ; preds = %1201, %1117
  %1202 = phi i64 [ %1196, %1117 ], [ %1281, %1201 ]
  %1203 = phi i64 [ %1199, %1117 ], [ %1284, %1201 ]
  %1204 = phi i64 [ %1200, %1117 ], [ %1285, %1201 ]
  %1205 = phi i64 [ %1186, %1117 ], [ %1271, %1201 ]
  %1206 = phi i64 [ %1192, %1117 ], [ %1277, %1201 ]
  %1207 = phi i32 [ 1, %1117 ], [ %1286, %1201 ]
  %1208 = mul nuw nsw i64 %1206, 19
  %1209 = mul nuw nsw i64 %1206, 38
  %1210 = shl nuw nsw i64 %1206, 1
  %1211 = mul nuw nsw i64 %1205, 19
  %1212 = mul nuw nsw i64 %1205, 38
  %1213 = shl nuw nsw i64 %1205, 1
  %1214 = shl nsw i64 %1204, 1
  %1215 = shl nuw nsw i64 %1203, 1
  %1216 = zext i64 %1206 to i128
  %1217 = zext i64 %1208 to i128
  %1218 = mul nuw nsw i128 %1217, %1216
  %1219 = zext i64 %1205 to i128
  %1220 = zext i64 %1209 to i128
  %1221 = mul nuw nsw i128 %1220, %1219
  %1222 = zext i64 %1211 to i128
  %1223 = mul nuw nsw i128 %1222, %1219
  %1224 = zext i64 %1204 to i128
  %1225 = mul nuw nsw i128 %1220, %1224
  %1226 = zext i64 %1212 to i128
  %1227 = mul nuw nsw i128 %1226, %1224
  %1228 = mul nuw i128 %1224, %1224
  %1229 = zext i64 %1203 to i128
  %1230 = mul nuw nsw i128 %1220, %1229
  %1231 = zext i64 %1213 to i128
  %1232 = mul nuw nsw i128 %1231, %1229
  %1233 = zext i64 %1214 to i128
  %1234 = mul nuw nsw i128 %1233, %1229
  %1235 = mul nuw nsw i128 %1229, %1229
  %1236 = zext i64 %1202 to i128
  %1237 = zext i64 %1210 to i128
  %1238 = mul nuw nsw i128 %1237, %1236
  %1239 = mul nuw nsw i128 %1231, %1236
  %1240 = mul nuw nsw i128 %1233, %1236
  %1241 = zext i64 %1215 to i128
  %1242 = mul nuw nsw i128 %1241, %1236
  %1243 = mul nuw nsw i128 %1236, %1236
  %1244 = add nuw nsw i128 %1227, %1243
  %1245 = add nuw nsw i128 %1244, %1230
  %1246 = lshr i128 %1245, 51
  %1247 = trunc i128 %1245 to i64
  %1248 = and i64 %1247, 2251799813685247
  %1249 = add i128 %1232, %1228
  %1250 = add i128 %1249, %1238
  %1251 = and i128 %1246, 18446744073709551615
  %1252 = add nuw nsw i128 %1223, %1242
  %1253 = add nuw nsw i128 %1252, %1225
  %1254 = add nuw nsw i128 %1253, %1251
  %1255 = lshr i128 %1254, 51
  %1256 = trunc i128 %1254 to i64
  %1257 = and i64 %1256, 2251799813685247
  %1258 = and i128 %1255, 18446744073709551615
  %1259 = add nuw nsw i128 %1240, %1235
  %1260 = add nuw nsw i128 %1259, %1221
  %1261 = add nuw nsw i128 %1260, %1258
  %1262 = lshr i128 %1261, 51
  %1263 = trunc i128 %1261 to i64
  %1264 = and i64 %1263, 2251799813685247
  %1265 = and i128 %1262, 18446744073709551615
  %1266 = add nuw nsw i128 %1239, %1234
  %1267 = add nuw nsw i128 %1266, %1218
  %1268 = add nuw nsw i128 %1267, %1265
  %1269 = lshr i128 %1268, 51
  %1270 = trunc i128 %1268 to i64
  %1271 = and i64 %1270, 2251799813685247
  %1272 = and i128 %1269, 18446744073709551615
  %1273 = add i128 %1250, %1272
  %1274 = lshr i128 %1273, 51
  %1275 = trunc i128 %1274 to i64
  %1276 = trunc i128 %1273 to i64
  %1277 = and i64 %1276, 2251799813685247
  %1278 = mul i64 %1275, 19
  %1279 = add i64 %1278, %1248
  %1280 = lshr i64 %1279, 51
  %1281 = and i64 %1279, 2251799813685247
  %1282 = add nuw nsw i64 %1280, %1257
  %1283 = lshr i64 %1282, 51
  %1284 = and i64 %1282, 2251799813685247
  %1285 = add nuw nsw i64 %1283, %1264
  %1286 = add nuw nsw i32 %1207, 1
  %1287 = icmp eq i32 %1286, 20
  br i1 %1287, label %1288, label %1201

1288:                                             ; preds = %1201
  store i64 %1277, i64* %946, align 8
  store i64 %1271, i64* %370, align 8
  store i64 %1285, i64* %945, align 8
  store i64 %1284, i64* %369, align 8
  store i64 %1281, i64* %944, align 8
  call fastcc void @fe_mul_impl(i64* nonnull %363, i64* nonnull %944, i64* nonnull %363) #4
  %1289 = load i64, i64* %367, align 8
  %1290 = mul i64 %1289, 19
  %1291 = mul i64 %1289, 38
  %1292 = shl i64 %1289, 1
  %1293 = load i64, i64* %366, align 8
  %1294 = mul i64 %1293, 19
  %1295 = mul i64 %1293, 38
  %1296 = shl i64 %1293, 1
  %1297 = load i64, i64* %365, align 8
  %1298 = shl i64 %1297, 1
  %1299 = load i64, i64* %364, align 8
  %1300 = shl i64 %1299, 1
  %1301 = zext i64 %1289 to i128
  %1302 = zext i64 %1290 to i128
  %1303 = mul nuw i128 %1302, %1301
  %1304 = zext i64 %1293 to i128
  %1305 = zext i64 %1291 to i128
  %1306 = mul nuw i128 %1304, %1305
  %1307 = zext i64 %1294 to i128
  %1308 = mul nuw i128 %1307, %1304
  %1309 = zext i64 %1297 to i128
  %1310 = mul nuw i128 %1309, %1305
  %1311 = zext i64 %1295 to i128
  %1312 = mul nuw i128 %1309, %1311
  %1313 = mul nuw i128 %1309, %1309
  %1314 = zext i64 %1299 to i128
  %1315 = mul nuw i128 %1314, %1305
  %1316 = zext i64 %1296 to i128
  %1317 = mul nuw i128 %1314, %1316
  %1318 = zext i64 %1298 to i128
  %1319 = mul nuw i128 %1314, %1318
  %1320 = mul nuw i128 %1314, %1314
  %1321 = load i64, i64* %363, align 8
  %1322 = zext i64 %1321 to i128
  %1323 = zext i64 %1292 to i128
  %1324 = mul nuw i128 %1322, %1323
  %1325 = mul nuw i128 %1322, %1316
  %1326 = mul nuw i128 %1322, %1318
  %1327 = zext i64 %1300 to i128
  %1328 = mul nuw i128 %1322, %1327
  %1329 = mul nuw i128 %1322, %1322
  %1330 = add i128 %1315, %1312
  %1331 = add i128 %1330, %1329
  %1332 = lshr i128 %1331, 51
  %1333 = trunc i128 %1331 to i64
  %1334 = and i64 %1333, 2251799813685247
  %1335 = add i128 %1317, %1313
  %1336 = add i128 %1335, %1324
  %1337 = add i128 %1319, %1303
  %1338 = add i128 %1337, %1325
  %1339 = add i128 %1320, %1306
  %1340 = add i128 %1339, %1326
  %1341 = add i128 %1310, %1308
  %1342 = add i128 %1341, %1328
  %1343 = and i128 %1332, 18446744073709551615
  %1344 = add i128 %1342, %1343
  %1345 = lshr i128 %1344, 51
  %1346 = trunc i128 %1344 to i64
  %1347 = and i64 %1346, 2251799813685247
  %1348 = and i128 %1345, 18446744073709551615
  %1349 = add i128 %1340, %1348
  %1350 = lshr i128 %1349, 51
  %1351 = trunc i128 %1349 to i64
  %1352 = and i64 %1351, 2251799813685247
  %1353 = and i128 %1350, 18446744073709551615
  %1354 = add i128 %1338, %1353
  %1355 = lshr i128 %1354, 51
  %1356 = trunc i128 %1354 to i64
  %1357 = and i64 %1356, 2251799813685247
  %1358 = and i128 %1355, 18446744073709551615
  %1359 = add i128 %1336, %1358
  %1360 = lshr i128 %1359, 51
  %1361 = trunc i128 %1360 to i64
  %1362 = trunc i128 %1359 to i64
  %1363 = and i64 %1362, 2251799813685247
  %1364 = mul i64 %1361, 19
  %1365 = add i64 %1364, %1334
  %1366 = lshr i64 %1365, 51
  %1367 = and i64 %1365, 2251799813685247
  %1368 = add nuw nsw i64 %1366, %1347
  %1369 = lshr i64 %1368, 51
  %1370 = and i64 %1368, 2251799813685247
  %1371 = add nuw nsw i64 %1369, %1352
  store i64 %1367, i64* %363, align 8
  store i64 %1370, i64* %364, align 8
  store i64 %1371, i64* %365, align 8
  store i64 %1357, i64* %366, align 8
  store i64 %1363, i64* %367, align 8
  br label %1372

1372:                                             ; preds = %1372, %1288
  %1373 = phi i64 [ %1367, %1288 ], [ %1452, %1372 ]
  %1374 = phi i64 [ %1370, %1288 ], [ %1455, %1372 ]
  %1375 = phi i64 [ %1371, %1288 ], [ %1456, %1372 ]
  %1376 = phi i64 [ %1357, %1288 ], [ %1442, %1372 ]
  %1377 = phi i64 [ %1363, %1288 ], [ %1448, %1372 ]
  %1378 = phi i32 [ 1, %1288 ], [ %1457, %1372 ]
  %1379 = mul nuw nsw i64 %1377, 19
  %1380 = mul nuw nsw i64 %1377, 38
  %1381 = shl nuw nsw i64 %1377, 1
  %1382 = mul nuw nsw i64 %1376, 19
  %1383 = mul nuw nsw i64 %1376, 38
  %1384 = shl nuw nsw i64 %1376, 1
  %1385 = shl nsw i64 %1375, 1
  %1386 = shl nuw nsw i64 %1374, 1
  %1387 = zext i64 %1377 to i128
  %1388 = zext i64 %1379 to i128
  %1389 = mul nuw nsw i128 %1388, %1387
  %1390 = zext i64 %1376 to i128
  %1391 = zext i64 %1380 to i128
  %1392 = mul nuw nsw i128 %1391, %1390
  %1393 = zext i64 %1382 to i128
  %1394 = mul nuw nsw i128 %1393, %1390
  %1395 = zext i64 %1375 to i128
  %1396 = mul nuw nsw i128 %1391, %1395
  %1397 = zext i64 %1383 to i128
  %1398 = mul nuw nsw i128 %1397, %1395
  %1399 = mul nuw i128 %1395, %1395
  %1400 = zext i64 %1374 to i128
  %1401 = mul nuw nsw i128 %1391, %1400
  %1402 = zext i64 %1384 to i128
  %1403 = mul nuw nsw i128 %1402, %1400
  %1404 = zext i64 %1385 to i128
  %1405 = mul nuw nsw i128 %1404, %1400
  %1406 = mul nuw nsw i128 %1400, %1400
  %1407 = zext i64 %1373 to i128
  %1408 = zext i64 %1381 to i128
  %1409 = mul nuw nsw i128 %1408, %1407
  %1410 = mul nuw nsw i128 %1402, %1407
  %1411 = mul nuw nsw i128 %1404, %1407
  %1412 = zext i64 %1386 to i128
  %1413 = mul nuw nsw i128 %1412, %1407
  %1414 = mul nuw nsw i128 %1407, %1407
  %1415 = add nuw nsw i128 %1398, %1414
  %1416 = add nuw nsw i128 %1415, %1401
  %1417 = lshr i128 %1416, 51
  %1418 = trunc i128 %1416 to i64
  %1419 = and i64 %1418, 2251799813685247
  %1420 = add i128 %1403, %1399
  %1421 = add i128 %1420, %1409
  %1422 = and i128 %1417, 18446744073709551615
  %1423 = add nuw nsw i128 %1394, %1413
  %1424 = add nuw nsw i128 %1423, %1396
  %1425 = add nuw nsw i128 %1424, %1422
  %1426 = lshr i128 %1425, 51
  %1427 = trunc i128 %1425 to i64
  %1428 = and i64 %1427, 2251799813685247
  %1429 = and i128 %1426, 18446744073709551615
  %1430 = add nuw nsw i128 %1411, %1406
  %1431 = add nuw nsw i128 %1430, %1392
  %1432 = add nuw nsw i128 %1431, %1429
  %1433 = lshr i128 %1432, 51
  %1434 = trunc i128 %1432 to i64
  %1435 = and i64 %1434, 2251799813685247
  %1436 = and i128 %1433, 18446744073709551615
  %1437 = add nuw nsw i128 %1410, %1405
  %1438 = add nuw nsw i128 %1437, %1389
  %1439 = add nuw nsw i128 %1438, %1436
  %1440 = lshr i128 %1439, 51
  %1441 = trunc i128 %1439 to i64
  %1442 = and i64 %1441, 2251799813685247
  %1443 = and i128 %1440, 18446744073709551615
  %1444 = add i128 %1421, %1443
  %1445 = lshr i128 %1444, 51
  %1446 = trunc i128 %1445 to i64
  %1447 = trunc i128 %1444 to i64
  %1448 = and i64 %1447, 2251799813685247
  %1449 = mul i64 %1446, 19
  %1450 = add i64 %1449, %1419
  %1451 = lshr i64 %1450, 51
  %1452 = and i64 %1450, 2251799813685247
  %1453 = add nuw nsw i64 %1451, %1428
  %1454 = lshr i64 %1453, 51
  %1455 = and i64 %1453, 2251799813685247
  %1456 = add nuw nsw i64 %1454, %1435
  %1457 = add nuw nsw i32 %1378, 1
  %1458 = icmp eq i32 %1457, 10
  br i1 %1458, label %1459, label %1372

1459:                                             ; preds = %1372
  store i64 %1448, i64* %367, align 8
  store i64 %1442, i64* %366, align 8
  store i64 %1456, i64* %365, align 8
  store i64 %1455, i64* %364, align 8
  store i64 %1452, i64* %363, align 8
  call fastcc void @fe_mul_impl(i64* nonnull %356, i64* nonnull %363, i64* nonnull %356) #4
  %1460 = load i64, i64* %360, align 8
  %1461 = mul i64 %1460, 19
  %1462 = mul i64 %1460, 38
  %1463 = shl i64 %1460, 1
  %1464 = load i64, i64* %359, align 8
  %1465 = mul i64 %1464, 19
  %1466 = mul i64 %1464, 38
  %1467 = shl i64 %1464, 1
  %1468 = load i64, i64* %358, align 8
  %1469 = shl i64 %1468, 1
  %1470 = load i64, i64* %357, align 8
  %1471 = shl i64 %1470, 1
  %1472 = zext i64 %1460 to i128
  %1473 = zext i64 %1461 to i128
  %1474 = mul nuw i128 %1473, %1472
  %1475 = zext i64 %1464 to i128
  %1476 = zext i64 %1462 to i128
  %1477 = mul nuw i128 %1475, %1476
  %1478 = zext i64 %1465 to i128
  %1479 = mul nuw i128 %1478, %1475
  %1480 = zext i64 %1468 to i128
  %1481 = mul nuw i128 %1480, %1476
  %1482 = zext i64 %1466 to i128
  %1483 = mul nuw i128 %1480, %1482
  %1484 = mul nuw i128 %1480, %1480
  %1485 = zext i64 %1470 to i128
  %1486 = mul nuw i128 %1485, %1476
  %1487 = zext i64 %1467 to i128
  %1488 = mul nuw i128 %1485, %1487
  %1489 = zext i64 %1469 to i128
  %1490 = mul nuw i128 %1485, %1489
  %1491 = mul nuw i128 %1485, %1485
  %1492 = load i64, i64* %356, align 8
  %1493 = zext i64 %1492 to i128
  %1494 = zext i64 %1463 to i128
  %1495 = mul nuw i128 %1493, %1494
  %1496 = mul nuw i128 %1493, %1487
  %1497 = mul nuw i128 %1493, %1489
  %1498 = zext i64 %1471 to i128
  %1499 = mul nuw i128 %1493, %1498
  %1500 = mul nuw i128 %1493, %1493
  %1501 = add i128 %1486, %1483
  %1502 = add i128 %1501, %1500
  %1503 = lshr i128 %1502, 51
  %1504 = trunc i128 %1502 to i64
  %1505 = and i64 %1504, 2251799813685247
  %1506 = add i128 %1488, %1484
  %1507 = add i128 %1506, %1495
  %1508 = add i128 %1490, %1474
  %1509 = add i128 %1508, %1496
  %1510 = add i128 %1491, %1477
  %1511 = add i128 %1510, %1497
  %1512 = add i128 %1481, %1479
  %1513 = add i128 %1512, %1499
  %1514 = and i128 %1503, 18446744073709551615
  %1515 = add i128 %1513, %1514
  %1516 = lshr i128 %1515, 51
  %1517 = trunc i128 %1515 to i64
  %1518 = and i64 %1517, 2251799813685247
  %1519 = and i128 %1516, 18446744073709551615
  %1520 = add i128 %1511, %1519
  %1521 = lshr i128 %1520, 51
  %1522 = trunc i128 %1520 to i64
  %1523 = and i64 %1522, 2251799813685247
  %1524 = and i128 %1521, 18446744073709551615
  %1525 = add i128 %1509, %1524
  %1526 = lshr i128 %1525, 51
  %1527 = trunc i128 %1525 to i64
  %1528 = and i64 %1527, 2251799813685247
  %1529 = and i128 %1526, 18446744073709551615
  %1530 = add i128 %1507, %1529
  %1531 = lshr i128 %1530, 51
  %1532 = trunc i128 %1531 to i64
  %1533 = trunc i128 %1530 to i64
  %1534 = and i64 %1533, 2251799813685247
  %1535 = mul i64 %1532, 19
  %1536 = add i64 %1535, %1505
  %1537 = lshr i64 %1536, 51
  %1538 = and i64 %1536, 2251799813685247
  %1539 = add nuw nsw i64 %1537, %1518
  %1540 = lshr i64 %1539, 51
  %1541 = and i64 %1539, 2251799813685247
  %1542 = add nuw nsw i64 %1540, %1523
  store i64 %1538, i64* %363, align 8
  store i64 %1541, i64* %364, align 8
  store i64 %1542, i64* %365, align 8
  store i64 %1528, i64* %366, align 8
  store i64 %1534, i64* %367, align 8
  br label %1543

1543:                                             ; preds = %1543, %1459
  %1544 = phi i64 [ %1538, %1459 ], [ %1623, %1543 ]
  %1545 = phi i64 [ %1541, %1459 ], [ %1626, %1543 ]
  %1546 = phi i64 [ %1542, %1459 ], [ %1627, %1543 ]
  %1547 = phi i64 [ %1528, %1459 ], [ %1613, %1543 ]
  %1548 = phi i64 [ %1534, %1459 ], [ %1619, %1543 ]
  %1549 = phi i32 [ 1, %1459 ], [ %1628, %1543 ]
  %1550 = mul nuw nsw i64 %1548, 19
  %1551 = mul nuw nsw i64 %1548, 38
  %1552 = shl nuw nsw i64 %1548, 1
  %1553 = mul nuw nsw i64 %1547, 19
  %1554 = mul nuw nsw i64 %1547, 38
  %1555 = shl nuw nsw i64 %1547, 1
  %1556 = shl nsw i64 %1546, 1
  %1557 = shl nuw nsw i64 %1545, 1
  %1558 = zext i64 %1548 to i128
  %1559 = zext i64 %1550 to i128
  %1560 = mul nuw nsw i128 %1559, %1558
  %1561 = zext i64 %1547 to i128
  %1562 = zext i64 %1551 to i128
  %1563 = mul nuw nsw i128 %1562, %1561
  %1564 = zext i64 %1553 to i128
  %1565 = mul nuw nsw i128 %1564, %1561
  %1566 = zext i64 %1546 to i128
  %1567 = mul nuw nsw i128 %1562, %1566
  %1568 = zext i64 %1554 to i128
  %1569 = mul nuw nsw i128 %1568, %1566
  %1570 = mul nuw i128 %1566, %1566
  %1571 = zext i64 %1545 to i128
  %1572 = mul nuw nsw i128 %1562, %1571
  %1573 = zext i64 %1555 to i128
  %1574 = mul nuw nsw i128 %1573, %1571
  %1575 = zext i64 %1556 to i128
  %1576 = mul nuw nsw i128 %1575, %1571
  %1577 = mul nuw nsw i128 %1571, %1571
  %1578 = zext i64 %1544 to i128
  %1579 = zext i64 %1552 to i128
  %1580 = mul nuw nsw i128 %1579, %1578
  %1581 = mul nuw nsw i128 %1573, %1578
  %1582 = mul nuw nsw i128 %1575, %1578
  %1583 = zext i64 %1557 to i128
  %1584 = mul nuw nsw i128 %1583, %1578
  %1585 = mul nuw nsw i128 %1578, %1578
  %1586 = add nuw nsw i128 %1569, %1585
  %1587 = add nuw nsw i128 %1586, %1572
  %1588 = lshr i128 %1587, 51
  %1589 = trunc i128 %1587 to i64
  %1590 = and i64 %1589, 2251799813685247
  %1591 = add i128 %1574, %1570
  %1592 = add i128 %1591, %1580
  %1593 = and i128 %1588, 18446744073709551615
  %1594 = add nuw nsw i128 %1565, %1584
  %1595 = add nuw nsw i128 %1594, %1567
  %1596 = add nuw nsw i128 %1595, %1593
  %1597 = lshr i128 %1596, 51
  %1598 = trunc i128 %1596 to i64
  %1599 = and i64 %1598, 2251799813685247
  %1600 = and i128 %1597, 18446744073709551615
  %1601 = add nuw nsw i128 %1582, %1577
  %1602 = add nuw nsw i128 %1601, %1563
  %1603 = add nuw nsw i128 %1602, %1600
  %1604 = lshr i128 %1603, 51
  %1605 = trunc i128 %1603 to i64
  %1606 = and i64 %1605, 2251799813685247
  %1607 = and i128 %1604, 18446744073709551615
  %1608 = add nuw nsw i128 %1581, %1576
  %1609 = add nuw nsw i128 %1608, %1560
  %1610 = add nuw nsw i128 %1609, %1607
  %1611 = lshr i128 %1610, 51
  %1612 = trunc i128 %1610 to i64
  %1613 = and i64 %1612, 2251799813685247
  %1614 = and i128 %1611, 18446744073709551615
  %1615 = add i128 %1592, %1614
  %1616 = lshr i128 %1615, 51
  %1617 = trunc i128 %1616 to i64
  %1618 = trunc i128 %1615 to i64
  %1619 = and i64 %1618, 2251799813685247
  %1620 = mul i64 %1617, 19
  %1621 = add i64 %1620, %1590
  %1622 = lshr i64 %1621, 51
  %1623 = and i64 %1621, 2251799813685247
  %1624 = add nuw nsw i64 %1622, %1599
  %1625 = lshr i64 %1624, 51
  %1626 = and i64 %1624, 2251799813685247
  %1627 = add nuw nsw i64 %1625, %1606
  %1628 = add nuw nsw i32 %1549, 1
  %1629 = icmp eq i32 %1628, 50
  br i1 %1629, label %1630, label %1543

1630:                                             ; preds = %1543
  store i64 %1619, i64* %367, align 8
  store i64 %1613, i64* %366, align 8
  store i64 %1627, i64* %365, align 8
  store i64 %1626, i64* %364, align 8
  store i64 %1623, i64* %363, align 8
  call fastcc void @fe_mul_impl(i64* nonnull %363, i64* nonnull %363, i64* nonnull %356) #4
  %1631 = load i64, i64* %367, align 8
  %1632 = mul i64 %1631, 19
  %1633 = mul i64 %1631, 38
  %1634 = shl i64 %1631, 1
  %1635 = load i64, i64* %366, align 8
  %1636 = mul i64 %1635, 19
  %1637 = mul i64 %1635, 38
  %1638 = shl i64 %1635, 1
  %1639 = load i64, i64* %365, align 8
  %1640 = shl i64 %1639, 1
  %1641 = load i64, i64* %364, align 8
  %1642 = shl i64 %1641, 1
  %1643 = zext i64 %1631 to i128
  %1644 = zext i64 %1632 to i128
  %1645 = mul nuw i128 %1644, %1643
  %1646 = zext i64 %1635 to i128
  %1647 = zext i64 %1633 to i128
  %1648 = mul nuw i128 %1646, %1647
  %1649 = zext i64 %1636 to i128
  %1650 = mul nuw i128 %1649, %1646
  %1651 = zext i64 %1639 to i128
  %1652 = mul nuw i128 %1651, %1647
  %1653 = zext i64 %1637 to i128
  %1654 = mul nuw i128 %1651, %1653
  %1655 = mul nuw i128 %1651, %1651
  %1656 = zext i64 %1641 to i128
  %1657 = mul nuw i128 %1656, %1647
  %1658 = zext i64 %1638 to i128
  %1659 = mul nuw i128 %1656, %1658
  %1660 = zext i64 %1640 to i128
  %1661 = mul nuw i128 %1656, %1660
  %1662 = mul nuw i128 %1656, %1656
  %1663 = load i64, i64* %363, align 8
  %1664 = zext i64 %1663 to i128
  %1665 = zext i64 %1634 to i128
  %1666 = mul nuw i128 %1664, %1665
  %1667 = mul nuw i128 %1664, %1658
  %1668 = mul nuw i128 %1664, %1660
  %1669 = zext i64 %1642 to i128
  %1670 = mul nuw i128 %1664, %1669
  %1671 = mul nuw i128 %1664, %1664
  %1672 = add i128 %1657, %1654
  %1673 = add i128 %1672, %1671
  %1674 = lshr i128 %1673, 51
  %1675 = trunc i128 %1673 to i64
  %1676 = and i64 %1675, 2251799813685247
  %1677 = add i128 %1659, %1655
  %1678 = add i128 %1677, %1666
  %1679 = add i128 %1661, %1645
  %1680 = add i128 %1679, %1667
  %1681 = add i128 %1662, %1648
  %1682 = add i128 %1681, %1668
  %1683 = add i128 %1652, %1650
  %1684 = add i128 %1683, %1670
  %1685 = and i128 %1674, 18446744073709551615
  %1686 = add i128 %1684, %1685
  %1687 = lshr i128 %1686, 51
  %1688 = trunc i128 %1686 to i64
  %1689 = and i64 %1688, 2251799813685247
  %1690 = and i128 %1687, 18446744073709551615
  %1691 = add i128 %1682, %1690
  %1692 = lshr i128 %1691, 51
  %1693 = trunc i128 %1691 to i64
  %1694 = and i64 %1693, 2251799813685247
  %1695 = and i128 %1692, 18446744073709551615
  %1696 = add i128 %1680, %1695
  %1697 = lshr i128 %1696, 51
  %1698 = trunc i128 %1696 to i64
  %1699 = and i64 %1698, 2251799813685247
  %1700 = and i128 %1697, 18446744073709551615
  %1701 = add i128 %1678, %1700
  %1702 = lshr i128 %1701, 51
  %1703 = trunc i128 %1702 to i64
  %1704 = trunc i128 %1701 to i64
  %1705 = and i64 %1704, 2251799813685247
  %1706 = mul i64 %1703, 19
  %1707 = add i64 %1706, %1676
  %1708 = lshr i64 %1707, 51
  %1709 = and i64 %1707, 2251799813685247
  %1710 = add nuw nsw i64 %1708, %1689
  %1711 = lshr i64 %1710, 51
  %1712 = and i64 %1710, 2251799813685247
  %1713 = add nuw nsw i64 %1711, %1694
  store i64 %1709, i64* %944, align 8
  store i64 %1712, i64* %369, align 8
  store i64 %1713, i64* %945, align 8
  store i64 %1699, i64* %370, align 8
  store i64 %1705, i64* %946, align 8
  br label %1714

1714:                                             ; preds = %1714, %1630
  %1715 = phi i64 [ %1709, %1630 ], [ %1794, %1714 ]
  %1716 = phi i64 [ %1712, %1630 ], [ %1797, %1714 ]
  %1717 = phi i64 [ %1713, %1630 ], [ %1798, %1714 ]
  %1718 = phi i64 [ %1699, %1630 ], [ %1784, %1714 ]
  %1719 = phi i64 [ %1705, %1630 ], [ %1790, %1714 ]
  %1720 = phi i32 [ 1, %1630 ], [ %1799, %1714 ]
  %1721 = mul nuw nsw i64 %1719, 19
  %1722 = mul nuw nsw i64 %1719, 38
  %1723 = shl nuw nsw i64 %1719, 1
  %1724 = mul nuw nsw i64 %1718, 19
  %1725 = mul nuw nsw i64 %1718, 38
  %1726 = shl nuw nsw i64 %1718, 1
  %1727 = shl nsw i64 %1717, 1
  %1728 = shl nuw nsw i64 %1716, 1
  %1729 = zext i64 %1719 to i128
  %1730 = zext i64 %1721 to i128
  %1731 = mul nuw nsw i128 %1730, %1729
  %1732 = zext i64 %1718 to i128
  %1733 = zext i64 %1722 to i128
  %1734 = mul nuw nsw i128 %1733, %1732
  %1735 = zext i64 %1724 to i128
  %1736 = mul nuw nsw i128 %1735, %1732
  %1737 = zext i64 %1717 to i128
  %1738 = mul nuw nsw i128 %1733, %1737
  %1739 = zext i64 %1725 to i128
  %1740 = mul nuw nsw i128 %1739, %1737
  %1741 = mul nuw i128 %1737, %1737
  %1742 = zext i64 %1716 to i128
  %1743 = mul nuw nsw i128 %1733, %1742
  %1744 = zext i64 %1726 to i128
  %1745 = mul nuw nsw i128 %1744, %1742
  %1746 = zext i64 %1727 to i128
  %1747 = mul nuw nsw i128 %1746, %1742
  %1748 = mul nuw nsw i128 %1742, %1742
  %1749 = zext i64 %1715 to i128
  %1750 = zext i64 %1723 to i128
  %1751 = mul nuw nsw i128 %1750, %1749
  %1752 = mul nuw nsw i128 %1744, %1749
  %1753 = mul nuw nsw i128 %1746, %1749
  %1754 = zext i64 %1728 to i128
  %1755 = mul nuw nsw i128 %1754, %1749
  %1756 = mul nuw nsw i128 %1749, %1749
  %1757 = add nuw nsw i128 %1740, %1756
  %1758 = add nuw nsw i128 %1757, %1743
  %1759 = lshr i128 %1758, 51
  %1760 = trunc i128 %1758 to i64
  %1761 = and i64 %1760, 2251799813685247
  %1762 = add i128 %1745, %1741
  %1763 = add i128 %1762, %1751
  %1764 = and i128 %1759, 18446744073709551615
  %1765 = add nuw nsw i128 %1736, %1755
  %1766 = add nuw nsw i128 %1765, %1738
  %1767 = add nuw nsw i128 %1766, %1764
  %1768 = lshr i128 %1767, 51
  %1769 = trunc i128 %1767 to i64
  %1770 = and i64 %1769, 2251799813685247
  %1771 = and i128 %1768, 18446744073709551615
  %1772 = add nuw nsw i128 %1753, %1748
  %1773 = add nuw nsw i128 %1772, %1734
  %1774 = add nuw nsw i128 %1773, %1771
  %1775 = lshr i128 %1774, 51
  %1776 = trunc i128 %1774 to i64
  %1777 = and i64 %1776, 2251799813685247
  %1778 = and i128 %1775, 18446744073709551615
  %1779 = add nuw nsw i128 %1752, %1747
  %1780 = add nuw nsw i128 %1779, %1731
  %1781 = add nuw nsw i128 %1780, %1778
  %1782 = lshr i128 %1781, 51
  %1783 = trunc i128 %1781 to i64
  %1784 = and i64 %1783, 2251799813685247
  %1785 = and i128 %1782, 18446744073709551615
  %1786 = add i128 %1763, %1785
  %1787 = lshr i128 %1786, 51
  %1788 = trunc i128 %1787 to i64
  %1789 = trunc i128 %1786 to i64
  %1790 = and i64 %1789, 2251799813685247
  %1791 = mul i64 %1788, 19
  %1792 = add i64 %1791, %1761
  %1793 = lshr i64 %1792, 51
  %1794 = and i64 %1792, 2251799813685247
  %1795 = add nuw nsw i64 %1793, %1770
  %1796 = lshr i64 %1795, 51
  %1797 = and i64 %1795, 2251799813685247
  %1798 = add nuw nsw i64 %1796, %1777
  %1799 = add nuw nsw i32 %1720, 1
  %1800 = icmp eq i32 %1799, 100
  br i1 %1800, label %1801, label %1714

1801:                                             ; preds = %1714
  store i64 %1790, i64* %946, align 8
  store i64 %1784, i64* %370, align 8
  store i64 %1798, i64* %945, align 8
  store i64 %1797, i64* %369, align 8
  store i64 %1794, i64* %944, align 8
  call fastcc void @fe_mul_impl(i64* nonnull %363, i64* nonnull %944, i64* nonnull %363) #4
  %1802 = load i64, i64* %367, align 8
  %1803 = mul i64 %1802, 19
  %1804 = mul i64 %1802, 38
  %1805 = shl i64 %1802, 1
  %1806 = load i64, i64* %366, align 8
  %1807 = mul i64 %1806, 19
  %1808 = mul i64 %1806, 38
  %1809 = shl i64 %1806, 1
  %1810 = load i64, i64* %365, align 8
  %1811 = shl i64 %1810, 1
  %1812 = load i64, i64* %364, align 8
  %1813 = shl i64 %1812, 1
  %1814 = zext i64 %1802 to i128
  %1815 = zext i64 %1803 to i128
  %1816 = mul nuw i128 %1815, %1814
  %1817 = zext i64 %1806 to i128
  %1818 = zext i64 %1804 to i128
  %1819 = mul nuw i128 %1817, %1818
  %1820 = zext i64 %1807 to i128
  %1821 = mul nuw i128 %1820, %1817
  %1822 = zext i64 %1810 to i128
  %1823 = mul nuw i128 %1822, %1818
  %1824 = zext i64 %1808 to i128
  %1825 = mul nuw i128 %1822, %1824
  %1826 = mul nuw i128 %1822, %1822
  %1827 = zext i64 %1812 to i128
  %1828 = mul nuw i128 %1827, %1818
  %1829 = zext i64 %1809 to i128
  %1830 = mul nuw i128 %1827, %1829
  %1831 = zext i64 %1811 to i128
  %1832 = mul nuw i128 %1827, %1831
  %1833 = mul nuw i128 %1827, %1827
  %1834 = load i64, i64* %363, align 8
  %1835 = zext i64 %1834 to i128
  %1836 = zext i64 %1805 to i128
  %1837 = mul nuw i128 %1835, %1836
  %1838 = mul nuw i128 %1835, %1829
  %1839 = mul nuw i128 %1835, %1831
  %1840 = zext i64 %1813 to i128
  %1841 = mul nuw i128 %1835, %1840
  %1842 = mul nuw i128 %1835, %1835
  %1843 = add i128 %1828, %1825
  %1844 = add i128 %1843, %1842
  %1845 = lshr i128 %1844, 51
  %1846 = trunc i128 %1844 to i64
  %1847 = and i64 %1846, 2251799813685247
  %1848 = add i128 %1830, %1826
  %1849 = add i128 %1848, %1837
  %1850 = add i128 %1832, %1816
  %1851 = add i128 %1850, %1838
  %1852 = add i128 %1833, %1819
  %1853 = add i128 %1852, %1839
  %1854 = add i128 %1823, %1821
  %1855 = add i128 %1854, %1841
  %1856 = and i128 %1845, 18446744073709551615
  %1857 = add i128 %1855, %1856
  %1858 = lshr i128 %1857, 51
  %1859 = trunc i128 %1857 to i64
  %1860 = and i64 %1859, 2251799813685247
  %1861 = and i128 %1858, 18446744073709551615
  %1862 = add i128 %1853, %1861
  %1863 = lshr i128 %1862, 51
  %1864 = trunc i128 %1862 to i64
  %1865 = and i64 %1864, 2251799813685247
  %1866 = and i128 %1863, 18446744073709551615
  %1867 = add i128 %1851, %1866
  %1868 = lshr i128 %1867, 51
  %1869 = trunc i128 %1867 to i64
  %1870 = and i64 %1869, 2251799813685247
  %1871 = and i128 %1868, 18446744073709551615
  %1872 = add i128 %1849, %1871
  %1873 = lshr i128 %1872, 51
  %1874 = trunc i128 %1873 to i64
  %1875 = trunc i128 %1872 to i64
  %1876 = and i64 %1875, 2251799813685247
  %1877 = mul i64 %1874, 19
  %1878 = add i64 %1877, %1847
  %1879 = lshr i64 %1878, 51
  %1880 = and i64 %1878, 2251799813685247
  %1881 = add nuw nsw i64 %1879, %1860
  %1882 = lshr i64 %1881, 51
  %1883 = and i64 %1881, 2251799813685247
  %1884 = add nuw nsw i64 %1882, %1865
  store i64 %1880, i64* %363, align 8
  store i64 %1883, i64* %364, align 8
  store i64 %1884, i64* %365, align 8
  store i64 %1870, i64* %366, align 8
  store i64 %1876, i64* %367, align 8
  br label %1885

1885:                                             ; preds = %1885, %1801
  %1886 = phi i64 [ %1880, %1801 ], [ %1965, %1885 ]
  %1887 = phi i64 [ %1883, %1801 ], [ %1968, %1885 ]
  %1888 = phi i64 [ %1884, %1801 ], [ %1969, %1885 ]
  %1889 = phi i64 [ %1870, %1801 ], [ %1955, %1885 ]
  %1890 = phi i64 [ %1876, %1801 ], [ %1961, %1885 ]
  %1891 = phi i32 [ 1, %1801 ], [ %1970, %1885 ]
  %1892 = mul nuw nsw i64 %1890, 19
  %1893 = mul nuw nsw i64 %1890, 38
  %1894 = shl nuw nsw i64 %1890, 1
  %1895 = mul nuw nsw i64 %1889, 19
  %1896 = mul nuw nsw i64 %1889, 38
  %1897 = shl nuw nsw i64 %1889, 1
  %1898 = shl nsw i64 %1888, 1
  %1899 = shl nuw nsw i64 %1887, 1
  %1900 = zext i64 %1890 to i128
  %1901 = zext i64 %1892 to i128
  %1902 = mul nuw nsw i128 %1901, %1900
  %1903 = zext i64 %1889 to i128
  %1904 = zext i64 %1893 to i128
  %1905 = mul nuw nsw i128 %1904, %1903
  %1906 = zext i64 %1895 to i128
  %1907 = mul nuw nsw i128 %1906, %1903
  %1908 = zext i64 %1888 to i128
  %1909 = mul nuw nsw i128 %1904, %1908
  %1910 = zext i64 %1896 to i128
  %1911 = mul nuw nsw i128 %1910, %1908
  %1912 = mul nuw i128 %1908, %1908
  %1913 = zext i64 %1887 to i128
  %1914 = mul nuw nsw i128 %1904, %1913
  %1915 = zext i64 %1897 to i128
  %1916 = mul nuw nsw i128 %1915, %1913
  %1917 = zext i64 %1898 to i128
  %1918 = mul nuw nsw i128 %1917, %1913
  %1919 = mul nuw nsw i128 %1913, %1913
  %1920 = zext i64 %1886 to i128
  %1921 = zext i64 %1894 to i128
  %1922 = mul nuw nsw i128 %1921, %1920
  %1923 = mul nuw nsw i128 %1915, %1920
  %1924 = mul nuw nsw i128 %1917, %1920
  %1925 = zext i64 %1899 to i128
  %1926 = mul nuw nsw i128 %1925, %1920
  %1927 = mul nuw nsw i128 %1920, %1920
  %1928 = add nuw nsw i128 %1911, %1927
  %1929 = add nuw nsw i128 %1928, %1914
  %1930 = lshr i128 %1929, 51
  %1931 = trunc i128 %1929 to i64
  %1932 = and i64 %1931, 2251799813685247
  %1933 = add i128 %1916, %1912
  %1934 = add i128 %1933, %1922
  %1935 = and i128 %1930, 18446744073709551615
  %1936 = add nuw nsw i128 %1907, %1926
  %1937 = add nuw nsw i128 %1936, %1909
  %1938 = add nuw nsw i128 %1937, %1935
  %1939 = lshr i128 %1938, 51
  %1940 = trunc i128 %1938 to i64
  %1941 = and i64 %1940, 2251799813685247
  %1942 = and i128 %1939, 18446744073709551615
  %1943 = add nuw nsw i128 %1924, %1919
  %1944 = add nuw nsw i128 %1943, %1905
  %1945 = add nuw nsw i128 %1944, %1942
  %1946 = lshr i128 %1945, 51
  %1947 = trunc i128 %1945 to i64
  %1948 = and i64 %1947, 2251799813685247
  %1949 = and i128 %1946, 18446744073709551615
  %1950 = add nuw nsw i128 %1923, %1918
  %1951 = add nuw nsw i128 %1950, %1902
  %1952 = add nuw nsw i128 %1951, %1949
  %1953 = lshr i128 %1952, 51
  %1954 = trunc i128 %1952 to i64
  %1955 = and i64 %1954, 2251799813685247
  %1956 = and i128 %1953, 18446744073709551615
  %1957 = add i128 %1934, %1956
  %1958 = lshr i128 %1957, 51
  %1959 = trunc i128 %1958 to i64
  %1960 = trunc i128 %1957 to i64
  %1961 = and i64 %1960, 2251799813685247
  %1962 = mul i64 %1959, 19
  %1963 = add i64 %1962, %1932
  %1964 = lshr i64 %1963, 51
  %1965 = and i64 %1963, 2251799813685247
  %1966 = add nuw nsw i64 %1964, %1941
  %1967 = lshr i64 %1966, 51
  %1968 = and i64 %1966, 2251799813685247
  %1969 = add nuw nsw i64 %1967, %1948
  %1970 = add nuw nsw i32 %1891, 1
  %1971 = icmp eq i32 %1970, 50
  br i1 %1971, label %1972, label %1885

1972:                                             ; preds = %1885
  store i64 %1961, i64* %367, align 8
  store i64 %1955, i64* %366, align 8
  store i64 %1969, i64* %365, align 8
  store i64 %1968, i64* %364, align 8
  store i64 %1965, i64* %363, align 8
  call fastcc void @fe_mul_impl(i64* nonnull %356, i64* nonnull %363, i64* nonnull %356) #4
  %1973 = load i64, i64* %360, align 8
  %1974 = mul i64 %1973, 19
  %1975 = mul i64 %1973, 38
  %1976 = shl i64 %1973, 1
  %1977 = load i64, i64* %359, align 8
  %1978 = mul i64 %1977, 19
  %1979 = mul i64 %1977, 38
  %1980 = shl i64 %1977, 1
  %1981 = load i64, i64* %358, align 8
  %1982 = shl i64 %1981, 1
  %1983 = load i64, i64* %357, align 8
  %1984 = shl i64 %1983, 1
  %1985 = zext i64 %1973 to i128
  %1986 = zext i64 %1974 to i128
  %1987 = mul nuw i128 %1986, %1985
  %1988 = zext i64 %1977 to i128
  %1989 = zext i64 %1975 to i128
  %1990 = mul nuw i128 %1988, %1989
  %1991 = zext i64 %1978 to i128
  %1992 = mul nuw i128 %1991, %1988
  %1993 = zext i64 %1981 to i128
  %1994 = mul nuw i128 %1993, %1989
  %1995 = zext i64 %1979 to i128
  %1996 = mul nuw i128 %1993, %1995
  %1997 = mul nuw i128 %1993, %1993
  %1998 = zext i64 %1983 to i128
  %1999 = mul nuw i128 %1998, %1989
  %2000 = zext i64 %1980 to i128
  %2001 = mul nuw i128 %1998, %2000
  %2002 = zext i64 %1982 to i128
  %2003 = mul nuw i128 %1998, %2002
  %2004 = mul nuw i128 %1998, %1998
  %2005 = load i64, i64* %356, align 8
  %2006 = zext i64 %2005 to i128
  %2007 = zext i64 %1976 to i128
  %2008 = mul nuw i128 %2006, %2007
  %2009 = mul nuw i128 %2006, %2000
  %2010 = mul nuw i128 %2006, %2002
  %2011 = zext i64 %1984 to i128
  %2012 = mul nuw i128 %2006, %2011
  %2013 = mul nuw i128 %2006, %2006
  %2014 = add i128 %1999, %1996
  %2015 = add i128 %2014, %2013
  %2016 = lshr i128 %2015, 51
  %2017 = trunc i128 %2015 to i64
  %2018 = and i64 %2017, 2251799813685247
  %2019 = add i128 %2001, %1997
  %2020 = add i128 %2019, %2008
  %2021 = add i128 %2003, %1987
  %2022 = add i128 %2021, %2009
  %2023 = add i128 %2004, %1990
  %2024 = add i128 %2023, %2010
  %2025 = add i128 %1994, %1992
  %2026 = add i128 %2025, %2012
  %2027 = and i128 %2016, 18446744073709551615
  %2028 = add i128 %2026, %2027
  %2029 = lshr i128 %2028, 51
  %2030 = trunc i128 %2028 to i64
  %2031 = and i64 %2030, 2251799813685247
  %2032 = and i128 %2029, 18446744073709551615
  %2033 = add i128 %2024, %2032
  %2034 = lshr i128 %2033, 51
  %2035 = trunc i128 %2033 to i64
  %2036 = and i64 %2035, 2251799813685247
  %2037 = and i128 %2034, 18446744073709551615
  %2038 = add i128 %2022, %2037
  %2039 = lshr i128 %2038, 51
  %2040 = trunc i128 %2038 to i64
  %2041 = and i64 %2040, 2251799813685247
  %2042 = and i128 %2039, 18446744073709551615
  %2043 = add i128 %2020, %2042
  %2044 = lshr i128 %2043, 51
  %2045 = trunc i128 %2044 to i64
  %2046 = mul i64 %2045, 19
  %2047 = add i64 %2046, %2018
  %2048 = lshr i64 %2047, 51
  %2049 = and i64 %2047, 2251799813685247
  %2050 = add nuw nsw i64 %2048, %2031
  %2051 = lshr i64 %2050, 51
  %2052 = and i64 %2050, 2251799813685247
  %2053 = add nuw nsw i64 %2051, %2036
  store i64 %2049, i64* %356, align 8
  store i64 %2052, i64* %357, align 8
  store i64 %2053, i64* %358, align 8
  store i64 %2041, i64* %359, align 8
  %2054 = trunc i128 %2043 to i51
  %2055 = mul i51 %2054, 19
  %2056 = mul i51 %2055, %2054
  %2057 = trunc i64 %2051 to i51
  %2058 = trunc i128 %2033 to i51
  %2059 = add i51 %2057, %2058
  %2060 = trunc i64 %2048 to i51
  %2061 = trunc i128 %2028 to i51
  %2062 = add i51 %2060, %2061
  %2063 = mul i51 %2059, %2062
  %2064 = trunc i128 %2044 to i51
  %2065 = mul i51 %2064, 19
  %2066 = trunc i128 %2015 to i51
  %2067 = add i51 %2065, %2066
  %2068 = trunc i128 %2038 to i51
  %2069 = mul i51 %2067, %2068
  %2070 = zext i51 %2062 to i128
  %2071 = and i128 %2043, 2251799813685247
  %2072 = mul nuw nsw i128 %2071, %2070
  %2073 = zext i64 %2053 to i128
  %2074 = and i128 %2038, 2251799813685247
  %2075 = mul nuw nsw i128 %2074, %2073
  %2076 = add nuw nsw i128 %2075, %2072
  %2077 = mul nuw nsw i128 %2076, 38
  %2078 = zext i51 %2067 to i128
  %2079 = mul nuw nsw i128 %2078, %2078
  %2080 = add nuw nsw i128 %2077, %2079
  %2081 = lshr i128 %2080, 51
  %2082 = mul nuw nsw i128 %2071, 38
  %2083 = mul nuw nsw i128 %2082, %2073
  %2084 = mul nuw nsw i128 %2074, 19
  %2085 = mul nuw nsw i128 %2084, %2074
  %2086 = shl nuw nsw i128 %2078, 1
  %2087 = mul nuw nsw i128 %2086, %2070
  %2088 = add nuw nsw i128 %2087, %2085
  %2089 = add nuw nsw i128 %2088, %2083
  %2090 = add nuw nsw i128 %2089, %2081
  %2091 = lshr i128 %2090, 51
  %2092 = mul nuw nsw i128 %2082, %2074
  %2093 = mul nuw nsw i128 %2086, %2073
  %2094 = mul nuw nsw i128 %2070, %2070
  %2095 = add nuw nsw i128 %2094, %2092
  %2096 = add nuw nsw i128 %2095, %2093
  %2097 = add nuw nsw i128 %2096, %2091
  %2098 = lshr i128 %2097, 51
  %2099 = trunc i128 %2098 to i51
  %2100 = add i51 %2063, %2069
  %2101 = shl i51 %2100, 1
  %2102 = add i51 %2101, %2056
  %2103 = add i51 %2102, %2099
  %2104 = mul i51 %2067, %2054
  %2105 = mul i51 %2062, %2068
  %2106 = add i51 %2105, %2104
  %2107 = shl i51 %2106, 1
  %2108 = mul i51 %2059, %2059
  %2109 = add i51 %2107, %2108
  %2110 = mul nuw nsw i128 %2071, 19
  %2111 = mul nuw nsw i128 %2110, %2071
  %2112 = mul nuw nsw i128 %2073, %2070
  %2113 = mul nuw nsw i128 %2074, %2078
  %2114 = add nuw nsw i128 %2112, %2113
  %2115 = shl nuw nsw i128 %2114, 1
  %2116 = add nuw nsw i128 %2115, %2111
  %2117 = add nuw nsw i128 %2116, %2098
  %2118 = lshr i128 %2117, 51
  %2119 = trunc i128 %2118 to i51
  %2120 = add i51 %2109, %2119
  %2121 = mul nuw nsw i128 %2071, %2078
  %2122 = mul nuw nsw i128 %2074, %2070
  %2123 = mul nuw nsw i128 %2073, %2073
  %2124 = add nuw nsw i128 %2122, %2121
  %2125 = shl nuw nsw i128 %2124, 1
  %2126 = add nuw nsw i128 %2125, %2123
  %2127 = add nuw nsw i128 %2126, %2118
  %2128 = lshr i128 %2127, 51
  %2129 = trunc i128 %2128 to i51
  %2130 = mul i51 %2129, 19
  %2131 = mul i51 %2062, %2054
  %2132 = mul i51 %2059, %2068
  %2133 = add i51 %2132, %2131
  %2134 = mul i51 %2133, 38
  %2135 = mul i51 %2067, %2067
  %2136 = add i51 %2134, %2135
  %2137 = add i51 %2130, %2136
  %2138 = mul i51 %2054, 38
  %2139 = mul i51 %2138, %2059
  %2140 = mul i51 %2068, 19
  %2141 = mul i51 %2140, %2068
  %2142 = shl i51 %2067, 1
  %2143 = mul i51 %2142, %2062
  %2144 = trunc i128 %2081 to i51
  %2145 = add i51 %2143, %2141
  %2146 = add i51 %2145, %2139
  %2147 = add i51 %2146, %2144
  %2148 = trunc i128 %2128 to i64
  %2149 = mul nuw nsw i64 %2148, 19
  %2150 = zext i51 %2136 to i64
  %2151 = add nuw nsw i64 %2149, %2150
  %2152 = lshr i64 %2151, 51
  %2153 = trunc i64 %2152 to i51
  %2154 = add i51 %2147, %2153
  %2155 = zext i51 %2147 to i64
  %2156 = add nuw nsw i64 %2152, %2155
  %2157 = mul i51 %2138, %2068
  %2158 = mul i51 %2142, %2059
  %2159 = mul i51 %2062, %2062
  %2160 = trunc i128 %2091 to i51
  %2161 = add i51 %2159, %2157
  %2162 = add i51 %2161, %2158
  %2163 = add i51 %2162, %2160
  %2164 = zext i51 %2163 to i64
  %2165 = zext i51 %2103 to i64
  %2166 = zext i51 %2120 to i64
  %2167 = zext i51 %2137 to i64
  %2168 = zext i51 %2154 to i64
  %2169 = lshr i64 %2156, 51
  %2170 = add nuw nsw i64 %2169, %2164
  store i64 %2166, i64* %360, align 8
  store i64 %2165, i64* %359, align 8
  store i64 %2170, i64* %358, align 8
  store i64 %2168, i64* %357, align 8
  store i64 %2167, i64* %356, align 8
  call fastcc void @fe_mul_impl(i64* %266, i64* nonnull %356, i64* %266) #4
  call void @llvm.lifetime.end.p0i8(i64 40, i8* nonnull %368) #4
  call void @llvm.lifetime.end.p0i8(i64 40, i8* nonnull %361) #4
  call void @llvm.lifetime.end.p0i8(i64 40, i8* nonnull %355) #4
  call fastcc void @fe_mul_impl(i64* %266, i64* %266, i64* nonnull %28) #4
  call fastcc void @fe_mul_impl(i64* %266, i64* %266, i64* nonnull %14) #4
  %2171 = load i64, i64* %353, align 8
  %2172 = mul i64 %2171, 19
  %2173 = mul i64 %2171, 38
  %2174 = shl i64 %2171, 1
  %2175 = load i64, i64* %352, align 8
  %2176 = mul i64 %2175, 19
  %2177 = mul i64 %2175, 38
  %2178 = shl i64 %2175, 1
  %2179 = load i64, i64* %351, align 8
  %2180 = shl i64 %2179, 1
  %2181 = load i64, i64* %350, align 8
  %2182 = shl i64 %2181, 1
  %2183 = zext i64 %2171 to i128
  %2184 = zext i64 %2172 to i128
  %2185 = mul nuw i128 %2184, %2183
  %2186 = zext i64 %2175 to i128
  %2187 = zext i64 %2173 to i128
  %2188 = mul nuw i128 %2186, %2187
  %2189 = zext i64 %2176 to i128
  %2190 = mul nuw i128 %2189, %2186
  %2191 = zext i64 %2179 to i128
  %2192 = mul nuw i128 %2191, %2187
  %2193 = zext i64 %2177 to i128
  %2194 = mul nuw i128 %2191, %2193
  %2195 = mul nuw i128 %2191, %2191
  %2196 = zext i64 %2181 to i128
  %2197 = mul nuw i128 %2196, %2187
  %2198 = zext i64 %2178 to i128
  %2199 = mul nuw i128 %2196, %2198
  %2200 = zext i64 %2180 to i128
  %2201 = mul nuw i128 %2196, %2200
  %2202 = mul nuw i128 %2196, %2196
  %2203 = load i64, i64* %266, align 8
  %2204 = zext i64 %2203 to i128
  %2205 = zext i64 %2174 to i128
  %2206 = mul nuw i128 %2204, %2205
  %2207 = mul nuw i128 %2204, %2198
  %2208 = mul nuw i128 %2204, %2200
  %2209 = zext i64 %2182 to i128
  %2210 = mul nuw i128 %2204, %2209
  %2211 = mul nuw i128 %2204, %2204
  %2212 = add i128 %2197, %2194
  %2213 = add i128 %2212, %2211
  %2214 = lshr i128 %2213, 51
  %2215 = trunc i128 %2213 to i64
  %2216 = and i64 %2215, 2251799813685247
  %2217 = add i128 %2199, %2195
  %2218 = add i128 %2217, %2206
  %2219 = add i128 %2201, %2185
  %2220 = add i128 %2219, %2207
  %2221 = add i128 %2202, %2188
  %2222 = add i128 %2221, %2208
  %2223 = add i128 %2192, %2190
  %2224 = add i128 %2223, %2210
  %2225 = and i128 %2214, 18446744073709551615
  %2226 = add i128 %2224, %2225
  %2227 = lshr i128 %2226, 51
  %2228 = trunc i128 %2226 to i64
  %2229 = and i64 %2228, 2251799813685247
  %2230 = and i128 %2227, 18446744073709551615
  %2231 = add i128 %2222, %2230
  %2232 = lshr i128 %2231, 51
  %2233 = trunc i128 %2231 to i64
  %2234 = and i64 %2233, 2251799813685247
  %2235 = and i128 %2232, 18446744073709551615
  %2236 = add i128 %2220, %2235
  %2237 = lshr i128 %2236, 51
  %2238 = and i128 %2237, 18446744073709551615
  %2239 = add i128 %2218, %2238
  %2240 = lshr i128 %2239, 51
  %2241 = trunc i128 %2240 to i64
  %2242 = insertelement <2 x i128> undef, i128 %2236, i32 0
  %2243 = insertelement <2 x i128> %2242, i128 %2239, i32 1
  %2244 = trunc <2 x i128> %2243 to <2 x i64>
  %2245 = and <2 x i64> %2244, <i64 2251799813685247, i64 2251799813685247>
  %2246 = mul i64 %2241, 19
  %2247 = add i64 %2246, %2216
  %2248 = lshr i64 %2247, 51
  %2249 = and i64 %2247, 2251799813685247
  %2250 = add nuw nsw i64 %2248, %2229
  %2251 = lshr i64 %2250, 51
  %2252 = and i64 %2250, 2251799813685247
  %2253 = add nuw nsw i64 %2251, %2234
  store i64 %2249, i64* %35, align 8
  store i64 %2252, i64* %36, align 8
  store i64 %2253, i64* %37, align 8
  %2254 = bitcast i64* %38 to <2 x i64>*
  store <2 x i64> %2245, <2 x i64>* %2254, align 8
  call fastcc void @fe_mul_impl(i64* nonnull %35, i64* nonnull %35, i64* nonnull %21) #4
  %2255 = load i64, i64* %35, align 8
  %2256 = add i64 %2255, 4503599627370458
  %2257 = load i64, i64* %14, align 8
  %2258 = sub i64 %2256, %2257
  %2259 = load i64, i64* %36, align 8
  %2260 = add i64 %2259, 4503599627370494
  %2261 = load i64, i64* %15, align 8
  %2262 = sub i64 %2260, %2261
  %2263 = load i64, i64* %37, align 8
  %2264 = add i64 %2263, 4503599627370494
  %2265 = load i64, i64* %16, align 8
  %2266 = sub i64 %2264, %2265
  %2267 = load i64, i64* %38, align 8
  %2268 = add i64 %2267, 4503599627370494
  %2269 = load i64, i64* %17, align 8
  %2270 = sub i64 %2268, %2269
  %2271 = load i64, i64* %39, align 8
  %2272 = add i64 %2271, 4503599627370494
  %2273 = load i64, i64* %18, align 8
  %2274 = sub i64 %2272, %2273
  %2275 = bitcast %struct.fe* %6 to i8*
  call void @llvm.lifetime.start.p0i8(i64 40, i8* nonnull %2275) #4
  %2276 = getelementptr inbounds %struct.fe, %struct.fe* %6, i64 0, i32 0, i64 0
  %2277 = lshr i64 %2258, 51
  %2278 = add i64 %2262, %2277
  %2279 = lshr i64 %2278, 51
  %2280 = add i64 %2266, %2279
  %2281 = lshr i64 %2280, 51
  %2282 = add i64 %2270, %2281
  %2283 = lshr i64 %2282, 51
  %2284 = add i64 %2274, %2283
  %2285 = and i64 %2258, 2251799813685247
  %2286 = lshr i64 %2284, 51
  %2287 = mul nuw nsw i64 %2286, 19
  %2288 = add nuw nsw i64 %2287, %2285
  %2289 = lshr i64 %2288, 51
  %2290 = and i64 %2278, 2251799813685247
  %2291 = add nuw nsw i64 %2289, %2290
  %2292 = and i64 %2288, 2251799813685247
  %2293 = and i64 %2291, 2251799813685247
  %2294 = lshr i64 %2291, 51
  %2295 = and i64 %2280, 2251799813685247
  %2296 = add nuw nsw i64 %2294, %2295
  %2297 = and i64 %2282, 2251799813685247
  %2298 = and i64 %2284, 2251799813685247
  store i64 %2292, i64* %2276, align 8
  %2299 = getelementptr inbounds %struct.fe, %struct.fe* %6, i64 0, i32 0, i64 1
  store i64 %2293, i64* %2299, align 8
  %2300 = getelementptr inbounds %struct.fe, %struct.fe* %6, i64 0, i32 0, i64 2
  store i64 %2296, i64* %2300, align 8
  %2301 = getelementptr inbounds %struct.fe, %struct.fe* %6, i64 0, i32 0, i64 3
  store i64 %2297, i64* %2301, align 8
  %2302 = getelementptr inbounds %struct.fe, %struct.fe* %6, i64 0, i32 0, i64 4
  store i64 %2298, i64* %2302, align 8
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %41) #4
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %41, i8 -86, i64 32, i1 false) #4
  call fastcc void @fe_tobytes(i8* nonnull %41, %struct.fe* nonnull %6) #4
  %2303 = call i32 @CRYPTO_memcmp(i8* nonnull %41, i8* getelementptr inbounds ([32 x i8], [32 x i8]* @fe_isnonzero.zero, i64 0, i64 0), i64 32) #4
  %2304 = icmp eq i32 %2303, 0
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %41) #4
  call void @llvm.lifetime.end.p0i8(i64 40, i8* nonnull %2275) #4
  br i1 %2304, label %2342, label %2305

2305:                                             ; preds = %1972
  %2306 = add i64 %2257, %2255
  %2307 = add i64 %2261, %2259
  %2308 = add i64 %2265, %2263
  %2309 = add i64 %2269, %2267
  %2310 = add i64 %2273, %2271
  %2311 = bitcast %struct.fe* %7 to i8*
  call void @llvm.lifetime.start.p0i8(i64 40, i8* nonnull %2311) #4
  %2312 = getelementptr inbounds %struct.fe, %struct.fe* %7, i64 0, i32 0, i64 0
  %2313 = lshr i64 %2306, 51
  %2314 = add i64 %2307, %2313
  %2315 = lshr i64 %2314, 51
  %2316 = add i64 %2308, %2315
  %2317 = lshr i64 %2316, 51
  %2318 = add i64 %2309, %2317
  %2319 = lshr i64 %2318, 51
  %2320 = add i64 %2310, %2319
  %2321 = and i64 %2306, 2251799813685247
  %2322 = lshr i64 %2320, 51
  %2323 = mul nuw nsw i64 %2322, 19
  %2324 = add nuw nsw i64 %2323, %2321
  %2325 = lshr i64 %2324, 51
  %2326 = and i64 %2314, 2251799813685247
  %2327 = add nuw nsw i64 %2325, %2326
  %2328 = and i64 %2324, 2251799813685247
  %2329 = and i64 %2327, 2251799813685247
  %2330 = lshr i64 %2327, 51
  %2331 = and i64 %2316, 2251799813685247
  %2332 = add nuw nsw i64 %2330, %2331
  %2333 = and i64 %2318, 2251799813685247
  %2334 = and i64 %2320, 2251799813685247
  store i64 %2328, i64* %2312, align 8
  %2335 = getelementptr inbounds %struct.fe, %struct.fe* %7, i64 0, i32 0, i64 1
  store i64 %2329, i64* %2335, align 8
  %2336 = getelementptr inbounds %struct.fe, %struct.fe* %7, i64 0, i32 0, i64 2
  store i64 %2332, i64* %2336, align 8
  %2337 = getelementptr inbounds %struct.fe, %struct.fe* %7, i64 0, i32 0, i64 3
  store i64 %2333, i64* %2337, align 8
  %2338 = getelementptr inbounds %struct.fe, %struct.fe* %7, i64 0, i32 0, i64 4
  store i64 %2334, i64* %2338, align 8
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %41) #4
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %41, i8 -86, i64 32, i1 false) #4
  call fastcc void @fe_tobytes(i8* nonnull %41, %struct.fe* nonnull %7) #4
  %2339 = call i32 @CRYPTO_memcmp(i8* nonnull %41, i8* getelementptr inbounds ([32 x i8], [32 x i8]* @fe_isnonzero.zero, i64 0, i64 0), i64 32) #4
  %2340 = icmp eq i32 %2339, 0
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %41) #4
  call void @llvm.lifetime.end.p0i8(i64 40, i8* nonnull %2311) #4
  br i1 %2340, label %2341, label %2384

2341:                                             ; preds = %2305
  call fastcc void @fe_mul_impl(i64* %266, i64* %266, i64* getelementptr inbounds (%struct.fe, %struct.fe* @sqrtm1, i64 0, i32 0, i64 0)) #4
  br label %2342

2342:                                             ; preds = %1972, %2341
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %41) #4
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %41, i8 -86, i64 32, i1 false) #4
  call fastcc void @fe_tobytes(i8* nonnull %41, %struct.fe* %943) #4
  %2343 = load i8, i8* %41, align 16
  %2344 = and i8 %2343, 1
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %41) #4
  %2345 = getelementptr inbounds i8, i8* %1, i64 31
  %2346 = load i8, i8* %2345, align 1
  %2347 = lshr i8 %2346, 7
  %2348 = icmp eq i8 %2344, %2347
  br i1 %2348, label %2382, label %2349

2349:                                             ; preds = %2342
  %2350 = load i64, i64* %266, align 8
  %2351 = sub i64 4503599627370458, %2350
  %2352 = load i64, i64* %350, align 8
  %2353 = sub i64 4503599627370494, %2352
  %2354 = load i64, i64* %351, align 8
  %2355 = sub i64 4503599627370494, %2354
  %2356 = load i64, i64* %352, align 8
  %2357 = sub i64 4503599627370494, %2356
  %2358 = load i64, i64* %353, align 8
  %2359 = sub i64 4503599627370494, %2358
  %2360 = lshr i64 %2351, 51
  %2361 = add i64 %2360, %2353
  %2362 = lshr i64 %2361, 51
  %2363 = add i64 %2362, %2355
  %2364 = lshr i64 %2363, 51
  %2365 = add i64 %2364, %2357
  %2366 = lshr i64 %2365, 51
  %2367 = add i64 %2366, %2359
  %2368 = and i64 %2351, 2251799813685247
  %2369 = lshr i64 %2367, 51
  %2370 = mul nuw nsw i64 %2369, 19
  %2371 = add nuw nsw i64 %2370, %2368
  %2372 = lshr i64 %2371, 51
  %2373 = and i64 %2361, 2251799813685247
  %2374 = add nuw nsw i64 %2372, %2373
  %2375 = and i64 %2371, 2251799813685247
  %2376 = and i64 %2374, 2251799813685247
  %2377 = lshr i64 %2374, 51
  %2378 = and i64 %2363, 2251799813685247
  %2379 = add nuw nsw i64 %2377, %2378
  %2380 = and i64 %2365, 2251799813685247
  %2381 = and i64 %2367, 2251799813685247
  store i64 %2375, i64* %266, align 8
  store i64 %2376, i64* %350, align 8
  store i64 %2379, i64* %351, align 8
  store i64 %2380, i64* %352, align 8
  store i64 %2381, i64* %353, align 8
  br label %2382

2382:                                             ; preds = %2342, %2349
  %2383 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %0, i64 0, i32 3, i32 0, i64 0
  call fastcc void @fe_mul_impl(i64* %2383, i64* %266, i64* %49) #4
  br label %2384

2384:                                             ; preds = %2305, %2382
  %2385 = phi i32 [ 1, %2382 ], [ 0, %2305 ]
  call void @llvm.lifetime.end.p0i8(i64 40, i8* nonnull %33) #4
  call void @llvm.lifetime.end.p0i8(i64 40, i8* nonnull %26) #4
  call void @llvm.lifetime.end.p0i8(i64 40, i8* nonnull %19) #4
  call void @llvm.lifetime.end.p0i8(i64 40, i8* nonnull %13) #4
  ret i32 %2385
}

; Function Attrs: nounwind ssp uwtable
define hidden void @x25519_ge_p3_to_cached(%struct.ge_cached* nocapture, %struct.ge_p3* nocapture readonly) local_unnamed_addr #0 {
  %3 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %1, i64 0, i32 1, i32 0, i64 0
  %4 = bitcast i64* %3 to <2 x i64>*
  %5 = load <2 x i64>, <2 x i64>* %4, align 8
  %6 = bitcast %struct.ge_p3* %1 to <2 x i64>*
  %7 = load <2 x i64>, <2 x i64>* %6, align 8
  %8 = add <2 x i64> %7, %5
  %9 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %1, i64 0, i32 1, i32 0, i64 2
  %10 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %1, i64 0, i32 0, i32 0, i64 2
  %11 = bitcast i64* %9 to <2 x i64>*
  %12 = load <2 x i64>, <2 x i64>* %11, align 8
  %13 = bitcast i64* %10 to <2 x i64>*
  %14 = load <2 x i64>, <2 x i64>* %13, align 8
  %15 = add <2 x i64> %14, %12
  %16 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %1, i64 0, i32 1, i32 0, i64 4
  %17 = load i64, i64* %16, align 8
  %18 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %1, i64 0, i32 0, i32 0, i64 4
  %19 = load i64, i64* %18, align 8
  %20 = add i64 %19, %17
  %21 = bitcast %struct.ge_cached* %0 to <2 x i64>*
  store <2 x i64> %8, <2 x i64>* %21, align 8
  %22 = getelementptr inbounds %struct.ge_cached, %struct.ge_cached* %0, i64 0, i32 0, i32 0, i64 2
  %23 = bitcast i64* %22 to <2 x i64>*
  store <2 x i64> %15, <2 x i64>* %23, align 8
  %24 = getelementptr inbounds %struct.ge_cached, %struct.ge_cached* %0, i64 0, i32 0, i32 0, i64 4
  store i64 %20, i64* %24, align 8
  %25 = getelementptr inbounds %struct.ge_cached, %struct.ge_cached* %0, i64 0, i32 1, i32 0, i64 0
  %26 = bitcast i64* %3 to <2 x i64>*
  %27 = load <2 x i64>, <2 x i64>* %26, align 8
  %28 = add <2 x i64> %27, <i64 4503599627370458, i64 4503599627370494>
  %29 = bitcast %struct.ge_p3* %1 to <2 x i64>*
  %30 = load <2 x i64>, <2 x i64>* %29, align 8
  %31 = sub <2 x i64> %28, %30
  %32 = bitcast i64* %9 to <2 x i64>*
  %33 = load <2 x i64>, <2 x i64>* %32, align 8
  %34 = add <2 x i64> %33, <i64 4503599627370494, i64 4503599627370494>
  %35 = bitcast i64* %10 to <2 x i64>*
  %36 = load <2 x i64>, <2 x i64>* %35, align 8
  %37 = sub <2 x i64> %34, %36
  %38 = load i64, i64* %16, align 8
  %39 = add i64 %38, 4503599627370494
  %40 = load i64, i64* %18, align 8
  %41 = sub i64 %39, %40
  %42 = bitcast i64* %25 to <2 x i64>*
  store <2 x i64> %31, <2 x i64>* %42, align 8
  %43 = getelementptr inbounds %struct.ge_cached, %struct.ge_cached* %0, i64 0, i32 1, i32 0, i64 2
  %44 = bitcast i64* %43 to <2 x i64>*
  store <2 x i64> %37, <2 x i64>* %44, align 8
  %45 = getelementptr inbounds %struct.ge_cached, %struct.ge_cached* %0, i64 0, i32 1, i32 0, i64 4
  store i64 %41, i64* %45, align 8
  %46 = getelementptr inbounds %struct.ge_cached, %struct.ge_cached* %0, i64 0, i32 2
  %47 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %1, i64 0, i32 2
  %48 = bitcast %struct.fe_loose* %46 to i8*
  %49 = bitcast %struct.fe* %47 to i8*
  tail call void @llvm.memmove.p0i8.p0i8.i64(i8* align 1 %48, i8* align 1 %49, i64 40, i1 false) #4
  %50 = getelementptr inbounds %struct.ge_cached, %struct.ge_cached* %0, i64 0, i32 3, i32 0, i64 0
  %51 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %1, i64 0, i32 3, i32 0, i64 0
  tail call fastcc void @fe_mul_impl(i64* %50, i64* %51, i64* getelementptr inbounds (%struct.fe, %struct.fe* @d2, i64 0, i32 0, i64 0)) #4
  ret void
}

; Function Attrs: nofree norecurse nounwind ssp uwtable
define hidden void @x25519_ge_p1p1_to_p2(%struct.ge_p2* nocapture, %struct.ge_p1p1* nocapture readonly) local_unnamed_addr #2 {
  %3 = getelementptr inbounds %struct.ge_p2, %struct.ge_p2* %0, i64 0, i32 0, i32 0, i64 0
  %4 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %1, i64 0, i32 0, i32 0, i64 0
  %5 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %1, i64 0, i32 3, i32 0, i64 0
  tail call fastcc void @fe_mul_impl(i64* %3, i64* %4, i64* %5) #4
  %6 = getelementptr inbounds %struct.ge_p2, %struct.ge_p2* %0, i64 0, i32 1, i32 0, i64 0
  %7 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %1, i64 0, i32 1, i32 0, i64 0
  %8 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %1, i64 0, i32 2, i32 0, i64 0
  tail call fastcc void @fe_mul_impl(i64* %6, i64* %7, i64* %8) #4
  %9 = getelementptr inbounds %struct.ge_p2, %struct.ge_p2* %0, i64 0, i32 2, i32 0, i64 0
  tail call fastcc void @fe_mul_impl(i64* %9, i64* %8, i64* %5) #4
  ret void
}

; Function Attrs: nofree norecurse nounwind ssp uwtable
define hidden void @x25519_ge_p1p1_to_p3(%struct.ge_p3* nocapture, %struct.ge_p1p1* nocapture readonly) local_unnamed_addr #2 {
  %3 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %0, i64 0, i32 0, i32 0, i64 0
  %4 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %1, i64 0, i32 0, i32 0, i64 0
  %5 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %1, i64 0, i32 3, i32 0, i64 0
  tail call fastcc void @fe_mul_impl(i64* %3, i64* %4, i64* %5) #4
  %6 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %0, i64 0, i32 1, i32 0, i64 0
  %7 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %1, i64 0, i32 1, i32 0, i64 0
  %8 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %1, i64 0, i32 2, i32 0, i64 0
  tail call fastcc void @fe_mul_impl(i64* %6, i64* %7, i64* %8) #4
  %9 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %0, i64 0, i32 2, i32 0, i64 0
  tail call fastcc void @fe_mul_impl(i64* %9, i64* %8, i64* %5) #4
  %10 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %0, i64 0, i32 3, i32 0, i64 0
  tail call fastcc void @fe_mul_impl(i64* %10, i64* %4, i64* %7) #4
  ret void
}

; Function Attrs: nounwind ssp uwtable
define hidden void @x25519_ge_add(%struct.ge_p1p1* nocapture, %struct.ge_p3* nocapture readonly, %struct.ge_cached* nocapture readonly) local_unnamed_addr #0 {
  %4 = alloca %struct.fe, align 8
  %5 = alloca %struct.fe, align 16
  %6 = alloca %struct.fe, align 16
  %7 = alloca %struct.fe, align 8
  %8 = bitcast %struct.fe* %4 to i8*
  call void @llvm.lifetime.start.p0i8(i64 40, i8* nonnull %8) #4
  %9 = getelementptr inbounds %struct.fe, %struct.fe* %4, i64 0, i32 0, i64 0
  %10 = getelementptr inbounds %struct.fe, %struct.fe* %4, i64 0, i32 0, i64 1
  %11 = getelementptr inbounds %struct.fe, %struct.fe* %4, i64 0, i32 0, i64 3
  %12 = getelementptr inbounds %struct.fe, %struct.fe* %4, i64 0, i32 0, i64 4
  %13 = bitcast %struct.fe* %5 to i8*
  %14 = bitcast %struct.fe* %4 to i8*
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %14, i8 -86, i64 40, i1 false)
  call void @llvm.lifetime.start.p0i8(i64 40, i8* nonnull %13) #4
  %15 = getelementptr inbounds %struct.fe, %struct.fe* %5, i64 0, i32 0, i64 0
  %16 = getelementptr inbounds %struct.fe, %struct.fe* %5, i64 0, i32 0, i64 2
  %17 = getelementptr inbounds %struct.fe, %struct.fe* %5, i64 0, i32 0, i64 3
  %18 = bitcast %struct.fe* %6 to i8*
  %19 = bitcast %struct.fe* %5 to i8*
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %19, i8 -86, i64 40, i1 false)
  call void @llvm.lifetime.start.p0i8(i64 40, i8* nonnull %18) #4
  %20 = getelementptr inbounds %struct.fe, %struct.fe* %6, i64 0, i32 0, i64 0
  %21 = getelementptr inbounds %struct.fe, %struct.fe* %6, i64 0, i32 0, i64 2
  %22 = getelementptr inbounds %struct.fe, %struct.fe* %6, i64 0, i32 0, i64 3
  %23 = bitcast %struct.fe* %7 to i8*
  %24 = bitcast %struct.fe* %6 to i8*
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %24, i8 -86, i64 40, i1 false)
  call void @llvm.lifetime.start.p0i8(i64 40, i8* nonnull %23) #4
  %25 = getelementptr inbounds %struct.fe, %struct.fe* %7, i64 0, i32 0, i64 0
  %26 = getelementptr inbounds %struct.fe, %struct.fe* %7, i64 0, i32 0, i64 1
  %27 = getelementptr inbounds %struct.fe, %struct.fe* %7, i64 0, i32 0, i64 2
  %28 = getelementptr inbounds %struct.fe, %struct.fe* %7, i64 0, i32 0, i64 3
  %29 = getelementptr inbounds %struct.fe, %struct.fe* %7, i64 0, i32 0, i64 4
  %30 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %0, i64 0, i32 0, i32 0, i64 0
  %31 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %1, i64 0, i32 1, i32 0, i64 0
  %32 = bitcast %struct.fe* %7 to i8*
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %32, i8 -86, i64 40, i1 false)
  %33 = bitcast i64* %31 to <2 x i64>*
  %34 = load <2 x i64>, <2 x i64>* %33, align 8
  %35 = bitcast %struct.ge_p3* %1 to <2 x i64>*
  %36 = load <2 x i64>, <2 x i64>* %35, align 8
  %37 = add <2 x i64> %36, %34
  %38 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %1, i64 0, i32 1, i32 0, i64 2
  %39 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %1, i64 0, i32 0, i32 0, i64 2
  %40 = bitcast i64* %38 to <2 x i64>*
  %41 = load <2 x i64>, <2 x i64>* %40, align 8
  %42 = bitcast i64* %39 to <2 x i64>*
  %43 = load <2 x i64>, <2 x i64>* %42, align 8
  %44 = add <2 x i64> %43, %41
  %45 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %1, i64 0, i32 1, i32 0, i64 4
  %46 = load i64, i64* %45, align 8
  %47 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %1, i64 0, i32 0, i32 0, i64 4
  %48 = load i64, i64* %47, align 8
  %49 = add i64 %48, %46
  %50 = bitcast %struct.ge_p1p1* %0 to <2 x i64>*
  store <2 x i64> %37, <2 x i64>* %50, align 8
  %51 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %0, i64 0, i32 0, i32 0, i64 2
  %52 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %0, i64 0, i32 0, i32 0, i64 3
  %53 = bitcast i64* %51 to <2 x i64>*
  store <2 x i64> %44, <2 x i64>* %53, align 8
  %54 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %0, i64 0, i32 0, i32 0, i64 4
  store i64 %49, i64* %54, align 8
  %55 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %0, i64 0, i32 1, i32 0, i64 0
  %56 = bitcast i64* %31 to <2 x i64>*
  %57 = load <2 x i64>, <2 x i64>* %56, align 8
  %58 = add <2 x i64> %57, <i64 4503599627370458, i64 4503599627370494>
  %59 = bitcast %struct.ge_p3* %1 to <2 x i64>*
  %60 = load <2 x i64>, <2 x i64>* %59, align 8
  %61 = sub <2 x i64> %58, %60
  %62 = bitcast i64* %38 to <2 x i64>*
  %63 = load <2 x i64>, <2 x i64>* %62, align 8
  %64 = add <2 x i64> %63, <i64 4503599627370494, i64 4503599627370494>
  %65 = bitcast i64* %39 to <2 x i64>*
  %66 = load <2 x i64>, <2 x i64>* %65, align 8
  %67 = sub <2 x i64> %64, %66
  %68 = load i64, i64* %45, align 8
  %69 = add i64 %68, 4503599627370494
  %70 = load i64, i64* %47, align 8
  %71 = sub i64 %69, %70
  %72 = bitcast i64* %55 to <2 x i64>*
  store <2 x i64> %61, <2 x i64>* %72, align 8
  %73 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %0, i64 0, i32 1, i32 0, i64 2
  %74 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %0, i64 0, i32 1, i32 0, i64 3
  %75 = bitcast i64* %73 to <2 x i64>*
  store <2 x i64> %67, <2 x i64>* %75, align 8
  %76 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %0, i64 0, i32 1, i32 0, i64 4
  store i64 %71, i64* %76, align 8
  %77 = getelementptr inbounds %struct.ge_cached, %struct.ge_cached* %2, i64 0, i32 0, i32 0, i64 0
  call fastcc void @fe_mul_impl(i64* nonnull %20, i64* %30, i64* %77) #4
  %78 = getelementptr inbounds %struct.ge_cached, %struct.ge_cached* %2, i64 0, i32 1, i32 0, i64 0
  call fastcc void @fe_mul_impl(i64* nonnull %15, i64* %55, i64* %78) #4
  %79 = getelementptr inbounds %struct.ge_cached, %struct.ge_cached* %2, i64 0, i32 3, i32 0, i64 0
  %80 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %1, i64 0, i32 3, i32 0, i64 0
  call fastcc void @fe_mul_impl(i64* nonnull %25, i64* %79, i64* %80) #4
  %81 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %1, i64 0, i32 2, i32 0, i64 0
  %82 = getelementptr inbounds %struct.ge_cached, %struct.ge_cached* %2, i64 0, i32 2, i32 0, i64 0
  call fastcc void @fe_mul_impl(i64* nonnull %9, i64* %81, i64* %82) #4
  %83 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %0, i64 0, i32 3, i32 0, i64 0
  %84 = load i64, i64* %9, align 8
  %85 = shl i64 %84, 1
  %86 = bitcast i64* %10 to <2 x i64>*
  %87 = load <2 x i64>, <2 x i64>* %86, align 8
  %88 = shl <2 x i64> %87, <i64 1, i64 1>
  %89 = load i64, i64* %11, align 8
  %90 = shl i64 %89, 1
  %91 = load i64, i64* %12, align 8
  %92 = shl i64 %91, 1
  store i64 %85, i64* %83, align 8
  %93 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %0, i64 0, i32 3, i32 0, i64 1
  %94 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %0, i64 0, i32 3, i32 0, i64 2
  %95 = bitcast i64* %93 to <2 x i64>*
  store <2 x i64> %88, <2 x i64>* %95, align 8
  %96 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %0, i64 0, i32 3, i32 0, i64 3
  %97 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %0, i64 0, i32 3, i32 0, i64 4
  %98 = bitcast %struct.fe* %6 to <2 x i64>*
  %99 = load <2 x i64>, <2 x i64>* %98, align 16
  %100 = add <2 x i64> %99, <i64 4503599627370458, i64 4503599627370494>
  %101 = bitcast %struct.fe* %5 to <2 x i64>*
  %102 = load <2 x i64>, <2 x i64>* %101, align 16
  %103 = sub <2 x i64> %100, %102
  %104 = load i64, i64* %21, align 16
  %105 = add i64 %104, 4503599627370494
  %106 = load i64, i64* %16, align 16
  %107 = sub i64 %105, %106
  %108 = bitcast i64* %22 to <2 x i64>*
  %109 = load <2 x i64>, <2 x i64>* %108, align 8
  %110 = add <2 x i64> %109, <i64 4503599627370494, i64 4503599627370494>
  %111 = bitcast i64* %17 to <2 x i64>*
  %112 = load <2 x i64>, <2 x i64>* %111, align 8
  %113 = sub <2 x i64> %110, %112
  %114 = bitcast %struct.ge_p1p1* %0 to <2 x i64>*
  store <2 x i64> %103, <2 x i64>* %114, align 8
  store i64 %107, i64* %51, align 8
  %115 = bitcast i64* %52 to <2 x i64>*
  store <2 x i64> %113, <2 x i64>* %115, align 8
  %116 = add <2 x i64> %102, %99
  %117 = add i64 %106, %104
  %118 = add <2 x i64> %112, %109
  %119 = bitcast i64* %55 to <2 x i64>*
  store <2 x i64> %116, <2 x i64>* %119, align 8
  store i64 %117, i64* %73, align 8
  %120 = bitcast i64* %74 to <2 x i64>*
  store <2 x i64> %118, <2 x i64>* %120, align 8
  %121 = lshr i64 %84, 50
  %122 = and i64 %121, 8191
  %123 = extractelement <2 x i64> %88, i32 0
  %124 = add i64 %122, %123
  %125 = lshr i64 %124, 51
  %126 = extractelement <2 x i64> %88, i32 1
  %127 = add i64 %125, %126
  %128 = lshr i64 %127, 51
  %129 = add i64 %128, %90
  %130 = lshr i64 %129, 51
  %131 = add i64 %130, %92
  %132 = and i64 %85, 2251799813685246
  %133 = lshr i64 %131, 51
  %134 = mul nuw nsw i64 %133, 19
  %135 = add nuw nsw i64 %134, %132
  %136 = lshr i64 %135, 51
  %137 = and i64 %124, 2251799813685247
  %138 = add nuw nsw i64 %136, %137
  %139 = and i64 %135, 2251799813685247
  %140 = and i64 %138, 2251799813685247
  %141 = lshr i64 %138, 51
  %142 = and i64 %127, 2251799813685247
  %143 = add nuw nsw i64 %141, %142
  %144 = and i64 %129, 2251799813685247
  %145 = and i64 %131, 2251799813685247
  %146 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %0, i64 0, i32 2, i32 0, i64 0
  %147 = load i64, i64* %25, align 8
  %148 = add i64 %147, %139
  %149 = load i64, i64* %26, align 8
  %150 = add i64 %149, %140
  %151 = load i64, i64* %27, align 8
  %152 = add i64 %151, %143
  %153 = load i64, i64* %28, align 8
  %154 = add i64 %153, %144
  %155 = load i64, i64* %29, align 8
  %156 = add i64 %155, %145
  store i64 %148, i64* %146, align 8
  %157 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %0, i64 0, i32 2, i32 0, i64 1
  store i64 %150, i64* %157, align 8
  %158 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %0, i64 0, i32 2, i32 0, i64 2
  store i64 %152, i64* %158, align 8
  %159 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %0, i64 0, i32 2, i32 0, i64 3
  store i64 %154, i64* %159, align 8
  %160 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %0, i64 0, i32 2, i32 0, i64 4
  store i64 %156, i64* %160, align 8
  %161 = add nuw nsw i64 %139, 4503599627370458
  %162 = sub i64 %161, %147
  %163 = add nuw nsw i64 %140, 4503599627370494
  %164 = sub i64 %163, %149
  %165 = add nuw nsw i64 %143, 4503599627370494
  %166 = sub i64 %165, %151
  %167 = add nuw nsw i64 %144, 4503599627370494
  %168 = sub i64 %167, %153
  %169 = add nuw nsw i64 %145, 4503599627370494
  %170 = sub i64 %169, %155
  store i64 %162, i64* %83, align 8
  store i64 %164, i64* %93, align 8
  store i64 %166, i64* %94, align 8
  store i64 %168, i64* %96, align 8
  store i64 %170, i64* %97, align 8
  call void @llvm.lifetime.end.p0i8(i64 40, i8* nonnull %23) #4
  call void @llvm.lifetime.end.p0i8(i64 40, i8* nonnull %18) #4
  call void @llvm.lifetime.end.p0i8(i64 40, i8* nonnull %13) #4
  call void @llvm.lifetime.end.p0i8(i64 40, i8* nonnull %8) #4
  ret void
}

; Function Attrs: nounwind ssp uwtable
define hidden void @x25519_ge_sub(%struct.ge_p1p1* nocapture, %struct.ge_p3* nocapture readonly, %struct.ge_cached* nocapture readonly) local_unnamed_addr #0 {
  %4 = alloca %struct.fe, align 8
  %5 = alloca %struct.fe, align 16
  %6 = alloca %struct.fe, align 16
  %7 = alloca %struct.fe, align 8
  %8 = bitcast %struct.fe* %4 to i8*
  call void @llvm.lifetime.start.p0i8(i64 40, i8* nonnull %8) #4
  %9 = getelementptr inbounds %struct.fe, %struct.fe* %4, i64 0, i32 0, i64 0
  %10 = getelementptr inbounds %struct.fe, %struct.fe* %4, i64 0, i32 0, i64 1
  %11 = getelementptr inbounds %struct.fe, %struct.fe* %4, i64 0, i32 0, i64 3
  %12 = getelementptr inbounds %struct.fe, %struct.fe* %4, i64 0, i32 0, i64 4
  %13 = bitcast %struct.fe* %5 to i8*
  %14 = bitcast %struct.fe* %4 to i8*
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %14, i8 -86, i64 40, i1 false)
  call void @llvm.lifetime.start.p0i8(i64 40, i8* nonnull %13) #4
  %15 = getelementptr inbounds %struct.fe, %struct.fe* %5, i64 0, i32 0, i64 0
  %16 = getelementptr inbounds %struct.fe, %struct.fe* %5, i64 0, i32 0, i64 2
  %17 = getelementptr inbounds %struct.fe, %struct.fe* %5, i64 0, i32 0, i64 3
  %18 = bitcast %struct.fe* %6 to i8*
  %19 = bitcast %struct.fe* %5 to i8*
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %19, i8 -86, i64 40, i1 false)
  call void @llvm.lifetime.start.p0i8(i64 40, i8* nonnull %18) #4
  %20 = getelementptr inbounds %struct.fe, %struct.fe* %6, i64 0, i32 0, i64 0
  %21 = getelementptr inbounds %struct.fe, %struct.fe* %6, i64 0, i32 0, i64 2
  %22 = getelementptr inbounds %struct.fe, %struct.fe* %6, i64 0, i32 0, i64 3
  %23 = bitcast %struct.fe* %7 to i8*
  %24 = bitcast %struct.fe* %6 to i8*
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %24, i8 -86, i64 40, i1 false)
  call void @llvm.lifetime.start.p0i8(i64 40, i8* nonnull %23) #4
  %25 = getelementptr inbounds %struct.fe, %struct.fe* %7, i64 0, i32 0, i64 0
  %26 = getelementptr inbounds %struct.fe, %struct.fe* %7, i64 0, i32 0, i64 1
  %27 = getelementptr inbounds %struct.fe, %struct.fe* %7, i64 0, i32 0, i64 2
  %28 = getelementptr inbounds %struct.fe, %struct.fe* %7, i64 0, i32 0, i64 3
  %29 = getelementptr inbounds %struct.fe, %struct.fe* %7, i64 0, i32 0, i64 4
  %30 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %0, i64 0, i32 0, i32 0, i64 0
  %31 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %1, i64 0, i32 1, i32 0, i64 0
  %32 = bitcast %struct.fe* %7 to i8*
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %32, i8 -86, i64 40, i1 false)
  %33 = bitcast i64* %31 to <2 x i64>*
  %34 = load <2 x i64>, <2 x i64>* %33, align 8
  %35 = bitcast %struct.ge_p3* %1 to <2 x i64>*
  %36 = load <2 x i64>, <2 x i64>* %35, align 8
  %37 = add <2 x i64> %36, %34
  %38 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %1, i64 0, i32 1, i32 0, i64 2
  %39 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %1, i64 0, i32 0, i32 0, i64 2
  %40 = bitcast i64* %38 to <2 x i64>*
  %41 = load <2 x i64>, <2 x i64>* %40, align 8
  %42 = bitcast i64* %39 to <2 x i64>*
  %43 = load <2 x i64>, <2 x i64>* %42, align 8
  %44 = add <2 x i64> %43, %41
  %45 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %1, i64 0, i32 1, i32 0, i64 4
  %46 = load i64, i64* %45, align 8
  %47 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %1, i64 0, i32 0, i32 0, i64 4
  %48 = load i64, i64* %47, align 8
  %49 = add i64 %48, %46
  %50 = bitcast %struct.ge_p1p1* %0 to <2 x i64>*
  store <2 x i64> %37, <2 x i64>* %50, align 8
  %51 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %0, i64 0, i32 0, i32 0, i64 2
  %52 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %0, i64 0, i32 0, i32 0, i64 3
  %53 = bitcast i64* %51 to <2 x i64>*
  store <2 x i64> %44, <2 x i64>* %53, align 8
  %54 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %0, i64 0, i32 0, i32 0, i64 4
  store i64 %49, i64* %54, align 8
  %55 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %0, i64 0, i32 1, i32 0, i64 0
  %56 = bitcast i64* %31 to <2 x i64>*
  %57 = load <2 x i64>, <2 x i64>* %56, align 8
  %58 = add <2 x i64> %57, <i64 4503599627370458, i64 4503599627370494>
  %59 = bitcast %struct.ge_p3* %1 to <2 x i64>*
  %60 = load <2 x i64>, <2 x i64>* %59, align 8
  %61 = sub <2 x i64> %58, %60
  %62 = bitcast i64* %38 to <2 x i64>*
  %63 = load <2 x i64>, <2 x i64>* %62, align 8
  %64 = add <2 x i64> %63, <i64 4503599627370494, i64 4503599627370494>
  %65 = bitcast i64* %39 to <2 x i64>*
  %66 = load <2 x i64>, <2 x i64>* %65, align 8
  %67 = sub <2 x i64> %64, %66
  %68 = load i64, i64* %45, align 8
  %69 = add i64 %68, 4503599627370494
  %70 = load i64, i64* %47, align 8
  %71 = sub i64 %69, %70
  %72 = bitcast i64* %55 to <2 x i64>*
  store <2 x i64> %61, <2 x i64>* %72, align 8
  %73 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %0, i64 0, i32 1, i32 0, i64 2
  %74 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %0, i64 0, i32 1, i32 0, i64 3
  %75 = bitcast i64* %73 to <2 x i64>*
  store <2 x i64> %67, <2 x i64>* %75, align 8
  %76 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %0, i64 0, i32 1, i32 0, i64 4
  store i64 %71, i64* %76, align 8
  %77 = getelementptr inbounds %struct.ge_cached, %struct.ge_cached* %2, i64 0, i32 1, i32 0, i64 0
  call fastcc void @fe_mul_impl(i64* nonnull %20, i64* %30, i64* %77) #4
  %78 = getelementptr inbounds %struct.ge_cached, %struct.ge_cached* %2, i64 0, i32 0, i32 0, i64 0
  call fastcc void @fe_mul_impl(i64* nonnull %15, i64* %55, i64* %78) #4
  %79 = getelementptr inbounds %struct.ge_cached, %struct.ge_cached* %2, i64 0, i32 3, i32 0, i64 0
  %80 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %1, i64 0, i32 3, i32 0, i64 0
  call fastcc void @fe_mul_impl(i64* nonnull %25, i64* %79, i64* %80) #4
  %81 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %1, i64 0, i32 2, i32 0, i64 0
  %82 = getelementptr inbounds %struct.ge_cached, %struct.ge_cached* %2, i64 0, i32 2, i32 0, i64 0
  call fastcc void @fe_mul_impl(i64* nonnull %9, i64* %81, i64* %82) #4
  %83 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %0, i64 0, i32 3, i32 0, i64 0
  %84 = load i64, i64* %9, align 8
  %85 = shl i64 %84, 1
  %86 = bitcast i64* %10 to <2 x i64>*
  %87 = load <2 x i64>, <2 x i64>* %86, align 8
  %88 = shl <2 x i64> %87, <i64 1, i64 1>
  %89 = load i64, i64* %11, align 8
  %90 = shl i64 %89, 1
  %91 = load i64, i64* %12, align 8
  %92 = shl i64 %91, 1
  store i64 %85, i64* %83, align 8
  %93 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %0, i64 0, i32 3, i32 0, i64 1
  %94 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %0, i64 0, i32 3, i32 0, i64 2
  %95 = bitcast i64* %93 to <2 x i64>*
  store <2 x i64> %88, <2 x i64>* %95, align 8
  %96 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %0, i64 0, i32 3, i32 0, i64 3
  %97 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %0, i64 0, i32 3, i32 0, i64 4
  %98 = bitcast %struct.fe* %6 to <2 x i64>*
  %99 = load <2 x i64>, <2 x i64>* %98, align 16
  %100 = add <2 x i64> %99, <i64 4503599627370458, i64 4503599627370494>
  %101 = bitcast %struct.fe* %5 to <2 x i64>*
  %102 = load <2 x i64>, <2 x i64>* %101, align 16
  %103 = sub <2 x i64> %100, %102
  %104 = load i64, i64* %21, align 16
  %105 = add i64 %104, 4503599627370494
  %106 = load i64, i64* %16, align 16
  %107 = sub i64 %105, %106
  %108 = bitcast i64* %22 to <2 x i64>*
  %109 = load <2 x i64>, <2 x i64>* %108, align 8
  %110 = add <2 x i64> %109, <i64 4503599627370494, i64 4503599627370494>
  %111 = bitcast i64* %17 to <2 x i64>*
  %112 = load <2 x i64>, <2 x i64>* %111, align 8
  %113 = sub <2 x i64> %110, %112
  %114 = bitcast %struct.ge_p1p1* %0 to <2 x i64>*
  store <2 x i64> %103, <2 x i64>* %114, align 8
  store i64 %107, i64* %51, align 8
  %115 = bitcast i64* %52 to <2 x i64>*
  store <2 x i64> %113, <2 x i64>* %115, align 8
  %116 = add <2 x i64> %102, %99
  %117 = add i64 %106, %104
  %118 = add <2 x i64> %112, %109
  %119 = bitcast i64* %55 to <2 x i64>*
  store <2 x i64> %116, <2 x i64>* %119, align 8
  store i64 %117, i64* %73, align 8
  %120 = bitcast i64* %74 to <2 x i64>*
  store <2 x i64> %118, <2 x i64>* %120, align 8
  %121 = lshr i64 %84, 50
  %122 = and i64 %121, 8191
  %123 = extractelement <2 x i64> %88, i32 0
  %124 = add i64 %122, %123
  %125 = lshr i64 %124, 51
  %126 = extractelement <2 x i64> %88, i32 1
  %127 = add i64 %125, %126
  %128 = lshr i64 %127, 51
  %129 = add i64 %128, %90
  %130 = lshr i64 %129, 51
  %131 = add i64 %130, %92
  %132 = and i64 %85, 2251799813685246
  %133 = lshr i64 %131, 51
  %134 = mul nuw nsw i64 %133, 19
  %135 = add nuw nsw i64 %134, %132
  %136 = lshr i64 %135, 51
  %137 = and i64 %124, 2251799813685247
  %138 = add nuw nsw i64 %136, %137
  %139 = and i64 %135, 2251799813685247
  %140 = and i64 %138, 2251799813685247
  %141 = lshr i64 %138, 51
  %142 = and i64 %127, 2251799813685247
  %143 = add nuw nsw i64 %141, %142
  %144 = and i64 %129, 2251799813685247
  %145 = and i64 %131, 2251799813685247
  %146 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %0, i64 0, i32 2, i32 0, i64 0
  %147 = add nuw nsw i64 %139, 4503599627370458
  %148 = load i64, i64* %25, align 8
  %149 = sub i64 %147, %148
  %150 = add nuw nsw i64 %140, 4503599627370494
  %151 = load i64, i64* %26, align 8
  %152 = sub i64 %150, %151
  %153 = add nuw nsw i64 %143, 4503599627370494
  %154 = load i64, i64* %27, align 8
  %155 = sub i64 %153, %154
  %156 = add nuw nsw i64 %144, 4503599627370494
  %157 = load i64, i64* %28, align 8
  %158 = sub i64 %156, %157
  %159 = add nuw nsw i64 %145, 4503599627370494
  %160 = load i64, i64* %29, align 8
  %161 = sub i64 %159, %160
  store i64 %149, i64* %146, align 8
  %162 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %0, i64 0, i32 2, i32 0, i64 1
  store i64 %152, i64* %162, align 8
  %163 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %0, i64 0, i32 2, i32 0, i64 2
  store i64 %155, i64* %163, align 8
  %164 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %0, i64 0, i32 2, i32 0, i64 3
  store i64 %158, i64* %164, align 8
  %165 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %0, i64 0, i32 2, i32 0, i64 4
  store i64 %161, i64* %165, align 8
  %166 = add i64 %148, %139
  %167 = add i64 %151, %140
  %168 = add i64 %154, %143
  %169 = add i64 %157, %144
  %170 = add i64 %160, %145
  store i64 %166, i64* %83, align 8
  store i64 %167, i64* %93, align 8
  store i64 %168, i64* %94, align 8
  store i64 %169, i64* %96, align 8
  store i64 %170, i64* %97, align 8
  call void @llvm.lifetime.end.p0i8(i64 40, i8* nonnull %23) #4
  call void @llvm.lifetime.end.p0i8(i64 40, i8* nonnull %18) #4
  call void @llvm.lifetime.end.p0i8(i64 40, i8* nonnull %13) #4
  call void @llvm.lifetime.end.p0i8(i64 40, i8* nonnull %8) #4
  ret void
}

; Function Attrs: nounwind ssp uwtable
define hidden void @x25519_ge_scalarmult_small_precomp(%struct.ge_p3*, i8* nocapture readonly, i8* readonly) local_unnamed_addr #0 {
  %4 = alloca [15 x %struct.ge_precomp], align 16
  %5 = alloca %struct.fe, align 16
  %6 = alloca %struct.fe, align 16
  %7 = alloca %struct.ge_precomp, align 16
  %8 = alloca %struct.ge_cached, align 8
  %9 = alloca %struct.ge_p1p1, align 8
  %10 = bitcast [15 x %struct.ge_precomp]* %4 to i8*
  call void @llvm.lifetime.start.p0i8(i64 1800, i8* nonnull %10) #4
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %10, i8 -86, i64 1800, i1 false)
  %11 = bitcast %struct.fe* %5 to i8*
  %12 = getelementptr inbounds %struct.fe, %struct.fe* %5, i64 0, i32 0, i64 0
  %13 = getelementptr inbounds %struct.fe, %struct.fe* %5, i64 0, i32 0, i64 2
  %14 = getelementptr inbounds %struct.fe, %struct.fe* %5, i64 0, i32 0, i64 4
  %15 = bitcast %struct.fe* %6 to i8*
  %16 = getelementptr inbounds %struct.fe, %struct.fe* %6, i64 0, i32 0, i64 0
  %17 = getelementptr inbounds %struct.fe, %struct.fe* %6, i64 0, i32 0, i64 2
  %18 = getelementptr inbounds %struct.fe, %struct.fe* %6, i64 0, i32 0, i64 4
  %19 = bitcast %struct.fe* %5 to i8*
  %20 = bitcast %struct.fe* %6 to i8*
  %21 = bitcast %struct.fe* %6 to <2 x i64>*
  %22 = bitcast %struct.fe* %5 to <2 x i64>*
  %23 = bitcast i64* %17 to <2 x i64>*
  %24 = bitcast i64* %13 to <2 x i64>*
  br label %25

25:                                               ; preds = %25, %3
  %26 = phi i64 [ 0, %3 ], [ %66, %25 ]
  %27 = shl i64 %26, 6
  %28 = getelementptr inbounds i8, i8* %2, i64 %27
  call void @llvm.lifetime.start.p0i8(i64 40, i8* nonnull %11) #4
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %19, i8 -86, i64 40, i1 false)
  call void @llvm.lifetime.start.p0i8(i64 40, i8* nonnull %15) #4
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %20, i8 -86, i64 40, i1 false)
  call fastcc void @fe_frombytes_strict(%struct.fe* nonnull %5, i8* %28)
  %29 = getelementptr inbounds i8, i8* %28, i64 32
  call fastcc void @fe_frombytes_strict(%struct.fe* nonnull %6, i8* %29)
  %30 = getelementptr inbounds [15 x %struct.ge_precomp], [15 x %struct.ge_precomp]* %4, i64 0, i64 %26, i32 0, i32 0, i64 0
  %31 = load <2 x i64>, <2 x i64>* %21, align 16
  %32 = load <2 x i64>, <2 x i64>* %22, align 16
  %33 = extractelement <2 x i64> %31, i32 0
  %34 = extractelement <2 x i64> %32, i32 0
  %35 = add i64 %34, %33
  %36 = extractelement <2 x i64> %31, i32 1
  %37 = extractelement <2 x i64> %32, i32 1
  %38 = add i64 %37, %36
  %39 = load <2 x i64>, <2 x i64>* %23, align 16
  %40 = load <2 x i64>, <2 x i64>* %24, align 16
  %41 = extractelement <2 x i64> %39, i32 0
  %42 = extractelement <2 x i64> %40, i32 0
  %43 = add i64 %42, %41
  %44 = extractelement <2 x i64> %39, i32 1
  %45 = extractelement <2 x i64> %40, i32 1
  %46 = add i64 %45, %44
  %47 = load i64, i64* %18, align 16
  %48 = load i64, i64* %14, align 16
  %49 = add i64 %48, %47
  store i64 %35, i64* %30, align 8
  %50 = getelementptr inbounds [15 x %struct.ge_precomp], [15 x %struct.ge_precomp]* %4, i64 0, i64 %26, i32 0, i32 0, i64 1
  store i64 %38, i64* %50, align 8
  %51 = getelementptr inbounds [15 x %struct.ge_precomp], [15 x %struct.ge_precomp]* %4, i64 0, i64 %26, i32 0, i32 0, i64 2
  store i64 %43, i64* %51, align 8
  %52 = getelementptr inbounds [15 x %struct.ge_precomp], [15 x %struct.ge_precomp]* %4, i64 0, i64 %26, i32 0, i32 0, i64 3
  store i64 %46, i64* %52, align 8
  %53 = getelementptr inbounds [15 x %struct.ge_precomp], [15 x %struct.ge_precomp]* %4, i64 0, i64 %26, i32 0, i32 0, i64 4
  store i64 %49, i64* %53, align 8
  %54 = getelementptr inbounds [15 x %struct.ge_precomp], [15 x %struct.ge_precomp]* %4, i64 0, i64 %26, i32 1, i32 0, i64 0
  %55 = add <2 x i64> %31, <i64 4503599627370458, i64 4503599627370494>
  %56 = sub <2 x i64> %55, %32
  %57 = add <2 x i64> %39, <i64 4503599627370494, i64 4503599627370494>
  %58 = sub <2 x i64> %57, %40
  %59 = add i64 %47, 4503599627370494
  %60 = sub i64 %59, %48
  %61 = bitcast i64* %54 to <2 x i64>*
  store <2 x i64> %56, <2 x i64>* %61, align 8
  %62 = getelementptr inbounds [15 x %struct.ge_precomp], [15 x %struct.ge_precomp]* %4, i64 0, i64 %26, i32 1, i32 0, i64 2
  %63 = bitcast i64* %62 to <2 x i64>*
  store <2 x i64> %58, <2 x i64>* %63, align 8
  %64 = getelementptr inbounds [15 x %struct.ge_precomp], [15 x %struct.ge_precomp]* %4, i64 0, i64 %26, i32 1, i32 0, i64 4
  store i64 %60, i64* %64, align 8
  %65 = getelementptr inbounds [15 x %struct.ge_precomp], [15 x %struct.ge_precomp]* %4, i64 0, i64 %26, i32 2, i32 0, i64 0
  call fastcc void @fe_mul_impl(i64* %65, i64* nonnull %12, i64* nonnull %16) #4
  call fastcc void @fe_mul_impl(i64* %65, i64* %65, i64* getelementptr inbounds (%struct.fe, %struct.fe* @d2, i64 0, i32 0, i64 0)) #4
  call void @llvm.lifetime.end.p0i8(i64 40, i8* nonnull %15) #4
  call void @llvm.lifetime.end.p0i8(i64 40, i8* nonnull %11) #4
  %66 = add nuw nsw i64 %26, 1
  %67 = icmp eq i64 %66, 15
  br i1 %67, label %68, label %25

68:                                               ; preds = %25
  %69 = bitcast %struct.ge_p3* %0 to i8*
  tail call void @llvm.memset.p0i8.i64(i8* align 1 %69, i8 0, i64 40, i1 false) #4
  %70 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %0, i64 0, i32 1, i32 0, i64 1
  %71 = bitcast i64* %70 to i8*
  tail call void @llvm.memset.p0i8.i64(i8* align 1 %71, i8 0, i64 32, i1 false) #4
  %72 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %0, i64 0, i32 1, i32 0, i64 0
  store i64 1, i64* %72, align 8
  %73 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %0, i64 0, i32 2, i32 0, i64 1
  %74 = bitcast i64* %73 to i8*
  tail call void @llvm.memset.p0i8.i64(i8* align 1 %74, i8 0, i64 32, i1 false) #4
  %75 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %0, i64 0, i32 2, i32 0, i64 0
  store i64 1, i64* %75, align 8
  %76 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %0, i64 0, i32 3
  %77 = bitcast %struct.fe* %76 to i8*
  tail call void @llvm.memset.p0i8.i64(i8* align 1 %77, i8 0, i64 40, i1 false) #4
  %78 = bitcast %struct.ge_precomp* %7 to i8*
  %79 = getelementptr inbounds %struct.ge_precomp, %struct.ge_precomp* %7, i64 0, i32 0, i32 0, i64 1
  %80 = bitcast i64* %79 to i8*
  %81 = getelementptr inbounds %struct.ge_precomp, %struct.ge_precomp* %7, i64 0, i32 0, i32 0, i64 0
  %82 = getelementptr inbounds %struct.ge_precomp, %struct.ge_precomp* %7, i64 0, i32 1, i32 0, i64 1
  %83 = bitcast i64* %82 to i8*
  %84 = getelementptr inbounds %struct.ge_precomp, %struct.ge_precomp* %7, i64 0, i32 1, i32 0, i64 0
  %85 = getelementptr inbounds %struct.ge_precomp, %struct.ge_precomp* %7, i64 0, i32 2
  %86 = bitcast %struct.fe_loose* %85 to i8*
  %87 = getelementptr inbounds %struct.ge_precomp, %struct.ge_precomp* %7, i64 0, i32 0, i32 0, i64 2
  %88 = getelementptr inbounds %struct.ge_precomp, %struct.ge_precomp* %7, i64 0, i32 0, i32 0, i64 4
  %89 = getelementptr inbounds %struct.ge_precomp, %struct.ge_precomp* %7, i64 0, i32 1, i32 0, i64 3
  %90 = getelementptr inbounds %struct.ge_precomp, %struct.ge_precomp* %7, i64 0, i32 2, i32 0, i64 0
  %91 = getelementptr inbounds %struct.ge_precomp, %struct.ge_precomp* %7, i64 0, i32 2, i32 0, i64 2
  %92 = getelementptr inbounds %struct.ge_precomp, %struct.ge_precomp* %7, i64 0, i32 2, i32 0, i64 4
  %93 = bitcast %struct.ge_cached* %8 to i8*
  %94 = bitcast %struct.ge_p1p1* %9 to i8*
  %95 = getelementptr inbounds %struct.ge_cached, %struct.ge_cached* %8, i64 0, i32 0, i32 0, i64 0
  %96 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %0, i64 0, i32 0, i32 0, i64 0
  %97 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %0, i64 0, i32 1, i32 0, i64 2
  %98 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %0, i64 0, i32 0, i32 0, i64 2
  %99 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %0, i64 0, i32 1, i32 0, i64 4
  %100 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %0, i64 0, i32 0, i32 0, i64 4
  %101 = getelementptr inbounds %struct.ge_cached, %struct.ge_cached* %8, i64 0, i32 0, i32 0, i64 1
  %102 = getelementptr inbounds %struct.ge_cached, %struct.ge_cached* %8, i64 0, i32 0, i32 0, i64 2
  %103 = getelementptr inbounds %struct.ge_cached, %struct.ge_cached* %8, i64 0, i32 0, i32 0, i64 3
  %104 = getelementptr inbounds %struct.ge_cached, %struct.ge_cached* %8, i64 0, i32 0, i32 0, i64 4
  %105 = getelementptr inbounds %struct.ge_cached, %struct.ge_cached* %8, i64 0, i32 1, i32 0, i64 0
  %106 = getelementptr inbounds %struct.ge_cached, %struct.ge_cached* %8, i64 0, i32 1, i32 0, i64 2
  %107 = getelementptr inbounds %struct.ge_cached, %struct.ge_cached* %8, i64 0, i32 1, i32 0, i64 4
  %108 = getelementptr inbounds %struct.ge_cached, %struct.ge_cached* %8, i64 0, i32 2
  %109 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %0, i64 0, i32 2
  %110 = bitcast %struct.fe_loose* %108 to i8*
  %111 = bitcast %struct.fe* %109 to i8*
  %112 = getelementptr inbounds %struct.ge_cached, %struct.ge_cached* %8, i64 0, i32 3, i32 0, i64 0
  %113 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %0, i64 0, i32 3, i32 0, i64 0
  %114 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %9, i64 0, i32 0, i32 0, i64 0
  %115 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %9, i64 0, i32 3, i32 0, i64 0
  %116 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %9, i64 0, i32 1, i32 0, i64 0
  %117 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %9, i64 0, i32 2, i32 0, i64 0
  %118 = getelementptr inbounds %struct.ge_cached, %struct.ge_cached* %8, i64 0, i32 3
  %119 = bitcast %struct.fe_loose* %118 to i8*
  %120 = bitcast %struct.ge_precomp* %7 to <2 x i64>*
  %121 = bitcast i64* %87 to <2 x i64>*
  %122 = bitcast i64* %88 to <2 x i64>*
  %123 = bitcast i64* %82 to <2 x i64>*
  %124 = bitcast i64* %89 to <2 x i64>*
  %125 = bitcast i64* %90 to <2 x i64>*
  %126 = bitcast i64* %91 to <2 x i64>*
  %127 = bitcast i64* %72 to <2 x i64>*
  %128 = bitcast %struct.ge_p3* %0 to <2 x i64>*
  %129 = bitcast i64* %97 to <2 x i64>*
  %130 = bitcast i64* %98 to <2 x i64>*
  %131 = bitcast i64* %105 to <2 x i64>*
  %132 = bitcast i64* %106 to <2 x i64>*
  br label %133

133:                                              ; preds = %68, %238
  %134 = phi i32 [ 63, %68 ], [ %264, %238 ]
  %135 = lshr i32 %134, 3
  %136 = and i32 %134, 7
  %137 = zext i32 %135 to i64
  %138 = getelementptr inbounds i8, i8* %1, i64 %137
  %139 = load i8, i8* %138, align 1
  %140 = zext i8 %139 to i32
  %141 = lshr i32 %140, %136
  %142 = and i32 %141, 1
  %143 = add nuw nsw i64 %137, 8
  %144 = getelementptr inbounds i8, i8* %1, i64 %143
  %145 = load i8, i8* %144, align 1
  %146 = zext i8 %145 to i32
  %147 = lshr i32 %146, %136
  %148 = shl nuw nsw i32 %147, 1
  %149 = and i32 %148, 2
  %150 = or i32 %142, %149
  %151 = add nuw nsw i64 %137, 16
  %152 = getelementptr inbounds i8, i8* %1, i64 %151
  %153 = load i8, i8* %152, align 1
  %154 = zext i8 %153 to i32
  %155 = lshr i32 %154, %136
  %156 = shl nuw nsw i32 %155, 2
  %157 = and i32 %156, 4
  %158 = or i32 %150, %157
  %159 = add nuw nsw i64 %137, 24
  %160 = getelementptr inbounds i8, i8* %1, i64 %159
  %161 = load i8, i8* %160, align 1
  %162 = zext i8 %161 to i32
  %163 = lshr i32 %162, %136
  %164 = shl nuw nsw i32 %163, 3
  %165 = and i32 %164, 8
  %166 = or i32 %158, %165
  call void @llvm.lifetime.start.p0i8(i64 120, i8* nonnull %78) #4
  call void @llvm.memset.p0i8.i64(i8* align 8 %80, i8 0, i64 32, i1 false) #4
  store i64 1, i64* %81, align 16
  call void @llvm.memset.p0i8.i64(i8* align 8 %83, i8 0, i64 32, i1 false) #4
  store i64 1, i64* %84, align 8
  call void @llvm.memset.p0i8.i64(i8* align 16 %86, i8 0, i64 40, i1 false) #4
  br label %167

167:                                              ; preds = %167, %133
  %168 = phi i64 [ 1, %133 ], [ %235, %167 ]
  %169 = phi i64 [ 0, %133 ], [ %234, %167 ]
  %170 = phi i32 [ 1, %133 ], [ %236, %167 ]
  %171 = phi <2 x i64> [ <i64 1, i64 0>, %133 ], [ %193, %167 ]
  %172 = phi <2 x i64> [ zeroinitializer, %133 ], [ %199, %167 ]
  %173 = phi <2 x i64> [ <i64 0, i64 1>, %133 ], [ %205, %167 ]
  %174 = phi <2 x i64> [ zeroinitializer, %133 ], [ %211, %167 ]
  %175 = phi <2 x i64> [ zeroinitializer, %133 ], [ %217, %167 ]
  %176 = phi <2 x i64> [ zeroinitializer, %133 ], [ %223, %167 ]
  %177 = phi <2 x i64> [ zeroinitializer, %133 ], [ %229, %167 ]
  %178 = add nsw i64 %168, -1
  %179 = and i32 %170, 255
  %180 = xor i32 %179, %166
  %181 = zext i32 %180 to i64
  %182 = add nuw nsw i64 %181, 4294967295
  %183 = lshr i64 %182, 31
  %184 = and i64 %183, 1
  %185 = sub nsw i64 0, %184
  %186 = getelementptr inbounds [15 x %struct.ge_precomp], [15 x %struct.ge_precomp]* %4, i64 0, i64 %178, i32 0, i32 0, i64 0
  %187 = bitcast i64* %186 to <2 x i64>*
  %188 = load <2 x i64>, <2 x i64>* %187, align 8
  %189 = xor <2 x i64> %188, %171
  %190 = insertelement <2 x i64> undef, i64 %185, i32 0
  %191 = shufflevector <2 x i64> %190, <2 x i64> undef, <2 x i32> zeroinitializer
  %192 = and <2 x i64> %189, %191
  %193 = xor <2 x i64> %192, %171
  %194 = getelementptr inbounds [15 x %struct.ge_precomp], [15 x %struct.ge_precomp]* %4, i64 0, i64 %178, i32 0, i32 0, i64 2
  %195 = bitcast i64* %194 to <2 x i64>*
  %196 = load <2 x i64>, <2 x i64>* %195, align 8
  %197 = xor <2 x i64> %196, %172
  %198 = and <2 x i64> %197, %191
  %199 = xor <2 x i64> %198, %172
  %200 = getelementptr inbounds [15 x %struct.ge_precomp], [15 x %struct.ge_precomp]* %4, i64 0, i64 %178, i32 0, i32 0, i64 4
  %201 = bitcast i64* %200 to <2 x i64>*
  %202 = load <2 x i64>, <2 x i64>* %201, align 8
  %203 = xor <2 x i64> %202, %173
  %204 = and <2 x i64> %203, %191
  %205 = xor <2 x i64> %204, %173
  %206 = getelementptr inbounds [15 x %struct.ge_precomp], [15 x %struct.ge_precomp]* %4, i64 0, i64 %178, i32 1, i32 0, i64 1
  %207 = bitcast i64* %206 to <2 x i64>*
  %208 = load <2 x i64>, <2 x i64>* %207, align 8
  %209 = xor <2 x i64> %208, %174
  %210 = and <2 x i64> %209, %191
  %211 = xor <2 x i64> %210, %174
  %212 = getelementptr inbounds [15 x %struct.ge_precomp], [15 x %struct.ge_precomp]* %4, i64 0, i64 %178, i32 1, i32 0, i64 3
  %213 = bitcast i64* %212 to <2 x i64>*
  %214 = load <2 x i64>, <2 x i64>* %213, align 8
  %215 = xor <2 x i64> %214, %175
  %216 = and <2 x i64> %215, %191
  %217 = xor <2 x i64> %216, %175
  %218 = getelementptr inbounds [15 x %struct.ge_precomp], [15 x %struct.ge_precomp]* %4, i64 0, i64 %178, i32 2, i32 0, i64 0
  %219 = bitcast i64* %218 to <2 x i64>*
  %220 = load <2 x i64>, <2 x i64>* %219, align 8
  %221 = xor <2 x i64> %220, %176
  %222 = and <2 x i64> %221, %191
  %223 = xor <2 x i64> %222, %176
  %224 = getelementptr inbounds [15 x %struct.ge_precomp], [15 x %struct.ge_precomp]* %4, i64 0, i64 %178, i32 2, i32 0, i64 2
  %225 = bitcast i64* %224 to <2 x i64>*
  %226 = load <2 x i64>, <2 x i64>* %225, align 8
  %227 = xor <2 x i64> %226, %177
  %228 = and <2 x i64> %227, %191
  %229 = xor <2 x i64> %228, %177
  %230 = getelementptr inbounds [15 x %struct.ge_precomp], [15 x %struct.ge_precomp]* %4, i64 0, i64 %178, i32 2, i32 0, i64 4
  %231 = load i64, i64* %230, align 8
  %232 = xor i64 %231, %169
  %233 = and i64 %232, %185
  %234 = xor i64 %233, %169
  %235 = add nuw nsw i64 %168, 1
  %236 = add nuw nsw i32 %170, 1
  %237 = icmp eq i64 %235, 16
  br i1 %237, label %238, label %167

238:                                              ; preds = %167
  store <2 x i64> %193, <2 x i64>* %120, align 16
  store <2 x i64> %199, <2 x i64>* %121, align 16
  store <2 x i64> %205, <2 x i64>* %122, align 16
  store <2 x i64> %211, <2 x i64>* %123, align 8
  store <2 x i64> %217, <2 x i64>* %124, align 8
  store <2 x i64> %223, <2 x i64>* %125, align 16
  store <2 x i64> %229, <2 x i64>* %126, align 16
  store i64 %234, i64* %92, align 16
  call void @llvm.lifetime.start.p0i8(i64 160, i8* nonnull %93) #4
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %119, i8 -86, i64 40, i1 false)
  call void @llvm.lifetime.start.p0i8(i64 160, i8* nonnull %94) #4
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %94, i8 -86, i64 160, i1 false)
  %239 = load <2 x i64>, <2 x i64>* %127, align 8
  %240 = load <2 x i64>, <2 x i64>* %128, align 8
  %241 = extractelement <2 x i64> %239, i32 0
  %242 = extractelement <2 x i64> %240, i32 0
  %243 = add i64 %242, %241
  %244 = extractelement <2 x i64> %239, i32 1
  %245 = extractelement <2 x i64> %240, i32 1
  %246 = add i64 %245, %244
  %247 = load <2 x i64>, <2 x i64>* %129, align 8
  %248 = load <2 x i64>, <2 x i64>* %130, align 8
  %249 = extractelement <2 x i64> %247, i32 0
  %250 = extractelement <2 x i64> %248, i32 0
  %251 = add i64 %250, %249
  %252 = extractelement <2 x i64> %247, i32 1
  %253 = extractelement <2 x i64> %248, i32 1
  %254 = add i64 %253, %252
  %255 = load i64, i64* %99, align 8
  %256 = load i64, i64* %100, align 8
  %257 = add i64 %256, %255
  store i64 %243, i64* %95, align 8
  store i64 %246, i64* %101, align 8
  store i64 %251, i64* %102, align 8
  store i64 %254, i64* %103, align 8
  store i64 %257, i64* %104, align 8
  %258 = add <2 x i64> %239, <i64 4503599627370458, i64 4503599627370494>
  %259 = sub <2 x i64> %258, %240
  %260 = add <2 x i64> %247, <i64 4503599627370494, i64 4503599627370494>
  %261 = sub <2 x i64> %260, %248
  %262 = add i64 %255, 4503599627370494
  %263 = sub i64 %262, %256
  store <2 x i64> %259, <2 x i64>* %131, align 8
  store <2 x i64> %261, <2 x i64>* %132, align 8
  store i64 %263, i64* %107, align 8
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 8 %110, i8* align 1 %111, i64 40, i1 false) #4
  call fastcc void @fe_mul_impl(i64* %112, i64* %113, i64* getelementptr inbounds (%struct.fe, %struct.fe* @d2, i64 0, i32 0, i64 0)) #4
  call void @x25519_ge_add(%struct.ge_p1p1* nonnull %9, %struct.ge_p3* %0, %struct.ge_cached* nonnull %8)
  call fastcc void @fe_mul_impl(i64* %96, i64* nonnull %114, i64* %115) #4
  call fastcc void @fe_mul_impl(i64* %72, i64* %116, i64* %117) #4
  call fastcc void @fe_mul_impl(i64* %75, i64* %117, i64* %115) #4
  call fastcc void @fe_mul_impl(i64* %113, i64* nonnull %114, i64* %116) #4
  call fastcc void @ge_madd(%struct.ge_p1p1* nonnull %9, %struct.ge_p3* %0, %struct.ge_precomp* nonnull %7)
  call fastcc void @fe_mul_impl(i64* %96, i64* nonnull %114, i64* %115) #4
  call fastcc void @fe_mul_impl(i64* %72, i64* %116, i64* %117) #4
  call fastcc void @fe_mul_impl(i64* %75, i64* %117, i64* %115) #4
  call fastcc void @fe_mul_impl(i64* %113, i64* nonnull %114, i64* %116) #4
  call void @llvm.lifetime.end.p0i8(i64 160, i8* nonnull %94) #4
  call void @llvm.lifetime.end.p0i8(i64 160, i8* nonnull %93) #4
  call void @llvm.lifetime.end.p0i8(i64 120, i8* nonnull %78) #4
  %264 = add nsw i32 %134, -1
  %265 = icmp ult i32 %264, 64
  br i1 %265, label %133, label %266

266:                                              ; preds = %238
  call void @llvm.lifetime.end.p0i8(i64 1800, i8* nonnull %10) #4
  ret void
}

; Function Attrs: nofree norecurse nounwind ssp uwtable
define internal fastcc void @fe_frombytes_strict(%struct.fe* nocapture, i8* readonly) unnamed_addr #2 {
  %3 = getelementptr inbounds %struct.fe, %struct.fe* %0, i64 0, i32 0, i64 0
  %4 = getelementptr inbounds i8, i8* %1, i64 31
  %5 = load i8, i8* %4, align 1
  %6 = zext i8 %5 to i64
  %7 = shl nuw nsw i64 %6, 44
  %8 = getelementptr inbounds i8, i8* %1, i64 30
  %9 = load i8, i8* %8, align 1
  %10 = zext i8 %9 to i64
  %11 = shl nuw nsw i64 %10, 36
  %12 = getelementptr inbounds i8, i8* %1, i64 29
  %13 = load i8, i8* %12, align 1
  %14 = zext i8 %13 to i64
  %15 = shl nuw nsw i64 %14, 28
  %16 = getelementptr inbounds i8, i8* %1, i64 28
  %17 = load i8, i8* %16, align 1
  %18 = zext i8 %17 to i64
  %19 = shl nuw nsw i64 %18, 20
  %20 = getelementptr inbounds i8, i8* %1, i64 27
  %21 = load i8, i8* %20, align 1
  %22 = zext i8 %21 to i64
  %23 = shl nuw nsw i64 %22, 12
  %24 = getelementptr inbounds i8, i8* %1, i64 26
  %25 = load i8, i8* %24, align 1
  %26 = zext i8 %25 to i64
  %27 = shl nuw nsw i64 %26, 4
  %28 = getelementptr inbounds i8, i8* %1, i64 25
  %29 = load i8, i8* %28, align 1
  %30 = zext i8 %29 to i64
  %31 = shl nuw nsw i64 %30, 47
  %32 = getelementptr inbounds i8, i8* %1, i64 24
  %33 = load i8, i8* %32, align 1
  %34 = zext i8 %33 to i64
  %35 = shl nuw nsw i64 %34, 39
  %36 = getelementptr inbounds i8, i8* %1, i64 23
  %37 = load i8, i8* %36, align 1
  %38 = zext i8 %37 to i64
  %39 = shl nuw nsw i64 %38, 31
  %40 = getelementptr inbounds i8, i8* %1, i64 22
  %41 = load i8, i8* %40, align 1
  %42 = zext i8 %41 to i64
  %43 = shl nuw nsw i64 %42, 23
  %44 = getelementptr inbounds i8, i8* %1, i64 21
  %45 = load i8, i8* %44, align 1
  %46 = zext i8 %45 to i64
  %47 = shl nuw nsw i64 %46, 15
  %48 = getelementptr inbounds i8, i8* %1, i64 20
  %49 = load i8, i8* %48, align 1
  %50 = zext i8 %49 to i64
  %51 = shl nuw nsw i64 %50, 7
  %52 = getelementptr inbounds i8, i8* %1, i64 19
  %53 = load i8, i8* %52, align 1
  %54 = zext i8 %53 to i64
  %55 = shl nuw nsw i64 %54, 50
  %56 = getelementptr inbounds i8, i8* %1, i64 18
  %57 = load i8, i8* %56, align 1
  %58 = zext i8 %57 to i64
  %59 = shl nuw nsw i64 %58, 42
  %60 = getelementptr inbounds i8, i8* %1, i64 17
  %61 = load i8, i8* %60, align 1
  %62 = zext i8 %61 to i64
  %63 = shl nuw nsw i64 %62, 34
  %64 = getelementptr inbounds i8, i8* %1, i64 16
  %65 = load i8, i8* %64, align 1
  %66 = zext i8 %65 to i64
  %67 = shl nuw nsw i64 %66, 26
  %68 = getelementptr inbounds i8, i8* %1, i64 15
  %69 = load i8, i8* %68, align 1
  %70 = zext i8 %69 to i64
  %71 = shl nuw nsw i64 %70, 18
  %72 = getelementptr inbounds i8, i8* %1, i64 14
  %73 = load i8, i8* %72, align 1
  %74 = zext i8 %73 to i64
  %75 = shl nuw nsw i64 %74, 10
  %76 = getelementptr inbounds i8, i8* %1, i64 13
  %77 = load i8, i8* %76, align 1
  %78 = zext i8 %77 to i64
  %79 = shl nuw nsw i64 %78, 2
  %80 = getelementptr inbounds i8, i8* %1, i64 12
  %81 = load i8, i8* %80, align 1
  %82 = zext i8 %81 to i64
  %83 = shl nuw nsw i64 %82, 45
  %84 = getelementptr inbounds i8, i8* %1, i64 11
  %85 = load i8, i8* %84, align 1
  %86 = zext i8 %85 to i64
  %87 = shl nuw nsw i64 %86, 37
  %88 = getelementptr inbounds i8, i8* %1, i64 10
  %89 = load i8, i8* %88, align 1
  %90 = zext i8 %89 to i64
  %91 = shl nuw nsw i64 %90, 29
  %92 = getelementptr inbounds i8, i8* %1, i64 9
  %93 = load i8, i8* %92, align 1
  %94 = zext i8 %93 to i64
  %95 = shl nuw nsw i64 %94, 21
  %96 = getelementptr inbounds i8, i8* %1, i64 8
  %97 = load i8, i8* %96, align 1
  %98 = zext i8 %97 to i64
  %99 = shl nuw nsw i64 %98, 13
  %100 = getelementptr inbounds i8, i8* %1, i64 7
  %101 = load i8, i8* %100, align 1
  %102 = zext i8 %101 to i64
  %103 = shl nuw nsw i64 %102, 5
  %104 = getelementptr inbounds i8, i8* %1, i64 6
  %105 = load i8, i8* %104, align 1
  %106 = zext i8 %105 to i64
  %107 = shl nuw nsw i64 %106, 48
  %108 = getelementptr inbounds i8, i8* %1, i64 5
  %109 = load i8, i8* %108, align 1
  %110 = zext i8 %109 to i64
  %111 = shl nuw nsw i64 %110, 40
  %112 = getelementptr inbounds i8, i8* %1, i64 4
  %113 = load i8, i8* %112, align 1
  %114 = zext i8 %113 to i64
  %115 = shl nuw nsw i64 %114, 32
  %116 = getelementptr inbounds i8, i8* %1, i64 3
  %117 = load i8, i8* %116, align 1
  %118 = zext i8 %117 to i64
  %119 = shl nuw nsw i64 %118, 24
  %120 = getelementptr inbounds i8, i8* %1, i64 2
  %121 = load i8, i8* %120, align 1
  %122 = zext i8 %121 to i64
  %123 = shl nuw nsw i64 %122, 16
  %124 = getelementptr inbounds i8, i8* %1, i64 1
  %125 = load i8, i8* %124, align 1
  %126 = zext i8 %125 to i64
  %127 = shl nuw nsw i64 %126, 8
  %128 = load i8, i8* %1, align 1
  %129 = zext i8 %128 to i64
  %130 = lshr i8 %105, 3
  %131 = and i64 %107, 1970324836974592
  %132 = or i64 %111, %131
  %133 = or i64 %132, %115
  %134 = or i64 %133, %119
  %135 = or i64 %134, %123
  %136 = or i64 %135, %127
  %137 = or i64 %136, %129
  %138 = or i64 %11, %7
  %139 = or i64 %138, %15
  %140 = or i64 %139, %19
  %141 = or i64 %140, %23
  %142 = or i64 %141, %27
  %143 = or i64 %35, %31
  %144 = or i64 %143, %39
  %145 = or i64 %144, %43
  %146 = or i64 %145, %47
  %147 = or i64 %146, %51
  %148 = or i64 %59, %55
  %149 = or i64 %148, %63
  %150 = or i64 %149, %67
  %151 = or i64 %150, %71
  %152 = or i64 %151, %75
  %153 = or i64 %152, %79
  %154 = zext i8 %130 to i64
  %155 = lshr i8 %81, 6
  %156 = and i64 %83, 2216615441596416
  %157 = or i64 %87, %156
  %158 = or i64 %157, %91
  %159 = or i64 %158, %95
  %160 = or i64 %159, %99
  %161 = or i64 %160, %103
  %162 = or i64 %161, %154
  %163 = zext i8 %155 to i64
  %164 = add i64 %153, %163
  %165 = lshr i64 %164, 51
  %166 = and i64 %164, 2251799813685247
  %167 = and i64 %165, 255
  %168 = add i64 %167, %147
  %169 = lshr i64 %168, 51
  %170 = and i64 %168, 2251799813685247
  %171 = and i64 %169, 255
  %172 = add i64 %171, %142
  store i64 %137, i64* %3, align 8
  %173 = getelementptr inbounds %struct.fe, %struct.fe* %0, i64 0, i32 0, i64 1
  store i64 %162, i64* %173, align 8
  %174 = getelementptr inbounds %struct.fe, %struct.fe* %0, i64 0, i32 0, i64 2
  store i64 %166, i64* %174, align 8
  %175 = getelementptr inbounds %struct.fe, %struct.fe* %0, i64 0, i32 0, i64 3
  store i64 %170, i64* %175, align 8
  %176 = getelementptr inbounds %struct.fe, %struct.fe* %0, i64 0, i32 0, i64 4
  store i64 %172, i64* %176, align 8
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal fastcc void @ge_madd(%struct.ge_p1p1* nocapture, %struct.ge_p3* nocapture readonly, %struct.ge_precomp* nocapture readonly) unnamed_addr #0 {
  %4 = alloca %struct.fe, align 16
  %5 = alloca %struct.fe, align 16
  %6 = alloca %struct.fe, align 8
  %7 = bitcast %struct.fe* %4 to i8*
  call void @llvm.lifetime.start.p0i8(i64 40, i8* nonnull %7) #4
  %8 = getelementptr inbounds %struct.fe, %struct.fe* %4, i64 0, i32 0, i64 0
  %9 = getelementptr inbounds %struct.fe, %struct.fe* %4, i64 0, i32 0, i64 2
  %10 = getelementptr inbounds %struct.fe, %struct.fe* %4, i64 0, i32 0, i64 3
  %11 = bitcast %struct.fe* %5 to i8*
  %12 = bitcast %struct.fe* %4 to i8*
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %12, i8 -86, i64 40, i1 false)
  call void @llvm.lifetime.start.p0i8(i64 40, i8* nonnull %11) #4
  %13 = getelementptr inbounds %struct.fe, %struct.fe* %5, i64 0, i32 0, i64 0
  %14 = getelementptr inbounds %struct.fe, %struct.fe* %5, i64 0, i32 0, i64 2
  %15 = getelementptr inbounds %struct.fe, %struct.fe* %5, i64 0, i32 0, i64 3
  %16 = bitcast %struct.fe* %6 to i8*
  %17 = bitcast %struct.fe* %5 to i8*
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %17, i8 -86, i64 40, i1 false)
  call void @llvm.lifetime.start.p0i8(i64 40, i8* nonnull %16) #4
  %18 = getelementptr inbounds %struct.fe, %struct.fe* %6, i64 0, i32 0, i64 0
  %19 = getelementptr inbounds %struct.fe, %struct.fe* %6, i64 0, i32 0, i64 1
  %20 = getelementptr inbounds %struct.fe, %struct.fe* %6, i64 0, i32 0, i64 2
  %21 = getelementptr inbounds %struct.fe, %struct.fe* %6, i64 0, i32 0, i64 3
  %22 = getelementptr inbounds %struct.fe, %struct.fe* %6, i64 0, i32 0, i64 4
  %23 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %0, i64 0, i32 0, i32 0, i64 0
  %24 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %1, i64 0, i32 1, i32 0, i64 0
  %25 = bitcast %struct.fe* %6 to i8*
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %25, i8 -86, i64 40, i1 false)
  %26 = bitcast i64* %24 to <2 x i64>*
  %27 = load <2 x i64>, <2 x i64>* %26, align 8
  %28 = bitcast %struct.ge_p3* %1 to <2 x i64>*
  %29 = load <2 x i64>, <2 x i64>* %28, align 8
  %30 = add <2 x i64> %29, %27
  %31 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %1, i64 0, i32 1, i32 0, i64 2
  %32 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %1, i64 0, i32 0, i32 0, i64 2
  %33 = bitcast i64* %31 to <2 x i64>*
  %34 = load <2 x i64>, <2 x i64>* %33, align 8
  %35 = bitcast i64* %32 to <2 x i64>*
  %36 = load <2 x i64>, <2 x i64>* %35, align 8
  %37 = add <2 x i64> %36, %34
  %38 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %1, i64 0, i32 1, i32 0, i64 4
  %39 = load i64, i64* %38, align 8
  %40 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %1, i64 0, i32 0, i32 0, i64 4
  %41 = load i64, i64* %40, align 8
  %42 = add i64 %41, %39
  %43 = bitcast %struct.ge_p1p1* %0 to <2 x i64>*
  store <2 x i64> %30, <2 x i64>* %43, align 8
  %44 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %0, i64 0, i32 0, i32 0, i64 2
  %45 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %0, i64 0, i32 0, i32 0, i64 3
  %46 = bitcast i64* %44 to <2 x i64>*
  store <2 x i64> %37, <2 x i64>* %46, align 8
  %47 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %0, i64 0, i32 0, i32 0, i64 4
  store i64 %42, i64* %47, align 8
  %48 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %0, i64 0, i32 1, i32 0, i64 0
  %49 = bitcast i64* %24 to <2 x i64>*
  %50 = load <2 x i64>, <2 x i64>* %49, align 8
  %51 = add <2 x i64> %50, <i64 4503599627370458, i64 4503599627370494>
  %52 = bitcast %struct.ge_p3* %1 to <2 x i64>*
  %53 = load <2 x i64>, <2 x i64>* %52, align 8
  %54 = sub <2 x i64> %51, %53
  %55 = bitcast i64* %31 to <2 x i64>*
  %56 = load <2 x i64>, <2 x i64>* %55, align 8
  %57 = add <2 x i64> %56, <i64 4503599627370494, i64 4503599627370494>
  %58 = bitcast i64* %32 to <2 x i64>*
  %59 = load <2 x i64>, <2 x i64>* %58, align 8
  %60 = sub <2 x i64> %57, %59
  %61 = load i64, i64* %38, align 8
  %62 = add i64 %61, 4503599627370494
  %63 = load i64, i64* %40, align 8
  %64 = sub i64 %62, %63
  %65 = bitcast i64* %48 to <2 x i64>*
  store <2 x i64> %54, <2 x i64>* %65, align 8
  %66 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %0, i64 0, i32 1, i32 0, i64 2
  %67 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %0, i64 0, i32 1, i32 0, i64 3
  %68 = bitcast i64* %66 to <2 x i64>*
  store <2 x i64> %60, <2 x i64>* %68, align 8
  %69 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %0, i64 0, i32 1, i32 0, i64 4
  store i64 %64, i64* %69, align 8
  %70 = getelementptr inbounds %struct.ge_precomp, %struct.ge_precomp* %2, i64 0, i32 0, i32 0, i64 0
  call fastcc void @fe_mul_impl(i64* nonnull %13, i64* %23, i64* %70) #4
  %71 = getelementptr inbounds %struct.ge_precomp, %struct.ge_precomp* %2, i64 0, i32 1, i32 0, i64 0
  call fastcc void @fe_mul_impl(i64* nonnull %8, i64* %48, i64* %71) #4
  %72 = getelementptr inbounds %struct.ge_precomp, %struct.ge_precomp* %2, i64 0, i32 2, i32 0, i64 0
  %73 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %1, i64 0, i32 3, i32 0, i64 0
  call fastcc void @fe_mul_impl(i64* nonnull %18, i64* %72, i64* %73) #4
  %74 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %0, i64 0, i32 3, i32 0, i64 0
  %75 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %1, i64 0, i32 2, i32 0, i64 0
  %76 = load i64, i64* %75, align 8
  %77 = shl i64 %76, 1
  %78 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %1, i64 0, i32 2, i32 0, i64 1
  %79 = bitcast i64* %78 to <2 x i64>*
  %80 = load <2 x i64>, <2 x i64>* %79, align 8
  %81 = shl <2 x i64> %80, <i64 1, i64 1>
  %82 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %1, i64 0, i32 2, i32 0, i64 3
  %83 = load i64, i64* %82, align 8
  %84 = shl i64 %83, 1
  %85 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %1, i64 0, i32 2, i32 0, i64 4
  %86 = load i64, i64* %85, align 8
  %87 = shl i64 %86, 1
  store i64 %77, i64* %74, align 8
  %88 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %0, i64 0, i32 3, i32 0, i64 1
  %89 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %0, i64 0, i32 3, i32 0, i64 2
  %90 = bitcast i64* %88 to <2 x i64>*
  store <2 x i64> %81, <2 x i64>* %90, align 8
  %91 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %0, i64 0, i32 3, i32 0, i64 3
  %92 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %0, i64 0, i32 3, i32 0, i64 4
  %93 = bitcast %struct.fe* %5 to <2 x i64>*
  %94 = load <2 x i64>, <2 x i64>* %93, align 16
  %95 = add <2 x i64> %94, <i64 4503599627370458, i64 4503599627370494>
  %96 = bitcast %struct.fe* %4 to <2 x i64>*
  %97 = load <2 x i64>, <2 x i64>* %96, align 16
  %98 = sub <2 x i64> %95, %97
  %99 = load i64, i64* %14, align 16
  %100 = add i64 %99, 4503599627370494
  %101 = load i64, i64* %9, align 16
  %102 = sub i64 %100, %101
  %103 = bitcast i64* %15 to <2 x i64>*
  %104 = load <2 x i64>, <2 x i64>* %103, align 8
  %105 = add <2 x i64> %104, <i64 4503599627370494, i64 4503599627370494>
  %106 = bitcast i64* %10 to <2 x i64>*
  %107 = load <2 x i64>, <2 x i64>* %106, align 8
  %108 = sub <2 x i64> %105, %107
  %109 = bitcast %struct.ge_p1p1* %0 to <2 x i64>*
  store <2 x i64> %98, <2 x i64>* %109, align 8
  store i64 %102, i64* %44, align 8
  %110 = bitcast i64* %45 to <2 x i64>*
  store <2 x i64> %108, <2 x i64>* %110, align 8
  %111 = add <2 x i64> %97, %94
  %112 = add i64 %101, %99
  %113 = add <2 x i64> %107, %104
  %114 = bitcast i64* %48 to <2 x i64>*
  store <2 x i64> %111, <2 x i64>* %114, align 8
  store i64 %112, i64* %66, align 8
  %115 = bitcast i64* %67 to <2 x i64>*
  store <2 x i64> %113, <2 x i64>* %115, align 8
  %116 = lshr i64 %76, 50
  %117 = and i64 %116, 8191
  %118 = extractelement <2 x i64> %81, i32 0
  %119 = add i64 %117, %118
  %120 = lshr i64 %119, 51
  %121 = extractelement <2 x i64> %81, i32 1
  %122 = add i64 %120, %121
  %123 = lshr i64 %122, 51
  %124 = add i64 %123, %84
  %125 = lshr i64 %124, 51
  %126 = add i64 %125, %87
  %127 = and i64 %77, 2251799813685246
  %128 = lshr i64 %126, 51
  %129 = mul nuw nsw i64 %128, 19
  %130 = add nuw nsw i64 %129, %127
  %131 = lshr i64 %130, 51
  %132 = and i64 %119, 2251799813685247
  %133 = add nuw nsw i64 %131, %132
  %134 = and i64 %130, 2251799813685247
  %135 = and i64 %133, 2251799813685247
  %136 = lshr i64 %133, 51
  %137 = and i64 %122, 2251799813685247
  %138 = add nuw nsw i64 %136, %137
  %139 = and i64 %124, 2251799813685247
  %140 = and i64 %126, 2251799813685247
  %141 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %0, i64 0, i32 2, i32 0, i64 0
  %142 = load i64, i64* %18, align 8
  %143 = add i64 %142, %134
  %144 = load i64, i64* %19, align 8
  %145 = add i64 %144, %135
  %146 = load i64, i64* %20, align 8
  %147 = add i64 %146, %138
  %148 = load i64, i64* %21, align 8
  %149 = add i64 %148, %139
  %150 = load i64, i64* %22, align 8
  %151 = add i64 %150, %140
  store i64 %143, i64* %141, align 8
  %152 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %0, i64 0, i32 2, i32 0, i64 1
  store i64 %145, i64* %152, align 8
  %153 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %0, i64 0, i32 2, i32 0, i64 2
  store i64 %147, i64* %153, align 8
  %154 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %0, i64 0, i32 2, i32 0, i64 3
  store i64 %149, i64* %154, align 8
  %155 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %0, i64 0, i32 2, i32 0, i64 4
  store i64 %151, i64* %155, align 8
  %156 = add nuw nsw i64 %134, 4503599627370458
  %157 = sub i64 %156, %142
  %158 = add nuw nsw i64 %135, 4503599627370494
  %159 = sub i64 %158, %144
  %160 = add nuw nsw i64 %138, 4503599627370494
  %161 = sub i64 %160, %146
  %162 = add nuw nsw i64 %139, 4503599627370494
  %163 = sub i64 %162, %148
  %164 = add nuw nsw i64 %140, 4503599627370494
  %165 = sub i64 %164, %150
  store i64 %157, i64* %74, align 8
  store i64 %159, i64* %88, align 8
  store i64 %161, i64* %89, align 8
  store i64 %163, i64* %91, align 8
  store i64 %165, i64* %92, align 8
  call void @llvm.lifetime.end.p0i8(i64 40, i8* nonnull %16) #4
  call void @llvm.lifetime.end.p0i8(i64 40, i8* nonnull %11) #4
  call void @llvm.lifetime.end.p0i8(i64 40, i8* nonnull %7) #4
  ret void
}

; Function Attrs: nounwind ssp uwtable
define hidden void @x25519_ge_scalarmult_base(%struct.ge_p3*, i8* nocapture readonly) local_unnamed_addr #0 {
  tail call void @x25519_ge_scalarmult_small_precomp(%struct.ge_p3* %0, i8* %1, i8* getelementptr inbounds ([960 x i8], [960 x i8]* @k25519SmallPrecomp, i64 0, i64 0))
  ret void
}

; Function Attrs: nounwind ssp uwtable
define hidden void @x25519_ge_scalarmult(%struct.ge_p2*, i8* nocapture readonly, %struct.ge_p3* nocapture readonly) local_unnamed_addr #0 {
  %4 = alloca %struct.ge_p3, align 16
  %5 = alloca %struct.ge_p3, align 16
  %6 = alloca [8 x %struct.ge_p2], align 16
  %7 = alloca [16 x %struct.ge_cached], align 16
  %8 = alloca %struct.ge_p1p1, align 8
  %9 = alloca %struct.ge_p3, align 8
  %10 = alloca %struct.ge_cached, align 16
  %11 = bitcast [8 x %struct.ge_p2]* %6 to i8*
  call void @llvm.lifetime.start.p0i8(i64 960, i8* nonnull %11) #4
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %11, i8 -86, i64 960, i1 false)
  %12 = bitcast [16 x %struct.ge_cached]* %7 to i8*
  call void @llvm.lifetime.start.p0i8(i64 2560, i8* nonnull %12) #4
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %12, i8 -86, i64 2560, i1 false)
  %13 = bitcast %struct.ge_p1p1* %8 to i8*
  call void @llvm.lifetime.start.p0i8(i64 160, i8* nonnull %13) #4
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %13, i8 -86, i64 160, i1 false)
  %14 = getelementptr inbounds [16 x %struct.ge_cached], [16 x %struct.ge_cached]* %7, i64 0, i64 0, i32 0, i32 0, i64 1
  %15 = bitcast i64* %14 to i8*
  call void @llvm.memset.p0i8.i64(i8* align 8 %15, i8 0, i64 32, i1 false) #4
  %16 = getelementptr inbounds [16 x %struct.ge_cached], [16 x %struct.ge_cached]* %7, i64 0, i64 0, i32 0, i32 0, i64 0
  store i64 1, i64* %16, align 16
  %17 = getelementptr inbounds [16 x %struct.ge_cached], [16 x %struct.ge_cached]* %7, i64 0, i64 0, i32 1, i32 0, i64 1
  %18 = bitcast i64* %17 to i8*
  call void @llvm.memset.p0i8.i64(i8* align 8 %18, i8 0, i64 32, i1 false) #4
  %19 = getelementptr inbounds [16 x %struct.ge_cached], [16 x %struct.ge_cached]* %7, i64 0, i64 0, i32 1, i32 0, i64 0
  store i64 1, i64* %19, align 8
  %20 = getelementptr inbounds [16 x %struct.ge_cached], [16 x %struct.ge_cached]* %7, i64 0, i64 0, i32 2, i32 0, i64 1
  %21 = bitcast i64* %20 to i8*
  call void @llvm.memset.p0i8.i64(i8* align 8 %21, i8 0, i64 32, i1 false) #4
  %22 = getelementptr inbounds [16 x %struct.ge_cached], [16 x %struct.ge_cached]* %7, i64 0, i64 0, i32 2, i32 0, i64 0
  store i64 1, i64* %22, align 16
  %23 = getelementptr inbounds [16 x %struct.ge_cached], [16 x %struct.ge_cached]* %7, i64 0, i64 0, i32 3
  %24 = bitcast %struct.fe_loose* %23 to i8*
  call void @llvm.memset.p0i8.i64(i8* align 8 %24, i8 0, i64 40, i1 false) #4
  %25 = getelementptr inbounds [16 x %struct.ge_cached], [16 x %struct.ge_cached]* %7, i64 0, i64 1, i32 0, i32 0, i64 0
  %26 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %2, i64 0, i32 1, i32 0, i64 0
  %27 = bitcast i64* %26 to <2 x i64>*
  %28 = load <2 x i64>, <2 x i64>* %27, align 8
  %29 = bitcast %struct.ge_p3* %2 to <2 x i64>*
  %30 = load <2 x i64>, <2 x i64>* %29, align 8
  %31 = extractelement <2 x i64> %28, i32 0
  %32 = extractelement <2 x i64> %30, i32 0
  %33 = add i64 %32, %31
  %34 = extractelement <2 x i64> %28, i32 1
  %35 = extractelement <2 x i64> %30, i32 1
  %36 = add i64 %35, %34
  %37 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %2, i64 0, i32 1, i32 0, i64 2
  %38 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %2, i64 0, i32 0, i32 0, i64 2
  %39 = bitcast i64* %37 to <2 x i64>*
  %40 = load <2 x i64>, <2 x i64>* %39, align 8
  %41 = bitcast i64* %38 to <2 x i64>*
  %42 = load <2 x i64>, <2 x i64>* %41, align 8
  %43 = extractelement <2 x i64> %40, i32 0
  %44 = extractelement <2 x i64> %42, i32 0
  %45 = add i64 %44, %43
  %46 = extractelement <2 x i64> %40, i32 1
  %47 = extractelement <2 x i64> %42, i32 1
  %48 = add i64 %47, %46
  %49 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %2, i64 0, i32 1, i32 0, i64 4
  %50 = load i64, i64* %49, align 8
  %51 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %2, i64 0, i32 0, i32 0, i64 4
  %52 = load i64, i64* %51, align 8
  %53 = add i64 %52, %50
  store i64 %33, i64* %25, align 16
  %54 = getelementptr inbounds [16 x %struct.ge_cached], [16 x %struct.ge_cached]* %7, i64 0, i64 1, i32 0, i32 0, i64 1
  store i64 %36, i64* %54, align 8
  %55 = getelementptr inbounds [16 x %struct.ge_cached], [16 x %struct.ge_cached]* %7, i64 0, i64 1, i32 0, i32 0, i64 2
  store i64 %45, i64* %55, align 16
  %56 = getelementptr inbounds [16 x %struct.ge_cached], [16 x %struct.ge_cached]* %7, i64 0, i64 1, i32 0, i32 0, i64 3
  store i64 %48, i64* %56, align 8
  %57 = getelementptr inbounds [16 x %struct.ge_cached], [16 x %struct.ge_cached]* %7, i64 0, i64 1, i32 0, i32 0, i64 4
  store i64 %53, i64* %57, align 16
  %58 = getelementptr inbounds [16 x %struct.ge_cached], [16 x %struct.ge_cached]* %7, i64 0, i64 1, i32 1, i32 0, i64 0
  %59 = add <2 x i64> %28, <i64 4503599627370458, i64 4503599627370494>
  %60 = sub <2 x i64> %59, %30
  %61 = add <2 x i64> %40, <i64 4503599627370494, i64 4503599627370494>
  %62 = sub <2 x i64> %61, %42
  %63 = add i64 %50, 4503599627370494
  %64 = sub i64 %63, %52
  %65 = bitcast i64* %58 to <2 x i64>*
  store <2 x i64> %60, <2 x i64>* %65, align 8
  %66 = getelementptr inbounds [16 x %struct.ge_cached], [16 x %struct.ge_cached]* %7, i64 0, i64 1, i32 1, i32 0, i64 2
  %67 = bitcast i64* %66 to <2 x i64>*
  store <2 x i64> %62, <2 x i64>* %67, align 8
  %68 = getelementptr inbounds [16 x %struct.ge_cached], [16 x %struct.ge_cached]* %7, i64 0, i64 1, i32 1, i32 0, i64 4
  store i64 %64, i64* %68, align 8
  %69 = getelementptr inbounds [16 x %struct.ge_cached], [16 x %struct.ge_cached]* %7, i64 0, i64 1, i32 2
  %70 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %2, i64 0, i32 2
  %71 = bitcast %struct.fe_loose* %69 to i8*
  %72 = bitcast %struct.fe* %70 to i8*
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 16 %71, i8* align 1 %72, i64 40, i1 false) #4
  %73 = getelementptr inbounds [16 x %struct.ge_cached], [16 x %struct.ge_cached]* %7, i64 0, i64 1, i32 3, i32 0, i64 0
  %74 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %2, i64 0, i32 3, i32 0, i64 0
  call fastcc void @fe_mul_impl(i64* %73, i64* %74, i64* getelementptr inbounds (%struct.fe, %struct.fe* @d2, i64 0, i32 0, i64 0)) #4
  %75 = getelementptr inbounds [8 x %struct.ge_p2], [8 x %struct.ge_p2]* %6, i64 0, i64 1
  %76 = bitcast %struct.ge_p2* %75 to i8*
  %77 = bitcast %struct.ge_p3* %2 to i8*
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 8 %76, i8* align 1 %77, i64 40, i1 false) #4
  %78 = getelementptr inbounds [8 x %struct.ge_p2], [8 x %struct.ge_p2]* %6, i64 0, i64 1, i32 1
  %79 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %2, i64 0, i32 1
  %80 = bitcast %struct.fe* %78 to i8*
  %81 = bitcast %struct.fe* %79 to i8*
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 8 %80, i8* align 1 %81, i64 40, i1 false) #4
  %82 = getelementptr inbounds [8 x %struct.ge_p2], [8 x %struct.ge_p2]* %6, i64 0, i64 1, i32 2
  %83 = bitcast %struct.fe* %82 to i8*
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 8 %83, i8* align 1 %72, i64 40, i1 false) #4
  %84 = bitcast %struct.ge_p3* %5 to i8*
  %85 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %5, i64 0, i32 0, i32 0, i64 0
  %86 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %8, i64 0, i32 0, i32 0, i64 0
  %87 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %8, i64 0, i32 3, i32 0, i64 0
  %88 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %5, i64 0, i32 1, i32 0, i64 0
  %89 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %8, i64 0, i32 1, i32 0, i64 0
  %90 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %8, i64 0, i32 2, i32 0, i64 0
  %91 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %5, i64 0, i32 2, i32 0, i64 0
  %92 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %5, i64 0, i32 3, i32 0, i64 0
  %93 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %5, i64 0, i32 1, i32 0, i64 2
  %94 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %5, i64 0, i32 0, i32 0, i64 2
  %95 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %5, i64 0, i32 1, i32 0, i64 4
  %96 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %5, i64 0, i32 0, i32 0, i64 4
  %97 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %5, i64 0, i32 2
  %98 = bitcast %struct.fe* %97 to i8*
  %99 = bitcast %struct.ge_p3* %4 to i8*
  %100 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %4, i64 0, i32 0, i32 0, i64 0
  %101 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %4, i64 0, i32 1, i32 0, i64 0
  %102 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %4, i64 0, i32 2, i32 0, i64 0
  %103 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %4, i64 0, i32 3, i32 0, i64 0
  %104 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %4, i64 0, i32 1, i32 0, i64 2
  %105 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %4, i64 0, i32 0, i32 0, i64 2
  %106 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %4, i64 0, i32 1, i32 0, i64 4
  %107 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %4, i64 0, i32 0, i32 0, i64 4
  %108 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %4, i64 0, i32 2
  %109 = bitcast %struct.fe* %108 to i8*
  %110 = bitcast i64* %88 to <2 x i64>*
  %111 = bitcast %struct.ge_p3* %5 to <2 x i64>*
  %112 = bitcast i64* %93 to <2 x i64>*
  %113 = bitcast i64* %94 to <2 x i64>*
  %114 = bitcast i64* %101 to <2 x i64>*
  %115 = bitcast %struct.ge_p3* %4 to <2 x i64>*
  %116 = bitcast i64* %104 to <2 x i64>*
  %117 = bitcast i64* %105 to <2 x i64>*
  br label %118

118:                                              ; preds = %3, %211
  %119 = phi i64 [ 2, %3 ], [ %212, %211 ]
  %120 = lshr exact i64 %119, 1
  %121 = getelementptr inbounds [8 x %struct.ge_p2], [8 x %struct.ge_p2]* %6, i64 0, i64 %120
  call fastcc void @ge_p2_dbl(%struct.ge_p1p1* nonnull %8, %struct.ge_p2* %121)
  %122 = getelementptr inbounds [16 x %struct.ge_cached], [16 x %struct.ge_cached]* %7, i64 0, i64 %119
  call void @llvm.lifetime.start.p0i8(i64 160, i8* nonnull %84) #4
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %84, i8 -86, i64 160, i1 false) #4
  call fastcc void @fe_mul_impl(i64* nonnull %85, i64* nonnull %86, i64* %87) #4
  call fastcc void @fe_mul_impl(i64* %88, i64* %89, i64* %90) #4
  call fastcc void @fe_mul_impl(i64* %91, i64* %90, i64* %87) #4
  call fastcc void @fe_mul_impl(i64* %92, i64* nonnull %86, i64* %89) #4
  %123 = getelementptr inbounds %struct.ge_cached, %struct.ge_cached* %122, i64 0, i32 0, i32 0, i64 0
  %124 = load <2 x i64>, <2 x i64>* %110, align 8
  %125 = load <2 x i64>, <2 x i64>* %111, align 16
  %126 = extractelement <2 x i64> %124, i32 0
  %127 = extractelement <2 x i64> %125, i32 0
  %128 = add i64 %127, %126
  %129 = extractelement <2 x i64> %124, i32 1
  %130 = extractelement <2 x i64> %125, i32 1
  %131 = add i64 %130, %129
  %132 = load <2 x i64>, <2 x i64>* %112, align 8
  %133 = load <2 x i64>, <2 x i64>* %113, align 16
  %134 = extractelement <2 x i64> %132, i32 0
  %135 = extractelement <2 x i64> %133, i32 0
  %136 = add i64 %135, %134
  %137 = extractelement <2 x i64> %132, i32 1
  %138 = extractelement <2 x i64> %133, i32 1
  %139 = add i64 %138, %137
  %140 = load i64, i64* %95, align 8
  %141 = load i64, i64* %96, align 16
  %142 = add i64 %141, %140
  store i64 %128, i64* %123, align 16
  %143 = getelementptr inbounds [16 x %struct.ge_cached], [16 x %struct.ge_cached]* %7, i64 0, i64 %119, i32 0, i32 0, i64 1
  store i64 %131, i64* %143, align 8
  %144 = getelementptr inbounds [16 x %struct.ge_cached], [16 x %struct.ge_cached]* %7, i64 0, i64 %119, i32 0, i32 0, i64 2
  store i64 %136, i64* %144, align 16
  %145 = getelementptr inbounds [16 x %struct.ge_cached], [16 x %struct.ge_cached]* %7, i64 0, i64 %119, i32 0, i32 0, i64 3
  store i64 %139, i64* %145, align 8
  %146 = getelementptr inbounds [16 x %struct.ge_cached], [16 x %struct.ge_cached]* %7, i64 0, i64 %119, i32 0, i32 0, i64 4
  store i64 %142, i64* %146, align 16
  %147 = getelementptr inbounds [16 x %struct.ge_cached], [16 x %struct.ge_cached]* %7, i64 0, i64 %119, i32 1, i32 0, i64 0
  %148 = add <2 x i64> %124, <i64 4503599627370458, i64 4503599627370494>
  %149 = sub <2 x i64> %148, %125
  %150 = add <2 x i64> %132, <i64 4503599627370494, i64 4503599627370494>
  %151 = sub <2 x i64> %150, %133
  %152 = add i64 %140, 4503599627370494
  %153 = sub i64 %152, %141
  %154 = bitcast i64* %147 to <2 x i64>*
  store <2 x i64> %149, <2 x i64>* %154, align 8
  %155 = getelementptr inbounds [16 x %struct.ge_cached], [16 x %struct.ge_cached]* %7, i64 0, i64 %119, i32 1, i32 0, i64 2
  %156 = bitcast i64* %155 to <2 x i64>*
  store <2 x i64> %151, <2 x i64>* %156, align 8
  %157 = getelementptr inbounds [16 x %struct.ge_cached], [16 x %struct.ge_cached]* %7, i64 0, i64 %119, i32 1, i32 0, i64 4
  store i64 %153, i64* %157, align 8
  %158 = getelementptr inbounds [16 x %struct.ge_cached], [16 x %struct.ge_cached]* %7, i64 0, i64 %119, i32 2
  %159 = bitcast %struct.fe_loose* %158 to i8*
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 16 %159, i8* align 16 %98, i64 40, i1 false) #4
  %160 = getelementptr inbounds [16 x %struct.ge_cached], [16 x %struct.ge_cached]* %7, i64 0, i64 %119, i32 3, i32 0, i64 0
  call fastcc void @fe_mul_impl(i64* %160, i64* %92, i64* getelementptr inbounds (%struct.fe, %struct.fe* @d2, i64 0, i32 0, i64 0)) #4
  call void @llvm.lifetime.end.p0i8(i64 160, i8* nonnull %84) #4
  %161 = icmp ult i64 %119, 8
  br i1 %161, label %162, label %166

162:                                              ; preds = %118
  %163 = getelementptr inbounds [8 x %struct.ge_p2], [8 x %struct.ge_p2]* %6, i64 0, i64 %119, i32 0, i32 0, i64 0
  call fastcc void @fe_mul_impl(i64* %163, i64* nonnull %86, i64* %87) #4
  %164 = getelementptr inbounds [8 x %struct.ge_p2], [8 x %struct.ge_p2]* %6, i64 0, i64 %119, i32 1, i32 0, i64 0
  call fastcc void @fe_mul_impl(i64* %164, i64* %89, i64* %90) #4
  %165 = getelementptr inbounds [8 x %struct.ge_p2], [8 x %struct.ge_p2]* %6, i64 0, i64 %119, i32 2, i32 0, i64 0
  call fastcc void @fe_mul_impl(i64* %165, i64* %90, i64* %87) #4
  br label %166

166:                                              ; preds = %162, %118
  call void @x25519_ge_add(%struct.ge_p1p1* nonnull %8, %struct.ge_p3* %2, %struct.ge_cached* %122)
  %167 = or i64 %119, 1
  call void @llvm.lifetime.start.p0i8(i64 160, i8* nonnull %99) #4
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %99, i8 -86, i64 160, i1 false) #4
  call fastcc void @fe_mul_impl(i64* nonnull %100, i64* nonnull %86, i64* %87) #4
  call fastcc void @fe_mul_impl(i64* %101, i64* %89, i64* %90) #4
  call fastcc void @fe_mul_impl(i64* %102, i64* %90, i64* %87) #4
  call fastcc void @fe_mul_impl(i64* %103, i64* nonnull %86, i64* %89) #4
  %168 = getelementptr inbounds [16 x %struct.ge_cached], [16 x %struct.ge_cached]* %7, i64 0, i64 %167, i32 0, i32 0, i64 0
  %169 = load <2 x i64>, <2 x i64>* %114, align 8
  %170 = load <2 x i64>, <2 x i64>* %115, align 16
  %171 = extractelement <2 x i64> %169, i32 0
  %172 = extractelement <2 x i64> %170, i32 0
  %173 = add i64 %172, %171
  %174 = extractelement <2 x i64> %169, i32 1
  %175 = extractelement <2 x i64> %170, i32 1
  %176 = add i64 %175, %174
  %177 = load <2 x i64>, <2 x i64>* %116, align 8
  %178 = load <2 x i64>, <2 x i64>* %117, align 16
  %179 = extractelement <2 x i64> %177, i32 0
  %180 = extractelement <2 x i64> %178, i32 0
  %181 = add i64 %180, %179
  %182 = extractelement <2 x i64> %177, i32 1
  %183 = extractelement <2 x i64> %178, i32 1
  %184 = add i64 %183, %182
  %185 = load i64, i64* %106, align 8
  %186 = load i64, i64* %107, align 16
  %187 = add i64 %186, %185
  store i64 %173, i64* %168, align 16
  %188 = getelementptr inbounds [16 x %struct.ge_cached], [16 x %struct.ge_cached]* %7, i64 0, i64 %167, i32 0, i32 0, i64 1
  store i64 %176, i64* %188, align 8
  %189 = getelementptr inbounds [16 x %struct.ge_cached], [16 x %struct.ge_cached]* %7, i64 0, i64 %167, i32 0, i32 0, i64 2
  store i64 %181, i64* %189, align 16
  %190 = getelementptr inbounds [16 x %struct.ge_cached], [16 x %struct.ge_cached]* %7, i64 0, i64 %167, i32 0, i32 0, i64 3
  store i64 %184, i64* %190, align 8
  %191 = getelementptr inbounds [16 x %struct.ge_cached], [16 x %struct.ge_cached]* %7, i64 0, i64 %167, i32 0, i32 0, i64 4
  store i64 %187, i64* %191, align 16
  %192 = getelementptr inbounds [16 x %struct.ge_cached], [16 x %struct.ge_cached]* %7, i64 0, i64 %167, i32 1, i32 0, i64 0
  %193 = add <2 x i64> %169, <i64 4503599627370458, i64 4503599627370494>
  %194 = sub <2 x i64> %193, %170
  %195 = add <2 x i64> %177, <i64 4503599627370494, i64 4503599627370494>
  %196 = sub <2 x i64> %195, %178
  %197 = add i64 %185, 4503599627370494
  %198 = sub i64 %197, %186
  %199 = bitcast i64* %192 to <2 x i64>*
  store <2 x i64> %194, <2 x i64>* %199, align 8
  %200 = getelementptr inbounds [16 x %struct.ge_cached], [16 x %struct.ge_cached]* %7, i64 0, i64 %167, i32 1, i32 0, i64 2
  %201 = bitcast i64* %200 to <2 x i64>*
  store <2 x i64> %196, <2 x i64>* %201, align 8
  %202 = getelementptr inbounds [16 x %struct.ge_cached], [16 x %struct.ge_cached]* %7, i64 0, i64 %167, i32 1, i32 0, i64 4
  store i64 %198, i64* %202, align 8
  %203 = getelementptr inbounds [16 x %struct.ge_cached], [16 x %struct.ge_cached]* %7, i64 0, i64 %167, i32 2
  %204 = bitcast %struct.fe_loose* %203 to i8*
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 16 %204, i8* align 16 %109, i64 40, i1 false) #4
  %205 = getelementptr inbounds [16 x %struct.ge_cached], [16 x %struct.ge_cached]* %7, i64 0, i64 %167, i32 3, i32 0, i64 0
  call fastcc void @fe_mul_impl(i64* %205, i64* %103, i64* getelementptr inbounds (%struct.fe, %struct.fe* @d2, i64 0, i32 0, i64 0)) #4
  call void @llvm.lifetime.end.p0i8(i64 160, i8* nonnull %99) #4
  %206 = icmp ult i64 %119, 7
  br i1 %206, label %207, label %211

207:                                              ; preds = %166
  %208 = getelementptr inbounds [8 x %struct.ge_p2], [8 x %struct.ge_p2]* %6, i64 0, i64 %167, i32 0, i32 0, i64 0
  call fastcc void @fe_mul_impl(i64* %208, i64* nonnull %86, i64* %87) #4
  %209 = getelementptr inbounds [8 x %struct.ge_p2], [8 x %struct.ge_p2]* %6, i64 0, i64 %167, i32 1, i32 0, i64 0
  call fastcc void @fe_mul_impl(i64* %209, i64* %89, i64* %90) #4
  %210 = getelementptr inbounds [8 x %struct.ge_p2], [8 x %struct.ge_p2]* %6, i64 0, i64 %167, i32 2, i32 0, i64 0
  call fastcc void @fe_mul_impl(i64* %210, i64* %90, i64* %87) #4
  br label %211

211:                                              ; preds = %166, %207
  %212 = add nuw nsw i64 %119, 2
  %213 = icmp ult i64 %212, 16
  br i1 %213, label %118, label %214

214:                                              ; preds = %211
  %215 = bitcast %struct.ge_p2* %0 to i8*
  tail call void @llvm.memset.p0i8.i64(i8* align 1 %215, i8 0, i64 40, i1 false) #4
  %216 = getelementptr inbounds %struct.ge_p2, %struct.ge_p2* %0, i64 0, i32 1, i32 0, i64 1
  %217 = bitcast i64* %216 to i8*
  tail call void @llvm.memset.p0i8.i64(i8* align 1 %217, i8 0, i64 32, i1 false) #4
  %218 = getelementptr inbounds %struct.ge_p2, %struct.ge_p2* %0, i64 0, i32 1, i32 0, i64 0
  store i64 1, i64* %218, align 8
  %219 = getelementptr inbounds %struct.ge_p2, %struct.ge_p2* %0, i64 0, i32 2, i32 0, i64 1
  %220 = bitcast i64* %219 to i8*
  tail call void @llvm.memset.p0i8.i64(i8* align 1 %220, i8 0, i64 32, i1 false) #4
  %221 = getelementptr inbounds %struct.ge_p2, %struct.ge_p2* %0, i64 0, i32 2, i32 0, i64 0
  store i64 1, i64* %221, align 8
  %222 = bitcast %struct.ge_p3* %9 to i8*
  call void @llvm.lifetime.start.p0i8(i64 160, i8* nonnull %222) #4
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %222, i8 -86, i64 160, i1 false)
  %223 = getelementptr inbounds %struct.ge_p2, %struct.ge_p2* %0, i64 0, i32 0, i32 0, i64 0
  %224 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %9, i64 0, i32 0, i32 0, i64 0
  %225 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %9, i64 0, i32 1, i32 0, i64 0
  %226 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %9, i64 0, i32 2, i32 0, i64 0
  %227 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %9, i64 0, i32 3, i32 0, i64 0
  %228 = bitcast %struct.ge_cached* %10 to i8*
  %229 = getelementptr inbounds %struct.ge_cached, %struct.ge_cached* %10, i64 0, i32 0, i32 0, i64 1
  %230 = bitcast i64* %229 to i8*
  %231 = getelementptr inbounds %struct.ge_cached, %struct.ge_cached* %10, i64 0, i32 0, i32 0, i64 0
  %232 = getelementptr inbounds %struct.ge_cached, %struct.ge_cached* %10, i64 0, i32 1, i32 0, i64 1
  %233 = bitcast i64* %232 to i8*
  %234 = getelementptr inbounds %struct.ge_cached, %struct.ge_cached* %10, i64 0, i32 1, i32 0, i64 0
  %235 = getelementptr inbounds %struct.ge_cached, %struct.ge_cached* %10, i64 0, i32 2, i32 0, i64 1
  %236 = bitcast i64* %235 to i8*
  %237 = getelementptr inbounds %struct.ge_cached, %struct.ge_cached* %10, i64 0, i32 2, i32 0, i64 0
  %238 = getelementptr inbounds %struct.ge_cached, %struct.ge_cached* %10, i64 0, i32 3
  %239 = bitcast %struct.fe_loose* %238 to i8*
  %240 = getelementptr inbounds %struct.ge_cached, %struct.ge_cached* %10, i64 0, i32 0, i32 0, i64 2
  %241 = getelementptr inbounds %struct.ge_cached, %struct.ge_cached* %10, i64 0, i32 0, i32 0, i64 4
  %242 = getelementptr inbounds %struct.ge_cached, %struct.ge_cached* %10, i64 0, i32 1, i32 0, i64 3
  %243 = getelementptr inbounds %struct.ge_cached, %struct.ge_cached* %10, i64 0, i32 2, i32 0, i64 2
  %244 = getelementptr inbounds %struct.ge_cached, %struct.ge_cached* %10, i64 0, i32 2, i32 0, i64 4
  %245 = getelementptr inbounds %struct.ge_cached, %struct.ge_cached* %10, i64 0, i32 3, i32 0, i64 1
  %246 = getelementptr inbounds %struct.ge_cached, %struct.ge_cached* %10, i64 0, i32 3, i32 0, i64 3
  %247 = bitcast %struct.ge_cached* %10 to <2 x i64>*
  %248 = bitcast i64* %240 to <2 x i64>*
  %249 = bitcast i64* %241 to <2 x i64>*
  %250 = bitcast i64* %232 to <2 x i64>*
  %251 = bitcast i64* %242 to <2 x i64>*
  %252 = bitcast i64* %237 to <2 x i64>*
  %253 = bitcast i64* %243 to <2 x i64>*
  %254 = bitcast i64* %244 to <2 x i64>*
  %255 = bitcast i64* %245 to <2 x i64>*
  %256 = bitcast i64* %246 to <2 x i64>*
  br label %257

257:                                              ; preds = %214, %351
  %258 = phi i32 [ 0, %214 ], [ %352, %351 ]
  call fastcc void @ge_p2_dbl(%struct.ge_p1p1* nonnull %8, %struct.ge_p2* %0)
  call fastcc void @fe_mul_impl(i64* %223, i64* nonnull %86, i64* %87) #4
  call fastcc void @fe_mul_impl(i64* %218, i64* %89, i64* %90) #4
  call fastcc void @fe_mul_impl(i64* %221, i64* %90, i64* %87) #4
  call fastcc void @ge_p2_dbl(%struct.ge_p1p1* nonnull %8, %struct.ge_p2* %0)
  call fastcc void @fe_mul_impl(i64* %223, i64* nonnull %86, i64* %87) #4
  call fastcc void @fe_mul_impl(i64* %218, i64* %89, i64* %90) #4
  call fastcc void @fe_mul_impl(i64* %221, i64* %90, i64* %87) #4
  call fastcc void @ge_p2_dbl(%struct.ge_p1p1* nonnull %8, %struct.ge_p2* %0)
  call fastcc void @fe_mul_impl(i64* %223, i64* nonnull %86, i64* %87) #4
  call fastcc void @fe_mul_impl(i64* %218, i64* %89, i64* %90) #4
  call fastcc void @fe_mul_impl(i64* %221, i64* %90, i64* %87) #4
  call fastcc void @ge_p2_dbl(%struct.ge_p1p1* nonnull %8, %struct.ge_p2* %0)
  call fastcc void @fe_mul_impl(i64* nonnull %224, i64* nonnull %86, i64* %87) #4
  call fastcc void @fe_mul_impl(i64* %225, i64* %89, i64* %90) #4
  call fastcc void @fe_mul_impl(i64* %226, i64* %90, i64* %87) #4
  call fastcc void @fe_mul_impl(i64* %227, i64* nonnull %86, i64* %89) #4
  %259 = lshr i32 %258, 3
  %260 = sub nuw nsw i32 31, %259
  %261 = zext i32 %260 to i64
  %262 = getelementptr inbounds i8, i8* %1, i64 %261
  %263 = load i8, i8* %262, align 1
  %264 = and i32 %258, 4
  %265 = xor i32 %264, 4
  %266 = zext i8 %263 to i32
  %267 = lshr i32 %266, %265
  %268 = and i32 %267, 15
  call void @llvm.lifetime.start.p0i8(i64 160, i8* nonnull %228) #4
  call void @llvm.memset.p0i8.i64(i8* align 8 %230, i8 0, i64 32, i1 false) #4
  store i64 1, i64* %231, align 16
  call void @llvm.memset.p0i8.i64(i8* align 8 %233, i8 0, i64 32, i1 false) #4
  store i64 1, i64* %234, align 8
  call void @llvm.memset.p0i8.i64(i8* align 8 %236, i8 0, i64 32, i1 false) #4
  store i64 1, i64* %237, align 16
  call void @llvm.memset.p0i8.i64(i8* align 8 %239, i8 0, i64 40, i1 false) #4
  br label %269

269:                                              ; preds = %269, %257
  %270 = phi i64 [ 0, %257 ], [ %349, %269 ]
  %271 = phi <2 x i64> [ <i64 1, i64 0>, %257 ], [ %294, %269 ]
  %272 = phi <2 x i64> [ zeroinitializer, %257 ], [ %300, %269 ]
  %273 = phi <2 x i64> [ <i64 0, i64 1>, %257 ], [ %306, %269 ]
  %274 = phi <2 x i64> [ zeroinitializer, %257 ], [ %312, %269 ]
  %275 = phi <2 x i64> [ zeroinitializer, %257 ], [ %318, %269 ]
  %276 = phi <2 x i64> [ <i64 1, i64 0>, %257 ], [ %324, %269 ]
  %277 = phi <2 x i64> [ zeroinitializer, %257 ], [ %330, %269 ]
  %278 = phi <2 x i64> [ zeroinitializer, %257 ], [ %336, %269 ]
  %279 = phi <2 x i64> [ zeroinitializer, %257 ], [ %342, %269 ]
  %280 = phi <2 x i64> [ zeroinitializer, %257 ], [ %348, %269 ]
  %281 = trunc i64 %270 to i32
  %282 = xor i32 %268, %281
  %283 = add nsw i32 %282, -1
  %284 = lshr i32 %283, 31
  %285 = zext i32 %284 to i64
  %286 = sub nsw i64 0, %285
  %287 = getelementptr inbounds [16 x %struct.ge_cached], [16 x %struct.ge_cached]* %7, i64 0, i64 %270, i32 0, i32 0, i64 0
  %288 = bitcast i64* %287 to <2 x i64>*
  %289 = load <2 x i64>, <2 x i64>* %288, align 16
  %290 = xor <2 x i64> %289, %271
  %291 = insertelement <2 x i64> undef, i64 %286, i32 0
  %292 = shufflevector <2 x i64> %291, <2 x i64> undef, <2 x i32> zeroinitializer
  %293 = and <2 x i64> %290, %292
  %294 = xor <2 x i64> %293, %271
  %295 = getelementptr inbounds [16 x %struct.ge_cached], [16 x %struct.ge_cached]* %7, i64 0, i64 %270, i32 0, i32 0, i64 2
  %296 = bitcast i64* %295 to <2 x i64>*
  %297 = load <2 x i64>, <2 x i64>* %296, align 16
  %298 = xor <2 x i64> %297, %272
  %299 = and <2 x i64> %298, %292
  %300 = xor <2 x i64> %299, %272
  %301 = getelementptr inbounds [16 x %struct.ge_cached], [16 x %struct.ge_cached]* %7, i64 0, i64 %270, i32 0, i32 0, i64 4
  %302 = bitcast i64* %301 to <2 x i64>*
  %303 = load <2 x i64>, <2 x i64>* %302, align 16
  %304 = xor <2 x i64> %303, %273
  %305 = and <2 x i64> %304, %292
  %306 = xor <2 x i64> %305, %273
  %307 = getelementptr inbounds [16 x %struct.ge_cached], [16 x %struct.ge_cached]* %7, i64 0, i64 %270, i32 1, i32 0, i64 1
  %308 = bitcast i64* %307 to <2 x i64>*
  %309 = load <2 x i64>, <2 x i64>* %308, align 8
  %310 = xor <2 x i64> %309, %274
  %311 = and <2 x i64> %310, %292
  %312 = xor <2 x i64> %311, %274
  %313 = getelementptr inbounds [16 x %struct.ge_cached], [16 x %struct.ge_cached]* %7, i64 0, i64 %270, i32 1, i32 0, i64 3
  %314 = bitcast i64* %313 to <2 x i64>*
  %315 = load <2 x i64>, <2 x i64>* %314, align 8
  %316 = xor <2 x i64> %315, %275
  %317 = and <2 x i64> %316, %292
  %318 = xor <2 x i64> %317, %275
  %319 = getelementptr inbounds [16 x %struct.ge_cached], [16 x %struct.ge_cached]* %7, i64 0, i64 %270, i32 2, i32 0, i64 0
  %320 = bitcast i64* %319 to <2 x i64>*
  %321 = load <2 x i64>, <2 x i64>* %320, align 16
  %322 = xor <2 x i64> %321, %276
  %323 = and <2 x i64> %322, %292
  %324 = xor <2 x i64> %323, %276
  %325 = getelementptr inbounds [16 x %struct.ge_cached], [16 x %struct.ge_cached]* %7, i64 0, i64 %270, i32 2, i32 0, i64 2
  %326 = bitcast i64* %325 to <2 x i64>*
  %327 = load <2 x i64>, <2 x i64>* %326, align 16
  %328 = xor <2 x i64> %327, %277
  %329 = and <2 x i64> %328, %292
  %330 = xor <2 x i64> %329, %277
  %331 = getelementptr inbounds [16 x %struct.ge_cached], [16 x %struct.ge_cached]* %7, i64 0, i64 %270, i32 2, i32 0, i64 4
  %332 = bitcast i64* %331 to <2 x i64>*
  %333 = load <2 x i64>, <2 x i64>* %332, align 16
  %334 = xor <2 x i64> %333, %278
  %335 = and <2 x i64> %334, %292
  %336 = xor <2 x i64> %335, %278
  %337 = getelementptr inbounds [16 x %struct.ge_cached], [16 x %struct.ge_cached]* %7, i64 0, i64 %270, i32 3, i32 0, i64 1
  %338 = bitcast i64* %337 to <2 x i64>*
  %339 = load <2 x i64>, <2 x i64>* %338, align 8
  %340 = xor <2 x i64> %339, %279
  %341 = and <2 x i64> %340, %292
  %342 = xor <2 x i64> %341, %279
  %343 = getelementptr inbounds [16 x %struct.ge_cached], [16 x %struct.ge_cached]* %7, i64 0, i64 %270, i32 3, i32 0, i64 3
  %344 = bitcast i64* %343 to <2 x i64>*
  %345 = load <2 x i64>, <2 x i64>* %344, align 8
  %346 = xor <2 x i64> %345, %280
  %347 = and <2 x i64> %346, %292
  %348 = xor <2 x i64> %347, %280
  %349 = add nuw nsw i64 %270, 1
  %350 = icmp eq i64 %349, 16
  br i1 %350, label %351, label %269

351:                                              ; preds = %269
  store <2 x i64> %294, <2 x i64>* %247, align 16
  store <2 x i64> %300, <2 x i64>* %248, align 16
  store <2 x i64> %306, <2 x i64>* %249, align 16
  store <2 x i64> %312, <2 x i64>* %250, align 8
  store <2 x i64> %318, <2 x i64>* %251, align 8
  store <2 x i64> %324, <2 x i64>* %252, align 16
  store <2 x i64> %330, <2 x i64>* %253, align 16
  store <2 x i64> %336, <2 x i64>* %254, align 16
  store <2 x i64> %342, <2 x i64>* %255, align 8
  store <2 x i64> %348, <2 x i64>* %256, align 8
  call void @x25519_ge_add(%struct.ge_p1p1* nonnull %8, %struct.ge_p3* nonnull %9, %struct.ge_cached* nonnull %10)
  call fastcc void @fe_mul_impl(i64* %223, i64* nonnull %86, i64* %87) #4
  call fastcc void @fe_mul_impl(i64* %218, i64* %89, i64* %90) #4
  call fastcc void @fe_mul_impl(i64* %221, i64* %90, i64* %87) #4
  call void @llvm.lifetime.end.p0i8(i64 160, i8* nonnull %228) #4
  %352 = add nuw nsw i32 %258, 4
  %353 = icmp ult i32 %352, 256
  br i1 %353, label %257, label %354

354:                                              ; preds = %351
  call void @llvm.lifetime.end.p0i8(i64 160, i8* nonnull %222) #4
  call void @llvm.lifetime.end.p0i8(i64 160, i8* nonnull %13) #4
  call void @llvm.lifetime.end.p0i8(i64 2560, i8* nonnull %12) #4
  call void @llvm.lifetime.end.p0i8(i64 960, i8* nonnull %11) #4
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal fastcc void @ge_p2_dbl(%struct.ge_p1p1* nocapture, %struct.ge_p2* nocapture readonly) unnamed_addr #0 {
  %3 = getelementptr inbounds %struct.ge_p2, %struct.ge_p2* %1, i64 0, i32 0, i32 0, i64 0
  %4 = getelementptr inbounds %struct.ge_p2, %struct.ge_p2* %1, i64 0, i32 0, i32 0, i64 4
  %5 = load i64, i64* %4, align 8
  %6 = mul i64 %5, 19
  %7 = mul i64 %5, 38
  %8 = shl i64 %5, 1
  %9 = getelementptr inbounds %struct.ge_p2, %struct.ge_p2* %1, i64 0, i32 0, i32 0, i64 3
  %10 = load i64, i64* %9, align 8
  %11 = mul i64 %10, 19
  %12 = mul i64 %10, 38
  %13 = shl i64 %10, 1
  %14 = getelementptr inbounds %struct.ge_p2, %struct.ge_p2* %1, i64 0, i32 0, i32 0, i64 2
  %15 = load i64, i64* %14, align 8
  %16 = shl i64 %15, 1
  %17 = getelementptr inbounds %struct.ge_p2, %struct.ge_p2* %1, i64 0, i32 0, i32 0, i64 1
  %18 = load i64, i64* %17, align 8
  %19 = shl i64 %18, 1
  %20 = zext i64 %5 to i128
  %21 = zext i64 %6 to i128
  %22 = mul nuw i128 %21, %20
  %23 = zext i64 %10 to i128
  %24 = zext i64 %7 to i128
  %25 = mul nuw i128 %23, %24
  %26 = zext i64 %11 to i128
  %27 = mul nuw i128 %26, %23
  %28 = zext i64 %15 to i128
  %29 = mul nuw i128 %28, %24
  %30 = zext i64 %12 to i128
  %31 = mul nuw i128 %28, %30
  %32 = mul nuw i128 %28, %28
  %33 = zext i64 %18 to i128
  %34 = mul nuw i128 %33, %24
  %35 = zext i64 %13 to i128
  %36 = mul nuw i128 %33, %35
  %37 = zext i64 %16 to i128
  %38 = mul nuw i128 %33, %37
  %39 = mul nuw i128 %33, %33
  %40 = load i64, i64* %3, align 8
  %41 = zext i64 %40 to i128
  %42 = zext i64 %8 to i128
  %43 = mul nuw i128 %41, %42
  %44 = mul nuw i128 %41, %35
  %45 = mul nuw i128 %41, %37
  %46 = zext i64 %19 to i128
  %47 = mul nuw i128 %41, %46
  %48 = mul nuw i128 %41, %41
  %49 = add i128 %34, %31
  %50 = add i128 %49, %48
  %51 = lshr i128 %50, 51
  %52 = trunc i128 %50 to i64
  %53 = and i64 %52, 2251799813685247
  %54 = add i128 %36, %32
  %55 = add i128 %54, %43
  %56 = add i128 %38, %22
  %57 = add i128 %56, %44
  %58 = add i128 %39, %25
  %59 = add i128 %58, %45
  %60 = add i128 %29, %27
  %61 = add i128 %60, %47
  %62 = and i128 %51, 18446744073709551615
  %63 = add i128 %61, %62
  %64 = lshr i128 %63, 51
  %65 = trunc i128 %63 to i64
  %66 = and i64 %65, 2251799813685247
  %67 = and i128 %64, 18446744073709551615
  %68 = add i128 %59, %67
  %69 = lshr i128 %68, 51
  %70 = trunc i128 %68 to i64
  %71 = and i64 %70, 2251799813685247
  %72 = and i128 %69, 18446744073709551615
  %73 = add i128 %57, %72
  %74 = lshr i128 %73, 51
  %75 = trunc i128 %73 to i64
  %76 = and i64 %75, 2251799813685247
  %77 = and i128 %74, 18446744073709551615
  %78 = add i128 %55, %77
  %79 = lshr i128 %78, 51
  %80 = trunc i128 %79 to i64
  %81 = trunc i128 %78 to i64
  %82 = and i64 %81, 2251799813685247
  %83 = mul i64 %80, 19
  %84 = add i64 %83, %53
  %85 = lshr i64 %84, 51
  %86 = and i64 %84, 2251799813685247
  %87 = add nuw nsw i64 %85, %66
  %88 = lshr i64 %87, 51
  %89 = and i64 %87, 2251799813685247
  %90 = add nuw nsw i64 %88, %71
  %91 = getelementptr inbounds %struct.ge_p2, %struct.ge_p2* %1, i64 0, i32 1, i32 0, i64 0
  %92 = getelementptr inbounds %struct.ge_p2, %struct.ge_p2* %1, i64 0, i32 1, i32 0, i64 4
  %93 = load i64, i64* %92, align 8
  %94 = mul i64 %93, 19
  %95 = mul i64 %93, 38
  %96 = shl i64 %93, 1
  %97 = getelementptr inbounds %struct.ge_p2, %struct.ge_p2* %1, i64 0, i32 1, i32 0, i64 3
  %98 = load i64, i64* %97, align 8
  %99 = mul i64 %98, 19
  %100 = mul i64 %98, 38
  %101 = shl i64 %98, 1
  %102 = getelementptr inbounds %struct.ge_p2, %struct.ge_p2* %1, i64 0, i32 1, i32 0, i64 2
  %103 = load i64, i64* %102, align 8
  %104 = shl i64 %103, 1
  %105 = getelementptr inbounds %struct.ge_p2, %struct.ge_p2* %1, i64 0, i32 1, i32 0, i64 1
  %106 = load i64, i64* %105, align 8
  %107 = shl i64 %106, 1
  %108 = zext i64 %93 to i128
  %109 = zext i64 %94 to i128
  %110 = mul nuw i128 %109, %108
  %111 = zext i64 %98 to i128
  %112 = zext i64 %95 to i128
  %113 = mul nuw i128 %111, %112
  %114 = zext i64 %99 to i128
  %115 = mul nuw i128 %114, %111
  %116 = zext i64 %103 to i128
  %117 = mul nuw i128 %116, %112
  %118 = zext i64 %100 to i128
  %119 = mul nuw i128 %116, %118
  %120 = mul nuw i128 %116, %116
  %121 = zext i64 %106 to i128
  %122 = mul nuw i128 %121, %112
  %123 = zext i64 %101 to i128
  %124 = mul nuw i128 %121, %123
  %125 = zext i64 %104 to i128
  %126 = mul nuw i128 %121, %125
  %127 = mul nuw i128 %121, %121
  %128 = load i64, i64* %91, align 8
  %129 = zext i64 %128 to i128
  %130 = zext i64 %96 to i128
  %131 = mul nuw i128 %129, %130
  %132 = mul nuw i128 %129, %123
  %133 = mul nuw i128 %129, %125
  %134 = zext i64 %107 to i128
  %135 = mul nuw i128 %129, %134
  %136 = mul nuw i128 %129, %129
  %137 = add i128 %122, %119
  %138 = add i128 %137, %136
  %139 = lshr i128 %138, 51
  %140 = trunc i128 %138 to i64
  %141 = and i64 %140, 2251799813685247
  %142 = add i128 %124, %120
  %143 = add i128 %142, %131
  %144 = add i128 %126, %110
  %145 = add i128 %144, %132
  %146 = add i128 %127, %113
  %147 = add i128 %146, %133
  %148 = add i128 %117, %115
  %149 = add i128 %148, %135
  %150 = and i128 %139, 18446744073709551615
  %151 = add i128 %149, %150
  %152 = lshr i128 %151, 51
  %153 = trunc i128 %151 to i64
  %154 = and i64 %153, 2251799813685247
  %155 = and i128 %152, 18446744073709551615
  %156 = add i128 %147, %155
  %157 = lshr i128 %156, 51
  %158 = trunc i128 %156 to i64
  %159 = and i64 %158, 2251799813685247
  %160 = and i128 %157, 18446744073709551615
  %161 = add i128 %145, %160
  %162 = lshr i128 %161, 51
  %163 = trunc i128 %161 to i64
  %164 = and i64 %163, 2251799813685247
  %165 = and i128 %162, 18446744073709551615
  %166 = add i128 %143, %165
  %167 = lshr i128 %166, 51
  %168 = trunc i128 %167 to i64
  %169 = trunc i128 %166 to i64
  %170 = and i64 %169, 2251799813685247
  %171 = mul i64 %168, 19
  %172 = add i64 %171, %141
  %173 = lshr i64 %172, 51
  %174 = and i64 %172, 2251799813685247
  %175 = add nuw nsw i64 %173, %154
  %176 = lshr i64 %175, 51
  %177 = and i64 %175, 2251799813685247
  %178 = add nuw nsw i64 %176, %159
  %179 = getelementptr inbounds %struct.ge_p2, %struct.ge_p2* %1, i64 0, i32 2, i32 0, i64 0
  %180 = getelementptr inbounds %struct.ge_p2, %struct.ge_p2* %1, i64 0, i32 2, i32 0, i64 4
  %181 = load i64, i64* %180, align 8
  %182 = mul i64 %181, 19
  %183 = mul i64 %181, 38
  %184 = shl i64 %181, 1
  %185 = getelementptr inbounds %struct.ge_p2, %struct.ge_p2* %1, i64 0, i32 2, i32 0, i64 3
  %186 = load i64, i64* %185, align 8
  %187 = mul i64 %186, 19
  %188 = mul i64 %186, 38
  %189 = shl i64 %186, 1
  %190 = getelementptr inbounds %struct.ge_p2, %struct.ge_p2* %1, i64 0, i32 2, i32 0, i64 2
  %191 = load i64, i64* %190, align 8
  %192 = shl i64 %191, 1
  %193 = getelementptr inbounds %struct.ge_p2, %struct.ge_p2* %1, i64 0, i32 2, i32 0, i64 1
  %194 = load i64, i64* %193, align 8
  %195 = shl i64 %194, 1
  %196 = zext i64 %181 to i128
  %197 = zext i64 %182 to i128
  %198 = mul nuw i128 %197, %196
  %199 = zext i64 %186 to i128
  %200 = zext i64 %183 to i128
  %201 = mul nuw i128 %199, %200
  %202 = zext i64 %187 to i128
  %203 = mul nuw i128 %202, %199
  %204 = zext i64 %191 to i128
  %205 = mul nuw i128 %204, %200
  %206 = zext i64 %188 to i128
  %207 = mul nuw i128 %204, %206
  %208 = mul nuw i128 %204, %204
  %209 = zext i64 %194 to i128
  %210 = mul nuw i128 %209, %200
  %211 = zext i64 %189 to i128
  %212 = mul nuw i128 %209, %211
  %213 = zext i64 %192 to i128
  %214 = mul nuw i128 %209, %213
  %215 = mul nuw i128 %209, %209
  %216 = load i64, i64* %179, align 8
  %217 = zext i64 %216 to i128
  %218 = zext i64 %184 to i128
  %219 = mul nuw i128 %217, %218
  %220 = mul nuw i128 %217, %211
  %221 = mul nuw i128 %217, %213
  %222 = zext i64 %195 to i128
  %223 = mul nuw i128 %217, %222
  %224 = mul nuw i128 %217, %217
  %225 = add i128 %210, %207
  %226 = add i128 %225, %224
  %227 = lshr i128 %226, 51
  %228 = trunc i128 %226 to i64
  %229 = and i64 %228, 2251799813685247
  %230 = add i128 %212, %208
  %231 = add i128 %230, %219
  %232 = add i128 %214, %198
  %233 = add i128 %232, %220
  %234 = add i128 %215, %201
  %235 = add i128 %234, %221
  %236 = add i128 %205, %203
  %237 = add i128 %236, %223
  %238 = and i128 %227, 18446744073709551615
  %239 = add i128 %237, %238
  %240 = lshr i128 %239, 51
  %241 = trunc i128 %239 to i64
  %242 = and i64 %241, 2251799813685247
  %243 = and i128 %240, 18446744073709551615
  %244 = add i128 %235, %243
  %245 = lshr i128 %244, 51
  %246 = trunc i128 %244 to i64
  %247 = and i64 %246, 2251799813685247
  %248 = and i128 %245, 18446744073709551615
  %249 = add i128 %233, %248
  %250 = lshr i128 %249, 51
  %251 = trunc i128 %249 to i64
  %252 = and i128 %250, 18446744073709551615
  %253 = add i128 %231, %252
  %254 = lshr i128 %253, 51
  %255 = trunc i128 %254 to i64
  %256 = trunc i128 %253 to i64
  %257 = mul i64 %255, 19
  %258 = add i64 %257, %229
  %259 = lshr i64 %258, 51
  %260 = and i64 %258, 2251799813685247
  %261 = add nuw nsw i64 %259, %242
  %262 = lshr i64 %261, 51
  %263 = add nuw nsw i64 %262, %247
  %264 = shl nuw nsw i64 %260, 1
  %265 = shl nuw nsw i64 %261, 1
  %266 = shl nuw nsw i64 %263, 1
  %267 = shl i64 %251, 1
  %268 = and i64 %267, 4503599627370494
  %269 = shl i64 %256, 1
  %270 = and i64 %269, 4503599627370494
  %271 = lshr i64 %260, 50
  %272 = lshr i64 %261, 50
  %273 = and i64 %272, 1
  %274 = lshr i64 %263, 50
  %275 = add nuw nsw i64 %274, %268
  %276 = lshr i64 %275, 51
  %277 = add nuw nsw i64 %276, %270
  %278 = and i64 %264, 2251799813685246
  %279 = lshr i64 %277, 51
  %280 = mul nuw nsw i64 %279, 19
  %281 = add nuw nsw i64 %280, %278
  %282 = lshr i64 %281, 51
  %283 = and i64 %265, 2251799813685246
  %284 = or i64 %283, %271
  %285 = add nuw nsw i64 %282, %284
  %286 = and i64 %281, 2251799813685247
  %287 = and i64 %285, 2251799813685247
  %288 = lshr i64 %285, 51
  %289 = and i64 %266, 2251799813685246
  %290 = or i64 %289, %273
  %291 = insertelement <2 x i64> undef, i64 %275, i32 0
  %292 = insertelement <2 x i64> %291, i64 %277, i32 1
  %293 = and <2 x i64> %292, <i64 2251799813685247, i64 2251799813685247>
  %294 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %0, i64 0, i32 1, i32 0, i64 0
  %295 = add i64 %128, %40
  %296 = load i64, i64* %17, align 8
  %297 = load i64, i64* %105, align 8
  %298 = add i64 %297, %296
  %299 = load i64, i64* %14, align 8
  %300 = load i64, i64* %102, align 8
  %301 = add i64 %300, %299
  %302 = load i64, i64* %9, align 8
  %303 = load i64, i64* %97, align 8
  %304 = add i64 %303, %302
  %305 = load i64, i64* %4, align 8
  %306 = load i64, i64* %92, align 8
  %307 = add i64 %306, %305
  %308 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %0, i64 0, i32 1, i32 0, i64 1
  %309 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %0, i64 0, i32 1, i32 0, i64 2
  %310 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %0, i64 0, i32 1, i32 0, i64 3
  %311 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %0, i64 0, i32 1, i32 0, i64 4
  %312 = mul i64 %307, 19
  %313 = mul i64 %307, 38
  %314 = shl i64 %307, 1
  %315 = mul i64 %304, 19
  %316 = mul i64 %304, 38
  %317 = shl i64 %304, 1
  %318 = shl i64 %301, 1
  %319 = shl i64 %298, 1
  %320 = zext i64 %307 to i128
  %321 = zext i64 %312 to i128
  %322 = mul nuw i128 %321, %320
  %323 = zext i64 %304 to i128
  %324 = zext i64 %313 to i128
  %325 = mul nuw i128 %323, %324
  %326 = zext i64 %315 to i128
  %327 = mul nuw i128 %326, %323
  %328 = zext i64 %301 to i128
  %329 = mul nuw i128 %328, %324
  %330 = zext i64 %316 to i128
  %331 = mul nuw i128 %328, %330
  %332 = mul nuw i128 %328, %328
  %333 = zext i64 %298 to i128
  %334 = mul nuw i128 %333, %324
  %335 = zext i64 %317 to i128
  %336 = mul nuw i128 %333, %335
  %337 = zext i64 %318 to i128
  %338 = mul nuw i128 %333, %337
  %339 = mul nuw i128 %333, %333
  %340 = zext i64 %295 to i128
  %341 = zext i64 %314 to i128
  %342 = mul nuw i128 %341, %340
  %343 = mul nuw i128 %335, %340
  %344 = mul nuw i128 %337, %340
  %345 = zext i64 %319 to i128
  %346 = mul nuw i128 %345, %340
  %347 = mul nuw i128 %340, %340
  %348 = add i128 %331, %347
  %349 = add i128 %348, %334
  %350 = lshr i128 %349, 51
  %351 = trunc i128 %349 to i64
  %352 = and i64 %351, 2251799813685247
  %353 = add i128 %339, %325
  %354 = add i128 %353, %344
  %355 = add i128 %329, %327
  %356 = add i128 %355, %346
  %357 = and i128 %350, 18446744073709551615
  %358 = add i128 %356, %357
  %359 = lshr i128 %358, 51
  %360 = trunc i128 %358 to i64
  %361 = and i64 %360, 2251799813685247
  %362 = and i128 %359, 18446744073709551615
  %363 = add i128 %354, %362
  %364 = lshr i128 %363, 51
  %365 = trunc i128 %363 to i64
  %366 = and i64 %365, 2251799813685247
  %367 = and i128 %364, 18446744073709551615
  %368 = add i128 %343, %322
  %369 = add i128 %368, %338
  %370 = add i128 %369, %367
  %371 = lshr i128 %370, 51
  %372 = and i128 %371, 18446744073709551615
  %373 = add i128 %332, %342
  %374 = add i128 %373, %336
  %375 = add i128 %374, %372
  %376 = lshr i128 %375, 51
  %377 = trunc i128 %376 to i64
  %378 = insertelement <2 x i128> undef, i128 %370, i32 0
  %379 = insertelement <2 x i128> %378, i128 %375, i32 1
  %380 = trunc <2 x i128> %379 to <2 x i64>
  %381 = and <2 x i64> %380, <i64 2251799813685247, i64 2251799813685247>
  %382 = mul i64 %377, 19
  %383 = add i64 %382, %352
  %384 = lshr i64 %383, 51
  %385 = add nuw nsw i64 %384, %361
  %386 = lshr i64 %385, 51
  %387 = insertelement <2 x i64> undef, i64 %383, i32 0
  %388 = insertelement <2 x i64> %387, i64 %385, i32 1
  %389 = and <2 x i64> %388, <i64 2251799813685247, i64 2251799813685247>
  %390 = add nuw nsw i64 %174, %86
  %391 = add nuw nsw i64 %177, %89
  %392 = add nuw nsw i64 %178, %90
  %393 = add nuw nsw i64 %164, %76
  %394 = add nuw nsw i64 %170, %82
  store i64 %390, i64* %294, align 8
  store i64 %391, i64* %308, align 8
  store i64 %392, i64* %309, align 8
  store i64 %393, i64* %310, align 8
  store i64 %394, i64* %311, align 8
  %395 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %0, i64 0, i32 2, i32 0, i64 0
  %396 = sub nuw nsw i64 4503599627370458, %86
  %397 = add nuw nsw i64 %396, %174
  %398 = sub nuw nsw i64 4503599627370494, %89
  %399 = add nuw nsw i64 %398, %177
  %400 = sub nuw nsw i64 4503599627370494, %90
  %401 = add nuw nsw i64 %400, %178
  %402 = sub nuw nsw i64 4503599627370494, %76
  %403 = add nuw nsw i64 %402, %164
  %404 = sub nuw nsw i64 4503599627370494, %82
  %405 = add nuw nsw i64 %404, %170
  store i64 %397, i64* %395, align 8
  %406 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %0, i64 0, i32 2, i32 0, i64 1
  store i64 %399, i64* %406, align 8
  %407 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %0, i64 0, i32 2, i32 0, i64 2
  store i64 %401, i64* %407, align 8
  %408 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %0, i64 0, i32 2, i32 0, i64 3
  store i64 %403, i64* %408, align 8
  %409 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %0, i64 0, i32 2, i32 0, i64 4
  store i64 %405, i64* %409, align 8
  %410 = lshr i64 %390, 51
  %411 = add nuw nsw i64 %391, %410
  %412 = lshr i64 %411, 51
  %413 = add nuw nsw i64 %412, %392
  %414 = lshr i64 %413, 51
  %415 = add nuw nsw i64 %414, %393
  %416 = lshr i64 %415, 51
  %417 = add nuw nsw i64 %416, %394
  %418 = and i64 %390, 2251799813685247
  %419 = lshr i64 %417, 51
  %420 = mul nuw nsw i64 %419, 19
  %421 = add nuw nsw i64 %420, %418
  %422 = lshr i64 %421, 51
  %423 = and i64 %411, 2251799813685247
  %424 = add nuw nsw i64 %422, %423
  %425 = insertelement <2 x i64> undef, i64 %421, i32 0
  %426 = insertelement <2 x i64> %425, i64 %424, i32 1
  %427 = and <2 x i64> %426, <i64 2251799813685247, i64 2251799813685247>
  %428 = lshr i64 %424, 51
  %429 = and i64 %413, 2251799813685247
  %430 = insertelement <2 x i64> undef, i64 %415, i32 0
  %431 = insertelement <2 x i64> %430, i64 %417, i32 1
  %432 = and <2 x i64> %431, <i64 2251799813685247, i64 2251799813685247>
  %433 = sub nuw nsw <2 x i64> <i64 4503599627370458, i64 4503599627370494>, %427
  %434 = add nuw nsw <2 x i64> %433, %389
  %435 = sub nuw nsw i64 4503599627370494, %429
  %436 = add nuw nsw i64 %435, %366
  %437 = sub nuw nsw i64 %436, %428
  %438 = add nuw nsw i64 %437, %386
  %439 = sub nuw nsw <2 x i64> <i64 4503599627370494, i64 4503599627370494>, %432
  %440 = add nuw nsw <2 x i64> %439, %381
  %441 = bitcast %struct.ge_p1p1* %0 to <2 x i64>*
  store <2 x i64> %434, <2 x i64>* %441, align 8
  %442 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %0, i64 0, i32 0, i32 0, i64 2
  store i64 %438, i64* %442, align 8
  %443 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %0, i64 0, i32 0, i32 0, i64 3
  %444 = bitcast i64* %443 to <2 x i64>*
  store <2 x i64> %440, <2 x i64>* %444, align 8
  %445 = lshr i64 %397, 51
  %446 = add nuw nsw i64 %399, %445
  %447 = lshr i64 %446, 51
  %448 = add nuw nsw i64 %447, %401
  %449 = lshr i64 %448, 51
  %450 = add nuw nsw i64 %449, %403
  %451 = lshr i64 %450, 51
  %452 = add nuw nsw i64 %451, %405
  %453 = and i64 %397, 2251799813685247
  %454 = lshr i64 %452, 51
  %455 = mul nuw nsw i64 %454, 19
  %456 = add nuw nsw i64 %455, %453
  %457 = lshr i64 %456, 51
  %458 = and i64 %446, 2251799813685247
  %459 = add nuw nsw i64 %457, %458
  %460 = and i64 %456, 2251799813685247
  %461 = and i64 %459, 2251799813685247
  %462 = lshr i64 %459, 51
  %463 = and i64 %448, 2251799813685247
  %464 = insertelement <2 x i64> undef, i64 %450, i32 0
  %465 = insertelement <2 x i64> %464, i64 %452, i32 1
  %466 = and <2 x i64> %465, <i64 2251799813685247, i64 2251799813685247>
  %467 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %0, i64 0, i32 3, i32 0, i64 0
  %468 = sub nuw nsw i64 4503599627370458, %460
  %469 = add nuw nsw i64 %468, %286
  %470 = sub nuw nsw i64 4503599627370494, %461
  %471 = add nuw nsw i64 %470, %287
  %472 = sub nuw nsw i64 4503599627370494, %463
  %473 = add nuw nsw i64 %472, %290
  %474 = sub nsw i64 %473, %462
  %475 = add nsw i64 %474, %288
  %476 = sub nuw nsw <2 x i64> <i64 4503599627370494, i64 4503599627370494>, %466
  %477 = add nuw nsw <2 x i64> %476, %293
  store i64 %469, i64* %467, align 8
  %478 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %0, i64 0, i32 3, i32 0, i64 1
  store i64 %471, i64* %478, align 8
  %479 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %0, i64 0, i32 3, i32 0, i64 2
  store i64 %475, i64* %479, align 8
  %480 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %0, i64 0, i32 3, i32 0, i64 3
  %481 = bitcast i64* %480 to <2 x i64>*
  store <2 x i64> %477, <2 x i64>* %481, align 8
  ret void
}

; Function Attrs: nofree norecurse nounwind ssp uwtable
define hidden void @x25519_sc_reduce(i8*) local_unnamed_addr #2 {
  %2 = load i8, i8* %0, align 1
  %3 = zext i8 %2 to i64
  %4 = getelementptr inbounds i8, i8* %0, i64 1
  %5 = load i8, i8* %4, align 1
  %6 = zext i8 %5 to i64
  %7 = shl nuw nsw i64 %6, 8
  %8 = or i64 %7, %3
  %9 = getelementptr inbounds i8, i8* %0, i64 2
  %10 = load i8, i8* %9, align 1
  %11 = zext i8 %10 to i64
  %12 = shl nuw nsw i64 %11, 16
  %13 = and i64 %12, 2031616
  %14 = or i64 %8, %13
  %15 = getelementptr inbounds i8, i8* %0, i64 3
  %16 = load i8, i8* %15, align 1
  %17 = zext i8 %16 to i64
  %18 = shl nuw nsw i64 %17, 8
  %19 = or i64 %18, %11
  %20 = getelementptr inbounds i8, i8* %0, i64 4
  %21 = load i8, i8* %20, align 1
  %22 = zext i8 %21 to i64
  %23 = shl nuw nsw i64 %22, 16
  %24 = or i64 %19, %23
  %25 = getelementptr inbounds i8, i8* %0, i64 5
  %26 = load i8, i8* %25, align 1
  %27 = zext i8 %26 to i64
  %28 = shl nuw nsw i64 %27, 24
  %29 = or i64 %24, %28
  %30 = lshr i64 %29, 5
  %31 = and i64 %30, 2097151
  %32 = getelementptr inbounds i8, i8* %0, i64 6
  %33 = load i8, i8* %32, align 1
  %34 = zext i8 %33 to i64
  %35 = shl nuw nsw i64 %34, 8
  %36 = or i64 %35, %27
  %37 = getelementptr inbounds i8, i8* %0, i64 7
  %38 = load i8, i8* %37, align 1
  %39 = zext i8 %38 to i64
  %40 = shl nuw nsw i64 %39, 16
  %41 = or i64 %36, %40
  %42 = lshr i64 %41, 2
  %43 = and i64 %42, 2097151
  %44 = getelementptr inbounds i8, i8* %0, i64 8
  %45 = load i8, i8* %44, align 1
  %46 = zext i8 %45 to i64
  %47 = shl nuw nsw i64 %46, 8
  %48 = or i64 %47, %39
  %49 = getelementptr inbounds i8, i8* %0, i64 9
  %50 = load i8, i8* %49, align 1
  %51 = zext i8 %50 to i64
  %52 = shl nuw nsw i64 %51, 16
  %53 = or i64 %48, %52
  %54 = getelementptr inbounds i8, i8* %0, i64 10
  %55 = load i8, i8* %54, align 1
  %56 = zext i8 %55 to i64
  %57 = shl nuw nsw i64 %56, 24
  %58 = or i64 %53, %57
  %59 = lshr i64 %58, 7
  %60 = and i64 %59, 2097151
  %61 = getelementptr inbounds i8, i8* %0, i64 11
  %62 = load i8, i8* %61, align 1
  %63 = zext i8 %62 to i64
  %64 = shl nuw nsw i64 %63, 8
  %65 = or i64 %64, %56
  %66 = getelementptr inbounds i8, i8* %0, i64 12
  %67 = load i8, i8* %66, align 1
  %68 = zext i8 %67 to i64
  %69 = shl nuw nsw i64 %68, 16
  %70 = or i64 %65, %69
  %71 = getelementptr inbounds i8, i8* %0, i64 13
  %72 = load i8, i8* %71, align 1
  %73 = zext i8 %72 to i64
  %74 = shl nuw nsw i64 %73, 24
  %75 = or i64 %70, %74
  %76 = lshr i64 %75, 4
  %77 = and i64 %76, 2097151
  %78 = getelementptr inbounds i8, i8* %0, i64 14
  %79 = load i8, i8* %78, align 1
  %80 = zext i8 %79 to i64
  %81 = shl nuw nsw i64 %80, 8
  %82 = or i64 %81, %73
  %83 = getelementptr inbounds i8, i8* %0, i64 15
  %84 = load i8, i8* %83, align 1
  %85 = zext i8 %84 to i64
  %86 = shl nuw nsw i64 %85, 16
  %87 = or i64 %82, %86
  %88 = lshr i64 %87, 1
  %89 = and i64 %88, 2097151
  %90 = getelementptr inbounds i8, i8* %0, i64 16
  %91 = load i8, i8* %90, align 1
  %92 = zext i8 %91 to i64
  %93 = shl nuw nsw i64 %92, 8
  %94 = or i64 %93, %85
  %95 = getelementptr inbounds i8, i8* %0, i64 17
  %96 = load i8, i8* %95, align 1
  %97 = zext i8 %96 to i64
  %98 = shl nuw nsw i64 %97, 16
  %99 = or i64 %94, %98
  %100 = getelementptr inbounds i8, i8* %0, i64 18
  %101 = load i8, i8* %100, align 1
  %102 = zext i8 %101 to i64
  %103 = shl nuw nsw i64 %102, 24
  %104 = or i64 %99, %103
  %105 = lshr i64 %104, 6
  %106 = and i64 %105, 2097151
  %107 = getelementptr inbounds i8, i8* %0, i64 19
  %108 = load i8, i8* %107, align 1
  %109 = zext i8 %108 to i64
  %110 = shl nuw nsw i64 %109, 8
  %111 = or i64 %110, %102
  %112 = getelementptr inbounds i8, i8* %0, i64 20
  %113 = load i8, i8* %112, align 1
  %114 = zext i8 %113 to i64
  %115 = shl nuw nsw i64 %114, 16
  %116 = or i64 %111, %115
  %117 = lshr i64 %116, 3
  %118 = getelementptr inbounds i8, i8* %0, i64 21
  %119 = load i8, i8* %118, align 1
  %120 = zext i8 %119 to i64
  %121 = getelementptr inbounds i8, i8* %0, i64 22
  %122 = load i8, i8* %121, align 1
  %123 = zext i8 %122 to i64
  %124 = shl nuw nsw i64 %123, 8
  %125 = or i64 %124, %120
  %126 = getelementptr inbounds i8, i8* %0, i64 23
  %127 = load i8, i8* %126, align 1
  %128 = zext i8 %127 to i64
  %129 = shl nuw nsw i64 %128, 16
  %130 = and i64 %129, 2031616
  %131 = or i64 %125, %130
  %132 = getelementptr inbounds i8, i8* %0, i64 24
  %133 = load i8, i8* %132, align 1
  %134 = zext i8 %133 to i64
  %135 = shl nuw nsw i64 %134, 8
  %136 = or i64 %135, %128
  %137 = getelementptr inbounds i8, i8* %0, i64 25
  %138 = load i8, i8* %137, align 1
  %139 = zext i8 %138 to i64
  %140 = shl nuw nsw i64 %139, 16
  %141 = or i64 %136, %140
  %142 = getelementptr inbounds i8, i8* %0, i64 26
  %143 = load i8, i8* %142, align 1
  %144 = zext i8 %143 to i64
  %145 = shl nuw nsw i64 %144, 24
  %146 = or i64 %141, %145
  %147 = lshr i64 %146, 5
  %148 = and i64 %147, 2097151
  %149 = getelementptr inbounds i8, i8* %0, i64 27
  %150 = load i8, i8* %149, align 1
  %151 = zext i8 %150 to i64
  %152 = shl nuw nsw i64 %151, 8
  %153 = or i64 %152, %144
  %154 = getelementptr inbounds i8, i8* %0, i64 28
  %155 = load i8, i8* %154, align 1
  %156 = zext i8 %155 to i64
  %157 = shl nuw nsw i64 %156, 16
  %158 = or i64 %153, %157
  %159 = lshr i64 %158, 2
  %160 = and i64 %159, 2097151
  %161 = getelementptr inbounds i8, i8* %0, i64 29
  %162 = load i8, i8* %161, align 1
  %163 = zext i8 %162 to i64
  %164 = shl nuw nsw i64 %163, 8
  %165 = or i64 %164, %156
  %166 = getelementptr inbounds i8, i8* %0, i64 30
  %167 = load i8, i8* %166, align 1
  %168 = zext i8 %167 to i64
  %169 = shl nuw nsw i64 %168, 16
  %170 = or i64 %165, %169
  %171 = getelementptr inbounds i8, i8* %0, i64 31
  %172 = load i8, i8* %171, align 1
  %173 = zext i8 %172 to i64
  %174 = shl nuw nsw i64 %173, 24
  %175 = or i64 %170, %174
  %176 = lshr i64 %175, 7
  %177 = and i64 %176, 2097151
  %178 = getelementptr inbounds i8, i8* %0, i64 32
  %179 = load i8, i8* %178, align 1
  %180 = zext i8 %179 to i64
  %181 = shl nuw nsw i64 %180, 8
  %182 = or i64 %181, %173
  %183 = getelementptr inbounds i8, i8* %0, i64 33
  %184 = load i8, i8* %183, align 1
  %185 = zext i8 %184 to i64
  %186 = shl nuw nsw i64 %185, 16
  %187 = or i64 %182, %186
  %188 = getelementptr inbounds i8, i8* %0, i64 34
  %189 = load i8, i8* %188, align 1
  %190 = zext i8 %189 to i64
  %191 = shl nuw nsw i64 %190, 24
  %192 = or i64 %187, %191
  %193 = lshr i64 %192, 4
  %194 = and i64 %193, 2097151
  %195 = getelementptr inbounds i8, i8* %0, i64 35
  %196 = load i8, i8* %195, align 1
  %197 = zext i8 %196 to i64
  %198 = shl nuw nsw i64 %197, 8
  %199 = or i64 %198, %190
  %200 = getelementptr inbounds i8, i8* %0, i64 36
  %201 = load i8, i8* %200, align 1
  %202 = zext i8 %201 to i64
  %203 = shl nuw nsw i64 %202, 16
  %204 = or i64 %199, %203
  %205 = lshr i64 %204, 1
  %206 = and i64 %205, 2097151
  %207 = getelementptr inbounds i8, i8* %0, i64 37
  %208 = load i8, i8* %207, align 1
  %209 = zext i8 %208 to i64
  %210 = shl nuw nsw i64 %209, 8
  %211 = or i64 %210, %202
  %212 = getelementptr inbounds i8, i8* %0, i64 38
  %213 = load i8, i8* %212, align 1
  %214 = zext i8 %213 to i64
  %215 = shl nuw nsw i64 %214, 16
  %216 = or i64 %211, %215
  %217 = getelementptr inbounds i8, i8* %0, i64 39
  %218 = load i8, i8* %217, align 1
  %219 = zext i8 %218 to i64
  %220 = shl nuw nsw i64 %219, 24
  %221 = or i64 %216, %220
  %222 = lshr i64 %221, 6
  %223 = and i64 %222, 2097151
  %224 = getelementptr inbounds i8, i8* %0, i64 40
  %225 = load i8, i8* %224, align 1
  %226 = zext i8 %225 to i64
  %227 = shl nuw nsw i64 %226, 8
  %228 = or i64 %227, %219
  %229 = getelementptr inbounds i8, i8* %0, i64 41
  %230 = load i8, i8* %229, align 1
  %231 = zext i8 %230 to i64
  %232 = shl nuw nsw i64 %231, 16
  %233 = or i64 %228, %232
  %234 = lshr i64 %233, 3
  %235 = getelementptr inbounds i8, i8* %0, i64 42
  %236 = load i8, i8* %235, align 1
  %237 = zext i8 %236 to i64
  %238 = getelementptr inbounds i8, i8* %0, i64 43
  %239 = load i8, i8* %238, align 1
  %240 = zext i8 %239 to i64
  %241 = shl nuw nsw i64 %240, 8
  %242 = or i64 %241, %237
  %243 = getelementptr inbounds i8, i8* %0, i64 44
  %244 = load i8, i8* %243, align 1
  %245 = zext i8 %244 to i64
  %246 = shl nuw nsw i64 %245, 16
  %247 = and i64 %246, 2031616
  %248 = or i64 %242, %247
  %249 = getelementptr inbounds i8, i8* %0, i64 45
  %250 = load i8, i8* %249, align 1
  %251 = zext i8 %250 to i64
  %252 = shl nuw nsw i64 %251, 8
  %253 = or i64 %252, %245
  %254 = getelementptr inbounds i8, i8* %0, i64 46
  %255 = load i8, i8* %254, align 1
  %256 = zext i8 %255 to i64
  %257 = shl nuw nsw i64 %256, 16
  %258 = or i64 %253, %257
  %259 = getelementptr inbounds i8, i8* %0, i64 47
  %260 = load i8, i8* %259, align 1
  %261 = zext i8 %260 to i64
  %262 = shl nuw nsw i64 %261, 24
  %263 = or i64 %258, %262
  %264 = lshr i64 %263, 5
  %265 = and i64 %264, 2097151
  %266 = getelementptr inbounds i8, i8* %0, i64 48
  %267 = load i8, i8* %266, align 1
  %268 = zext i8 %267 to i64
  %269 = shl nuw nsw i64 %268, 8
  %270 = or i64 %269, %261
  %271 = getelementptr inbounds i8, i8* %0, i64 49
  %272 = load i8, i8* %271, align 1
  %273 = zext i8 %272 to i64
  %274 = shl nuw nsw i64 %273, 16
  %275 = or i64 %270, %274
  %276 = lshr i64 %275, 2
  %277 = and i64 %276, 2097151
  %278 = getelementptr inbounds i8, i8* %0, i64 50
  %279 = load i8, i8* %278, align 1
  %280 = zext i8 %279 to i64
  %281 = shl nuw nsw i64 %280, 8
  %282 = or i64 %281, %273
  %283 = getelementptr inbounds i8, i8* %0, i64 51
  %284 = load i8, i8* %283, align 1
  %285 = zext i8 %284 to i64
  %286 = shl nuw nsw i64 %285, 16
  %287 = or i64 %282, %286
  %288 = getelementptr inbounds i8, i8* %0, i64 52
  %289 = load i8, i8* %288, align 1
  %290 = zext i8 %289 to i64
  %291 = shl nuw nsw i64 %290, 24
  %292 = or i64 %287, %291
  %293 = lshr i64 %292, 7
  %294 = and i64 %293, 2097151
  %295 = getelementptr inbounds i8, i8* %0, i64 53
  %296 = load i8, i8* %295, align 1
  %297 = zext i8 %296 to i64
  %298 = shl nuw nsw i64 %297, 8
  %299 = or i64 %298, %290
  %300 = getelementptr inbounds i8, i8* %0, i64 54
  %301 = load i8, i8* %300, align 1
  %302 = zext i8 %301 to i64
  %303 = shl nuw nsw i64 %302, 16
  %304 = or i64 %299, %303
  %305 = getelementptr inbounds i8, i8* %0, i64 55
  %306 = load i8, i8* %305, align 1
  %307 = zext i8 %306 to i64
  %308 = shl nuw nsw i64 %307, 24
  %309 = or i64 %304, %308
  %310 = lshr i64 %309, 4
  %311 = and i64 %310, 2097151
  %312 = getelementptr inbounds i8, i8* %0, i64 56
  %313 = load i8, i8* %312, align 1
  %314 = zext i8 %313 to i64
  %315 = shl nuw nsw i64 %314, 8
  %316 = or i64 %315, %307
  %317 = getelementptr inbounds i8, i8* %0, i64 57
  %318 = load i8, i8* %317, align 1
  %319 = zext i8 %318 to i64
  %320 = shl nuw nsw i64 %319, 16
  %321 = or i64 %316, %320
  %322 = lshr i64 %321, 1
  %323 = and i64 %322, 2097151
  %324 = getelementptr inbounds i8, i8* %0, i64 58
  %325 = load i8, i8* %324, align 1
  %326 = zext i8 %325 to i64
  %327 = shl nuw nsw i64 %326, 8
  %328 = or i64 %327, %319
  %329 = getelementptr inbounds i8, i8* %0, i64 59
  %330 = load i8, i8* %329, align 1
  %331 = zext i8 %330 to i64
  %332 = shl nuw nsw i64 %331, 16
  %333 = or i64 %328, %332
  %334 = getelementptr inbounds i8, i8* %0, i64 60
  %335 = load i8, i8* %334, align 1
  %336 = zext i8 %335 to i64
  %337 = shl nuw nsw i64 %336, 24
  %338 = or i64 %333, %337
  %339 = lshr i64 %338, 6
  %340 = and i64 %339, 2097151
  %341 = getelementptr inbounds i8, i8* %0, i64 61
  %342 = load i8, i8* %341, align 1
  %343 = zext i8 %342 to i64
  %344 = shl nuw nsw i64 %343, 8
  %345 = or i64 %344, %336
  %346 = getelementptr inbounds i8, i8* %0, i64 62
  %347 = load i8, i8* %346, align 1
  %348 = zext i8 %347 to i64
  %349 = shl nuw nsw i64 %348, 16
  %350 = or i64 %345, %349
  %351 = getelementptr inbounds i8, i8* %0, i64 63
  %352 = load i8, i8* %351, align 1
  %353 = zext i8 %352 to i64
  %354 = shl nuw nsw i64 %353, 24
  %355 = or i64 %350, %354
  %356 = lshr i64 %355, 3
  %357 = mul nuw nsw i64 %356, 666643
  %358 = mul nuw nsw i64 %356, 470296
  %359 = mul nuw nsw i64 %356, 654183
  %360 = mul nsw i64 %356, -997805
  %361 = mul nuw nsw i64 %356, 136657
  %362 = mul nsw i64 %356, -683901
  %363 = add i64 %362, %248
  %364 = mul nuw nsw i64 %340, 666643
  %365 = mul nuw nsw i64 %340, 470296
  %366 = mul nuw nsw i64 %340, 654183
  %367 = mul nsw i64 %340, -997805
  %368 = mul nuw nsw i64 %340, 136657
  %369 = mul nsw i64 %340, -683901
  %370 = mul nuw nsw i64 %323, 666643
  %371 = mul nuw nsw i64 %323, 470296
  %372 = mul nuw nsw i64 %323, 654183
  %373 = mul nsw i64 %323, -997805
  %374 = mul nuw nsw i64 %323, 136657
  %375 = mul nsw i64 %323, -683901
  %376 = add nsw i64 %375, %223
  %377 = add nsw i64 %376, %368
  %378 = add i64 %377, %360
  %379 = mul nuw nsw i64 %311, 666643
  %380 = mul nuw nsw i64 %311, 470296
  %381 = mul nuw nsw i64 %311, 654183
  %382 = mul nsw i64 %311, -997805
  %383 = mul nuw nsw i64 %311, 136657
  %384 = mul nsw i64 %311, -683901
  %385 = mul nuw nsw i64 %294, 666643
  %386 = mul nuw nsw i64 %294, 470296
  %387 = mul nuw nsw i64 %294, 654183
  %388 = mul nsw i64 %294, -997805
  %389 = mul nuw nsw i64 %294, 136657
  %390 = mul nsw i64 %294, -683901
  %391 = add nsw i64 %390, %194
  %392 = add nsw i64 %391, %383
  %393 = add nsw i64 %392, %373
  %394 = add nsw i64 %393, %366
  %395 = add i64 %394, %358
  %396 = mul nuw nsw i64 %277, 666643
  %397 = add nuw nsw i64 %396, %106
  %398 = mul nuw nsw i64 %277, 470296
  %399 = mul nuw nsw i64 %277, 654183
  %400 = add nuw nsw i64 %399, %131
  %401 = add nuw nsw i64 %400, %386
  %402 = add nuw nsw i64 %401, %379
  %403 = mul nsw i64 %277, -997805
  %404 = mul nuw nsw i64 %277, 136657
  %405 = add nuw nsw i64 %404, %160
  %406 = add nsw i64 %405, %388
  %407 = add nsw i64 %406, %381
  %408 = add nsw i64 %407, %371
  %409 = add nsw i64 %408, %364
  %410 = mul nsw i64 %277, -683901
  %411 = add nuw nsw i64 %397, 1048576
  %412 = lshr i64 %411, 21
  %413 = add nuw nsw i64 %398, %117
  %414 = add nuw nsw i64 %413, %412
  %415 = add nuw nsw i64 %414, %385
  %416 = and i64 %411, 8796090925056
  %417 = sub nsw i64 %397, %416
  %418 = add nuw nsw i64 %402, 1048576
  %419 = lshr i64 %418, 21
  %420 = add nsw i64 %403, %148
  %421 = add nsw i64 %420, %387
  %422 = add nsw i64 %421, %380
  %423 = add nsw i64 %422, %370
  %424 = add nsw i64 %423, %419
  %425 = and i64 %418, -2097152
  %426 = add nsw i64 %409, 1048576
  %427 = ashr i64 %426, 21
  %428 = add nsw i64 %410, %177
  %429 = add nsw i64 %428, %389
  %430 = add nsw i64 %429, %382
  %431 = add nsw i64 %430, %372
  %432 = add nsw i64 %431, %365
  %433 = add i64 %432, %357
  %434 = add i64 %433, %427
  %435 = and i64 %426, -2097152
  %436 = add nsw i64 %395, 1048576
  %437 = ashr i64 %436, 21
  %438 = add nsw i64 %384, %206
  %439 = add nsw i64 %438, %374
  %440 = add nsw i64 %439, %367
  %441 = add i64 %440, %359
  %442 = add i64 %441, %437
  %443 = and i64 %436, -2097152
  %444 = sub nsw i64 %395, %443
  %445 = add nsw i64 %378, 1048576
  %446 = ashr i64 %445, 21
  %447 = add nsw i64 %369, %234
  %448 = add i64 %447, %361
  %449 = add i64 %448, %446
  %450 = and i64 %445, -2097152
  %451 = sub nsw i64 %378, %450
  %452 = add nsw i64 %363, 1048576
  %453 = ashr i64 %452, 21
  %454 = add nsw i64 %453, %265
  %455 = and i64 %452, -2097152
  %456 = sub nsw i64 %363, %455
  %457 = add nuw nsw i64 %415, 1048576
  %458 = lshr i64 %457, 21
  %459 = and i64 %457, -2097152
  %460 = sub nsw i64 %415, %459
  %461 = add nsw i64 %424, 1048576
  %462 = ashr i64 %461, 21
  %463 = and i64 %461, -2097152
  %464 = sub i64 %424, %463
  %465 = add nsw i64 %434, 1048576
  %466 = ashr i64 %465, 21
  %467 = add nsw i64 %466, %444
  %468 = and i64 %465, -2097152
  %469 = sub nsw i64 %434, %468
  %470 = add nsw i64 %442, 1048576
  %471 = ashr i64 %470, 21
  %472 = add nsw i64 %471, %451
  %473 = and i64 %470, -2097152
  %474 = sub nsw i64 %442, %473
  %475 = add nsw i64 %449, 1048576
  %476 = ashr i64 %475, 21
  %477 = add nsw i64 %476, %456
  %478 = and i64 %475, -2097152
  %479 = sub nsw i64 %449, %478
  %480 = mul nsw i64 %454, 666643
  %481 = add nsw i64 %480, %89
  %482 = mul nsw i64 %454, 470296
  %483 = add nsw i64 %417, %482
  %484 = mul nsw i64 %454, 654183
  %485 = add nsw i64 %460, %484
  %486 = mul i64 %454, -997805
  %487 = mul nsw i64 %454, 136657
  %488 = add nsw i64 %464, %487
  %489 = mul i64 %454, -683901
  %490 = add nsw i64 %409, %462
  %491 = sub i64 %490, %435
  %492 = add i64 %491, %489
  %493 = mul nsw i64 %477, 666643
  %494 = add nsw i64 %493, %77
  %495 = mul nsw i64 %477, 470296
  %496 = add nsw i64 %481, %495
  %497 = mul nsw i64 %477, 654183
  %498 = add nsw i64 %483, %497
  %499 = mul i64 %477, -997805
  %500 = add i64 %485, %499
  %501 = mul nsw i64 %477, 136657
  %502 = mul i64 %477, -683901
  %503 = add i64 %488, %502
  %504 = mul nsw i64 %479, 666643
  %505 = add nsw i64 %504, %60
  %506 = mul nsw i64 %479, 470296
  %507 = add nsw i64 %494, %506
  %508 = mul nsw i64 %479, 654183
  %509 = add nsw i64 %496, %508
  %510 = mul i64 %479, -997805
  %511 = add i64 %498, %510
  %512 = mul nsw i64 %479, 136657
  %513 = add nsw i64 %500, %512
  %514 = mul i64 %479, -683901
  %515 = add i64 %402, %458
  %516 = sub i64 %515, %425
  %517 = add i64 %516, %486
  %518 = add i64 %517, %501
  %519 = add i64 %518, %514
  %520 = mul nsw i64 %472, 666643
  %521 = mul nsw i64 %472, 470296
  %522 = mul nsw i64 %472, 654183
  %523 = mul i64 %472, -997805
  %524 = mul nsw i64 %472, 136657
  %525 = add nsw i64 %511, %524
  %526 = mul i64 %472, -683901
  %527 = add i64 %513, %526
  %528 = mul nsw i64 %474, 666643
  %529 = mul nsw i64 %474, 470296
  %530 = mul nsw i64 %474, 654183
  %531 = mul i64 %474, -997805
  %532 = mul nsw i64 %474, 136657
  %533 = mul i64 %474, -683901
  %534 = add i64 %525, %533
  %535 = mul nsw i64 %467, 666643
  %536 = add nsw i64 %535, %14
  %537 = mul nsw i64 %467, 470296
  %538 = mul nsw i64 %467, 654183
  %539 = add i64 %538, %43
  %540 = add i64 %539, %520
  %541 = add i64 %540, %529
  %542 = mul i64 %467, -997805
  %543 = mul nsw i64 %467, 136657
  %544 = add i64 %507, %543
  %545 = add i64 %544, %522
  %546 = add i64 %545, %531
  %547 = mul i64 %467, -683901
  %548 = add nsw i64 %536, 1048576
  %549 = ashr i64 %548, 21
  %550 = add i64 %537, %31
  %551 = add i64 %550, %528
  %552 = add i64 %551, %549
  %553 = and i64 %548, -2097152
  %554 = sub nsw i64 %536, %553
  %555 = add nsw i64 %541, 1048576
  %556 = ashr i64 %555, 21
  %557 = add i64 %505, %542
  %558 = add i64 %557, %521
  %559 = add i64 %558, %530
  %560 = add i64 %559, %556
  %561 = and i64 %555, -2097152
  %562 = add nsw i64 %546, 1048576
  %563 = ashr i64 %562, 21
  %564 = add i64 %509, %547
  %565 = add i64 %564, %523
  %566 = add i64 %565, %532
  %567 = add i64 %566, %563
  %568 = and i64 %562, -2097152
  %569 = add nsw i64 %534, 1048576
  %570 = ashr i64 %569, 21
  %571 = add nsw i64 %527, %570
  %572 = and i64 %569, -2097152
  %573 = sub i64 %534, %572
  %574 = add nsw i64 %519, 1048576
  %575 = ashr i64 %574, 21
  %576 = add nsw i64 %503, %575
  %577 = and i64 %574, -2097152
  %578 = sub i64 %519, %577
  %579 = add nsw i64 %492, 1048576
  %580 = ashr i64 %579, 21
  %581 = add nsw i64 %469, %580
  %582 = and i64 %579, -2097152
  %583 = sub i64 %492, %582
  %584 = add nsw i64 %552, 1048576
  %585 = ashr i64 %584, 21
  %586 = and i64 %584, -2097152
  %587 = add nsw i64 %560, 1048576
  %588 = ashr i64 %587, 21
  %589 = and i64 %587, -2097152
  %590 = add nsw i64 %567, 1048576
  %591 = ashr i64 %590, 21
  %592 = add nsw i64 %573, %591
  %593 = and i64 %590, -2097152
  %594 = add nsw i64 %571, 1048576
  %595 = ashr i64 %594, 21
  %596 = add nsw i64 %578, %595
  %597 = and i64 %594, -2097152
  %598 = sub nsw i64 %571, %597
  %599 = add nsw i64 %576, 1048576
  %600 = ashr i64 %599, 21
  %601 = add nsw i64 %583, %600
  %602 = and i64 %599, -2097152
  %603 = sub nsw i64 %576, %602
  %604 = add nsw i64 %581, 1048576
  %605 = ashr i64 %604, 21
  %606 = and i64 %604, -2097152
  %607 = sub nsw i64 %581, %606
  %608 = mul nsw i64 %605, 666643
  %609 = add nsw i64 %554, %608
  %610 = mul nsw i64 %605, 470296
  %611 = mul nsw i64 %605, 654183
  %612 = mul nsw i64 %605, -997805
  %613 = mul nsw i64 %605, 136657
  %614 = mul nsw i64 %605, -683901
  %615 = ashr i64 %609, 21
  %616 = add i64 %552, %610
  %617 = sub i64 %616, %586
  %618 = add i64 %617, %615
  %619 = and i64 %609, 2097151
  %620 = ashr i64 %618, 21
  %621 = add i64 %541, %611
  %622 = sub i64 %621, %561
  %623 = add i64 %622, %585
  %624 = add i64 %623, %620
  %625 = and i64 %618, 2097151
  %626 = ashr i64 %624, 21
  %627 = add i64 %560, %612
  %628 = sub i64 %627, %589
  %629 = add i64 %628, %626
  %630 = and i64 %624, 2097151
  %631 = ashr i64 %629, 21
  %632 = add i64 %546, %613
  %633 = sub i64 %632, %568
  %634 = add i64 %633, %588
  %635 = add i64 %634, %631
  %636 = and i64 %629, 2097151
  %637 = ashr i64 %635, 21
  %638 = add i64 %567, %614
  %639 = sub i64 %638, %593
  %640 = add i64 %639, %637
  %641 = and i64 %635, 2097151
  %642 = ashr i64 %640, 21
  %643 = add nsw i64 %592, %642
  %644 = and i64 %640, 2097151
  %645 = ashr i64 %643, 21
  %646 = add nsw i64 %645, %598
  %647 = and i64 %643, 2097151
  %648 = ashr i64 %646, 21
  %649 = add nsw i64 %596, %648
  %650 = and i64 %646, 2097151
  %651 = ashr i64 %649, 21
  %652 = add nsw i64 %651, %603
  %653 = and i64 %649, 2097151
  %654 = ashr i64 %652, 21
  %655 = add nsw i64 %601, %654
  %656 = and i64 %652, 2097151
  %657 = ashr i64 %655, 21
  %658 = add nsw i64 %657, %607
  %659 = and i64 %655, 2097151
  %660 = ashr i64 %658, 21
  %661 = and i64 %658, 2097151
  %662 = mul nsw i64 %660, 666643
  %663 = add nsw i64 %662, %619
  %664 = mul nsw i64 %660, 470296
  %665 = add nsw i64 %664, %625
  %666 = mul nsw i64 %660, 654183
  %667 = add nsw i64 %666, %630
  %668 = mul nsw i64 %660, -997805
  %669 = add nsw i64 %668, %636
  %670 = mul nsw i64 %660, 136657
  %671 = add nsw i64 %670, %641
  %672 = mul nsw i64 %660, -683901
  %673 = add nsw i64 %672, %644
  %674 = ashr i64 %663, 21
  %675 = add nsw i64 %665, %674
  %676 = ashr i64 %675, 21
  %677 = add nsw i64 %667, %676
  %678 = and i64 %675, 2097151
  %679 = ashr i64 %677, 21
  %680 = add nsw i64 %669, %679
  %681 = and i64 %677, 2097151
  %682 = ashr i64 %680, 21
  %683 = add nsw i64 %671, %682
  %684 = and i64 %680, 2097151
  %685 = ashr i64 %683, 21
  %686 = add nsw i64 %673, %685
  %687 = and i64 %683, 2097151
  %688 = ashr i64 %686, 21
  %689 = add nsw i64 %688, %647
  %690 = and i64 %686, 2097151
  %691 = ashr i64 %689, 21
  %692 = add nsw i64 %691, %650
  %693 = and i64 %689, 2097151
  %694 = ashr i64 %692, 21
  %695 = add nsw i64 %694, %653
  %696 = ashr i64 %695, 21
  %697 = add nsw i64 %696, %656
  %698 = ashr i64 %697, 21
  %699 = add nsw i64 %698, %659
  %700 = and i64 %697, 2097151
  %701 = ashr i64 %699, 21
  %702 = add nsw i64 %701, %661
  %703 = and i64 %699, 2097151
  %704 = trunc i64 %663 to i8
  store i8 %704, i8* %0, align 1
  %705 = lshr i64 %663, 8
  %706 = trunc i64 %705 to i8
  store i8 %706, i8* %4, align 1
  %707 = lshr i64 %663, 16
  %708 = and i64 %707, 31
  %709 = shl nuw nsw i64 %678, 5
  %710 = or i64 %709, %708
  %711 = trunc i64 %710 to i8
  store i8 %711, i8* %9, align 1
  %712 = lshr i64 %675, 3
  %713 = trunc i64 %712 to i8
  store i8 %713, i8* %15, align 1
  %714 = lshr i64 %675, 11
  %715 = trunc i64 %714 to i8
  store i8 %715, i8* %20, align 1
  %716 = lshr i64 %678, 19
  %717 = shl nuw nsw i64 %681, 2
  %718 = or i64 %717, %716
  %719 = trunc i64 %718 to i8
  store i8 %719, i8* %25, align 1
  %720 = lshr i64 %677, 6
  %721 = trunc i64 %720 to i8
  store i8 %721, i8* %32, align 1
  %722 = lshr i64 %681, 14
  %723 = shl nuw nsw i64 %684, 7
  %724 = or i64 %723, %722
  %725 = trunc i64 %724 to i8
  store i8 %725, i8* %37, align 1
  %726 = lshr i64 %680, 1
  %727 = trunc i64 %726 to i8
  store i8 %727, i8* %44, align 1
  %728 = lshr i64 %680, 9
  %729 = trunc i64 %728 to i8
  store i8 %729, i8* %49, align 1
  %730 = lshr i64 %684, 17
  %731 = shl nuw nsw i64 %687, 4
  %732 = or i64 %731, %730
  %733 = trunc i64 %732 to i8
  store i8 %733, i8* %54, align 1
  %734 = lshr i64 %683, 4
  %735 = trunc i64 %734 to i8
  store i8 %735, i8* %61, align 1
  %736 = lshr i64 %683, 12
  %737 = trunc i64 %736 to i8
  store i8 %737, i8* %66, align 1
  %738 = lshr i64 %687, 20
  %739 = shl nuw nsw i64 %690, 1
  %740 = or i64 %739, %738
  %741 = trunc i64 %740 to i8
  store i8 %741, i8* %71, align 1
  %742 = lshr i64 %686, 7
  %743 = trunc i64 %742 to i8
  store i8 %743, i8* %78, align 1
  %744 = lshr i64 %690, 15
  %745 = shl nuw nsw i64 %693, 6
  %746 = or i64 %745, %744
  %747 = trunc i64 %746 to i8
  store i8 %747, i8* %83, align 1
  %748 = lshr i64 %689, 2
  %749 = trunc i64 %748 to i8
  store i8 %749, i8* %90, align 1
  %750 = lshr i64 %689, 10
  %751 = trunc i64 %750 to i8
  store i8 %751, i8* %95, align 1
  %752 = lshr i64 %693, 18
  %753 = shl nsw i64 %692, 3
  %754 = or i64 %753, %752
  %755 = trunc i64 %754 to i8
  store i8 %755, i8* %100, align 1
  %756 = lshr i64 %692, 5
  %757 = trunc i64 %756 to i8
  store i8 %757, i8* %107, align 1
  %758 = lshr i64 %692, 13
  %759 = trunc i64 %758 to i8
  store i8 %759, i8* %112, align 1
  %760 = trunc i64 %695 to i8
  store i8 %760, i8* %118, align 1
  %761 = lshr i64 %695, 8
  %762 = trunc i64 %761 to i8
  store i8 %762, i8* %121, align 1
  %763 = lshr i64 %695, 16
  %764 = and i64 %763, 31
  %765 = shl nuw nsw i64 %700, 5
  %766 = or i64 %765, %764
  %767 = trunc i64 %766 to i8
  store i8 %767, i8* %126, align 1
  %768 = lshr i64 %697, 3
  %769 = trunc i64 %768 to i8
  store i8 %769, i8* %132, align 1
  %770 = lshr i64 %697, 11
  %771 = trunc i64 %770 to i8
  store i8 %771, i8* %137, align 1
  %772 = lshr i64 %700, 19
  %773 = shl nuw nsw i64 %703, 2
  %774 = or i64 %773, %772
  %775 = trunc i64 %774 to i8
  store i8 %775, i8* %142, align 1
  %776 = lshr i64 %699, 6
  %777 = trunc i64 %776 to i8
  store i8 %777, i8* %149, align 1
  %778 = lshr i64 %703, 14
  %779 = shl nsw i64 %702, 7
  %780 = or i64 %779, %778
  %781 = trunc i64 %780 to i8
  store i8 %781, i8* %154, align 1
  %782 = lshr i64 %702, 1
  %783 = trunc i64 %782 to i8
  store i8 %783, i8* %161, align 1
  %784 = lshr i64 %702, 9
  %785 = trunc i64 %784 to i8
  store i8 %785, i8* %166, align 1
  %786 = lshr i64 %702, 17
  %787 = trunc i64 %786 to i8
  store i8 %787, i8* %171, align 1
  ret void
}

; Function Attrs: nounwind ssp uwtable
define hidden void @ED25519_keypair(i8*, i8* nocapture) local_unnamed_addr #0 {
  %3 = alloca [32 x i8], align 16
  %4 = getelementptr inbounds [32 x i8], [32 x i8]* %3, i64 0, i64 0
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %4) #4
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %4, i8 -86, i64 32, i1 false)
  %5 = call i32 @RAND_bytes(i8* nonnull %4, i64 32) #4
  call void @ED25519_keypair_from_seed(i8* %0, i8* %1, i8* nonnull %4)
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %4) #4
  ret void
}

declare i32 @RAND_bytes(i8*, i64) local_unnamed_addr #3

; Function Attrs: nounwind ssp uwtable
define hidden void @ED25519_keypair_from_seed(i8*, i8* nocapture, i8*) local_unnamed_addr #0 {
  %4 = alloca [32 x i8], align 16
  %5 = alloca %struct.fe_loose, align 8
  %6 = alloca %struct.fe, align 8
  %7 = alloca %struct.fe, align 8
  %8 = alloca %struct.fe, align 8
  %9 = alloca [64 x i8], align 16
  %10 = alloca %struct.ge_p3, align 8
  %11 = getelementptr inbounds [64 x i8], [64 x i8]* %9, i64 0, i64 0
  call void @llvm.lifetime.start.p0i8(i64 64, i8* nonnull %11) #4
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %11, i8 -86, i64 64, i1 false)
  %12 = call i8* @SHA512(i8* %2, i64 32, i8* nonnull %11) #4
  %13 = load i8, i8* %11, align 16
  %14 = and i8 %13, -8
  store i8 %14, i8* %11, align 16
  %15 = getelementptr inbounds [64 x i8], [64 x i8]* %9, i64 0, i64 31
  %16 = load i8, i8* %15, align 1
  %17 = and i8 %16, 63
  %18 = or i8 %17, 64
  store i8 %18, i8* %15, align 1
  %19 = bitcast %struct.ge_p3* %10 to i8*
  call void @llvm.lifetime.start.p0i8(i64 160, i8* nonnull %19) #4
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %19, i8 -86, i64 160, i1 false)
  call void @x25519_ge_scalarmult_small_precomp(%struct.ge_p3* nonnull %10, i8* nonnull %11, i8* getelementptr inbounds ([960 x i8], [960 x i8]* @k25519SmallPrecomp, i64 0, i64 0)) #4
  %20 = bitcast %struct.fe* %6 to i8*
  call void @llvm.lifetime.start.p0i8(i64 40, i8* nonnull %20) #4
  %21 = getelementptr inbounds %struct.fe, %struct.fe* %6, i64 0, i32 0, i64 0
  %22 = bitcast %struct.fe* %7 to i8*
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %20, i8 -86, i64 40, i1 false) #4
  call void @llvm.lifetime.start.p0i8(i64 40, i8* nonnull %22) #4
  %23 = getelementptr inbounds %struct.fe, %struct.fe* %7, i64 0, i32 0, i64 0
  %24 = bitcast %struct.fe* %8 to i8*
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %22, i8 -86, i64 40, i1 false) #4
  call void @llvm.lifetime.start.p0i8(i64 40, i8* nonnull %24) #4
  %25 = getelementptr inbounds %struct.fe, %struct.fe* %8, i64 0, i32 0, i64 0
  %26 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %10, i64 0, i32 2
  %27 = bitcast %struct.fe_loose* %5 to i8*
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %24, i8 -86, i64 40, i1 false) #4
  call void @llvm.lifetime.start.p0i8(i64 40, i8* nonnull %27) #4
  %28 = bitcast %struct.fe* %26 to i8*
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 8 %27, i8* align 8 %28, i64 40, i1 false) #4
  call fastcc void @fe_loose_invert(%struct.fe* nonnull %6, %struct.fe_loose* nonnull %5) #4
  call void @llvm.lifetime.end.p0i8(i64 40, i8* nonnull %27) #4
  %29 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %10, i64 0, i32 0, i32 0, i64 0
  call fastcc void @fe_mul_impl(i64* nonnull %23, i64* nonnull %29, i64* nonnull %21) #4
  %30 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %10, i64 0, i32 1, i32 0, i64 0
  call fastcc void @fe_mul_impl(i64* nonnull %25, i64* %30, i64* nonnull %21) #4
  call fastcc void @fe_tobytes(i8* %0, %struct.fe* nonnull %8) #4
  %31 = getelementptr inbounds [32 x i8], [32 x i8]* %4, i64 0, i64 0
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %31) #4
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %31, i8 -86, i64 32, i1 false) #4
  call fastcc void @fe_tobytes(i8* nonnull %31, %struct.fe* nonnull %7) #4
  %32 = load i8, i8* %31, align 16
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %31) #4
  %33 = getelementptr inbounds i8, i8* %0, i64 31
  %34 = load i8, i8* %33, align 1
  %35 = shl i8 %32, 7
  %36 = xor i8 %34, %35
  store i8 %36, i8* %33, align 1
  call void @llvm.lifetime.end.p0i8(i64 40, i8* nonnull %24) #4
  call void @llvm.lifetime.end.p0i8(i64 40, i8* nonnull %22) #4
  call void @llvm.lifetime.end.p0i8(i64 40, i8* nonnull %20) #4
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 1 %1, i8* align 1 %2, i64 32, i1 false) #4
  %37 = getelementptr inbounds i8, i8* %1, i64 32
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 1 %37, i8* align 1 %0, i64 32, i1 false) #4
  call void @llvm.lifetime.end.p0i8(i64 160, i8* nonnull %19) #4
  call void @llvm.lifetime.end.p0i8(i64 64, i8* nonnull %11) #4
  ret void
}

; Function Attrs: nounwind ssp uwtable
define hidden i32 @ED25519_sign(i8*, i8*, i64, i8*) local_unnamed_addr #0 {
  %5 = alloca [32 x i8], align 16
  %6 = alloca %struct.fe_loose, align 8
  %7 = alloca %struct.fe, align 8
  %8 = alloca %struct.fe, align 8
  %9 = alloca %struct.fe, align 8
  %10 = alloca [64 x i8], align 16
  %11 = alloca %struct.sha512_state_st, align 8
  %12 = alloca [64 x i8], align 16
  %13 = alloca %struct.ge_p3, align 8
  %14 = alloca [64 x i8], align 16
  %15 = getelementptr inbounds [64 x i8], [64 x i8]* %10, i64 0, i64 0
  call void @llvm.lifetime.start.p0i8(i64 64, i8* nonnull %15) #4
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %15, i8 -86, i64 64, i1 false)
  %16 = call i8* @SHA512(i8* %3, i64 32, i8* nonnull %15) #4
  %17 = load i8, i8* %15, align 16
  %18 = and i8 %17, -8
  store i8 %18, i8* %15, align 16
  %19 = getelementptr inbounds [64 x i8], [64 x i8]* %10, i64 0, i64 31
  %20 = load i8, i8* %19, align 1
  %21 = and i8 %20, 63
  %22 = or i8 %21, 64
  store i8 %22, i8* %19, align 1
  %23 = bitcast %struct.sha512_state_st* %11 to i8*
  call void @llvm.lifetime.start.p0i8(i64 216, i8* nonnull %23) #4
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %23, i8 -86, i64 216, i1 false)
  %24 = call i32 @SHA512_Init(%struct.sha512_state_st* nonnull %11) #4
  %25 = getelementptr inbounds [64 x i8], [64 x i8]* %10, i64 0, i64 32
  %26 = call i32 @SHA512_Update(%struct.sha512_state_st* nonnull %11, i8* %25, i64 32) #4
  %27 = call i32 @SHA512_Update(%struct.sha512_state_st* nonnull %11, i8* %1, i64 %2) #4
  %28 = getelementptr inbounds [64 x i8], [64 x i8]* %12, i64 0, i64 0
  call void @llvm.lifetime.start.p0i8(i64 64, i8* nonnull %28) #4
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %28, i8 -86, i64 64, i1 false)
  %29 = call i32 @SHA512_Final(i8* nonnull %28, %struct.sha512_state_st* nonnull %11) #4
  call void @x25519_sc_reduce(i8* nonnull %28)
  %30 = bitcast %struct.ge_p3* %13 to i8*
  call void @llvm.lifetime.start.p0i8(i64 160, i8* nonnull %30) #4
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %30, i8 -86, i64 160, i1 false)
  call void @x25519_ge_scalarmult_small_precomp(%struct.ge_p3* nonnull %13, i8* nonnull %28, i8* getelementptr inbounds ([960 x i8], [960 x i8]* @k25519SmallPrecomp, i64 0, i64 0)) #4
  %31 = bitcast %struct.fe* %7 to i8*
  call void @llvm.lifetime.start.p0i8(i64 40, i8* nonnull %31) #4
  %32 = getelementptr inbounds %struct.fe, %struct.fe* %7, i64 0, i32 0, i64 0
  %33 = bitcast %struct.fe* %8 to i8*
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %31, i8 -86, i64 40, i1 false) #4
  call void @llvm.lifetime.start.p0i8(i64 40, i8* nonnull %33) #4
  %34 = getelementptr inbounds %struct.fe, %struct.fe* %8, i64 0, i32 0, i64 0
  %35 = bitcast %struct.fe* %9 to i8*
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %33, i8 -86, i64 40, i1 false) #4
  call void @llvm.lifetime.start.p0i8(i64 40, i8* nonnull %35) #4
  %36 = getelementptr inbounds %struct.fe, %struct.fe* %9, i64 0, i32 0, i64 0
  %37 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %13, i64 0, i32 2
  %38 = bitcast %struct.fe_loose* %6 to i8*
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %35, i8 -86, i64 40, i1 false) #4
  call void @llvm.lifetime.start.p0i8(i64 40, i8* nonnull %38) #4
  %39 = bitcast %struct.fe* %37 to i8*
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 8 %38, i8* align 8 %39, i64 40, i1 false) #4
  call fastcc void @fe_loose_invert(%struct.fe* nonnull %7, %struct.fe_loose* nonnull %6) #4
  call void @llvm.lifetime.end.p0i8(i64 40, i8* nonnull %38) #4
  %40 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %13, i64 0, i32 0, i32 0, i64 0
  call fastcc void @fe_mul_impl(i64* nonnull %34, i64* nonnull %40, i64* nonnull %32) #4
  %41 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %13, i64 0, i32 1, i32 0, i64 0
  call fastcc void @fe_mul_impl(i64* nonnull %36, i64* %41, i64* nonnull %32) #4
  call fastcc void @fe_tobytes(i8* %0, %struct.fe* nonnull %9) #4
  %42 = getelementptr inbounds [32 x i8], [32 x i8]* %5, i64 0, i64 0
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %42) #4
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %42, i8 -86, i64 32, i1 false) #4
  call fastcc void @fe_tobytes(i8* nonnull %42, %struct.fe* nonnull %8) #4
  %43 = load i8, i8* %42, align 16
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %42) #4
  %44 = getelementptr inbounds i8, i8* %0, i64 31
  %45 = load i8, i8* %44, align 1
  %46 = shl i8 %43, 7
  %47 = xor i8 %45, %46
  store i8 %47, i8* %44, align 1
  call void @llvm.lifetime.end.p0i8(i64 40, i8* nonnull %35) #4
  call void @llvm.lifetime.end.p0i8(i64 40, i8* nonnull %33) #4
  call void @llvm.lifetime.end.p0i8(i64 40, i8* nonnull %31) #4
  %48 = call i32 @SHA512_Init(%struct.sha512_state_st* nonnull %11) #4
  %49 = call i32 @SHA512_Update(%struct.sha512_state_st* nonnull %11, i8* %0, i64 32) #4
  %50 = getelementptr inbounds i8, i8* %3, i64 32
  %51 = call i32 @SHA512_Update(%struct.sha512_state_st* nonnull %11, i8* %50, i64 32) #4
  %52 = call i32 @SHA512_Update(%struct.sha512_state_st* nonnull %11, i8* %1, i64 %2) #4
  %53 = getelementptr inbounds [64 x i8], [64 x i8]* %14, i64 0, i64 0
  call void @llvm.lifetime.start.p0i8(i64 64, i8* nonnull %53) #4
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %53, i8 -86, i64 64, i1 false)
  %54 = call i32 @SHA512_Final(i8* nonnull %53, %struct.sha512_state_st* nonnull %11) #4
  call void @x25519_sc_reduce(i8* nonnull %53)
  %55 = getelementptr inbounds i8, i8* %0, i64 32
  %56 = load i8, i8* %53, align 16
  %57 = zext i8 %56 to i64
  %58 = getelementptr inbounds [64 x i8], [64 x i8]* %14, i64 0, i64 1
  %59 = load i8, i8* %58, align 1
  %60 = zext i8 %59 to i64
  %61 = shl nuw nsw i64 %60, 8
  %62 = or i64 %61, %57
  %63 = getelementptr inbounds [64 x i8], [64 x i8]* %14, i64 0, i64 2
  %64 = load i8, i8* %63, align 2
  %65 = zext i8 %64 to i64
  %66 = shl nuw nsw i64 %65, 16
  %67 = and i64 %66, 2031616
  %68 = or i64 %62, %67
  %69 = getelementptr inbounds [64 x i8], [64 x i8]* %14, i64 0, i64 3
  %70 = load i8, i8* %69, align 1
  %71 = zext i8 %70 to i64
  %72 = shl nuw nsw i64 %71, 8
  %73 = or i64 %72, %65
  %74 = getelementptr inbounds [64 x i8], [64 x i8]* %14, i64 0, i64 4
  %75 = load i8, i8* %74, align 4
  %76 = zext i8 %75 to i64
  %77 = shl nuw nsw i64 %76, 16
  %78 = or i64 %73, %77
  %79 = getelementptr inbounds [64 x i8], [64 x i8]* %14, i64 0, i64 5
  %80 = load i8, i8* %79, align 1
  %81 = zext i8 %80 to i64
  %82 = shl nuw nsw i64 %81, 24
  %83 = or i64 %78, %82
  %84 = lshr i64 %83, 5
  %85 = and i64 %84, 2097151
  %86 = getelementptr inbounds [64 x i8], [64 x i8]* %14, i64 0, i64 6
  %87 = load i8, i8* %86, align 2
  %88 = zext i8 %87 to i64
  %89 = shl nuw nsw i64 %88, 8
  %90 = or i64 %89, %81
  %91 = getelementptr inbounds [64 x i8], [64 x i8]* %14, i64 0, i64 7
  %92 = load i8, i8* %91, align 1
  %93 = zext i8 %92 to i64
  %94 = shl nuw nsw i64 %93, 16
  %95 = or i64 %90, %94
  %96 = lshr i64 %95, 2
  %97 = and i64 %96, 2097151
  %98 = getelementptr inbounds [64 x i8], [64 x i8]* %14, i64 0, i64 8
  %99 = load i8, i8* %98, align 8
  %100 = zext i8 %99 to i64
  %101 = shl nuw nsw i64 %100, 8
  %102 = or i64 %101, %93
  %103 = getelementptr inbounds [64 x i8], [64 x i8]* %14, i64 0, i64 9
  %104 = load i8, i8* %103, align 1
  %105 = zext i8 %104 to i64
  %106 = shl nuw nsw i64 %105, 16
  %107 = or i64 %102, %106
  %108 = getelementptr inbounds [64 x i8], [64 x i8]* %14, i64 0, i64 10
  %109 = load i8, i8* %108, align 2
  %110 = zext i8 %109 to i64
  %111 = shl nuw nsw i64 %110, 24
  %112 = or i64 %107, %111
  %113 = lshr i64 %112, 7
  %114 = and i64 %113, 2097151
  %115 = getelementptr inbounds [64 x i8], [64 x i8]* %14, i64 0, i64 11
  %116 = load i8, i8* %115, align 1
  %117 = zext i8 %116 to i64
  %118 = shl nuw nsw i64 %117, 8
  %119 = or i64 %118, %110
  %120 = getelementptr inbounds [64 x i8], [64 x i8]* %14, i64 0, i64 12
  %121 = load i8, i8* %120, align 4
  %122 = zext i8 %121 to i64
  %123 = shl nuw nsw i64 %122, 16
  %124 = or i64 %119, %123
  %125 = getelementptr inbounds [64 x i8], [64 x i8]* %14, i64 0, i64 13
  %126 = load i8, i8* %125, align 1
  %127 = zext i8 %126 to i64
  %128 = shl nuw nsw i64 %127, 24
  %129 = or i64 %124, %128
  %130 = lshr i64 %129, 4
  %131 = and i64 %130, 2097151
  %132 = getelementptr inbounds [64 x i8], [64 x i8]* %14, i64 0, i64 14
  %133 = load i8, i8* %132, align 2
  %134 = zext i8 %133 to i64
  %135 = shl nuw nsw i64 %134, 8
  %136 = or i64 %135, %127
  %137 = getelementptr inbounds [64 x i8], [64 x i8]* %14, i64 0, i64 15
  %138 = load i8, i8* %137, align 1
  %139 = zext i8 %138 to i64
  %140 = shl nuw nsw i64 %139, 16
  %141 = or i64 %136, %140
  %142 = lshr i64 %141, 1
  %143 = and i64 %142, 2097151
  %144 = getelementptr inbounds [64 x i8], [64 x i8]* %14, i64 0, i64 16
  %145 = load i8, i8* %144, align 16
  %146 = zext i8 %145 to i64
  %147 = shl nuw nsw i64 %146, 8
  %148 = or i64 %147, %139
  %149 = getelementptr inbounds [64 x i8], [64 x i8]* %14, i64 0, i64 17
  %150 = load i8, i8* %149, align 1
  %151 = zext i8 %150 to i64
  %152 = shl nuw nsw i64 %151, 16
  %153 = or i64 %148, %152
  %154 = getelementptr inbounds [64 x i8], [64 x i8]* %14, i64 0, i64 18
  %155 = load i8, i8* %154, align 2
  %156 = zext i8 %155 to i64
  %157 = shl nuw nsw i64 %156, 24
  %158 = or i64 %153, %157
  %159 = lshr i64 %158, 6
  %160 = and i64 %159, 2097151
  %161 = getelementptr inbounds [64 x i8], [64 x i8]* %14, i64 0, i64 19
  %162 = load i8, i8* %161, align 1
  %163 = zext i8 %162 to i64
  %164 = shl nuw nsw i64 %163, 8
  %165 = or i64 %164, %156
  %166 = getelementptr inbounds [64 x i8], [64 x i8]* %14, i64 0, i64 20
  %167 = load i8, i8* %166, align 4
  %168 = zext i8 %167 to i64
  %169 = shl nuw nsw i64 %168, 16
  %170 = or i64 %165, %169
  %171 = lshr i64 %170, 3
  %172 = getelementptr inbounds [64 x i8], [64 x i8]* %14, i64 0, i64 21
  %173 = load i8, i8* %172, align 1
  %174 = zext i8 %173 to i64
  %175 = getelementptr inbounds [64 x i8], [64 x i8]* %14, i64 0, i64 22
  %176 = load i8, i8* %175, align 2
  %177 = zext i8 %176 to i64
  %178 = shl nuw nsw i64 %177, 8
  %179 = or i64 %178, %174
  %180 = getelementptr inbounds [64 x i8], [64 x i8]* %14, i64 0, i64 23
  %181 = load i8, i8* %180, align 1
  %182 = zext i8 %181 to i64
  %183 = shl nuw nsw i64 %182, 16
  %184 = and i64 %183, 2031616
  %185 = or i64 %179, %184
  %186 = getelementptr inbounds [64 x i8], [64 x i8]* %14, i64 0, i64 24
  %187 = load i8, i8* %186, align 8
  %188 = zext i8 %187 to i64
  %189 = shl nuw nsw i64 %188, 8
  %190 = or i64 %189, %182
  %191 = getelementptr inbounds [64 x i8], [64 x i8]* %14, i64 0, i64 25
  %192 = load i8, i8* %191, align 1
  %193 = zext i8 %192 to i64
  %194 = shl nuw nsw i64 %193, 16
  %195 = or i64 %190, %194
  %196 = getelementptr inbounds [64 x i8], [64 x i8]* %14, i64 0, i64 26
  %197 = load i8, i8* %196, align 2
  %198 = zext i8 %197 to i64
  %199 = shl nuw nsw i64 %198, 24
  %200 = or i64 %195, %199
  %201 = lshr i64 %200, 5
  %202 = and i64 %201, 2097151
  %203 = getelementptr inbounds [64 x i8], [64 x i8]* %14, i64 0, i64 27
  %204 = load i8, i8* %203, align 1
  %205 = zext i8 %204 to i64
  %206 = shl nuw nsw i64 %205, 8
  %207 = or i64 %206, %198
  %208 = getelementptr inbounds [64 x i8], [64 x i8]* %14, i64 0, i64 28
  %209 = load i8, i8* %208, align 4
  %210 = zext i8 %209 to i64
  %211 = shl nuw nsw i64 %210, 16
  %212 = or i64 %207, %211
  %213 = lshr i64 %212, 2
  %214 = and i64 %213, 2097151
  %215 = getelementptr inbounds [64 x i8], [64 x i8]* %14, i64 0, i64 29
  %216 = load i8, i8* %215, align 1
  %217 = zext i8 %216 to i64
  %218 = shl nuw nsw i64 %217, 8
  %219 = or i64 %218, %210
  %220 = getelementptr inbounds [64 x i8], [64 x i8]* %14, i64 0, i64 30
  %221 = load i8, i8* %220, align 2
  %222 = zext i8 %221 to i64
  %223 = shl nuw nsw i64 %222, 16
  %224 = or i64 %219, %223
  %225 = getelementptr inbounds [64 x i8], [64 x i8]* %14, i64 0, i64 31
  %226 = load i8, i8* %225, align 1
  %227 = zext i8 %226 to i64
  %228 = shl nuw nsw i64 %227, 24
  %229 = or i64 %224, %228
  %230 = lshr i64 %229, 7
  %231 = load i8, i8* %15, align 16
  %232 = zext i8 %231 to i64
  %233 = getelementptr inbounds [64 x i8], [64 x i8]* %10, i64 0, i64 1
  %234 = load i8, i8* %233, align 1
  %235 = zext i8 %234 to i64
  %236 = shl nuw nsw i64 %235, 8
  %237 = or i64 %236, %232
  %238 = getelementptr inbounds [64 x i8], [64 x i8]* %10, i64 0, i64 2
  %239 = load i8, i8* %238, align 2
  %240 = zext i8 %239 to i64
  %241 = shl nuw nsw i64 %240, 16
  %242 = and i64 %241, 2031616
  %243 = or i64 %237, %242
  %244 = getelementptr inbounds [64 x i8], [64 x i8]* %10, i64 0, i64 3
  %245 = load i8, i8* %244, align 1
  %246 = zext i8 %245 to i64
  %247 = shl nuw nsw i64 %246, 8
  %248 = or i64 %247, %240
  %249 = getelementptr inbounds [64 x i8], [64 x i8]* %10, i64 0, i64 4
  %250 = load i8, i8* %249, align 4
  %251 = zext i8 %250 to i64
  %252 = shl nuw nsw i64 %251, 16
  %253 = or i64 %248, %252
  %254 = getelementptr inbounds [64 x i8], [64 x i8]* %10, i64 0, i64 5
  %255 = load i8, i8* %254, align 1
  %256 = zext i8 %255 to i64
  %257 = shl nuw nsw i64 %256, 24
  %258 = or i64 %253, %257
  %259 = lshr i64 %258, 5
  %260 = and i64 %259, 2097151
  %261 = getelementptr inbounds [64 x i8], [64 x i8]* %10, i64 0, i64 6
  %262 = load i8, i8* %261, align 2
  %263 = zext i8 %262 to i64
  %264 = shl nuw nsw i64 %263, 8
  %265 = or i64 %264, %256
  %266 = getelementptr inbounds [64 x i8], [64 x i8]* %10, i64 0, i64 7
  %267 = load i8, i8* %266, align 1
  %268 = zext i8 %267 to i64
  %269 = shl nuw nsw i64 %268, 16
  %270 = or i64 %265, %269
  %271 = lshr i64 %270, 2
  %272 = and i64 %271, 2097151
  %273 = getelementptr inbounds [64 x i8], [64 x i8]* %10, i64 0, i64 8
  %274 = load i8, i8* %273, align 8
  %275 = zext i8 %274 to i64
  %276 = shl nuw nsw i64 %275, 8
  %277 = or i64 %276, %268
  %278 = getelementptr inbounds [64 x i8], [64 x i8]* %10, i64 0, i64 9
  %279 = load i8, i8* %278, align 1
  %280 = zext i8 %279 to i64
  %281 = shl nuw nsw i64 %280, 16
  %282 = or i64 %277, %281
  %283 = getelementptr inbounds [64 x i8], [64 x i8]* %10, i64 0, i64 10
  %284 = load i8, i8* %283, align 2
  %285 = zext i8 %284 to i64
  %286 = shl nuw nsw i64 %285, 24
  %287 = or i64 %282, %286
  %288 = lshr i64 %287, 7
  %289 = and i64 %288, 2097151
  %290 = getelementptr inbounds [64 x i8], [64 x i8]* %10, i64 0, i64 11
  %291 = load i8, i8* %290, align 1
  %292 = zext i8 %291 to i64
  %293 = shl nuw nsw i64 %292, 8
  %294 = or i64 %293, %285
  %295 = getelementptr inbounds [64 x i8], [64 x i8]* %10, i64 0, i64 12
  %296 = load i8, i8* %295, align 4
  %297 = zext i8 %296 to i64
  %298 = shl nuw nsw i64 %297, 16
  %299 = or i64 %294, %298
  %300 = getelementptr inbounds [64 x i8], [64 x i8]* %10, i64 0, i64 13
  %301 = load i8, i8* %300, align 1
  %302 = zext i8 %301 to i64
  %303 = shl nuw nsw i64 %302, 24
  %304 = or i64 %299, %303
  %305 = lshr i64 %304, 4
  %306 = and i64 %305, 2097151
  %307 = getelementptr inbounds [64 x i8], [64 x i8]* %10, i64 0, i64 14
  %308 = load i8, i8* %307, align 2
  %309 = zext i8 %308 to i64
  %310 = shl nuw nsw i64 %309, 8
  %311 = or i64 %310, %302
  %312 = getelementptr inbounds [64 x i8], [64 x i8]* %10, i64 0, i64 15
  %313 = load i8, i8* %312, align 1
  %314 = zext i8 %313 to i64
  %315 = shl nuw nsw i64 %314, 16
  %316 = or i64 %311, %315
  %317 = lshr i64 %316, 1
  %318 = and i64 %317, 2097151
  %319 = getelementptr inbounds [64 x i8], [64 x i8]* %10, i64 0, i64 16
  %320 = load i8, i8* %319, align 16
  %321 = zext i8 %320 to i64
  %322 = shl nuw nsw i64 %321, 8
  %323 = or i64 %322, %314
  %324 = getelementptr inbounds [64 x i8], [64 x i8]* %10, i64 0, i64 17
  %325 = load i8, i8* %324, align 1
  %326 = zext i8 %325 to i64
  %327 = shl nuw nsw i64 %326, 16
  %328 = or i64 %323, %327
  %329 = getelementptr inbounds [64 x i8], [64 x i8]* %10, i64 0, i64 18
  %330 = load i8, i8* %329, align 2
  %331 = zext i8 %330 to i64
  %332 = shl nuw nsw i64 %331, 24
  %333 = or i64 %328, %332
  %334 = lshr i64 %333, 6
  %335 = and i64 %334, 2097151
  %336 = getelementptr inbounds [64 x i8], [64 x i8]* %10, i64 0, i64 19
  %337 = load i8, i8* %336, align 1
  %338 = zext i8 %337 to i64
  %339 = shl nuw nsw i64 %338, 8
  %340 = or i64 %339, %331
  %341 = getelementptr inbounds [64 x i8], [64 x i8]* %10, i64 0, i64 20
  %342 = load i8, i8* %341, align 4
  %343 = zext i8 %342 to i64
  %344 = shl nuw nsw i64 %343, 16
  %345 = or i64 %340, %344
  %346 = lshr i64 %345, 3
  %347 = getelementptr inbounds [64 x i8], [64 x i8]* %10, i64 0, i64 21
  %348 = load i8, i8* %347, align 1
  %349 = zext i8 %348 to i64
  %350 = getelementptr inbounds [64 x i8], [64 x i8]* %10, i64 0, i64 22
  %351 = load i8, i8* %350, align 2
  %352 = zext i8 %351 to i64
  %353 = shl nuw nsw i64 %352, 8
  %354 = or i64 %353, %349
  %355 = getelementptr inbounds [64 x i8], [64 x i8]* %10, i64 0, i64 23
  %356 = load i8, i8* %355, align 1
  %357 = zext i8 %356 to i64
  %358 = shl nuw nsw i64 %357, 16
  %359 = and i64 %358, 2031616
  %360 = or i64 %354, %359
  %361 = getelementptr inbounds [64 x i8], [64 x i8]* %10, i64 0, i64 24
  %362 = load i8, i8* %361, align 8
  %363 = zext i8 %362 to i64
  %364 = shl nuw nsw i64 %363, 8
  %365 = or i64 %364, %357
  %366 = getelementptr inbounds [64 x i8], [64 x i8]* %10, i64 0, i64 25
  %367 = load i8, i8* %366, align 1
  %368 = zext i8 %367 to i64
  %369 = shl nuw nsw i64 %368, 16
  %370 = or i64 %365, %369
  %371 = getelementptr inbounds [64 x i8], [64 x i8]* %10, i64 0, i64 26
  %372 = load i8, i8* %371, align 2
  %373 = zext i8 %372 to i64
  %374 = shl nuw nsw i64 %373, 24
  %375 = or i64 %370, %374
  %376 = lshr i64 %375, 5
  %377 = and i64 %376, 2097151
  %378 = getelementptr inbounds [64 x i8], [64 x i8]* %10, i64 0, i64 27
  %379 = load i8, i8* %378, align 1
  %380 = zext i8 %379 to i64
  %381 = shl nuw nsw i64 %380, 8
  %382 = or i64 %381, %373
  %383 = getelementptr inbounds [64 x i8], [64 x i8]* %10, i64 0, i64 28
  %384 = load i8, i8* %383, align 4
  %385 = zext i8 %384 to i64
  %386 = shl nuw nsw i64 %385, 16
  %387 = or i64 %382, %386
  %388 = lshr i64 %387, 2
  %389 = and i64 %388, 2097151
  %390 = getelementptr inbounds [64 x i8], [64 x i8]* %10, i64 0, i64 29
  %391 = load i8, i8* %390, align 1
  %392 = zext i8 %391 to i64
  %393 = shl nuw nsw i64 %392, 8
  %394 = or i64 %393, %385
  %395 = getelementptr inbounds [64 x i8], [64 x i8]* %10, i64 0, i64 30
  %396 = load i8, i8* %395, align 2
  %397 = zext i8 %396 to i64
  %398 = shl nuw nsw i64 %397, 16
  %399 = or i64 %394, %398
  %400 = load i8, i8* %19, align 1
  %401 = zext i8 %400 to i64
  %402 = shl nuw nsw i64 %401, 24
  %403 = or i64 %399, %402
  %404 = lshr i64 %403, 7
  %405 = load i8, i8* %28, align 16
  %406 = zext i8 %405 to i64
  %407 = getelementptr inbounds [64 x i8], [64 x i8]* %12, i64 0, i64 1
  %408 = load i8, i8* %407, align 1
  %409 = zext i8 %408 to i64
  %410 = shl nuw nsw i64 %409, 8
  %411 = or i64 %410, %406
  %412 = getelementptr inbounds [64 x i8], [64 x i8]* %12, i64 0, i64 2
  %413 = load i8, i8* %412, align 2
  %414 = zext i8 %413 to i64
  %415 = shl nuw nsw i64 %414, 16
  %416 = and i64 %415, 2031616
  %417 = or i64 %411, %416
  %418 = getelementptr inbounds [64 x i8], [64 x i8]* %12, i64 0, i64 3
  %419 = load i8, i8* %418, align 1
  %420 = zext i8 %419 to i64
  %421 = shl nuw nsw i64 %420, 8
  %422 = or i64 %421, %414
  %423 = getelementptr inbounds [64 x i8], [64 x i8]* %12, i64 0, i64 4
  %424 = load i8, i8* %423, align 4
  %425 = zext i8 %424 to i64
  %426 = shl nuw nsw i64 %425, 16
  %427 = or i64 %422, %426
  %428 = getelementptr inbounds [64 x i8], [64 x i8]* %12, i64 0, i64 5
  %429 = load i8, i8* %428, align 1
  %430 = zext i8 %429 to i64
  %431 = shl nuw nsw i64 %430, 24
  %432 = or i64 %427, %431
  %433 = lshr i64 %432, 5
  %434 = and i64 %433, 2097151
  %435 = getelementptr inbounds [64 x i8], [64 x i8]* %12, i64 0, i64 6
  %436 = load i8, i8* %435, align 2
  %437 = zext i8 %436 to i64
  %438 = shl nuw nsw i64 %437, 8
  %439 = or i64 %438, %430
  %440 = getelementptr inbounds [64 x i8], [64 x i8]* %12, i64 0, i64 7
  %441 = load i8, i8* %440, align 1
  %442 = zext i8 %441 to i64
  %443 = shl nuw nsw i64 %442, 16
  %444 = or i64 %439, %443
  %445 = lshr i64 %444, 2
  %446 = and i64 %445, 2097151
  %447 = getelementptr inbounds [64 x i8], [64 x i8]* %12, i64 0, i64 8
  %448 = load i8, i8* %447, align 8
  %449 = zext i8 %448 to i64
  %450 = shl nuw nsw i64 %449, 8
  %451 = or i64 %450, %442
  %452 = getelementptr inbounds [64 x i8], [64 x i8]* %12, i64 0, i64 9
  %453 = load i8, i8* %452, align 1
  %454 = zext i8 %453 to i64
  %455 = shl nuw nsw i64 %454, 16
  %456 = or i64 %451, %455
  %457 = getelementptr inbounds [64 x i8], [64 x i8]* %12, i64 0, i64 10
  %458 = load i8, i8* %457, align 2
  %459 = zext i8 %458 to i64
  %460 = shl nuw nsw i64 %459, 24
  %461 = or i64 %456, %460
  %462 = lshr i64 %461, 7
  %463 = and i64 %462, 2097151
  %464 = getelementptr inbounds [64 x i8], [64 x i8]* %12, i64 0, i64 11
  %465 = load i8, i8* %464, align 1
  %466 = zext i8 %465 to i64
  %467 = shl nuw nsw i64 %466, 8
  %468 = or i64 %467, %459
  %469 = getelementptr inbounds [64 x i8], [64 x i8]* %12, i64 0, i64 12
  %470 = load i8, i8* %469, align 4
  %471 = zext i8 %470 to i64
  %472 = shl nuw nsw i64 %471, 16
  %473 = or i64 %468, %472
  %474 = getelementptr inbounds [64 x i8], [64 x i8]* %12, i64 0, i64 13
  %475 = load i8, i8* %474, align 1
  %476 = zext i8 %475 to i64
  %477 = shl nuw nsw i64 %476, 24
  %478 = or i64 %473, %477
  %479 = lshr i64 %478, 4
  %480 = and i64 %479, 2097151
  %481 = getelementptr inbounds [64 x i8], [64 x i8]* %12, i64 0, i64 14
  %482 = load i8, i8* %481, align 2
  %483 = zext i8 %482 to i64
  %484 = shl nuw nsw i64 %483, 8
  %485 = or i64 %484, %476
  %486 = getelementptr inbounds [64 x i8], [64 x i8]* %12, i64 0, i64 15
  %487 = load i8, i8* %486, align 1
  %488 = zext i8 %487 to i64
  %489 = shl nuw nsw i64 %488, 16
  %490 = or i64 %485, %489
  %491 = lshr i64 %490, 1
  %492 = and i64 %491, 2097151
  %493 = getelementptr inbounds [64 x i8], [64 x i8]* %12, i64 0, i64 16
  %494 = load i8, i8* %493, align 16
  %495 = zext i8 %494 to i64
  %496 = shl nuw nsw i64 %495, 8
  %497 = or i64 %496, %488
  %498 = getelementptr inbounds [64 x i8], [64 x i8]* %12, i64 0, i64 17
  %499 = load i8, i8* %498, align 1
  %500 = zext i8 %499 to i64
  %501 = shl nuw nsw i64 %500, 16
  %502 = or i64 %497, %501
  %503 = getelementptr inbounds [64 x i8], [64 x i8]* %12, i64 0, i64 18
  %504 = load i8, i8* %503, align 2
  %505 = zext i8 %504 to i64
  %506 = shl nuw nsw i64 %505, 24
  %507 = or i64 %502, %506
  %508 = lshr i64 %507, 6
  %509 = and i64 %508, 2097151
  %510 = getelementptr inbounds [64 x i8], [64 x i8]* %12, i64 0, i64 19
  %511 = load i8, i8* %510, align 1
  %512 = zext i8 %511 to i64
  %513 = shl nuw nsw i64 %512, 8
  %514 = or i64 %513, %505
  %515 = getelementptr inbounds [64 x i8], [64 x i8]* %12, i64 0, i64 20
  %516 = load i8, i8* %515, align 4
  %517 = zext i8 %516 to i64
  %518 = shl nuw nsw i64 %517, 16
  %519 = or i64 %514, %518
  %520 = lshr i64 %519, 3
  %521 = getelementptr inbounds [64 x i8], [64 x i8]* %12, i64 0, i64 21
  %522 = load i8, i8* %521, align 1
  %523 = zext i8 %522 to i64
  %524 = getelementptr inbounds [64 x i8], [64 x i8]* %12, i64 0, i64 22
  %525 = load i8, i8* %524, align 2
  %526 = zext i8 %525 to i64
  %527 = shl nuw nsw i64 %526, 8
  %528 = or i64 %527, %523
  %529 = getelementptr inbounds [64 x i8], [64 x i8]* %12, i64 0, i64 23
  %530 = load i8, i8* %529, align 1
  %531 = zext i8 %530 to i64
  %532 = shl nuw nsw i64 %531, 16
  %533 = and i64 %532, 2031616
  %534 = or i64 %528, %533
  %535 = getelementptr inbounds [64 x i8], [64 x i8]* %12, i64 0, i64 24
  %536 = load i8, i8* %535, align 8
  %537 = zext i8 %536 to i64
  %538 = shl nuw nsw i64 %537, 8
  %539 = or i64 %538, %531
  %540 = getelementptr inbounds [64 x i8], [64 x i8]* %12, i64 0, i64 25
  %541 = load i8, i8* %540, align 1
  %542 = zext i8 %541 to i64
  %543 = shl nuw nsw i64 %542, 16
  %544 = or i64 %539, %543
  %545 = getelementptr inbounds [64 x i8], [64 x i8]* %12, i64 0, i64 26
  %546 = load i8, i8* %545, align 2
  %547 = zext i8 %546 to i64
  %548 = shl nuw nsw i64 %547, 24
  %549 = or i64 %544, %548
  %550 = lshr i64 %549, 5
  %551 = and i64 %550, 2097151
  %552 = getelementptr inbounds [64 x i8], [64 x i8]* %12, i64 0, i64 27
  %553 = load i8, i8* %552, align 1
  %554 = zext i8 %553 to i64
  %555 = shl nuw nsw i64 %554, 8
  %556 = or i64 %555, %547
  %557 = getelementptr inbounds [64 x i8], [64 x i8]* %12, i64 0, i64 28
  %558 = load i8, i8* %557, align 4
  %559 = zext i8 %558 to i64
  %560 = shl nuw nsw i64 %559, 16
  %561 = or i64 %556, %560
  %562 = lshr i64 %561, 2
  %563 = and i64 %562, 2097151
  %564 = getelementptr inbounds [64 x i8], [64 x i8]* %12, i64 0, i64 29
  %565 = load i8, i8* %564, align 1
  %566 = zext i8 %565 to i64
  %567 = shl nuw nsw i64 %566, 8
  %568 = or i64 %567, %559
  %569 = getelementptr inbounds [64 x i8], [64 x i8]* %12, i64 0, i64 30
  %570 = load i8, i8* %569, align 2
  %571 = zext i8 %570 to i64
  %572 = shl nuw nsw i64 %571, 16
  %573 = or i64 %568, %572
  %574 = getelementptr inbounds [64 x i8], [64 x i8]* %12, i64 0, i64 31
  %575 = load i8, i8* %574, align 1
  %576 = zext i8 %575 to i64
  %577 = shl nuw nsw i64 %576, 24
  %578 = or i64 %573, %577
  %579 = lshr i64 %578, 7
  %580 = mul nuw nsw i64 %243, %68
  %581 = add nuw nsw i64 %417, %580
  %582 = mul nuw nsw i64 %260, %68
  %583 = mul nuw nsw i64 %243, %85
  %584 = mul nuw nsw i64 %272, %68
  %585 = mul nuw nsw i64 %260, %85
  %586 = mul nuw nsw i64 %243, %97
  %587 = add nuw nsw i64 %585, %586
  %588 = add nuw nsw i64 %587, %584
  %589 = add i64 %588, %446
  %590 = mul nuw nsw i64 %289, %68
  %591 = mul nuw nsw i64 %272, %85
  %592 = mul nuw nsw i64 %260, %97
  %593 = mul nuw nsw i64 %243, %114
  %594 = mul nuw nsw i64 %306, %68
  %595 = mul nuw nsw i64 %289, %85
  %596 = mul nuw nsw i64 %272, %97
  %597 = mul nuw nsw i64 %260, %114
  %598 = mul nuw nsw i64 %243, %131
  %599 = add nuw nsw i64 %597, %598
  %600 = add nuw nsw i64 %599, %596
  %601 = add i64 %600, %595
  %602 = add i64 %601, %594
  %603 = add i64 %602, %480
  %604 = mul nuw nsw i64 %318, %68
  %605 = mul nuw nsw i64 %306, %85
  %606 = mul nuw nsw i64 %289, %97
  %607 = mul nuw nsw i64 %272, %114
  %608 = mul nuw nsw i64 %260, %131
  %609 = mul nuw nsw i64 %243, %143
  %610 = mul nuw nsw i64 %335, %68
  %611 = mul nuw nsw i64 %318, %85
  %612 = mul nuw nsw i64 %306, %97
  %613 = mul nuw nsw i64 %289, %114
  %614 = mul nuw nsw i64 %272, %131
  %615 = mul nuw nsw i64 %260, %143
  %616 = mul nuw nsw i64 %243, %160
  %617 = add nuw nsw i64 %615, %616
  %618 = add nuw nsw i64 %617, %614
  %619 = add i64 %618, %613
  %620 = add i64 %619, %612
  %621 = add i64 %620, %611
  %622 = add i64 %621, %610
  %623 = add i64 %622, %509
  %624 = mul nuw nsw i64 %346, %68
  %625 = mul nuw nsw i64 %335, %85
  %626 = mul nuw nsw i64 %318, %97
  %627 = mul nuw nsw i64 %306, %114
  %628 = mul nuw nsw i64 %289, %131
  %629 = mul nuw nsw i64 %272, %143
  %630 = mul nuw nsw i64 %260, %160
  %631 = mul nuw nsw i64 %243, %171
  %632 = mul nuw nsw i64 %360, %68
  %633 = mul nuw nsw i64 %346, %85
  %634 = mul nuw nsw i64 %335, %97
  %635 = mul nuw nsw i64 %318, %114
  %636 = mul nuw nsw i64 %306, %131
  %637 = mul nuw nsw i64 %289, %143
  %638 = mul nuw nsw i64 %272, %160
  %639 = mul nuw nsw i64 %260, %171
  %640 = mul nuw nsw i64 %243, %185
  %641 = add nuw nsw i64 %639, %640
  %642 = add nuw i64 %641, %638
  %643 = add i64 %642, %637
  %644 = add i64 %643, %636
  %645 = add i64 %644, %635
  %646 = add i64 %645, %634
  %647 = add i64 %646, %633
  %648 = add i64 %647, %632
  %649 = add i64 %648, %534
  %650 = mul nuw nsw i64 %377, %68
  %651 = mul nuw nsw i64 %360, %85
  %652 = mul nuw nsw i64 %346, %97
  %653 = mul nuw nsw i64 %335, %114
  %654 = mul nuw nsw i64 %318, %131
  %655 = mul nuw nsw i64 %306, %143
  %656 = mul nuw nsw i64 %289, %160
  %657 = mul nuw nsw i64 %272, %171
  %658 = mul nuw nsw i64 %260, %185
  %659 = mul nuw nsw i64 %243, %202
  %660 = mul nuw nsw i64 %389, %68
  %661 = mul nuw nsw i64 %377, %85
  %662 = mul nuw nsw i64 %360, %97
  %663 = mul nuw nsw i64 %346, %114
  %664 = mul nuw nsw i64 %335, %131
  %665 = mul nuw nsw i64 %318, %143
  %666 = mul nuw nsw i64 %306, %160
  %667 = mul nuw nsw i64 %289, %171
  %668 = mul nuw nsw i64 %272, %185
  %669 = mul nuw nsw i64 %260, %202
  %670 = mul nuw nsw i64 %243, %214
  %671 = add nuw nsw i64 %669, %670
  %672 = add nuw nsw i64 %671, %668
  %673 = add i64 %672, %667
  %674 = add i64 %673, %666
  %675 = add i64 %674, %665
  %676 = add i64 %675, %664
  %677 = add i64 %676, %663
  %678 = add i64 %677, %662
  %679 = add i64 %678, %661
  %680 = add i64 %679, %660
  %681 = add i64 %680, %563
  %682 = mul nuw nsw i64 %404, %68
  %683 = mul nuw nsw i64 %389, %85
  %684 = mul nuw nsw i64 %377, %97
  %685 = mul nuw nsw i64 %360, %114
  %686 = mul nuw nsw i64 %346, %131
  %687 = mul nuw nsw i64 %335, %143
  %688 = mul nuw nsw i64 %318, %160
  %689 = mul nuw nsw i64 %306, %171
  %690 = mul nuw nsw i64 %289, %185
  %691 = mul nuw nsw i64 %272, %202
  %692 = mul nuw nsw i64 %260, %214
  %693 = mul nuw nsw i64 %243, %230
  %694 = mul nuw nsw i64 %404, %85
  %695 = mul nuw nsw i64 %389, %97
  %696 = mul nuw nsw i64 %377, %114
  %697 = mul nuw nsw i64 %360, %131
  %698 = mul nuw nsw i64 %346, %143
  %699 = mul nuw nsw i64 %335, %160
  %700 = mul nuw nsw i64 %318, %171
  %701 = mul nuw nsw i64 %306, %185
  %702 = mul nuw nsw i64 %289, %202
  %703 = mul nuw nsw i64 %272, %214
  %704 = mul nuw nsw i64 %260, %230
  %705 = add nuw i64 %703, %704
  %706 = add i64 %705, %702
  %707 = add i64 %706, %701
  %708 = add i64 %707, %700
  %709 = add i64 %708, %699
  %710 = add i64 %709, %698
  %711 = add i64 %710, %697
  %712 = add i64 %711, %696
  %713 = add i64 %712, %695
  %714 = add i64 %713, %694
  %715 = mul nuw nsw i64 %404, %97
  %716 = mul nuw nsw i64 %389, %114
  %717 = mul nuw nsw i64 %377, %131
  %718 = mul nuw nsw i64 %360, %143
  %719 = mul nuw nsw i64 %346, %160
  %720 = mul nuw nsw i64 %335, %171
  %721 = mul nuw nsw i64 %318, %185
  %722 = mul nuw nsw i64 %306, %202
  %723 = mul nuw nsw i64 %289, %214
  %724 = mul nuw nsw i64 %272, %230
  %725 = mul nuw nsw i64 %404, %114
  %726 = mul nuw nsw i64 %389, %131
  %727 = mul nuw nsw i64 %377, %143
  %728 = mul nuw nsw i64 %360, %160
  %729 = mul nuw nsw i64 %346, %171
  %730 = mul nuw nsw i64 %335, %185
  %731 = mul nuw nsw i64 %318, %202
  %732 = mul nuw nsw i64 %306, %214
  %733 = mul nuw nsw i64 %289, %230
  %734 = add nuw i64 %732, %733
  %735 = add i64 %734, %731
  %736 = add i64 %735, %730
  %737 = add i64 %736, %729
  %738 = add i64 %737, %728
  %739 = add i64 %738, %727
  %740 = add i64 %739, %726
  %741 = add i64 %740, %725
  %742 = mul nuw nsw i64 %404, %131
  %743 = mul nuw nsw i64 %389, %143
  %744 = mul nuw nsw i64 %377, %160
  %745 = mul nuw nsw i64 %360, %171
  %746 = mul nuw nsw i64 %346, %185
  %747 = mul nuw nsw i64 %335, %202
  %748 = mul nuw nsw i64 %318, %214
  %749 = mul nuw nsw i64 %306, %230
  %750 = mul nuw nsw i64 %404, %143
  %751 = mul nuw nsw i64 %389, %160
  %752 = mul nuw nsw i64 %377, %171
  %753 = mul nuw nsw i64 %360, %185
  %754 = mul nuw nsw i64 %346, %202
  %755 = mul nuw nsw i64 %335, %214
  %756 = mul nuw nsw i64 %318, %230
  %757 = add nuw i64 %755, %756
  %758 = add i64 %757, %754
  %759 = add i64 %758, %753
  %760 = add i64 %759, %752
  %761 = add i64 %760, %751
  %762 = add i64 %761, %750
  %763 = mul nuw nsw i64 %404, %160
  %764 = mul nuw nsw i64 %389, %171
  %765 = mul nuw nsw i64 %377, %185
  %766 = mul nuw nsw i64 %360, %202
  %767 = mul nuw nsw i64 %346, %214
  %768 = mul nuw nsw i64 %335, %230
  %769 = mul nuw nsw i64 %404, %171
  %770 = mul nuw nsw i64 %389, %185
  %771 = mul nuw nsw i64 %377, %202
  %772 = mul nuw nsw i64 %360, %214
  %773 = mul nuw nsw i64 %346, %230
  %774 = add nuw i64 %772, %773
  %775 = add i64 %774, %771
  %776 = add i64 %775, %770
  %777 = add i64 %776, %769
  %778 = mul nuw nsw i64 %404, %185
  %779 = mul nuw nsw i64 %389, %202
  %780 = mul nuw nsw i64 %377, %214
  %781 = mul nuw nsw i64 %360, %230
  %782 = mul nuw nsw i64 %404, %202
  %783 = mul nuw nsw i64 %389, %214
  %784 = mul nuw nsw i64 %377, %230
  %785 = add nuw i64 %783, %784
  %786 = add i64 %785, %782
  %787 = mul nuw nsw i64 %404, %214
  %788 = mul nuw nsw i64 %389, %230
  %789 = add nuw nsw i64 %787, %788
  %790 = mul nuw nsw i64 %404, %230
  %791 = add nuw nsw i64 %581, 1048576
  %792 = lshr i64 %791, 21
  %793 = add nuw nsw i64 %582, %583
  %794 = add nuw nsw i64 %793, %792
  %795 = add i64 %794, %434
  %796 = and i64 %791, -2097152
  %797 = sub nsw i64 %581, %796
  %798 = add nuw nsw i64 %589, 1048576
  %799 = lshr i64 %798, 21
  %800 = add nuw nsw i64 %592, %593
  %801 = add nuw nsw i64 %800, %591
  %802 = add i64 %801, %590
  %803 = add i64 %802, %463
  %804 = add i64 %803, %799
  %805 = and i64 %798, -2097152
  %806 = add nuw nsw i64 %603, 1048576
  %807 = ashr i64 %806, 21
  %808 = add nuw nsw i64 %608, %609
  %809 = add nuw nsw i64 %808, %607
  %810 = add i64 %809, %606
  %811 = add i64 %810, %605
  %812 = add i64 %811, %604
  %813 = add i64 %812, %492
  %814 = add i64 %813, %807
  %815 = and i64 %806, -2097152
  %816 = add nuw nsw i64 %623, 1048576
  %817 = ashr i64 %816, 21
  %818 = add nuw nsw i64 %630, %631
  %819 = add nuw i64 %818, %629
  %820 = add i64 %819, %628
  %821 = add i64 %820, %627
  %822 = add i64 %821, %626
  %823 = add i64 %822, %625
  %824 = add i64 %823, %624
  %825 = add i64 %824, %520
  %826 = add i64 %825, %817
  %827 = and i64 %816, -2097152
  %828 = add nuw nsw i64 %649, 1048576
  %829 = ashr i64 %828, 21
  %830 = add nuw nsw i64 %658, %659
  %831 = add nuw nsw i64 %830, %657
  %832 = add i64 %831, %656
  %833 = add i64 %832, %655
  %834 = add i64 %833, %654
  %835 = add i64 %834, %653
  %836 = add i64 %835, %652
  %837 = add i64 %836, %651
  %838 = add i64 %837, %650
  %839 = add i64 %838, %829
  %840 = add i64 %839, %551
  %841 = and i64 %828, -2097152
  %842 = add nuw nsw i64 %681, 1048576
  %843 = ashr i64 %842, 21
  %844 = add nuw i64 %692, %693
  %845 = add i64 %844, %691
  %846 = add i64 %845, %690
  %847 = add i64 %846, %689
  %848 = add i64 %847, %688
  %849 = add i64 %848, %687
  %850 = add i64 %849, %686
  %851 = add i64 %850, %685
  %852 = add i64 %851, %684
  %853 = add i64 %852, %683
  %854 = add i64 %853, %682
  %855 = add i64 %854, %579
  %856 = add i64 %855, %843
  %857 = and i64 %842, -2097152
  %858 = add nsw i64 %714, 1048576
  %859 = ashr i64 %858, 21
  %860 = add nuw i64 %723, %724
  %861 = add i64 %860, %722
  %862 = add i64 %861, %721
  %863 = add i64 %862, %720
  %864 = add i64 %863, %719
  %865 = add i64 %864, %718
  %866 = add i64 %865, %717
  %867 = add i64 %866, %716
  %868 = add i64 %867, %715
  %869 = add i64 %868, %859
  %870 = and i64 %858, -2097152
  %871 = add nsw i64 %741, 1048576
  %872 = ashr i64 %871, 21
  %873 = add nuw i64 %748, %749
  %874 = add i64 %873, %747
  %875 = add i64 %874, %746
  %876 = add i64 %875, %745
  %877 = add i64 %876, %744
  %878 = add i64 %877, %743
  %879 = add i64 %878, %742
  %880 = add i64 %879, %872
  %881 = and i64 %871, -2097152
  %882 = add nsw i64 %762, 1048576
  %883 = ashr i64 %882, 21
  %884 = add nuw i64 %767, %768
  %885 = add i64 %884, %766
  %886 = add i64 %885, %765
  %887 = add i64 %886, %764
  %888 = add i64 %887, %763
  %889 = add i64 %888, %883
  %890 = and i64 %882, -2097152
  %891 = add nuw nsw i64 %777, 1048576
  %892 = ashr i64 %891, 21
  %893 = add nuw i64 %780, %781
  %894 = add i64 %893, %779
  %895 = add i64 %894, %778
  %896 = add i64 %895, %892
  %897 = and i64 %891, -2097152
  %898 = sub nsw i64 %777, %897
  %899 = add nuw nsw i64 %786, 1048576
  %900 = lshr i64 %899, 21
  %901 = add nuw nsw i64 %789, %900
  %902 = and i64 %899, 9223372036852678656
  %903 = sub nsw i64 %786, %902
  %904 = add nuw nsw i64 %790, 1048576
  %905 = lshr i64 %904, 21
  %906 = and i64 %904, 9223372036852678656
  %907 = sub nsw i64 %790, %906
  %908 = add nuw nsw i64 %795, 1048576
  %909 = lshr i64 %908, 21
  %910 = and i64 %908, -2097152
  %911 = sub i64 %795, %910
  %912 = add nuw nsw i64 %804, 1048576
  %913 = ashr i64 %912, 21
  %914 = and i64 %912, -2097152
  %915 = sub i64 %804, %914
  %916 = add nuw nsw i64 %814, 1048576
  %917 = ashr i64 %916, 21
  %918 = and i64 %916, -2097152
  %919 = add nuw nsw i64 %826, 1048576
  %920 = ashr i64 %919, 21
  %921 = and i64 %919, -2097152
  %922 = add nuw nsw i64 %840, 1048576
  %923 = ashr i64 %922, 21
  %924 = and i64 %922, -2097152
  %925 = add nsw i64 %856, 1048576
  %926 = ashr i64 %925, 21
  %927 = and i64 %925, -2097152
  %928 = add nsw i64 %869, 1048576
  %929 = ashr i64 %928, 21
  %930 = and i64 %928, -2097152
  %931 = add nsw i64 %880, 1048576
  %932 = ashr i64 %931, 21
  %933 = and i64 %931, -2097152
  %934 = add nsw i64 %889, 1048576
  %935 = ashr i64 %934, 21
  %936 = add nsw i64 %935, %898
  %937 = and i64 %934, -2097152
  %938 = sub nsw i64 %889, %937
  %939 = add nsw i64 %896, 1048576
  %940 = ashr i64 %939, 21
  %941 = add nsw i64 %940, %903
  %942 = and i64 %939, -2097152
  %943 = sub nsw i64 %896, %942
  %944 = add nuw nsw i64 %901, 1048576
  %945 = lshr i64 %944, 21
  %946 = add nsw i64 %945, %907
  %947 = and i64 %944, 9223372036852678656
  %948 = sub nsw i64 %901, %947
  %949 = mul nuw nsw i64 %905, 666643
  %950 = mul nuw nsw i64 %905, 470296
  %951 = mul nuw nsw i64 %905, 654183
  %952 = mul nsw i64 %905, -997805
  %953 = mul nuw nsw i64 %905, 136657
  %954 = mul nsw i64 %905, -683901
  %955 = add i64 %954, %762
  %956 = sub i64 %955, %890
  %957 = add i64 %956, %932
  %958 = mul nsw i64 %946, 666643
  %959 = mul nsw i64 %946, 470296
  %960 = mul nsw i64 %946, 654183
  %961 = mul i64 %946, -997805
  %962 = mul nsw i64 %946, 136657
  %963 = mul i64 %946, -683901
  %964 = mul nsw i64 %948, 666643
  %965 = mul nsw i64 %948, 470296
  %966 = mul nsw i64 %948, 654183
  %967 = mul i64 %948, -997805
  %968 = mul nsw i64 %948, 136657
  %969 = mul i64 %948, -683901
  %970 = add i64 %952, %741
  %971 = sub i64 %970, %881
  %972 = add i64 %971, %929
  %973 = add i64 %972, %962
  %974 = add i64 %973, %969
  %975 = mul nsw i64 %941, 666643
  %976 = mul nsw i64 %941, 470296
  %977 = mul nsw i64 %941, 654183
  %978 = mul i64 %941, -997805
  %979 = mul nsw i64 %941, 136657
  %980 = mul i64 %941, -683901
  %981 = mul nsw i64 %943, 666643
  %982 = mul nsw i64 %943, 470296
  %983 = mul nsw i64 %943, 654183
  %984 = mul i64 %943, -997805
  %985 = mul nsw i64 %943, 136657
  %986 = mul i64 %943, -683901
  %987 = add i64 %950, %714
  %988 = sub i64 %987, %870
  %989 = add i64 %988, %960
  %990 = add i64 %989, %967
  %991 = add i64 %990, %979
  %992 = add i64 %991, %986
  %993 = add i64 %992, %926
  %994 = mul nsw i64 %936, 666643
  %995 = add i64 %917, %994
  %996 = add i64 %995, %623
  %997 = sub i64 %996, %827
  %998 = mul nsw i64 %936, 470296
  %999 = mul nsw i64 %936, 654183
  %1000 = add i64 %982, %975
  %1001 = add i64 %1000, %999
  %1002 = add i64 %1001, %649
  %1003 = add i64 %1002, %920
  %1004 = sub i64 %1003, %841
  %1005 = mul i64 %936, -997805
  %1006 = mul nsw i64 %936, 136657
  %1007 = add i64 %965, %958
  %1008 = add i64 %1007, %977
  %1009 = add i64 %1008, %984
  %1010 = add i64 %1009, %1006
  %1011 = add i64 %1010, %681
  %1012 = add i64 %1011, %923
  %1013 = sub i64 %1012, %857
  %1014 = mul i64 %936, -683901
  %1015 = add nsw i64 %997, 1048576
  %1016 = ashr i64 %1015, 21
  %1017 = add i64 %998, %981
  %1018 = add i64 %1017, %826
  %1019 = sub i64 %1018, %921
  %1020 = add i64 %1019, %1016
  %1021 = and i64 %1015, -2097152
  %1022 = add nsw i64 %1004, 1048576
  %1023 = ashr i64 %1022, 21
  %1024 = add i64 %976, %964
  %1025 = add i64 %1024, %983
  %1026 = add i64 %1025, %1005
  %1027 = add i64 %1026, %840
  %1028 = add i64 %1027, %1023
  %1029 = sub i64 %1028, %924
  %1030 = and i64 %1022, -2097152
  %1031 = add nsw i64 %1013, 1048576
  %1032 = ashr i64 %1031, 21
  %1033 = add i64 %959, %949
  %1034 = add i64 %1033, %966
  %1035 = add i64 %1034, %978
  %1036 = add i64 %1035, %985
  %1037 = add i64 %1036, %1014
  %1038 = add i64 %1037, %856
  %1039 = sub i64 %1038, %927
  %1040 = add i64 %1039, %1032
  %1041 = and i64 %1031, -2097152
  %1042 = add nsw i64 %993, 1048576
  %1043 = ashr i64 %1042, 21
  %1044 = add i64 %869, %951
  %1045 = sub i64 %1044, %930
  %1046 = add i64 %1045, %961
  %1047 = add i64 %1046, %968
  %1048 = add i64 %1047, %980
  %1049 = add i64 %1048, %1043
  %1050 = and i64 %1042, -2097152
  %1051 = sub nsw i64 %993, %1050
  %1052 = add nsw i64 %974, 1048576
  %1053 = ashr i64 %1052, 21
  %1054 = add i64 %880, %953
  %1055 = sub i64 %1054, %933
  %1056 = add i64 %1055, %963
  %1057 = add i64 %1056, %1053
  %1058 = and i64 %1052, -2097152
  %1059 = sub nsw i64 %974, %1058
  %1060 = add nsw i64 %957, 1048576
  %1061 = ashr i64 %1060, 21
  %1062 = add nsw i64 %1061, %938
  %1063 = and i64 %1060, -2097152
  %1064 = sub nsw i64 %957, %1063
  %1065 = add nsw i64 %1020, 1048576
  %1066 = ashr i64 %1065, 21
  %1067 = and i64 %1065, -2097152
  %1068 = add nsw i64 %1029, 1048576
  %1069 = ashr i64 %1068, 21
  %1070 = and i64 %1068, -2097152
  %1071 = add nsw i64 %1040, 1048576
  %1072 = ashr i64 %1071, 21
  %1073 = add nsw i64 %1072, %1051
  %1074 = and i64 %1071, -2097152
  %1075 = sub nsw i64 %1040, %1074
  %1076 = add nsw i64 %1049, 1048576
  %1077 = ashr i64 %1076, 21
  %1078 = add nsw i64 %1077, %1059
  %1079 = and i64 %1076, -2097152
  %1080 = sub nsw i64 %1049, %1079
  %1081 = add nsw i64 %1057, 1048576
  %1082 = ashr i64 %1081, 21
  %1083 = add nsw i64 %1082, %1064
  %1084 = and i64 %1081, -2097152
  %1085 = sub nsw i64 %1057, %1084
  %1086 = mul nsw i64 %1062, 666643
  %1087 = mul nsw i64 %1062, 470296
  %1088 = mul nsw i64 %1062, 654183
  %1089 = mul i64 %1062, -997805
  %1090 = mul nsw i64 %1062, 136657
  %1091 = mul i64 %1062, -683901
  %1092 = add i64 %1013, %1091
  %1093 = add i64 %1092, %1069
  %1094 = sub i64 %1093, %1041
  %1095 = mul nsw i64 %1083, 666643
  %1096 = mul nsw i64 %1083, 470296
  %1097 = mul nsw i64 %1083, 654183
  %1098 = mul i64 %1083, -997805
  %1099 = mul nsw i64 %1083, 136657
  %1100 = mul i64 %1083, -683901
  %1101 = mul nsw i64 %1085, 666643
  %1102 = add nsw i64 %915, %1101
  %1103 = mul nsw i64 %1085, 470296
  %1104 = mul nsw i64 %1085, 654183
  %1105 = mul i64 %1085, -997805
  %1106 = mul nsw i64 %1085, 136657
  %1107 = mul i64 %1085, -683901
  %1108 = add i64 %1099, %1089
  %1109 = add i64 %1108, %1107
  %1110 = add i64 %1109, %1004
  %1111 = add i64 %1110, %1066
  %1112 = sub i64 %1111, %1030
  %1113 = mul nsw i64 %1078, 666643
  %1114 = mul nsw i64 %1078, 470296
  %1115 = mul nsw i64 %1078, 654183
  %1116 = mul i64 %1078, -997805
  %1117 = mul nsw i64 %1078, 136657
  %1118 = mul i64 %1078, -683901
  %1119 = mul nsw i64 %1080, 666643
  %1120 = mul nsw i64 %1080, 470296
  %1121 = mul nsw i64 %1080, 654183
  %1122 = mul i64 %1080, -997805
  %1123 = mul nsw i64 %1080, 136657
  %1124 = mul i64 %1080, -683901
  %1125 = add i64 %1097, %1087
  %1126 = add i64 %1125, %1105
  %1127 = add i64 %1126, %997
  %1128 = sub i64 %1127, %1021
  %1129 = add i64 %1128, %1117
  %1130 = add i64 %1129, %1124
  %1131 = mul nsw i64 %1073, 666643
  %1132 = add nsw i64 %1131, %797
  %1133 = mul nsw i64 %1073, 470296
  %1134 = mul nsw i64 %1073, 654183
  %1135 = add i64 %589, %909
  %1136 = sub i64 %1135, %805
  %1137 = add i64 %1136, %1134
  %1138 = add i64 %1137, %1113
  %1139 = add i64 %1138, %1120
  %1140 = mul i64 %1073, -997805
  %1141 = mul nsw i64 %1073, 136657
  %1142 = add i64 %603, %913
  %1143 = sub i64 %1142, %815
  %1144 = add i64 %1143, %1095
  %1145 = add i64 %1144, %1103
  %1146 = add i64 %1145, %1141
  %1147 = add i64 %1146, %1115
  %1148 = add i64 %1147, %1122
  %1149 = mul i64 %1073, -683901
  %1150 = add nsw i64 %1132, 1048576
  %1151 = ashr i64 %1150, 21
  %1152 = add i64 %911, %1133
  %1153 = add i64 %1152, %1119
  %1154 = add i64 %1153, %1151
  %1155 = and i64 %1150, -2097152
  %1156 = sub nsw i64 %1132, %1155
  %1157 = add nsw i64 %1139, 1048576
  %1158 = ashr i64 %1157, 21
  %1159 = add i64 %1102, %1140
  %1160 = add i64 %1159, %1114
  %1161 = add i64 %1160, %1121
  %1162 = add i64 %1161, %1158
  %1163 = and i64 %1157, -2097152
  %1164 = add nsw i64 %1148, 1048576
  %1165 = ashr i64 %1164, 21
  %1166 = add i64 %814, %1086
  %1167 = add i64 %1166, %1096
  %1168 = add i64 %1167, %1104
  %1169 = sub i64 %1168, %918
  %1170 = add i64 %1169, %1149
  %1171 = add i64 %1170, %1116
  %1172 = add i64 %1171, %1123
  %1173 = add i64 %1172, %1165
  %1174 = and i64 %1164, -2097152
  %1175 = add nsw i64 %1130, 1048576
  %1176 = ashr i64 %1175, 21
  %1177 = add i64 %1098, %1088
  %1178 = add i64 %1177, %1106
  %1179 = add i64 %1178, %1020
  %1180 = sub i64 %1179, %1067
  %1181 = add i64 %1180, %1118
  %1182 = add i64 %1181, %1176
  %1183 = and i64 %1175, -2097152
  %1184 = sub i64 %1130, %1183
  %1185 = add nsw i64 %1112, 1048576
  %1186 = ashr i64 %1185, 21
  %1187 = add i64 %1100, %1090
  %1188 = add i64 %1187, %1029
  %1189 = add i64 %1188, %1186
  %1190 = sub i64 %1189, %1070
  %1191 = and i64 %1185, -2097152
  %1192 = sub i64 %1112, %1191
  %1193 = add nsw i64 %1094, 1048576
  %1194 = ashr i64 %1193, 21
  %1195 = add nsw i64 %1075, %1194
  %1196 = and i64 %1193, -2097152
  %1197 = add nsw i64 %1154, 1048576
  %1198 = ashr i64 %1197, 21
  %1199 = and i64 %1197, -2097152
  %1200 = add nsw i64 %1162, 1048576
  %1201 = ashr i64 %1200, 21
  %1202 = and i64 %1200, -2097152
  %1203 = add nsw i64 %1173, 1048576
  %1204 = ashr i64 %1203, 21
  %1205 = add nsw i64 %1184, %1204
  %1206 = and i64 %1203, -2097152
  %1207 = add nsw i64 %1182, 1048576
  %1208 = ashr i64 %1207, 21
  %1209 = add nsw i64 %1192, %1208
  %1210 = and i64 %1207, -2097152
  %1211 = sub nsw i64 %1182, %1210
  %1212 = add nsw i64 %1190, 1048576
  %1213 = ashr i64 %1212, 21
  %1214 = and i64 %1212, -2097152
  %1215 = sub nsw i64 %1190, %1214
  %1216 = add nsw i64 %1195, 1048576
  %1217 = ashr i64 %1216, 21
  %1218 = and i64 %1216, -2097152
  %1219 = sub nsw i64 %1195, %1218
  %1220 = mul nsw i64 %1217, 666643
  %1221 = add nsw i64 %1156, %1220
  %1222 = mul nsw i64 %1217, 470296
  %1223 = mul nsw i64 %1217, 654183
  %1224 = mul nsw i64 %1217, -997805
  %1225 = mul nsw i64 %1217, 136657
  %1226 = mul nsw i64 %1217, -683901
  %1227 = ashr i64 %1221, 21
  %1228 = add i64 %1154, %1222
  %1229 = sub i64 %1228, %1199
  %1230 = add i64 %1229, %1227
  %1231 = and i64 %1221, 2097151
  %1232 = ashr i64 %1230, 21
  %1233 = add i64 %1139, %1223
  %1234 = sub i64 %1233, %1163
  %1235 = add i64 %1234, %1198
  %1236 = add i64 %1235, %1232
  %1237 = and i64 %1230, 2097151
  %1238 = ashr i64 %1236, 21
  %1239 = add i64 %1162, %1224
  %1240 = sub i64 %1239, %1202
  %1241 = add i64 %1240, %1238
  %1242 = and i64 %1236, 2097151
  %1243 = ashr i64 %1241, 21
  %1244 = add i64 %1148, %1225
  %1245 = sub i64 %1244, %1174
  %1246 = add i64 %1245, %1201
  %1247 = add i64 %1246, %1243
  %1248 = and i64 %1241, 2097151
  %1249 = ashr i64 %1247, 21
  %1250 = add i64 %1173, %1226
  %1251 = sub i64 %1250, %1206
  %1252 = add i64 %1251, %1249
  %1253 = and i64 %1247, 2097151
  %1254 = ashr i64 %1252, 21
  %1255 = add nsw i64 %1205, %1254
  %1256 = and i64 %1252, 2097151
  %1257 = ashr i64 %1255, 21
  %1258 = add nsw i64 %1257, %1211
  %1259 = and i64 %1255, 2097151
  %1260 = ashr i64 %1258, 21
  %1261 = add nsw i64 %1209, %1260
  %1262 = and i64 %1258, 2097151
  %1263 = ashr i64 %1261, 21
  %1264 = add nsw i64 %1263, %1215
  %1265 = and i64 %1261, 2097151
  %1266 = ashr i64 %1264, 21
  %1267 = add i64 %1213, %1094
  %1268 = sub i64 %1267, %1196
  %1269 = add i64 %1268, %1266
  %1270 = and i64 %1264, 2097151
  %1271 = ashr i64 %1269, 21
  %1272 = add nsw i64 %1271, %1219
  %1273 = and i64 %1269, 2097151
  %1274 = ashr i64 %1272, 21
  %1275 = and i64 %1272, 2097151
  %1276 = mul nsw i64 %1274, 666643
  %1277 = add nsw i64 %1276, %1231
  %1278 = mul nsw i64 %1274, 470296
  %1279 = add nsw i64 %1278, %1237
  %1280 = mul nsw i64 %1274, 654183
  %1281 = add nsw i64 %1280, %1242
  %1282 = mul nsw i64 %1274, -997805
  %1283 = add nsw i64 %1282, %1248
  %1284 = mul nsw i64 %1274, 136657
  %1285 = add nsw i64 %1284, %1253
  %1286 = mul nsw i64 %1274, -683901
  %1287 = add nsw i64 %1286, %1256
  %1288 = ashr i64 %1277, 21
  %1289 = add nsw i64 %1279, %1288
  %1290 = ashr i64 %1289, 21
  %1291 = add nsw i64 %1281, %1290
  %1292 = and i64 %1289, 2097151
  %1293 = ashr i64 %1291, 21
  %1294 = add nsw i64 %1283, %1293
  %1295 = and i64 %1291, 2097151
  %1296 = ashr i64 %1294, 21
  %1297 = add nsw i64 %1285, %1296
  %1298 = and i64 %1294, 2097151
  %1299 = ashr i64 %1297, 21
  %1300 = add nsw i64 %1287, %1299
  %1301 = and i64 %1297, 2097151
  %1302 = ashr i64 %1300, 21
  %1303 = add nsw i64 %1302, %1259
  %1304 = and i64 %1300, 2097151
  %1305 = ashr i64 %1303, 21
  %1306 = add nsw i64 %1305, %1262
  %1307 = and i64 %1303, 2097151
  %1308 = ashr i64 %1306, 21
  %1309 = add nsw i64 %1308, %1265
  %1310 = ashr i64 %1309, 21
  %1311 = add nsw i64 %1310, %1270
  %1312 = ashr i64 %1311, 21
  %1313 = add nsw i64 %1312, %1273
  %1314 = and i64 %1311, 2097151
  %1315 = ashr i64 %1313, 21
  %1316 = add nsw i64 %1315, %1275
  %1317 = and i64 %1313, 2097151
  %1318 = trunc i64 %1277 to i8
  store i8 %1318, i8* %55, align 1
  %1319 = lshr i64 %1277, 8
  %1320 = trunc i64 %1319 to i8
  %1321 = getelementptr inbounds i8, i8* %0, i64 33
  store i8 %1320, i8* %1321, align 1
  %1322 = lshr i64 %1277, 16
  %1323 = and i64 %1322, 31
  %1324 = shl nuw nsw i64 %1292, 5
  %1325 = or i64 %1324, %1323
  %1326 = trunc i64 %1325 to i8
  %1327 = getelementptr inbounds i8, i8* %0, i64 34
  store i8 %1326, i8* %1327, align 1
  %1328 = lshr i64 %1289, 3
  %1329 = trunc i64 %1328 to i8
  %1330 = getelementptr inbounds i8, i8* %0, i64 35
  store i8 %1329, i8* %1330, align 1
  %1331 = lshr i64 %1289, 11
  %1332 = trunc i64 %1331 to i8
  %1333 = getelementptr inbounds i8, i8* %0, i64 36
  store i8 %1332, i8* %1333, align 1
  %1334 = lshr i64 %1292, 19
  %1335 = shl nuw nsw i64 %1295, 2
  %1336 = or i64 %1335, %1334
  %1337 = trunc i64 %1336 to i8
  %1338 = getelementptr inbounds i8, i8* %0, i64 37
  store i8 %1337, i8* %1338, align 1
  %1339 = lshr i64 %1291, 6
  %1340 = trunc i64 %1339 to i8
  %1341 = getelementptr inbounds i8, i8* %0, i64 38
  store i8 %1340, i8* %1341, align 1
  %1342 = lshr i64 %1295, 14
  %1343 = shl nuw nsw i64 %1298, 7
  %1344 = or i64 %1343, %1342
  %1345 = trunc i64 %1344 to i8
  %1346 = getelementptr inbounds i8, i8* %0, i64 39
  store i8 %1345, i8* %1346, align 1
  %1347 = lshr i64 %1294, 1
  %1348 = trunc i64 %1347 to i8
  %1349 = getelementptr inbounds i8, i8* %0, i64 40
  store i8 %1348, i8* %1349, align 1
  %1350 = lshr i64 %1294, 9
  %1351 = trunc i64 %1350 to i8
  %1352 = getelementptr inbounds i8, i8* %0, i64 41
  store i8 %1351, i8* %1352, align 1
  %1353 = lshr i64 %1298, 17
  %1354 = shl nuw nsw i64 %1301, 4
  %1355 = or i64 %1354, %1353
  %1356 = trunc i64 %1355 to i8
  %1357 = getelementptr inbounds i8, i8* %0, i64 42
  store i8 %1356, i8* %1357, align 1
  %1358 = lshr i64 %1297, 4
  %1359 = trunc i64 %1358 to i8
  %1360 = getelementptr inbounds i8, i8* %0, i64 43
  store i8 %1359, i8* %1360, align 1
  %1361 = lshr i64 %1297, 12
  %1362 = trunc i64 %1361 to i8
  %1363 = getelementptr inbounds i8, i8* %0, i64 44
  store i8 %1362, i8* %1363, align 1
  %1364 = lshr i64 %1301, 20
  %1365 = shl nuw nsw i64 %1304, 1
  %1366 = or i64 %1365, %1364
  %1367 = trunc i64 %1366 to i8
  %1368 = getelementptr inbounds i8, i8* %0, i64 45
  store i8 %1367, i8* %1368, align 1
  %1369 = lshr i64 %1300, 7
  %1370 = trunc i64 %1369 to i8
  %1371 = getelementptr inbounds i8, i8* %0, i64 46
  store i8 %1370, i8* %1371, align 1
  %1372 = lshr i64 %1304, 15
  %1373 = shl nuw nsw i64 %1307, 6
  %1374 = or i64 %1373, %1372
  %1375 = trunc i64 %1374 to i8
  %1376 = getelementptr inbounds i8, i8* %0, i64 47
  store i8 %1375, i8* %1376, align 1
  %1377 = lshr i64 %1303, 2
  %1378 = trunc i64 %1377 to i8
  %1379 = getelementptr inbounds i8, i8* %0, i64 48
  store i8 %1378, i8* %1379, align 1
  %1380 = lshr i64 %1303, 10
  %1381 = trunc i64 %1380 to i8
  %1382 = getelementptr inbounds i8, i8* %0, i64 49
  store i8 %1381, i8* %1382, align 1
  %1383 = lshr i64 %1307, 18
  %1384 = shl nsw i64 %1306, 3
  %1385 = or i64 %1384, %1383
  %1386 = trunc i64 %1385 to i8
  %1387 = getelementptr inbounds i8, i8* %0, i64 50
  store i8 %1386, i8* %1387, align 1
  %1388 = lshr i64 %1306, 5
  %1389 = trunc i64 %1388 to i8
  %1390 = getelementptr inbounds i8, i8* %0, i64 51
  store i8 %1389, i8* %1390, align 1
  %1391 = lshr i64 %1306, 13
  %1392 = trunc i64 %1391 to i8
  %1393 = getelementptr inbounds i8, i8* %0, i64 52
  store i8 %1392, i8* %1393, align 1
  %1394 = trunc i64 %1309 to i8
  %1395 = getelementptr inbounds i8, i8* %0, i64 53
  store i8 %1394, i8* %1395, align 1
  %1396 = lshr i64 %1309, 8
  %1397 = trunc i64 %1396 to i8
  %1398 = getelementptr inbounds i8, i8* %0, i64 54
  store i8 %1397, i8* %1398, align 1
  %1399 = lshr i64 %1309, 16
  %1400 = and i64 %1399, 31
  %1401 = shl nuw nsw i64 %1314, 5
  %1402 = or i64 %1401, %1400
  %1403 = trunc i64 %1402 to i8
  %1404 = getelementptr inbounds i8, i8* %0, i64 55
  store i8 %1403, i8* %1404, align 1
  %1405 = lshr i64 %1311, 3
  %1406 = trunc i64 %1405 to i8
  %1407 = getelementptr inbounds i8, i8* %0, i64 56
  store i8 %1406, i8* %1407, align 1
  %1408 = lshr i64 %1311, 11
  %1409 = trunc i64 %1408 to i8
  %1410 = getelementptr inbounds i8, i8* %0, i64 57
  store i8 %1409, i8* %1410, align 1
  %1411 = lshr i64 %1314, 19
  %1412 = shl nuw nsw i64 %1317, 2
  %1413 = or i64 %1412, %1411
  %1414 = trunc i64 %1413 to i8
  %1415 = getelementptr inbounds i8, i8* %0, i64 58
  store i8 %1414, i8* %1415, align 1
  %1416 = lshr i64 %1313, 6
  %1417 = trunc i64 %1416 to i8
  %1418 = getelementptr inbounds i8, i8* %0, i64 59
  store i8 %1417, i8* %1418, align 1
  %1419 = lshr i64 %1317, 14
  %1420 = shl nsw i64 %1316, 7
  %1421 = or i64 %1420, %1419
  %1422 = trunc i64 %1421 to i8
  %1423 = getelementptr inbounds i8, i8* %0, i64 60
  store i8 %1422, i8* %1423, align 1
  %1424 = lshr i64 %1316, 1
  %1425 = trunc i64 %1424 to i8
  %1426 = getelementptr inbounds i8, i8* %0, i64 61
  store i8 %1425, i8* %1426, align 1
  %1427 = lshr i64 %1316, 9
  %1428 = trunc i64 %1427 to i8
  %1429 = getelementptr inbounds i8, i8* %0, i64 62
  store i8 %1428, i8* %1429, align 1
  %1430 = lshr i64 %1316, 17
  %1431 = trunc i64 %1430 to i8
  %1432 = getelementptr inbounds i8, i8* %0, i64 63
  store i8 %1431, i8* %1432, align 1
  call void @llvm.lifetime.end.p0i8(i64 64, i8* nonnull %53) #4
  call void @llvm.lifetime.end.p0i8(i64 160, i8* nonnull %30) #4
  call void @llvm.lifetime.end.p0i8(i64 64, i8* nonnull %28) #4
  call void @llvm.lifetime.end.p0i8(i64 216, i8* nonnull %23) #4
  call void @llvm.lifetime.end.p0i8(i64 64, i8* nonnull %15) #4
  ret i32 1
}

declare i8* @SHA512(i8*, i64, i8*) local_unnamed_addr #3

declare i32 @SHA512_Init(%struct.sha512_state_st*) local_unnamed_addr #3

declare i32 @SHA512_Update(%struct.sha512_state_st*, i8*, i64) local_unnamed_addr #3

declare i32 @SHA512_Final(i8*, %struct.sha512_state_st*) local_unnamed_addr #3

; Function Attrs: nounwind ssp uwtable
define hidden i32 @ED25519_verify(i8*, i64, i8*, i8*) local_unnamed_addr #0 {
  %5 = alloca %struct.fe, align 16
  %6 = alloca %struct.fe, align 16
  %7 = alloca %struct.fe, align 8
  %8 = alloca %struct.ge_p2, align 8
  %9 = alloca [256 x i8], align 16
  %10 = alloca [256 x i8], align 16
  %11 = alloca [8 x %struct.ge_cached], align 16
  %12 = alloca %struct.ge_p1p1, align 16
  %13 = alloca %struct.ge_p3, align 16
  %14 = alloca %struct.ge_p3, align 8
  %15 = alloca [32 x i8], align 16
  %16 = alloca %struct.fe_loose, align 8
  %17 = alloca %struct.fe, align 8
  %18 = alloca %struct.fe, align 8
  %19 = alloca %struct.fe, align 8
  %20 = alloca %struct.ge_p3, align 16
  %21 = alloca [32 x i8], align 16
  %22 = alloca %union.anon, align 8
  %23 = alloca %struct.sha512_state_st, align 8
  %24 = alloca [64 x i8], align 16
  %25 = alloca %struct.ge_p2, align 8
  %26 = alloca [32 x i8], align 16
  %27 = bitcast %struct.ge_p3* %20 to i8*
  call void @llvm.lifetime.start.p0i8(i64 160, i8* nonnull %27) #4
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %27, i8 -86, i64 160, i1 false)
  %28 = getelementptr inbounds i8, i8* %2, i64 63
  %29 = load i8, i8* %28, align 1
  %30 = icmp ugt i8 %29, 31
  br i1 %30, label %905, label %31

31:                                               ; preds = %4
  %32 = call i32 @x25519_ge_frombytes_vartime(%struct.ge_p3* nonnull %20, i8* %3)
  %33 = icmp eq i32 %32, 0
  br i1 %33, label %905, label %34

34:                                               ; preds = %31
  %35 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %20, i64 0, i32 0, i32 0, i64 0
  %36 = load i64, i64* %35, align 16
  %37 = sub i64 4503599627370458, %36
  %38 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %20, i64 0, i32 0, i32 0, i64 1
  %39 = load i64, i64* %38, align 8
  %40 = sub i64 4503599627370494, %39
  %41 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %20, i64 0, i32 0, i32 0, i64 2
  %42 = load i64, i64* %41, align 16
  %43 = sub i64 4503599627370494, %42
  %44 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %20, i64 0, i32 0, i32 0, i64 3
  %45 = load i64, i64* %44, align 8
  %46 = sub i64 4503599627370494, %45
  %47 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %20, i64 0, i32 0, i32 0, i64 4
  %48 = load i64, i64* %47, align 16
  %49 = sub i64 4503599627370494, %48
  %50 = lshr i64 %37, 51
  %51 = add i64 %50, %40
  %52 = lshr i64 %51, 51
  %53 = add i64 %52, %43
  %54 = lshr i64 %53, 51
  %55 = add i64 %54, %46
  %56 = lshr i64 %55, 51
  %57 = add i64 %56, %49
  %58 = and i64 %37, 2251799813685247
  %59 = lshr i64 %57, 51
  %60 = mul nuw nsw i64 %59, 19
  %61 = add nuw nsw i64 %60, %58
  %62 = lshr i64 %61, 51
  %63 = and i64 %51, 2251799813685247
  %64 = add nuw nsw i64 %62, %63
  %65 = and i64 %61, 2251799813685247
  %66 = and i64 %64, 2251799813685247
  %67 = lshr i64 %64, 51
  %68 = and i64 %53, 2251799813685247
  %69 = add nuw nsw i64 %67, %68
  %70 = and i64 %55, 2251799813685247
  %71 = and i64 %57, 2251799813685247
  store i64 %65, i64* %35, align 16
  store i64 %66, i64* %38, align 8
  store i64 %69, i64* %41, align 16
  store i64 %70, i64* %44, align 8
  store i64 %71, i64* %47, align 16
  %72 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %20, i64 0, i32 3, i32 0, i64 0
  %73 = load i64, i64* %72, align 8
  %74 = sub i64 4503599627370458, %73
  %75 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %20, i64 0, i32 3, i32 0, i64 1
  %76 = load i64, i64* %75, align 8
  %77 = sub i64 4503599627370494, %76
  %78 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %20, i64 0, i32 3, i32 0, i64 2
  %79 = load i64, i64* %78, align 8
  %80 = sub i64 4503599627370494, %79
  %81 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %20, i64 0, i32 3, i32 0, i64 3
  %82 = load i64, i64* %81, align 8
  %83 = sub i64 4503599627370494, %82
  %84 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %20, i64 0, i32 3, i32 0, i64 4
  %85 = load i64, i64* %84, align 8
  %86 = sub i64 4503599627370494, %85
  %87 = lshr i64 %74, 51
  %88 = add i64 %87, %77
  %89 = lshr i64 %88, 51
  %90 = add i64 %89, %80
  %91 = lshr i64 %90, 51
  %92 = add i64 %91, %83
  %93 = lshr i64 %92, 51
  %94 = add i64 %93, %86
  %95 = and i64 %74, 2251799813685247
  %96 = lshr i64 %94, 51
  %97 = mul nuw nsw i64 %96, 19
  %98 = add nuw nsw i64 %97, %95
  %99 = lshr i64 %98, 51
  %100 = and i64 %88, 2251799813685247
  %101 = add nuw nsw i64 %99, %100
  %102 = and i64 %98, 2251799813685247
  %103 = and i64 %101, 2251799813685247
  %104 = lshr i64 %101, 51
  %105 = and i64 %90, 2251799813685247
  %106 = add nuw nsw i64 %104, %105
  %107 = and i64 %92, 2251799813685247
  %108 = and i64 %94, 2251799813685247
  store i64 %102, i64* %72, align 8
  store i64 %103, i64* %75, align 8
  store i64 %106, i64* %78, align 8
  store i64 %107, i64* %81, align 8
  store i64 %108, i64* %84, align 8
  %109 = getelementptr inbounds [32 x i8], [32 x i8]* %21, i64 0, i64 0
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %109) #4
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 16 %109, i8* align 1 %2, i64 32, i1 false) #4
  %110 = bitcast %union.anon* %22 to i8*
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %110) #4
  %111 = getelementptr inbounds %union.anon, %union.anon* %22, i64 0, i32 0, i64 3
  %112 = getelementptr inbounds i8, i8* %2, i64 32
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 8 %110, i8* align 1 %112, i64 32, i1 false) #4
  %113 = load i64, i64* %111, align 8
  %114 = icmp ugt i64 %113, 1152921504606846976
  br i1 %114, label %903, label %115

115:                                              ; preds = %34
  %116 = icmp ult i64 %113, 1152921504606846976
  br i1 %116, label %121, label %117

117:                                              ; preds = %115
  %118 = getelementptr inbounds %union.anon, %union.anon* %22, i64 0, i32 0, i64 2
  %119 = load i64, i64* %118, align 8
  %120 = icmp eq i64 %119, 0
  br i1 %120, label %915, label %903

121:                                              ; preds = %921, %919, %115
  %122 = bitcast %struct.sha512_state_st* %23 to i8*
  call void @llvm.lifetime.start.p0i8(i64 216, i8* nonnull %122) #4
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %122, i8 -86, i64 216, i1 false)
  %123 = call i32 @SHA512_Init(%struct.sha512_state_st* nonnull %23) #4
  %124 = call i32 @SHA512_Update(%struct.sha512_state_st* nonnull %23, i8* %2, i64 32) #4
  %125 = call i32 @SHA512_Update(%struct.sha512_state_st* nonnull %23, i8* %3, i64 32) #4
  %126 = call i32 @SHA512_Update(%struct.sha512_state_st* nonnull %23, i8* %0, i64 %1) #4
  %127 = getelementptr inbounds [64 x i8], [64 x i8]* %24, i64 0, i64 0
  call void @llvm.lifetime.start.p0i8(i64 64, i8* nonnull %127) #4
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %127, i8 -86, i64 64, i1 false)
  %128 = call i32 @SHA512_Final(i8* nonnull %127, %struct.sha512_state_st* nonnull %23) #4
  call void @x25519_sc_reduce(i8* nonnull %127)
  %129 = bitcast %struct.ge_p2* %25 to i8*
  call void @llvm.lifetime.start.p0i8(i64 120, i8* nonnull %129) #4
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %129, i8 -86, i64 120, i1 false)
  %130 = getelementptr inbounds [256 x i8], [256 x i8]* %9, i64 0, i64 0
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %130) #4
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %130, i8 -86, i64 256, i1 false) #4
  %131 = getelementptr inbounds [256 x i8], [256 x i8]* %10, i64 0, i64 0
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %131) #4
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %131, i8 -86, i64 256, i1 false) #4
  %132 = bitcast [8 x %struct.ge_cached]* %11 to i8*
  call void @llvm.lifetime.start.p0i8(i64 1280, i8* nonnull %132) #4
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %132, i8 -86, i64 1280, i1 false) #4
  %133 = bitcast %struct.ge_p1p1* %12 to i8*
  call void @llvm.lifetime.start.p0i8(i64 160, i8* nonnull %133) #4
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %133, i8 -86, i64 160, i1 false) #4
  %134 = bitcast %struct.ge_p3* %13 to i8*
  call void @llvm.lifetime.start.p0i8(i64 160, i8* nonnull %134) #4
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %134, i8 -86, i64 160, i1 false) #4
  %135 = bitcast %struct.ge_p3* %14 to i8*
  call void @llvm.lifetime.start.p0i8(i64 160, i8* nonnull %135) #4
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %135, i8 -86, i64 160, i1 false) #4
  br label %136

136:                                              ; preds = %136, %121
  %137 = phi i64 [ 0, %121 ], [ %161, %136 ]
  %138 = trunc i64 %137 to i32
  %139 = lshr i64 %137, 3
  %140 = and i64 %139, 536870911
  %141 = getelementptr inbounds [64 x i8], [64 x i8]* %24, i64 0, i64 %140
  %142 = load i8, i8* %141, align 1
  %143 = zext i8 %142 to i32
  %144 = and i32 %138, 6
  %145 = lshr i32 %143, %144
  %146 = trunc i32 %145 to i8
  %147 = and i8 %146, 1
  %148 = getelementptr inbounds [256 x i8], [256 x i8]* %9, i64 0, i64 %137
  store i8 %147, i8* %148, align 2
  %149 = or i64 %137, 1
  %150 = trunc i64 %149 to i32
  %151 = lshr i64 %137, 3
  %152 = and i64 %151, 536870911
  %153 = getelementptr inbounds [64 x i8], [64 x i8]* %24, i64 0, i64 %152
  %154 = load i8, i8* %153, align 1
  %155 = zext i8 %154 to i32
  %156 = and i32 %150, 7
  %157 = lshr i32 %155, %156
  %158 = trunc i32 %157 to i8
  %159 = and i8 %158, 1
  %160 = getelementptr inbounds [256 x i8], [256 x i8]* %9, i64 0, i64 %149
  store i8 %159, i8* %160, align 1
  %161 = add nuw nsw i64 %137, 2
  %162 = icmp eq i64 %161, 256
  br i1 %162, label %163, label %136

163:                                              ; preds = %136, %208
  %164 = phi i64 [ %209, %208 ], [ 0, %136 ]
  %165 = phi i64 [ %211, %208 ], [ 256, %136 ]
  %166 = phi i64 [ %210, %208 ], [ 1, %136 ]
  %167 = getelementptr inbounds [256 x i8], [256 x i8]* %9, i64 0, i64 %164
  %168 = load i8, i8* %167, align 1
  %169 = icmp eq i8 %168, 0
  br i1 %169, label %208, label %170

170:                                              ; preds = %163, %204
  %171 = phi i64 [ %205, %204 ], [ 1, %163 ]
  %172 = phi i64 [ %206, %204 ], [ %166, %163 ]
  %173 = add nuw nsw i64 %171, %164
  %174 = icmp eq i64 %171, %165
  br i1 %174, label %208, label %175

175:                                              ; preds = %170
  %176 = getelementptr inbounds [256 x i8], [256 x i8]* %9, i64 0, i64 %173
  %177 = load i8, i8* %176, align 1
  %178 = icmp eq i8 %177, 0
  br i1 %178, label %204, label %179

179:                                              ; preds = %175
  %180 = load i8, i8* %167, align 1
  %181 = sext i8 %180 to i32
  %182 = sext i8 %177 to i32
  %183 = trunc i64 %171 to i32
  %184 = shl i32 %182, %183
  %185 = add nsw i32 %184, %181
  %186 = icmp slt i32 %185, 16
  br i1 %186, label %187, label %189

187:                                              ; preds = %179
  %188 = trunc i32 %185 to i8
  store i8 %188, i8* %167, align 1
  store i8 0, i8* %176, align 1
  br label %204

189:                                              ; preds = %179
  %190 = sub nsw i32 %181, %184
  %191 = icmp sgt i32 %190, -16
  br i1 %191, label %192, label %208

192:                                              ; preds = %189
  %193 = trunc i32 %190 to i8
  store i8 %193, i8* %167, align 1
  %194 = icmp ult i64 %173, 256
  br i1 %194, label %195, label %204

195:                                              ; preds = %192, %201
  %196 = phi i64 [ %202, %201 ], [ %172, %192 ]
  %197 = getelementptr inbounds [256 x i8], [256 x i8]* %9, i64 0, i64 %196
  %198 = load i8, i8* %197, align 1
  %199 = icmp eq i8 %198, 0
  br i1 %199, label %200, label %201

200:                                              ; preds = %195
  store i8 1, i8* %197, align 1
  br label %204

201:                                              ; preds = %195
  store i8 0, i8* %197, align 1
  %202 = add nuw nsw i64 %196, 1
  %203 = icmp eq i64 %202, 256
  br i1 %203, label %204, label %195

204:                                              ; preds = %201, %200, %192, %187, %175
  %205 = add nuw nsw i64 %171, 1
  %206 = add nuw nsw i64 %172, 1
  %207 = icmp eq i64 %205, 7
  br i1 %207, label %208, label %170

208:                                              ; preds = %204, %189, %170, %163
  %209 = add nuw nsw i64 %164, 1
  %210 = add nuw nsw i64 %166, 1
  %211 = add nsw i64 %165, -1
  %212 = icmp eq i64 %209, 256
  br i1 %212, label %213, label %163

213:                                              ; preds = %208, %213
  %214 = phi i64 [ %238, %213 ], [ 0, %208 ]
  %215 = trunc i64 %214 to i32
  %216 = lshr i64 %214, 3
  %217 = and i64 %216, 536870911
  %218 = getelementptr inbounds i8, i8* %110, i64 %217
  %219 = load i8, i8* %218, align 1
  %220 = zext i8 %219 to i32
  %221 = and i32 %215, 6
  %222 = lshr i32 %220, %221
  %223 = trunc i32 %222 to i8
  %224 = and i8 %223, 1
  %225 = getelementptr inbounds [256 x i8], [256 x i8]* %10, i64 0, i64 %214
  store i8 %224, i8* %225, align 2
  %226 = or i64 %214, 1
  %227 = trunc i64 %226 to i32
  %228 = lshr i64 %214, 3
  %229 = and i64 %228, 536870911
  %230 = getelementptr inbounds i8, i8* %110, i64 %229
  %231 = load i8, i8* %230, align 1
  %232 = zext i8 %231 to i32
  %233 = and i32 %227, 7
  %234 = lshr i32 %232, %233
  %235 = trunc i32 %234 to i8
  %236 = and i8 %235, 1
  %237 = getelementptr inbounds [256 x i8], [256 x i8]* %10, i64 0, i64 %226
  store i8 %236, i8* %237, align 1
  %238 = add nuw nsw i64 %214, 2
  %239 = icmp eq i64 %238, 256
  br i1 %239, label %240, label %213

240:                                              ; preds = %213, %285
  %241 = phi i64 [ %286, %285 ], [ 0, %213 ]
  %242 = phi i64 [ %288, %285 ], [ 256, %213 ]
  %243 = phi i64 [ %287, %285 ], [ 1, %213 ]
  %244 = getelementptr inbounds [256 x i8], [256 x i8]* %10, i64 0, i64 %241
  %245 = load i8, i8* %244, align 1
  %246 = icmp eq i8 %245, 0
  br i1 %246, label %285, label %247

247:                                              ; preds = %240, %281
  %248 = phi i64 [ %282, %281 ], [ 1, %240 ]
  %249 = phi i64 [ %283, %281 ], [ %243, %240 ]
  %250 = add nuw nsw i64 %248, %241
  %251 = icmp eq i64 %248, %242
  br i1 %251, label %285, label %252

252:                                              ; preds = %247
  %253 = getelementptr inbounds [256 x i8], [256 x i8]* %10, i64 0, i64 %250
  %254 = load i8, i8* %253, align 1
  %255 = icmp eq i8 %254, 0
  br i1 %255, label %281, label %256

256:                                              ; preds = %252
  %257 = load i8, i8* %244, align 1
  %258 = sext i8 %257 to i32
  %259 = sext i8 %254 to i32
  %260 = trunc i64 %248 to i32
  %261 = shl i32 %259, %260
  %262 = add nsw i32 %261, %258
  %263 = icmp slt i32 %262, 16
  br i1 %263, label %264, label %266

264:                                              ; preds = %256
  %265 = trunc i32 %262 to i8
  store i8 %265, i8* %244, align 1
  store i8 0, i8* %253, align 1
  br label %281

266:                                              ; preds = %256
  %267 = sub nsw i32 %258, %261
  %268 = icmp sgt i32 %267, -16
  br i1 %268, label %269, label %285

269:                                              ; preds = %266
  %270 = trunc i32 %267 to i8
  store i8 %270, i8* %244, align 1
  %271 = icmp ult i64 %250, 256
  br i1 %271, label %272, label %281

272:                                              ; preds = %269, %278
  %273 = phi i64 [ %279, %278 ], [ %249, %269 ]
  %274 = getelementptr inbounds [256 x i8], [256 x i8]* %10, i64 0, i64 %273
  %275 = load i8, i8* %274, align 1
  %276 = icmp eq i8 %275, 0
  br i1 %276, label %277, label %278

277:                                              ; preds = %272
  store i8 1, i8* %274, align 1
  br label %281

278:                                              ; preds = %272
  store i8 0, i8* %274, align 1
  %279 = add nuw nsw i64 %273, 1
  %280 = icmp eq i64 %279, 256
  br i1 %280, label %281, label %272

281:                                              ; preds = %278, %277, %269, %264, %252
  %282 = add nuw nsw i64 %248, 1
  %283 = add nuw nsw i64 %249, 1
  %284 = icmp eq i64 %282, 7
  br i1 %284, label %285, label %247

285:                                              ; preds = %281, %266, %247, %240
  %286 = add nuw nsw i64 %241, 1
  %287 = add nuw nsw i64 %243, 1
  %288 = add nsw i64 %242, -1
  %289 = icmp eq i64 %286, 256
  br i1 %289, label %290, label %240

290:                                              ; preds = %285
  %291 = getelementptr inbounds [8 x %struct.ge_cached], [8 x %struct.ge_cached]* %11, i64 0, i64 0
  %292 = getelementptr inbounds [8 x %struct.ge_cached], [8 x %struct.ge_cached]* %11, i64 0, i64 0, i32 0, i32 0, i64 0
  %293 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %20, i64 0, i32 1, i32 0, i64 0
  %294 = bitcast i64* %293 to <2 x i64>*
  %295 = load <2 x i64>, <2 x i64>* %294, align 8
  %296 = bitcast %struct.ge_p3* %20 to <2 x i64>*
  %297 = load <2 x i64>, <2 x i64>* %296, align 16
  %298 = extractelement <2 x i64> %295, i32 0
  %299 = extractelement <2 x i64> %297, i32 0
  %300 = add i64 %299, %298
  %301 = extractelement <2 x i64> %295, i32 1
  %302 = extractelement <2 x i64> %297, i32 1
  %303 = add i64 %302, %301
  %304 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %20, i64 0, i32 1, i32 0, i64 2
  %305 = bitcast i64* %304 to <2 x i64>*
  %306 = load <2 x i64>, <2 x i64>* %305, align 8
  %307 = bitcast i64* %41 to <2 x i64>*
  %308 = load <2 x i64>, <2 x i64>* %307, align 16
  %309 = extractelement <2 x i64> %306, i32 0
  %310 = extractelement <2 x i64> %308, i32 0
  %311 = add i64 %310, %309
  %312 = extractelement <2 x i64> %306, i32 1
  %313 = extractelement <2 x i64> %308, i32 1
  %314 = add i64 %313, %312
  %315 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %20, i64 0, i32 1, i32 0, i64 4
  %316 = load i64, i64* %315, align 8
  %317 = load i64, i64* %47, align 16
  %318 = add i64 %317, %316
  store i64 %300, i64* %292, align 16
  %319 = getelementptr inbounds [8 x %struct.ge_cached], [8 x %struct.ge_cached]* %11, i64 0, i64 0, i32 0, i32 0, i64 1
  store i64 %303, i64* %319, align 8
  %320 = getelementptr inbounds [8 x %struct.ge_cached], [8 x %struct.ge_cached]* %11, i64 0, i64 0, i32 0, i32 0, i64 2
  store i64 %311, i64* %320, align 16
  %321 = getelementptr inbounds [8 x %struct.ge_cached], [8 x %struct.ge_cached]* %11, i64 0, i64 0, i32 0, i32 0, i64 3
  store i64 %314, i64* %321, align 8
  %322 = getelementptr inbounds [8 x %struct.ge_cached], [8 x %struct.ge_cached]* %11, i64 0, i64 0, i32 0, i32 0, i64 4
  store i64 %318, i64* %322, align 16
  %323 = getelementptr inbounds [8 x %struct.ge_cached], [8 x %struct.ge_cached]* %11, i64 0, i64 0, i32 1, i32 0, i64 0
  %324 = add <2 x i64> %295, <i64 4503599627370458, i64 4503599627370494>
  %325 = sub <2 x i64> %324, %297
  %326 = add <2 x i64> %306, <i64 4503599627370494, i64 4503599627370494>
  %327 = sub <2 x i64> %326, %308
  %328 = add i64 %316, 4503599627370494
  %329 = sub i64 %328, %317
  %330 = bitcast i64* %323 to <2 x i64>*
  store <2 x i64> %325, <2 x i64>* %330, align 8
  %331 = getelementptr inbounds [8 x %struct.ge_cached], [8 x %struct.ge_cached]* %11, i64 0, i64 0, i32 1, i32 0, i64 2
  %332 = bitcast i64* %331 to <2 x i64>*
  store <2 x i64> %327, <2 x i64>* %332, align 8
  %333 = getelementptr inbounds [8 x %struct.ge_cached], [8 x %struct.ge_cached]* %11, i64 0, i64 0, i32 1, i32 0, i64 4
  store i64 %329, i64* %333, align 8
  %334 = getelementptr inbounds [8 x %struct.ge_cached], [8 x %struct.ge_cached]* %11, i64 0, i64 0, i32 2
  %335 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %20, i64 0, i32 2
  %336 = bitcast %struct.fe_loose* %334 to i8*
  %337 = bitcast %struct.fe* %335 to i8*
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 16 %336, i8* align 16 %337, i64 40, i1 false) #4
  %338 = getelementptr inbounds [8 x %struct.ge_cached], [8 x %struct.ge_cached]* %11, i64 0, i64 0, i32 3, i32 0, i64 0
  call fastcc void @fe_mul_impl(i64* %338, i64* %72, i64* getelementptr inbounds (%struct.fe, %struct.fe* @d2, i64 0, i32 0, i64 0)) #4
  %339 = bitcast %struct.ge_p2* %8 to i8*
  call void @llvm.lifetime.start.p0i8(i64 120, i8* nonnull %339) #4
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 8 %339, i8* nonnull align 16 %27, i64 40, i1 false) #4
  %340 = getelementptr inbounds %struct.ge_p2, %struct.ge_p2* %8, i64 0, i32 1
  %341 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %20, i64 0, i32 1
  %342 = bitcast %struct.fe* %340 to i8*
  %343 = bitcast %struct.fe* %341 to i8*
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 8 %342, i8* align 8 %343, i64 40, i1 false) #4
  %344 = getelementptr inbounds %struct.ge_p2, %struct.ge_p2* %8, i64 0, i32 2
  %345 = bitcast %struct.fe* %344 to i8*
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 8 %345, i8* align 16 %337, i64 40, i1 false) #4
  call fastcc void @ge_p2_dbl(%struct.ge_p1p1* nonnull %12, %struct.ge_p2* nonnull %8) #4
  call void @llvm.lifetime.end.p0i8(i64 120, i8* nonnull %339) #4
  %346 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %14, i64 0, i32 0, i32 0, i64 0
  %347 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %12, i64 0, i32 0, i32 0, i64 0
  %348 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %12, i64 0, i32 3, i32 0, i64 0
  call fastcc void @fe_mul_impl(i64* nonnull %346, i64* nonnull %347, i64* %348) #4
  %349 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %14, i64 0, i32 1, i32 0, i64 0
  %350 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %12, i64 0, i32 1, i32 0, i64 0
  %351 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %12, i64 0, i32 2, i32 0, i64 0
  call fastcc void @fe_mul_impl(i64* %349, i64* %350, i64* %351) #4
  %352 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %14, i64 0, i32 2, i32 0, i64 0
  call fastcc void @fe_mul_impl(i64* %352, i64* %351, i64* %348) #4
  %353 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %14, i64 0, i32 3, i32 0, i64 0
  call fastcc void @fe_mul_impl(i64* %353, i64* nonnull %347, i64* %350) #4
  call void @x25519_ge_add(%struct.ge_p1p1* nonnull %12, %struct.ge_p3* nonnull %14, %struct.ge_cached* nonnull %291) #4
  %354 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %13, i64 0, i32 0, i32 0, i64 0
  call fastcc void @fe_mul_impl(i64* nonnull %354, i64* nonnull %347, i64* %348) #4
  %355 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %13, i64 0, i32 1, i32 0, i64 0
  call fastcc void @fe_mul_impl(i64* %355, i64* %350, i64* %351) #4
  %356 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %13, i64 0, i32 2, i32 0, i64 0
  call fastcc void @fe_mul_impl(i64* %356, i64* %351, i64* %348) #4
  %357 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %13, i64 0, i32 3, i32 0, i64 0
  call fastcc void @fe_mul_impl(i64* %357, i64* nonnull %347, i64* %350) #4
  %358 = getelementptr inbounds [8 x %struct.ge_cached], [8 x %struct.ge_cached]* %11, i64 0, i64 1
  %359 = getelementptr inbounds %struct.ge_cached, %struct.ge_cached* %358, i64 0, i32 0, i32 0, i64 0
  %360 = load i64, i64* %355, align 8
  %361 = load i64, i64* %354, align 16
  %362 = add i64 %361, %360
  %363 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %13, i64 0, i32 1, i32 0, i64 1
  %364 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %13, i64 0, i32 0, i32 0, i64 1
  %365 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %13, i64 0, i32 1, i32 0, i64 2
  %366 = bitcast i64* %363 to <2 x i64>*
  %367 = load <2 x i64>, <2 x i64>* %366, align 8
  %368 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %13, i64 0, i32 0, i32 0, i64 2
  %369 = bitcast i64* %364 to <2 x i64>*
  %370 = load <2 x i64>, <2 x i64>* %369, align 8
  %371 = extractelement <2 x i64> %367, i32 0
  %372 = extractelement <2 x i64> %370, i32 0
  %373 = add i64 %372, %371
  %374 = extractelement <2 x i64> %367, i32 1
  %375 = extractelement <2 x i64> %370, i32 1
  %376 = add i64 %375, %374
  %377 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %13, i64 0, i32 1, i32 0, i64 3
  %378 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %13, i64 0, i32 0, i32 0, i64 3
  %379 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %13, i64 0, i32 1, i32 0, i64 4
  %380 = bitcast i64* %377 to <2 x i64>*
  %381 = load <2 x i64>, <2 x i64>* %380, align 8
  %382 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %13, i64 0, i32 0, i32 0, i64 4
  %383 = bitcast i64* %378 to <2 x i64>*
  %384 = load <2 x i64>, <2 x i64>* %383, align 8
  %385 = extractelement <2 x i64> %381, i32 0
  %386 = extractelement <2 x i64> %384, i32 0
  %387 = add i64 %386, %385
  %388 = extractelement <2 x i64> %381, i32 1
  %389 = extractelement <2 x i64> %384, i32 1
  %390 = add i64 %389, %388
  store i64 %362, i64* %359, align 16
  %391 = getelementptr inbounds [8 x %struct.ge_cached], [8 x %struct.ge_cached]* %11, i64 0, i64 1, i32 0, i32 0, i64 1
  store i64 %373, i64* %391, align 8
  %392 = getelementptr inbounds [8 x %struct.ge_cached], [8 x %struct.ge_cached]* %11, i64 0, i64 1, i32 0, i32 0, i64 2
  store i64 %376, i64* %392, align 16
  %393 = getelementptr inbounds [8 x %struct.ge_cached], [8 x %struct.ge_cached]* %11, i64 0, i64 1, i32 0, i32 0, i64 3
  store i64 %387, i64* %393, align 8
  %394 = getelementptr inbounds [8 x %struct.ge_cached], [8 x %struct.ge_cached]* %11, i64 0, i64 1, i32 0, i32 0, i64 4
  store i64 %390, i64* %394, align 16
  %395 = getelementptr inbounds [8 x %struct.ge_cached], [8 x %struct.ge_cached]* %11, i64 0, i64 1, i32 1, i32 0, i64 0
  %396 = add i64 %360, 4503599627370458
  %397 = sub i64 %396, %361
  %398 = add <2 x i64> %367, <i64 4503599627370494, i64 4503599627370494>
  %399 = sub <2 x i64> %398, %370
  %400 = add <2 x i64> %381, <i64 4503599627370494, i64 4503599627370494>
  %401 = sub <2 x i64> %400, %384
  store i64 %397, i64* %395, align 8
  %402 = getelementptr inbounds [8 x %struct.ge_cached], [8 x %struct.ge_cached]* %11, i64 0, i64 1, i32 1, i32 0, i64 1
  %403 = bitcast i64* %402 to <2 x i64>*
  store <2 x i64> %399, <2 x i64>* %403, align 8
  %404 = getelementptr inbounds [8 x %struct.ge_cached], [8 x %struct.ge_cached]* %11, i64 0, i64 1, i32 1, i32 0, i64 3
  %405 = bitcast i64* %404 to <2 x i64>*
  store <2 x i64> %401, <2 x i64>* %405, align 8
  %406 = getelementptr inbounds [8 x %struct.ge_cached], [8 x %struct.ge_cached]* %11, i64 0, i64 1, i32 2
  %407 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %13, i64 0, i32 2
  %408 = bitcast %struct.fe_loose* %406 to i8*
  %409 = bitcast %struct.fe* %407 to i8*
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 16 %408, i8* align 16 %409, i64 40, i1 false) #4
  %410 = getelementptr inbounds [8 x %struct.ge_cached], [8 x %struct.ge_cached]* %11, i64 0, i64 1, i32 3, i32 0, i64 0
  call fastcc void @fe_mul_impl(i64* %410, i64* %357, i64* getelementptr inbounds (%struct.fe, %struct.fe* @d2, i64 0, i32 0, i64 0)) #4
  call void @x25519_ge_add(%struct.ge_p1p1* nonnull %12, %struct.ge_p3* nonnull %14, %struct.ge_cached* %358) #4
  call fastcc void @fe_mul_impl(i64* nonnull %354, i64* nonnull %347, i64* %348) #4
  call fastcc void @fe_mul_impl(i64* %355, i64* %350, i64* %351) #4
  call fastcc void @fe_mul_impl(i64* %356, i64* %351, i64* %348) #4
  call fastcc void @fe_mul_impl(i64* %357, i64* nonnull %347, i64* %350) #4
  %411 = getelementptr inbounds [8 x %struct.ge_cached], [8 x %struct.ge_cached]* %11, i64 0, i64 2
  %412 = getelementptr inbounds %struct.ge_cached, %struct.ge_cached* %411, i64 0, i32 0, i32 0, i64 0
  %413 = bitcast i64* %355 to <2 x i64>*
  %414 = load <2 x i64>, <2 x i64>* %413, align 8
  %415 = bitcast %struct.ge_p3* %13 to <2 x i64>*
  %416 = load <2 x i64>, <2 x i64>* %415, align 16
  %417 = extractelement <2 x i64> %414, i32 0
  %418 = extractelement <2 x i64> %416, i32 0
  %419 = add i64 %418, %417
  %420 = extractelement <2 x i64> %414, i32 1
  %421 = extractelement <2 x i64> %416, i32 1
  %422 = add i64 %421, %420
  %423 = bitcast i64* %365 to <2 x i64>*
  %424 = load <2 x i64>, <2 x i64>* %423, align 8
  %425 = bitcast i64* %368 to <2 x i64>*
  %426 = load <2 x i64>, <2 x i64>* %425, align 16
  %427 = extractelement <2 x i64> %424, i32 0
  %428 = extractelement <2 x i64> %426, i32 0
  %429 = add i64 %428, %427
  %430 = extractelement <2 x i64> %424, i32 1
  %431 = extractelement <2 x i64> %426, i32 1
  %432 = add i64 %431, %430
  %433 = load i64, i64* %379, align 8
  %434 = load i64, i64* %382, align 16
  %435 = add i64 %434, %433
  store i64 %419, i64* %412, align 16
  %436 = getelementptr inbounds [8 x %struct.ge_cached], [8 x %struct.ge_cached]* %11, i64 0, i64 2, i32 0, i32 0, i64 1
  store i64 %422, i64* %436, align 8
  %437 = getelementptr inbounds [8 x %struct.ge_cached], [8 x %struct.ge_cached]* %11, i64 0, i64 2, i32 0, i32 0, i64 2
  store i64 %429, i64* %437, align 16
  %438 = getelementptr inbounds [8 x %struct.ge_cached], [8 x %struct.ge_cached]* %11, i64 0, i64 2, i32 0, i32 0, i64 3
  store i64 %432, i64* %438, align 8
  %439 = getelementptr inbounds [8 x %struct.ge_cached], [8 x %struct.ge_cached]* %11, i64 0, i64 2, i32 0, i32 0, i64 4
  store i64 %435, i64* %439, align 16
  %440 = getelementptr inbounds [8 x %struct.ge_cached], [8 x %struct.ge_cached]* %11, i64 0, i64 2, i32 1, i32 0, i64 0
  %441 = add <2 x i64> %414, <i64 4503599627370458, i64 4503599627370494>
  %442 = sub <2 x i64> %441, %416
  %443 = add <2 x i64> %424, <i64 4503599627370494, i64 4503599627370494>
  %444 = sub <2 x i64> %443, %426
  %445 = add i64 %433, 4503599627370494
  %446 = sub i64 %445, %434
  %447 = bitcast i64* %440 to <2 x i64>*
  store <2 x i64> %442, <2 x i64>* %447, align 8
  %448 = getelementptr inbounds [8 x %struct.ge_cached], [8 x %struct.ge_cached]* %11, i64 0, i64 2, i32 1, i32 0, i64 2
  %449 = bitcast i64* %448 to <2 x i64>*
  store <2 x i64> %444, <2 x i64>* %449, align 8
  %450 = getelementptr inbounds [8 x %struct.ge_cached], [8 x %struct.ge_cached]* %11, i64 0, i64 2, i32 1, i32 0, i64 4
  store i64 %446, i64* %450, align 8
  %451 = getelementptr inbounds [8 x %struct.ge_cached], [8 x %struct.ge_cached]* %11, i64 0, i64 2, i32 2
  %452 = bitcast %struct.fe_loose* %451 to i8*
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 16 %452, i8* align 16 %409, i64 40, i1 false) #4
  %453 = getelementptr inbounds [8 x %struct.ge_cached], [8 x %struct.ge_cached]* %11, i64 0, i64 2, i32 3, i32 0, i64 0
  call fastcc void @fe_mul_impl(i64* %453, i64* %357, i64* getelementptr inbounds (%struct.fe, %struct.fe* @d2, i64 0, i32 0, i64 0)) #4
  call void @x25519_ge_add(%struct.ge_p1p1* nonnull %12, %struct.ge_p3* nonnull %14, %struct.ge_cached* %411) #4
  call fastcc void @fe_mul_impl(i64* nonnull %354, i64* nonnull %347, i64* %348) #4
  call fastcc void @fe_mul_impl(i64* %355, i64* %350, i64* %351) #4
  call fastcc void @fe_mul_impl(i64* %356, i64* %351, i64* %348) #4
  call fastcc void @fe_mul_impl(i64* %357, i64* nonnull %347, i64* %350) #4
  %454 = getelementptr inbounds [8 x %struct.ge_cached], [8 x %struct.ge_cached]* %11, i64 0, i64 3
  %455 = getelementptr inbounds %struct.ge_cached, %struct.ge_cached* %454, i64 0, i32 0, i32 0, i64 0
  %456 = bitcast i64* %355 to <2 x i64>*
  %457 = load <2 x i64>, <2 x i64>* %456, align 8
  %458 = bitcast %struct.ge_p3* %13 to <2 x i64>*
  %459 = load <2 x i64>, <2 x i64>* %458, align 16
  %460 = extractelement <2 x i64> %457, i32 0
  %461 = extractelement <2 x i64> %459, i32 0
  %462 = add i64 %461, %460
  %463 = extractelement <2 x i64> %457, i32 1
  %464 = extractelement <2 x i64> %459, i32 1
  %465 = add i64 %464, %463
  %466 = bitcast i64* %365 to <2 x i64>*
  %467 = load <2 x i64>, <2 x i64>* %466, align 8
  %468 = bitcast i64* %368 to <2 x i64>*
  %469 = load <2 x i64>, <2 x i64>* %468, align 16
  %470 = extractelement <2 x i64> %467, i32 0
  %471 = extractelement <2 x i64> %469, i32 0
  %472 = add i64 %471, %470
  %473 = extractelement <2 x i64> %467, i32 1
  %474 = extractelement <2 x i64> %469, i32 1
  %475 = add i64 %474, %473
  %476 = load i64, i64* %379, align 8
  %477 = load i64, i64* %382, align 16
  %478 = add i64 %477, %476
  store i64 %462, i64* %455, align 16
  %479 = getelementptr inbounds [8 x %struct.ge_cached], [8 x %struct.ge_cached]* %11, i64 0, i64 3, i32 0, i32 0, i64 1
  store i64 %465, i64* %479, align 8
  %480 = getelementptr inbounds [8 x %struct.ge_cached], [8 x %struct.ge_cached]* %11, i64 0, i64 3, i32 0, i32 0, i64 2
  store i64 %472, i64* %480, align 16
  %481 = getelementptr inbounds [8 x %struct.ge_cached], [8 x %struct.ge_cached]* %11, i64 0, i64 3, i32 0, i32 0, i64 3
  store i64 %475, i64* %481, align 8
  %482 = getelementptr inbounds [8 x %struct.ge_cached], [8 x %struct.ge_cached]* %11, i64 0, i64 3, i32 0, i32 0, i64 4
  store i64 %478, i64* %482, align 16
  %483 = getelementptr inbounds [8 x %struct.ge_cached], [8 x %struct.ge_cached]* %11, i64 0, i64 3, i32 1, i32 0, i64 0
  %484 = add <2 x i64> %457, <i64 4503599627370458, i64 4503599627370494>
  %485 = sub <2 x i64> %484, %459
  %486 = add <2 x i64> %467, <i64 4503599627370494, i64 4503599627370494>
  %487 = sub <2 x i64> %486, %469
  %488 = add i64 %476, 4503599627370494
  %489 = sub i64 %488, %477
  %490 = bitcast i64* %483 to <2 x i64>*
  store <2 x i64> %485, <2 x i64>* %490, align 8
  %491 = getelementptr inbounds [8 x %struct.ge_cached], [8 x %struct.ge_cached]* %11, i64 0, i64 3, i32 1, i32 0, i64 2
  %492 = bitcast i64* %491 to <2 x i64>*
  store <2 x i64> %487, <2 x i64>* %492, align 8
  %493 = getelementptr inbounds [8 x %struct.ge_cached], [8 x %struct.ge_cached]* %11, i64 0, i64 3, i32 1, i32 0, i64 4
  store i64 %489, i64* %493, align 8
  %494 = getelementptr inbounds [8 x %struct.ge_cached], [8 x %struct.ge_cached]* %11, i64 0, i64 3, i32 2
  %495 = bitcast %struct.fe_loose* %494 to i8*
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 16 %495, i8* align 16 %409, i64 40, i1 false) #4
  %496 = getelementptr inbounds [8 x %struct.ge_cached], [8 x %struct.ge_cached]* %11, i64 0, i64 3, i32 3, i32 0, i64 0
  call fastcc void @fe_mul_impl(i64* %496, i64* %357, i64* getelementptr inbounds (%struct.fe, %struct.fe* @d2, i64 0, i32 0, i64 0)) #4
  call void @x25519_ge_add(%struct.ge_p1p1* nonnull %12, %struct.ge_p3* nonnull %14, %struct.ge_cached* %454) #4
  call fastcc void @fe_mul_impl(i64* nonnull %354, i64* nonnull %347, i64* %348) #4
  call fastcc void @fe_mul_impl(i64* %355, i64* %350, i64* %351) #4
  call fastcc void @fe_mul_impl(i64* %356, i64* %351, i64* %348) #4
  call fastcc void @fe_mul_impl(i64* %357, i64* nonnull %347, i64* %350) #4
  %497 = getelementptr inbounds [8 x %struct.ge_cached], [8 x %struct.ge_cached]* %11, i64 0, i64 4
  %498 = getelementptr inbounds %struct.ge_cached, %struct.ge_cached* %497, i64 0, i32 0, i32 0, i64 0
  %499 = bitcast i64* %355 to <2 x i64>*
  %500 = load <2 x i64>, <2 x i64>* %499, align 8
  %501 = bitcast %struct.ge_p3* %13 to <2 x i64>*
  %502 = load <2 x i64>, <2 x i64>* %501, align 16
  %503 = extractelement <2 x i64> %500, i32 0
  %504 = extractelement <2 x i64> %502, i32 0
  %505 = add i64 %504, %503
  %506 = extractelement <2 x i64> %500, i32 1
  %507 = extractelement <2 x i64> %502, i32 1
  %508 = add i64 %507, %506
  %509 = load i64, i64* %365, align 8
  %510 = load i64, i64* %368, align 16
  %511 = add i64 %510, %509
  %512 = bitcast i64* %377 to <2 x i64>*
  %513 = load <2 x i64>, <2 x i64>* %512, align 8
  %514 = bitcast i64* %378 to <2 x i64>*
  %515 = load <2 x i64>, <2 x i64>* %514, align 8
  %516 = extractelement <2 x i64> %513, i32 0
  %517 = extractelement <2 x i64> %515, i32 0
  %518 = add i64 %517, %516
  %519 = extractelement <2 x i64> %513, i32 1
  %520 = extractelement <2 x i64> %515, i32 1
  %521 = add i64 %520, %519
  store i64 %505, i64* %498, align 16
  %522 = getelementptr inbounds [8 x %struct.ge_cached], [8 x %struct.ge_cached]* %11, i64 0, i64 4, i32 0, i32 0, i64 1
  store i64 %508, i64* %522, align 8
  %523 = getelementptr inbounds [8 x %struct.ge_cached], [8 x %struct.ge_cached]* %11, i64 0, i64 4, i32 0, i32 0, i64 2
  store i64 %511, i64* %523, align 16
  %524 = getelementptr inbounds [8 x %struct.ge_cached], [8 x %struct.ge_cached]* %11, i64 0, i64 4, i32 0, i32 0, i64 3
  store i64 %518, i64* %524, align 8
  %525 = getelementptr inbounds [8 x %struct.ge_cached], [8 x %struct.ge_cached]* %11, i64 0, i64 4, i32 0, i32 0, i64 4
  store i64 %521, i64* %525, align 16
  %526 = getelementptr inbounds [8 x %struct.ge_cached], [8 x %struct.ge_cached]* %11, i64 0, i64 4, i32 1, i32 0, i64 0
  %527 = add <2 x i64> %500, <i64 4503599627370458, i64 4503599627370494>
  %528 = sub <2 x i64> %527, %502
  %529 = add i64 %509, 4503599627370494
  %530 = sub i64 %529, %510
  %531 = add <2 x i64> %513, <i64 4503599627370494, i64 4503599627370494>
  %532 = sub <2 x i64> %531, %515
  %533 = bitcast i64* %526 to <2 x i64>*
  store <2 x i64> %528, <2 x i64>* %533, align 8
  %534 = getelementptr inbounds [8 x %struct.ge_cached], [8 x %struct.ge_cached]* %11, i64 0, i64 4, i32 1, i32 0, i64 2
  store i64 %530, i64* %534, align 8
  %535 = getelementptr inbounds [8 x %struct.ge_cached], [8 x %struct.ge_cached]* %11, i64 0, i64 4, i32 1, i32 0, i64 3
  %536 = bitcast i64* %535 to <2 x i64>*
  store <2 x i64> %532, <2 x i64>* %536, align 8
  %537 = getelementptr inbounds [8 x %struct.ge_cached], [8 x %struct.ge_cached]* %11, i64 0, i64 4, i32 2
  %538 = bitcast %struct.fe_loose* %537 to i8*
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 16 %538, i8* align 16 %409, i64 40, i1 false) #4
  %539 = getelementptr inbounds [8 x %struct.ge_cached], [8 x %struct.ge_cached]* %11, i64 0, i64 4, i32 3, i32 0, i64 0
  call fastcc void @fe_mul_impl(i64* %539, i64* %357, i64* getelementptr inbounds (%struct.fe, %struct.fe* @d2, i64 0, i32 0, i64 0)) #4
  call void @x25519_ge_add(%struct.ge_p1p1* nonnull %12, %struct.ge_p3* nonnull %14, %struct.ge_cached* %497) #4
  call fastcc void @fe_mul_impl(i64* nonnull %354, i64* nonnull %347, i64* %348) #4
  call fastcc void @fe_mul_impl(i64* %355, i64* %350, i64* %351) #4
  call fastcc void @fe_mul_impl(i64* %356, i64* %351, i64* %348) #4
  call fastcc void @fe_mul_impl(i64* %357, i64* nonnull %347, i64* %350) #4
  %540 = getelementptr inbounds [8 x %struct.ge_cached], [8 x %struct.ge_cached]* %11, i64 0, i64 5
  %541 = getelementptr inbounds %struct.ge_cached, %struct.ge_cached* %540, i64 0, i32 0, i32 0, i64 0
  %542 = bitcast i64* %355 to <2 x i64>*
  %543 = load <2 x i64>, <2 x i64>* %542, align 8
  %544 = bitcast %struct.ge_p3* %13 to <2 x i64>*
  %545 = load <2 x i64>, <2 x i64>* %544, align 16
  %546 = extractelement <2 x i64> %543, i32 0
  %547 = extractelement <2 x i64> %545, i32 0
  %548 = add i64 %547, %546
  %549 = extractelement <2 x i64> %543, i32 1
  %550 = extractelement <2 x i64> %545, i32 1
  %551 = add i64 %550, %549
  %552 = bitcast i64* %365 to <2 x i64>*
  %553 = load <2 x i64>, <2 x i64>* %552, align 8
  %554 = bitcast i64* %368 to <2 x i64>*
  %555 = load <2 x i64>, <2 x i64>* %554, align 16
  %556 = extractelement <2 x i64> %553, i32 0
  %557 = extractelement <2 x i64> %555, i32 0
  %558 = add i64 %557, %556
  %559 = extractelement <2 x i64> %553, i32 1
  %560 = extractelement <2 x i64> %555, i32 1
  %561 = add i64 %560, %559
  %562 = load i64, i64* %379, align 8
  %563 = load i64, i64* %382, align 16
  %564 = add i64 %563, %562
  store i64 %548, i64* %541, align 16
  %565 = getelementptr inbounds [8 x %struct.ge_cached], [8 x %struct.ge_cached]* %11, i64 0, i64 5, i32 0, i32 0, i64 1
  store i64 %551, i64* %565, align 8
  %566 = getelementptr inbounds [8 x %struct.ge_cached], [8 x %struct.ge_cached]* %11, i64 0, i64 5, i32 0, i32 0, i64 2
  store i64 %558, i64* %566, align 16
  %567 = getelementptr inbounds [8 x %struct.ge_cached], [8 x %struct.ge_cached]* %11, i64 0, i64 5, i32 0, i32 0, i64 3
  store i64 %561, i64* %567, align 8
  %568 = getelementptr inbounds [8 x %struct.ge_cached], [8 x %struct.ge_cached]* %11, i64 0, i64 5, i32 0, i32 0, i64 4
  store i64 %564, i64* %568, align 16
  %569 = getelementptr inbounds [8 x %struct.ge_cached], [8 x %struct.ge_cached]* %11, i64 0, i64 5, i32 1, i32 0, i64 0
  %570 = add <2 x i64> %543, <i64 4503599627370458, i64 4503599627370494>
  %571 = sub <2 x i64> %570, %545
  %572 = add <2 x i64> %553, <i64 4503599627370494, i64 4503599627370494>
  %573 = sub <2 x i64> %572, %555
  %574 = add i64 %562, 4503599627370494
  %575 = sub i64 %574, %563
  %576 = bitcast i64* %569 to <2 x i64>*
  store <2 x i64> %571, <2 x i64>* %576, align 8
  %577 = getelementptr inbounds [8 x %struct.ge_cached], [8 x %struct.ge_cached]* %11, i64 0, i64 5, i32 1, i32 0, i64 2
  %578 = bitcast i64* %577 to <2 x i64>*
  store <2 x i64> %573, <2 x i64>* %578, align 8
  %579 = getelementptr inbounds [8 x %struct.ge_cached], [8 x %struct.ge_cached]* %11, i64 0, i64 5, i32 1, i32 0, i64 4
  store i64 %575, i64* %579, align 8
  %580 = getelementptr inbounds [8 x %struct.ge_cached], [8 x %struct.ge_cached]* %11, i64 0, i64 5, i32 2
  %581 = bitcast %struct.fe_loose* %580 to i8*
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 16 %581, i8* align 16 %409, i64 40, i1 false) #4
  %582 = getelementptr inbounds [8 x %struct.ge_cached], [8 x %struct.ge_cached]* %11, i64 0, i64 5, i32 3, i32 0, i64 0
  call fastcc void @fe_mul_impl(i64* %582, i64* %357, i64* getelementptr inbounds (%struct.fe, %struct.fe* @d2, i64 0, i32 0, i64 0)) #4
  call void @x25519_ge_add(%struct.ge_p1p1* nonnull %12, %struct.ge_p3* nonnull %14, %struct.ge_cached* %540) #4
  call fastcc void @fe_mul_impl(i64* nonnull %354, i64* nonnull %347, i64* %348) #4
  call fastcc void @fe_mul_impl(i64* %355, i64* %350, i64* %351) #4
  call fastcc void @fe_mul_impl(i64* %356, i64* %351, i64* %348) #4
  call fastcc void @fe_mul_impl(i64* %357, i64* nonnull %347, i64* %350) #4
  %583 = getelementptr inbounds [8 x %struct.ge_cached], [8 x %struct.ge_cached]* %11, i64 0, i64 6
  %584 = getelementptr inbounds %struct.ge_cached, %struct.ge_cached* %583, i64 0, i32 0, i32 0, i64 0
  %585 = bitcast i64* %355 to <2 x i64>*
  %586 = load <2 x i64>, <2 x i64>* %585, align 8
  %587 = bitcast %struct.ge_p3* %13 to <2 x i64>*
  %588 = load <2 x i64>, <2 x i64>* %587, align 16
  %589 = extractelement <2 x i64> %586, i32 0
  %590 = extractelement <2 x i64> %588, i32 0
  %591 = add i64 %590, %589
  %592 = extractelement <2 x i64> %586, i32 1
  %593 = extractelement <2 x i64> %588, i32 1
  %594 = add i64 %593, %592
  %595 = bitcast i64* %365 to <2 x i64>*
  %596 = load <2 x i64>, <2 x i64>* %595, align 8
  %597 = bitcast i64* %368 to <2 x i64>*
  %598 = load <2 x i64>, <2 x i64>* %597, align 16
  %599 = extractelement <2 x i64> %596, i32 0
  %600 = extractelement <2 x i64> %598, i32 0
  %601 = add i64 %600, %599
  %602 = extractelement <2 x i64> %596, i32 1
  %603 = extractelement <2 x i64> %598, i32 1
  %604 = add i64 %603, %602
  %605 = load i64, i64* %379, align 8
  %606 = load i64, i64* %382, align 16
  %607 = add i64 %606, %605
  store i64 %591, i64* %584, align 16
  %608 = getelementptr inbounds [8 x %struct.ge_cached], [8 x %struct.ge_cached]* %11, i64 0, i64 6, i32 0, i32 0, i64 1
  store i64 %594, i64* %608, align 8
  %609 = getelementptr inbounds [8 x %struct.ge_cached], [8 x %struct.ge_cached]* %11, i64 0, i64 6, i32 0, i32 0, i64 2
  store i64 %601, i64* %609, align 16
  %610 = getelementptr inbounds [8 x %struct.ge_cached], [8 x %struct.ge_cached]* %11, i64 0, i64 6, i32 0, i32 0, i64 3
  store i64 %604, i64* %610, align 8
  %611 = getelementptr inbounds [8 x %struct.ge_cached], [8 x %struct.ge_cached]* %11, i64 0, i64 6, i32 0, i32 0, i64 4
  store i64 %607, i64* %611, align 16
  %612 = getelementptr inbounds [8 x %struct.ge_cached], [8 x %struct.ge_cached]* %11, i64 0, i64 6, i32 1, i32 0, i64 0
  %613 = add <2 x i64> %586, <i64 4503599627370458, i64 4503599627370494>
  %614 = sub <2 x i64> %613, %588
  %615 = add <2 x i64> %596, <i64 4503599627370494, i64 4503599627370494>
  %616 = sub <2 x i64> %615, %598
  %617 = add i64 %605, 4503599627370494
  %618 = sub i64 %617, %606
  %619 = bitcast i64* %612 to <2 x i64>*
  store <2 x i64> %614, <2 x i64>* %619, align 8
  %620 = getelementptr inbounds [8 x %struct.ge_cached], [8 x %struct.ge_cached]* %11, i64 0, i64 6, i32 1, i32 0, i64 2
  %621 = bitcast i64* %620 to <2 x i64>*
  store <2 x i64> %616, <2 x i64>* %621, align 8
  %622 = getelementptr inbounds [8 x %struct.ge_cached], [8 x %struct.ge_cached]* %11, i64 0, i64 6, i32 1, i32 0, i64 4
  store i64 %618, i64* %622, align 8
  %623 = getelementptr inbounds [8 x %struct.ge_cached], [8 x %struct.ge_cached]* %11, i64 0, i64 6, i32 2
  %624 = bitcast %struct.fe_loose* %623 to i8*
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 16 %624, i8* align 16 %409, i64 40, i1 false) #4
  %625 = getelementptr inbounds [8 x %struct.ge_cached], [8 x %struct.ge_cached]* %11, i64 0, i64 6, i32 3, i32 0, i64 0
  call fastcc void @fe_mul_impl(i64* %625, i64* %357, i64* getelementptr inbounds (%struct.fe, %struct.fe* @d2, i64 0, i32 0, i64 0)) #4
  call void @x25519_ge_add(%struct.ge_p1p1* nonnull %12, %struct.ge_p3* nonnull %14, %struct.ge_cached* %583) #4
  call fastcc void @fe_mul_impl(i64* nonnull %354, i64* nonnull %347, i64* %348) #4
  call fastcc void @fe_mul_impl(i64* %355, i64* %350, i64* %351) #4
  call fastcc void @fe_mul_impl(i64* %356, i64* %351, i64* %348) #4
  call fastcc void @fe_mul_impl(i64* %357, i64* nonnull %347, i64* %350) #4
  %626 = getelementptr inbounds [8 x %struct.ge_cached], [8 x %struct.ge_cached]* %11, i64 0, i64 7, i32 0, i32 0, i64 0
  %627 = bitcast i64* %355 to <2 x i64>*
  %628 = load <2 x i64>, <2 x i64>* %627, align 8
  %629 = bitcast %struct.ge_p3* %13 to <2 x i64>*
  %630 = load <2 x i64>, <2 x i64>* %629, align 16
  %631 = extractelement <2 x i64> %628, i32 0
  %632 = extractelement <2 x i64> %630, i32 0
  %633 = add i64 %632, %631
  %634 = extractelement <2 x i64> %628, i32 1
  %635 = extractelement <2 x i64> %630, i32 1
  %636 = add i64 %635, %634
  %637 = bitcast i64* %365 to <2 x i64>*
  %638 = load <2 x i64>, <2 x i64>* %637, align 8
  %639 = bitcast i64* %368 to <2 x i64>*
  %640 = load <2 x i64>, <2 x i64>* %639, align 16
  %641 = extractelement <2 x i64> %638, i32 0
  %642 = extractelement <2 x i64> %640, i32 0
  %643 = add i64 %642, %641
  %644 = extractelement <2 x i64> %638, i32 1
  %645 = extractelement <2 x i64> %640, i32 1
  %646 = add i64 %645, %644
  %647 = load i64, i64* %379, align 8
  %648 = load i64, i64* %382, align 16
  %649 = add i64 %648, %647
  store i64 %633, i64* %626, align 16
  %650 = getelementptr inbounds [8 x %struct.ge_cached], [8 x %struct.ge_cached]* %11, i64 0, i64 7, i32 0, i32 0, i64 1
  store i64 %636, i64* %650, align 8
  %651 = getelementptr inbounds [8 x %struct.ge_cached], [8 x %struct.ge_cached]* %11, i64 0, i64 7, i32 0, i32 0, i64 2
  store i64 %643, i64* %651, align 16
  %652 = getelementptr inbounds [8 x %struct.ge_cached], [8 x %struct.ge_cached]* %11, i64 0, i64 7, i32 0, i32 0, i64 3
  store i64 %646, i64* %652, align 8
  %653 = getelementptr inbounds [8 x %struct.ge_cached], [8 x %struct.ge_cached]* %11, i64 0, i64 7, i32 0, i32 0, i64 4
  store i64 %649, i64* %653, align 16
  %654 = getelementptr inbounds [8 x %struct.ge_cached], [8 x %struct.ge_cached]* %11, i64 0, i64 7, i32 1, i32 0, i64 0
  %655 = add <2 x i64> %628, <i64 4503599627370458, i64 4503599627370494>
  %656 = sub <2 x i64> %655, %630
  %657 = add <2 x i64> %638, <i64 4503599627370494, i64 4503599627370494>
  %658 = sub <2 x i64> %657, %640
  %659 = add i64 %647, 4503599627370494
  %660 = sub i64 %659, %648
  %661 = bitcast i64* %654 to <2 x i64>*
  store <2 x i64> %656, <2 x i64>* %661, align 8
  %662 = getelementptr inbounds [8 x %struct.ge_cached], [8 x %struct.ge_cached]* %11, i64 0, i64 7, i32 1, i32 0, i64 2
  %663 = bitcast i64* %662 to <2 x i64>*
  store <2 x i64> %658, <2 x i64>* %663, align 8
  %664 = getelementptr inbounds [8 x %struct.ge_cached], [8 x %struct.ge_cached]* %11, i64 0, i64 7, i32 1, i32 0, i64 4
  store i64 %660, i64* %664, align 8
  %665 = getelementptr inbounds [8 x %struct.ge_cached], [8 x %struct.ge_cached]* %11, i64 0, i64 7, i32 2
  %666 = bitcast %struct.fe_loose* %665 to i8*
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 16 %666, i8* align 16 %409, i64 40, i1 false) #4
  %667 = getelementptr inbounds [8 x %struct.ge_cached], [8 x %struct.ge_cached]* %11, i64 0, i64 7, i32 3, i32 0, i64 0
  call fastcc void @fe_mul_impl(i64* %667, i64* %357, i64* getelementptr inbounds (%struct.fe, %struct.fe* @d2, i64 0, i32 0, i64 0)) #4
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %129, i8 0, i64 40, i1 false) #4
  %668 = getelementptr inbounds %struct.ge_p2, %struct.ge_p2* %25, i64 0, i32 1, i32 0, i64 1
  %669 = bitcast i64* %668 to i8*
  call void @llvm.memset.p0i8.i64(i8* align 8 %669, i8 0, i64 32, i1 false) #4
  %670 = getelementptr inbounds %struct.ge_p2, %struct.ge_p2* %25, i64 0, i32 1, i32 0, i64 0
  store i64 1, i64* %670, align 8
  %671 = getelementptr inbounds %struct.ge_p2, %struct.ge_p2* %25, i64 0, i32 2, i32 0, i64 1
  %672 = bitcast i64* %671 to i8*
  call void @llvm.memset.p0i8.i64(i8* align 8 %672, i8 0, i64 32, i1 false) #4
  %673 = getelementptr inbounds %struct.ge_p2, %struct.ge_p2* %25, i64 0, i32 2, i32 0, i64 0
  store i64 1, i64* %673, align 8
  br label %674

674:                                              ; preds = %911, %290
  %675 = phi i64 [ 255, %290 ], [ %912, %911 ]
  %676 = phi i32 [ 255, %290 ], [ %913, %911 ]
  %677 = getelementptr inbounds [256 x i8], [256 x i8]* %9, i64 0, i64 %675
  %678 = load i8, i8* %677, align 1
  %679 = icmp eq i8 %678, 0
  br i1 %679, label %680, label %692

680:                                              ; preds = %674
  %681 = getelementptr inbounds [256 x i8], [256 x i8]* %10, i64 0, i64 %675
  %682 = load i8, i8* %681, align 1
  %683 = icmp eq i8 %682, 0
  br i1 %683, label %684, label %689

684:                                              ; preds = %680
  %685 = add nsw i64 %675, -1
  %686 = getelementptr inbounds [256 x i8], [256 x i8]* %9, i64 0, i64 %685
  %687 = load i8, i8* %686, align 1
  %688 = icmp eq i8 %687, 0
  br i1 %688, label %907, label %692

689:                                              ; preds = %907, %680
  %690 = phi i64 [ %675, %680 ], [ %685, %907 ]
  %691 = trunc i64 %690 to i32
  br label %695

692:                                              ; preds = %684, %674
  %693 = phi i64 [ %675, %674 ], [ %685, %684 ]
  %694 = trunc i64 %693 to i32
  br label %695

695:                                              ; preds = %911, %692, %689
  %696 = phi i32 [ %691, %689 ], [ %694, %692 ], [ %913, %911 ]
  %697 = icmp sgt i32 %696, -1
  br i1 %697, label %698, label %882

698:                                              ; preds = %695
  %699 = bitcast %struct.fe* %5 to i8*
  %700 = getelementptr inbounds %struct.fe, %struct.fe* %5, i64 0, i32 0, i64 0
  %701 = getelementptr inbounds %struct.fe, %struct.fe* %5, i64 0, i32 0, i64 2
  %702 = getelementptr inbounds %struct.fe, %struct.fe* %5, i64 0, i32 0, i64 4
  %703 = bitcast %struct.fe* %6 to i8*
  %704 = getelementptr inbounds %struct.fe, %struct.fe* %6, i64 0, i32 0, i64 0
  %705 = getelementptr inbounds %struct.fe, %struct.fe* %6, i64 0, i32 0, i64 2
  %706 = getelementptr inbounds %struct.fe, %struct.fe* %6, i64 0, i32 0, i64 4
  %707 = bitcast %struct.fe* %7 to i8*
  %708 = getelementptr inbounds %struct.fe, %struct.fe* %7, i64 0, i32 0, i64 0
  %709 = getelementptr inbounds %struct.fe, %struct.fe* %7, i64 0, i32 0, i64 1
  %710 = getelementptr inbounds %struct.fe, %struct.fe* %7, i64 0, i32 0, i64 2
  %711 = getelementptr inbounds %struct.fe, %struct.fe* %7, i64 0, i32 0, i64 3
  %712 = getelementptr inbounds %struct.fe, %struct.fe* %7, i64 0, i32 0, i64 4
  %713 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %12, i64 0, i32 0, i32 0, i64 1
  %714 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %12, i64 0, i32 0, i32 0, i64 2
  %715 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %12, i64 0, i32 0, i32 0, i64 3
  %716 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %12, i64 0, i32 0, i32 0, i64 4
  %717 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %12, i64 0, i32 1, i32 0, i64 1
  %718 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %12, i64 0, i32 1, i32 0, i64 2
  %719 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %12, i64 0, i32 1, i32 0, i64 3
  %720 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %12, i64 0, i32 1, i32 0, i64 4
  %721 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %13, i64 0, i32 2, i32 0, i64 1
  %722 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %13, i64 0, i32 2, i32 0, i64 2
  %723 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %13, i64 0, i32 2, i32 0, i64 3
  %724 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %13, i64 0, i32 2, i32 0, i64 4
  %725 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %12, i64 0, i32 3, i32 0, i64 1
  %726 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %12, i64 0, i32 3, i32 0, i64 2
  %727 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %12, i64 0, i32 3, i32 0, i64 3
  %728 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %12, i64 0, i32 3, i32 0, i64 4
  %729 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %12, i64 0, i32 2, i32 0, i64 1
  %730 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %12, i64 0, i32 2, i32 0, i64 2
  %731 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %12, i64 0, i32 2, i32 0, i64 3
  %732 = getelementptr inbounds %struct.ge_p1p1, %struct.ge_p1p1* %12, i64 0, i32 2, i32 0, i64 4
  %733 = getelementptr inbounds %struct.ge_p2, %struct.ge_p2* %25, i64 0, i32 0, i32 0, i64 0
  %734 = sext i32 %696 to i64
  %735 = bitcast i64* %355 to <2 x i64>*
  %736 = bitcast %struct.ge_p3* %13 to <2 x i64>*
  %737 = bitcast i64* %365 to <2 x i64>*
  %738 = bitcast i64* %368 to <2 x i64>*
  %739 = bitcast i64* %350 to <2 x i64>*
  %740 = bitcast i64* %718 to <2 x i64>*
  %741 = bitcast %struct.fe* %6 to <2 x i64>*
  %742 = bitcast %struct.fe* %5 to <2 x i64>*
  %743 = bitcast i64* %705 to <2 x i64>*
  %744 = bitcast i64* %701 to <2 x i64>*
  %745 = bitcast %struct.ge_p1p1* %12 to <2 x i64>*
  %746 = bitcast i64* %714 to <2 x i64>*
  %747 = bitcast i64* %717 to <2 x i64>*
  br label %748

748:                                              ; preds = %879, %698
  %749 = phi i64 [ %734, %698 ], [ %880, %879 ]
  call fastcc void @ge_p2_dbl(%struct.ge_p1p1* nonnull %12, %struct.ge_p2* nonnull %25) #4
  %750 = getelementptr inbounds [256 x i8], [256 x i8]* %9, i64 0, i64 %749
  %751 = load i8, i8* %750, align 1
  %752 = icmp sgt i8 %751, 0
  br i1 %752, label %753, label %757

753:                                              ; preds = %748
  call fastcc void @fe_mul_impl(i64* nonnull %354, i64* nonnull %347, i64* %348) #4
  call fastcc void @fe_mul_impl(i64* %355, i64* %350, i64* %351) #4
  call fastcc void @fe_mul_impl(i64* %356, i64* %351, i64* %348) #4
  call fastcc void @fe_mul_impl(i64* %357, i64* nonnull %347, i64* %350) #4
  %754 = lshr i8 %751, 1
  %755 = zext i8 %754 to i64
  %756 = getelementptr inbounds [8 x %struct.ge_cached], [8 x %struct.ge_cached]* %11, i64 0, i64 %755
  call void @x25519_ge_add(%struct.ge_p1p1* nonnull %12, %struct.ge_p3* nonnull %13, %struct.ge_cached* %756) #4
  br label %763

757:                                              ; preds = %748
  %758 = icmp slt i8 %751, 0
  br i1 %758, label %759, label %763

759:                                              ; preds = %757
  call fastcc void @fe_mul_impl(i64* nonnull %354, i64* nonnull %347, i64* %348) #4
  call fastcc void @fe_mul_impl(i64* %355, i64* %350, i64* %351) #4
  call fastcc void @fe_mul_impl(i64* %356, i64* %351, i64* %348) #4
  call fastcc void @fe_mul_impl(i64* %357, i64* nonnull %347, i64* %350) #4
  %760 = sdiv i8 %751, -2
  %761 = sext i8 %760 to i64
  %762 = getelementptr inbounds [8 x %struct.ge_cached], [8 x %struct.ge_cached]* %11, i64 0, i64 %761
  call void @x25519_ge_sub(%struct.ge_p1p1* nonnull %12, %struct.ge_p3* nonnull %13, %struct.ge_cached* %762) #4
  br label %763

763:                                              ; preds = %759, %757, %753
  %764 = getelementptr inbounds [256 x i8], [256 x i8]* %10, i64 0, i64 %749
  %765 = load i8, i8* %764, align 1
  %766 = icmp sgt i8 %765, 0
  br i1 %766, label %767, label %771

767:                                              ; preds = %763
  call fastcc void @fe_mul_impl(i64* nonnull %354, i64* nonnull %347, i64* %348) #4
  call fastcc void @fe_mul_impl(i64* %355, i64* %350, i64* %351) #4
  call fastcc void @fe_mul_impl(i64* %356, i64* %351, i64* %348) #4
  call fastcc void @fe_mul_impl(i64* %357, i64* nonnull %347, i64* %350) #4
  %768 = lshr i8 %765, 1
  %769 = zext i8 %768 to i64
  %770 = getelementptr inbounds [8 x %struct.ge_precomp], [8 x %struct.ge_precomp]* @Bi, i64 0, i64 %769
  call fastcc void @ge_madd(%struct.ge_p1p1* nonnull %12, %struct.ge_p3* nonnull %13, %struct.ge_precomp* %770) #4
  br label %879

771:                                              ; preds = %763
  %772 = icmp slt i8 %765, 0
  br i1 %772, label %773, label %879

773:                                              ; preds = %771
  call fastcc void @fe_mul_impl(i64* nonnull %354, i64* nonnull %347, i64* %348) #4
  call fastcc void @fe_mul_impl(i64* %355, i64* %350, i64* %351) #4
  call fastcc void @fe_mul_impl(i64* %356, i64* %351, i64* %348) #4
  call fastcc void @fe_mul_impl(i64* %357, i64* nonnull %347, i64* %350) #4
  %774 = sdiv i8 %765, -2
  %775 = sext i8 %774 to i64
  call void @llvm.lifetime.start.p0i8(i64 40, i8* nonnull %699) #4
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %699, i8 -86, i64 40, i1 false) #4
  call void @llvm.lifetime.start.p0i8(i64 40, i8* nonnull %703) #4
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %703, i8 -86, i64 40, i1 false) #4
  call void @llvm.lifetime.start.p0i8(i64 40, i8* nonnull %707) #4
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %707, i8 -86, i64 40, i1 false) #4
  %776 = load <2 x i64>, <2 x i64>* %735, align 8
  %777 = load <2 x i64>, <2 x i64>* %736, align 16
  %778 = extractelement <2 x i64> %776, i32 0
  %779 = extractelement <2 x i64> %777, i32 0
  %780 = add i64 %779, %778
  %781 = extractelement <2 x i64> %776, i32 1
  %782 = extractelement <2 x i64> %777, i32 1
  %783 = add i64 %782, %781
  %784 = load <2 x i64>, <2 x i64>* %737, align 8
  %785 = load <2 x i64>, <2 x i64>* %738, align 16
  %786 = extractelement <2 x i64> %784, i32 0
  %787 = extractelement <2 x i64> %785, i32 0
  %788 = add i64 %787, %786
  %789 = extractelement <2 x i64> %784, i32 1
  %790 = extractelement <2 x i64> %785, i32 1
  %791 = add i64 %790, %789
  %792 = load i64, i64* %379, align 8
  %793 = load i64, i64* %382, align 16
  %794 = add i64 %793, %792
  store i64 %780, i64* %347, align 16
  store i64 %783, i64* %713, align 8
  store i64 %788, i64* %714, align 16
  store i64 %791, i64* %715, align 8
  store i64 %794, i64* %716, align 16
  %795 = add <2 x i64> %776, <i64 4503599627370458, i64 4503599627370494>
  %796 = sub <2 x i64> %795, %777
  %797 = add <2 x i64> %784, <i64 4503599627370494, i64 4503599627370494>
  %798 = sub <2 x i64> %797, %785
  %799 = add i64 %792, 4503599627370494
  %800 = sub i64 %799, %793
  store <2 x i64> %796, <2 x i64>* %739, align 8
  store <2 x i64> %798, <2 x i64>* %740, align 8
  store i64 %800, i64* %720, align 8
  %801 = getelementptr inbounds [8 x %struct.ge_precomp], [8 x %struct.ge_precomp]* @Bi, i64 0, i64 %775, i32 1, i32 0, i64 0
  call fastcc void @fe_mul_impl(i64* nonnull %704, i64* nonnull %347, i64* %801) #4
  %802 = getelementptr inbounds [8 x %struct.ge_precomp], [8 x %struct.ge_precomp]* @Bi, i64 0, i64 %775, i32 0, i32 0, i64 0
  call fastcc void @fe_mul_impl(i64* nonnull %700, i64* %350, i64* %802) #4
  %803 = getelementptr inbounds [8 x %struct.ge_precomp], [8 x %struct.ge_precomp]* @Bi, i64 0, i64 %775, i32 2, i32 0, i64 0
  call fastcc void @fe_mul_impl(i64* nonnull %708, i64* %803, i64* %357) #4
  %804 = load i64, i64* %356, align 16
  %805 = shl i64 %804, 1
  %806 = load i64, i64* %721, align 8
  %807 = shl i64 %806, 1
  %808 = load i64, i64* %722, align 16
  %809 = shl i64 %808, 1
  %810 = load i64, i64* %723, align 8
  %811 = shl i64 %810, 1
  %812 = load i64, i64* %724, align 16
  %813 = shl i64 %812, 1
  %814 = load <2 x i64>, <2 x i64>* %741, align 16
  %815 = add <2 x i64> %814, <i64 4503599627370458, i64 4503599627370494>
  %816 = load <2 x i64>, <2 x i64>* %742, align 16
  %817 = sub <2 x i64> %815, %816
  %818 = load <2 x i64>, <2 x i64>* %743, align 16
  %819 = add <2 x i64> %818, <i64 4503599627370494, i64 4503599627370494>
  %820 = load <2 x i64>, <2 x i64>* %744, align 16
  %821 = sub <2 x i64> %819, %820
  %822 = load i64, i64* %706, align 16
  %823 = add i64 %822, 4503599627370494
  %824 = load i64, i64* %702, align 16
  %825 = sub i64 %823, %824
  store <2 x i64> %817, <2 x i64>* %745, align 16
  store <2 x i64> %821, <2 x i64>* %746, align 16
  store i64 %825, i64* %716, align 16
  %826 = extractelement <2 x i64> %814, i32 0
  %827 = extractelement <2 x i64> %816, i32 0
  %828 = add i64 %827, %826
  %829 = shufflevector <2 x i64> %816, <2 x i64> %820, <2 x i32> <i32 1, i32 2>
  %830 = shufflevector <2 x i64> %814, <2 x i64> %818, <2 x i32> <i32 1, i32 2>
  %831 = add <2 x i64> %829, %830
  %832 = extractelement <2 x i64> %818, i32 1
  %833 = extractelement <2 x i64> %820, i32 1
  %834 = add i64 %833, %832
  %835 = add i64 %824, %822
  store i64 %828, i64* %350, align 8
  store <2 x i64> %831, <2 x i64>* %747, align 8
  store i64 %834, i64* %719, align 8
  store i64 %835, i64* %720, align 8
  %836 = lshr i64 %804, 50
  %837 = and i64 %836, 8191
  %838 = add i64 %837, %807
  %839 = lshr i64 %838, 51
  %840 = add i64 %839, %809
  %841 = lshr i64 %840, 51
  %842 = add i64 %841, %811
  %843 = lshr i64 %842, 51
  %844 = add i64 %843, %813
  %845 = and i64 %805, 2251799813685246
  %846 = lshr i64 %844, 51
  %847 = mul nuw nsw i64 %846, 19
  %848 = add nuw nsw i64 %847, %845
  %849 = lshr i64 %848, 51
  %850 = and i64 %838, 2251799813685247
  %851 = add nuw nsw i64 %849, %850
  %852 = and i64 %848, 2251799813685247
  %853 = and i64 %851, 2251799813685247
  %854 = lshr i64 %851, 51
  %855 = and i64 %840, 2251799813685247
  %856 = add nuw nsw i64 %854, %855
  %857 = and i64 %842, 2251799813685247
  %858 = and i64 %844, 2251799813685247
  %859 = add nuw nsw i64 %852, 4503599627370458
  %860 = load i64, i64* %708, align 8
  %861 = sub i64 %859, %860
  %862 = add nuw nsw i64 %853, 4503599627370494
  %863 = load i64, i64* %709, align 8
  %864 = sub i64 %862, %863
  %865 = add nuw nsw i64 %856, 4503599627370494
  %866 = load i64, i64* %710, align 8
  %867 = sub i64 %865, %866
  %868 = add nuw nsw i64 %857, 4503599627370494
  %869 = load i64, i64* %711, align 8
  %870 = sub i64 %868, %869
  %871 = add nuw nsw i64 %858, 4503599627370494
  %872 = load i64, i64* %712, align 8
  %873 = sub i64 %871, %872
  store i64 %861, i64* %351, align 16
  store i64 %864, i64* %729, align 8
  store i64 %867, i64* %730, align 16
  store i64 %870, i64* %731, align 8
  store i64 %873, i64* %732, align 16
  %874 = add i64 %860, %852
  %875 = add i64 %863, %853
  %876 = add i64 %866, %856
  %877 = add i64 %869, %857
  %878 = add i64 %872, %858
  store i64 %874, i64* %348, align 8
  store i64 %875, i64* %725, align 8
  store i64 %876, i64* %726, align 8
  store i64 %877, i64* %727, align 8
  store i64 %878, i64* %728, align 8
  call void @llvm.lifetime.end.p0i8(i64 40, i8* nonnull %707) #4
  call void @llvm.lifetime.end.p0i8(i64 40, i8* nonnull %703) #4
  call void @llvm.lifetime.end.p0i8(i64 40, i8* nonnull %699) #4
  br label %879

879:                                              ; preds = %773, %771, %767
  call fastcc void @fe_mul_impl(i64* nonnull %733, i64* nonnull %347, i64* %348) #4
  call fastcc void @fe_mul_impl(i64* %670, i64* %350, i64* %351) #4
  call fastcc void @fe_mul_impl(i64* %673, i64* %351, i64* %348) #4
  %880 = add nsw i64 %749, -1
  %881 = icmp sgt i64 %749, 0
  br i1 %881, label %748, label %882

882:                                              ; preds = %879, %695
  call void @llvm.lifetime.end.p0i8(i64 160, i8* nonnull %135) #4
  call void @llvm.lifetime.end.p0i8(i64 160, i8* nonnull %134) #4
  call void @llvm.lifetime.end.p0i8(i64 160, i8* nonnull %133) #4
  call void @llvm.lifetime.end.p0i8(i64 1280, i8* nonnull %132) #4
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %131) #4
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %130) #4
  %883 = getelementptr inbounds [32 x i8], [32 x i8]* %26, i64 0, i64 0
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %883) #4
  %884 = getelementptr inbounds [32 x i8], [32 x i8]* %26, i64 0, i64 31
  %885 = bitcast %struct.fe* %17 to i8*
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %883, i8 -86, i64 32, i1 false)
  call void @llvm.lifetime.start.p0i8(i64 40, i8* nonnull %885) #4
  %886 = getelementptr inbounds %struct.fe, %struct.fe* %17, i64 0, i32 0, i64 0
  %887 = bitcast %struct.fe* %18 to i8*
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %885, i8 -86, i64 40, i1 false) #4
  call void @llvm.lifetime.start.p0i8(i64 40, i8* nonnull %887) #4
  %888 = getelementptr inbounds %struct.fe, %struct.fe* %18, i64 0, i32 0, i64 0
  %889 = bitcast %struct.fe* %19 to i8*
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %887, i8 -86, i64 40, i1 false) #4
  call void @llvm.lifetime.start.p0i8(i64 40, i8* nonnull %889) #4
  %890 = getelementptr inbounds %struct.fe, %struct.fe* %19, i64 0, i32 0, i64 0
  %891 = getelementptr inbounds %struct.ge_p2, %struct.ge_p2* %25, i64 0, i32 2
  %892 = bitcast %struct.fe_loose* %16 to i8*
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %889, i8 -86, i64 40, i1 false) #4
  call void @llvm.lifetime.start.p0i8(i64 40, i8* nonnull %892) #4
  %893 = bitcast %struct.fe* %891 to i8*
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 8 %892, i8* align 8 %893, i64 40, i1 false) #4
  call fastcc void @fe_loose_invert(%struct.fe* nonnull %17, %struct.fe_loose* nonnull %16) #4
  call void @llvm.lifetime.end.p0i8(i64 40, i8* nonnull %892) #4
  %894 = getelementptr inbounds %struct.ge_p2, %struct.ge_p2* %25, i64 0, i32 0, i32 0, i64 0
  call fastcc void @fe_mul_impl(i64* nonnull %888, i64* nonnull %894, i64* nonnull %886) #4
  call fastcc void @fe_mul_impl(i64* nonnull %890, i64* %670, i64* nonnull %886) #4
  call fastcc void @fe_tobytes(i8* nonnull %883, %struct.fe* nonnull %19) #4
  %895 = getelementptr inbounds [32 x i8], [32 x i8]* %15, i64 0, i64 0
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %895) #4
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %895, i8 -86, i64 32, i1 false) #4
  call fastcc void @fe_tobytes(i8* nonnull %895, %struct.fe* nonnull %18) #4
  %896 = load i8, i8* %895, align 16
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %895) #4
  %897 = load i8, i8* %884, align 1
  %898 = shl i8 %896, 7
  %899 = xor i8 %897, %898
  store i8 %899, i8* %884, align 1
  call void @llvm.lifetime.end.p0i8(i64 40, i8* nonnull %889) #4
  call void @llvm.lifetime.end.p0i8(i64 40, i8* nonnull %887) #4
  call void @llvm.lifetime.end.p0i8(i64 40, i8* nonnull %885) #4
  %900 = call i32 @CRYPTO_memcmp(i8* nonnull %883, i8* nonnull %109, i64 32) #4
  %901 = icmp eq i32 %900, 0
  %902 = zext i1 %901 to i32
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %883) #4
  call void @llvm.lifetime.end.p0i8(i64 120, i8* nonnull %129) #4
  call void @llvm.lifetime.end.p0i8(i64 64, i8* nonnull %127) #4
  call void @llvm.lifetime.end.p0i8(i64 216, i8* nonnull %122) #4
  br label %903

903:                                              ; preds = %915, %921, %117, %34, %882
  %904 = phi i32 [ %902, %882 ], [ 0, %34 ], [ 0, %117 ], [ 0, %921 ], [ 0, %915 ]
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %110) #4
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %109) #4
  br label %905

905:                                              ; preds = %4, %31, %903
  %906 = phi i32 [ %904, %903 ], [ 0, %31 ], [ 0, %4 ]
  call void @llvm.lifetime.end.p0i8(i64 160, i8* nonnull %27) #4
  ret i32 %906

907:                                              ; preds = %684
  %908 = getelementptr inbounds [256 x i8], [256 x i8]* %10, i64 0, i64 %685
  %909 = load i8, i8* %908, align 1
  %910 = icmp eq i8 %909, 0
  br i1 %910, label %911, label %689

911:                                              ; preds = %907
  %912 = add nsw i64 %675, -2
  %913 = add nsw i32 %676, -2
  %914 = icmp eq i64 %685, 0
  br i1 %914, label %695, label %674

915:                                              ; preds = %117
  %916 = getelementptr inbounds %union.anon, %union.anon* %22, i64 0, i32 0, i64 1
  %917 = load i64, i64* %916, align 8
  %918 = icmp ugt i64 %917, 1503914060200516822
  br i1 %918, label %903, label %919

919:                                              ; preds = %915
  %920 = icmp eq i64 %917, 1503914060200516822
  br i1 %920, label %921, label %121

921:                                              ; preds = %919
  %922 = getelementptr inbounds %union.anon, %union.anon* %22, i64 0, i32 0, i64 0
  %923 = load i64, i64* %922, align 8
  %924 = icmp ugt i64 %923, 6346243789798364141
  %925 = icmp eq i64 %923, 6346243789798364141
  %926 = or i1 %924, %925
  br i1 %926, label %903, label %121
}

declare i32 @CRYPTO_memcmp(i8*, i8*, i64) local_unnamed_addr #3

; Function Attrs: nounwind ssp uwtable
define hidden void @X25519_keypair(i8*, i8*) local_unnamed_addr #0 {
  %3 = tail call i32 @RAND_bytes(i8* %1, i64 32) #4
  %4 = load i8, i8* %1, align 1
  %5 = or i8 %4, 7
  store i8 %5, i8* %1, align 1
  %6 = getelementptr inbounds i8, i8* %1, i64 31
  %7 = load i8, i8* %6, align 1
  %8 = and i8 %7, 63
  %9 = or i8 %8, -128
  store i8 %9, i8* %6, align 1
  tail call void @X25519_public_from_private(i8* %0, i8* %1)
  ret void
}

; Function Attrs: nounwind ssp uwtable
define hidden void @X25519_public_from_private(i8*, i8* nocapture readonly) local_unnamed_addr #0 {
  %3 = alloca [32 x i8], align 16
  %4 = alloca %struct.ge_p3, align 8
  %5 = alloca %struct.fe_loose, align 8
  %6 = alloca %struct.fe_loose, align 16
  %7 = alloca %struct.fe, align 8
  %8 = getelementptr inbounds [32 x i8], [32 x i8]* %3, i64 0, i64 0
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %8) #4
  %9 = getelementptr inbounds [32 x i8], [32 x i8]* %3, i64 0, i64 31
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 16 %8, i8* align 1 %1, i64 32, i1 false) #4
  %10 = load i8, i8* %8, align 16
  %11 = and i8 %10, -8
  store i8 %11, i8* %8, align 16
  %12 = load i8, i8* %9, align 1
  %13 = and i8 %12, 63
  %14 = or i8 %13, 64
  store i8 %14, i8* %9, align 1
  %15 = bitcast %struct.ge_p3* %4 to i8*
  call void @llvm.lifetime.start.p0i8(i64 160, i8* nonnull %15) #4
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %15, i8 -86, i64 160, i1 false)
  call void @x25519_ge_scalarmult_small_precomp(%struct.ge_p3* nonnull %4, i8* nonnull %8, i8* getelementptr inbounds ([960 x i8], [960 x i8]* @k25519SmallPrecomp, i64 0, i64 0)) #4
  %16 = bitcast %struct.fe_loose* %5 to i8*
  call void @llvm.lifetime.start.p0i8(i64 40, i8* nonnull %16) #4
  %17 = getelementptr inbounds %struct.fe_loose, %struct.fe_loose* %5, i64 0, i32 0, i64 0
  %18 = getelementptr inbounds %struct.fe_loose, %struct.fe_loose* %5, i64 0, i32 0, i64 1
  %19 = getelementptr inbounds %struct.fe_loose, %struct.fe_loose* %5, i64 0, i32 0, i64 2
  %20 = getelementptr inbounds %struct.fe_loose, %struct.fe_loose* %5, i64 0, i32 0, i64 3
  %21 = getelementptr inbounds %struct.fe_loose, %struct.fe_loose* %5, i64 0, i32 0, i64 4
  %22 = bitcast %struct.fe_loose* %6 to i8*
  call void @llvm.lifetime.start.p0i8(i64 40, i8* nonnull %22) #4
  %23 = getelementptr inbounds %struct.fe_loose, %struct.fe_loose* %6, i64 0, i32 0, i64 2
  %24 = getelementptr inbounds %struct.fe_loose, %struct.fe_loose* %6, i64 0, i32 0, i64 4
  %25 = bitcast %struct.fe* %7 to i8*
  call void @llvm.lifetime.start.p0i8(i64 40, i8* nonnull %25) #4
  %26 = getelementptr inbounds %struct.fe, %struct.fe* %7, i64 0, i32 0, i64 0
  %27 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %4, i64 0, i32 2, i32 0, i64 0
  %28 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %4, i64 0, i32 1, i32 0, i64 0
  %29 = bitcast %struct.fe* %7 to i8*
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %29, i8 -86, i64 40, i1 false)
  %30 = bitcast i64* %27 to <2 x i64>*
  %31 = load <2 x i64>, <2 x i64>* %30, align 8
  %32 = bitcast i64* %28 to <2 x i64>*
  %33 = load <2 x i64>, <2 x i64>* %32, align 8
  %34 = extractelement <2 x i64> %31, i32 0
  %35 = extractelement <2 x i64> %33, i32 0
  %36 = add i64 %35, %34
  %37 = extractelement <2 x i64> %31, i32 1
  %38 = extractelement <2 x i64> %33, i32 1
  %39 = add i64 %38, %37
  %40 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %4, i64 0, i32 2, i32 0, i64 2
  %41 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %4, i64 0, i32 1, i32 0, i64 2
  %42 = bitcast i64* %40 to <2 x i64>*
  %43 = load <2 x i64>, <2 x i64>* %42, align 8
  %44 = bitcast i64* %41 to <2 x i64>*
  %45 = load <2 x i64>, <2 x i64>* %44, align 8
  %46 = extractelement <2 x i64> %43, i32 0
  %47 = extractelement <2 x i64> %45, i32 0
  %48 = add i64 %47, %46
  %49 = extractelement <2 x i64> %43, i32 1
  %50 = extractelement <2 x i64> %45, i32 1
  %51 = add i64 %50, %49
  %52 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %4, i64 0, i32 2, i32 0, i64 4
  %53 = load i64, i64* %52, align 8
  %54 = getelementptr inbounds %struct.ge_p3, %struct.ge_p3* %4, i64 0, i32 1, i32 0, i64 4
  %55 = load i64, i64* %54, align 8
  %56 = add i64 %55, %53
  store i64 %36, i64* %17, align 8
  store i64 %39, i64* %18, align 8
  store i64 %48, i64* %19, align 8
  store i64 %51, i64* %20, align 8
  store i64 %56, i64* %21, align 8
  %57 = add <2 x i64> %31, <i64 4503599627370458, i64 4503599627370494>
  %58 = sub <2 x i64> %57, %33
  %59 = add <2 x i64> %43, <i64 4503599627370494, i64 4503599627370494>
  %60 = sub <2 x i64> %59, %45
  %61 = add i64 %53, 4503599627370494
  %62 = sub i64 %61, %55
  %63 = bitcast %struct.fe_loose* %6 to <2 x i64>*
  store <2 x i64> %58, <2 x i64>* %63, align 16
  %64 = bitcast i64* %23 to <2 x i64>*
  store <2 x i64> %60, <2 x i64>* %64, align 16
  store i64 %62, i64* %24, align 16
  call fastcc void @fe_loose_invert(%struct.fe* nonnull %7, %struct.fe_loose* nonnull %6)
  call fastcc void @fe_mul_impl(i64* nonnull %26, i64* nonnull %17, i64* nonnull %26) #4
  call fastcc void @fe_tobytes(i8* %0, %struct.fe* nonnull %7)
  call void @llvm.lifetime.end.p0i8(i64 40, i8* nonnull %25) #4
  call void @llvm.lifetime.end.p0i8(i64 40, i8* nonnull %22) #4
  call void @llvm.lifetime.end.p0i8(i64 40, i8* nonnull %16) #4
  call void @llvm.lifetime.end.p0i8(i64 160, i8* nonnull %15) #4
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %8) #4
  ret void
}

; Function Attrs: nounwind ssp uwtable
define hidden i32 @X25519(i8*, i8* nocapture readonly, i8* readonly) local_unnamed_addr #0 {
  %4 = alloca %struct.fe_loose, align 8
  %5 = alloca %struct.fe, align 8
  %6 = alloca %struct.fe, align 16
  %7 = alloca %struct.fe, align 16
  %8 = alloca %struct.fe, align 16
  %9 = alloca %struct.fe, align 8
  %10 = alloca %struct.fe, align 8
  %11 = alloca %struct.fe_loose, align 8
  %12 = alloca %struct.fe_loose, align 8
  %13 = alloca %struct.fe_loose, align 8
  %14 = alloca %struct.fe_loose, align 8
  %15 = alloca [32 x i8], align 16
  %16 = bitcast %struct.fe* %5 to i8*
  call void @llvm.lifetime.start.p0i8(i64 40, i8* nonnull %16) #4
  %17 = getelementptr inbounds %struct.fe, %struct.fe* %5, i64 0, i32 0, i64 0
  %18 = getelementptr inbounds %struct.fe, %struct.fe* %5, i64 0, i32 0, i64 1
  %19 = getelementptr inbounds %struct.fe, %struct.fe* %5, i64 0, i32 0, i64 2
  %20 = getelementptr inbounds %struct.fe, %struct.fe* %5, i64 0, i32 0, i64 3
  %21 = getelementptr inbounds %struct.fe, %struct.fe* %5, i64 0, i32 0, i64 4
  %22 = bitcast %struct.fe* %6 to i8*
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %16, i8 -86, i64 40, i1 false) #4
  call void @llvm.lifetime.start.p0i8(i64 40, i8* nonnull %22) #4
  %23 = getelementptr inbounds %struct.fe, %struct.fe* %6, i64 0, i32 0, i64 0
  %24 = getelementptr inbounds %struct.fe, %struct.fe* %6, i64 0, i32 0, i64 1
  %25 = getelementptr inbounds %struct.fe, %struct.fe* %6, i64 0, i32 0, i64 2
  %26 = getelementptr inbounds %struct.fe, %struct.fe* %6, i64 0, i32 0, i64 3
  %27 = getelementptr inbounds %struct.fe, %struct.fe* %6, i64 0, i32 0, i64 4
  %28 = bitcast %struct.fe* %7 to i8*
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %22, i8 -86, i64 40, i1 false) #4
  call void @llvm.lifetime.start.p0i8(i64 40, i8* nonnull %28) #4
  %29 = getelementptr inbounds %struct.fe, %struct.fe* %7, i64 0, i32 0, i64 0
  %30 = getelementptr inbounds %struct.fe, %struct.fe* %7, i64 0, i32 0, i64 1
  %31 = getelementptr inbounds %struct.fe, %struct.fe* %7, i64 0, i32 0, i64 2
  %32 = getelementptr inbounds %struct.fe, %struct.fe* %7, i64 0, i32 0, i64 3
  %33 = getelementptr inbounds %struct.fe, %struct.fe* %7, i64 0, i32 0, i64 4
  %34 = bitcast %struct.fe* %8 to i8*
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %28, i8 -86, i64 40, i1 false) #4
  call void @llvm.lifetime.start.p0i8(i64 40, i8* nonnull %34) #4
  %35 = getelementptr inbounds %struct.fe, %struct.fe* %8, i64 0, i32 0, i64 0
  %36 = getelementptr inbounds %struct.fe, %struct.fe* %8, i64 0, i32 0, i64 1
  %37 = getelementptr inbounds %struct.fe, %struct.fe* %8, i64 0, i32 0, i64 2
  %38 = getelementptr inbounds %struct.fe, %struct.fe* %8, i64 0, i32 0, i64 3
  %39 = getelementptr inbounds %struct.fe, %struct.fe* %8, i64 0, i32 0, i64 4
  %40 = bitcast %struct.fe* %9 to i8*
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %34, i8 -86, i64 40, i1 false) #4
  call void @llvm.lifetime.start.p0i8(i64 40, i8* nonnull %40) #4
  %41 = getelementptr inbounds %struct.fe, %struct.fe* %9, i64 0, i32 0, i64 0
  %42 = getelementptr inbounds %struct.fe, %struct.fe* %9, i64 0, i32 0, i64 1
  %43 = getelementptr inbounds %struct.fe, %struct.fe* %9, i64 0, i32 0, i64 2
  %44 = getelementptr inbounds %struct.fe, %struct.fe* %9, i64 0, i32 0, i64 3
  %45 = getelementptr inbounds %struct.fe, %struct.fe* %9, i64 0, i32 0, i64 4
  %46 = bitcast %struct.fe* %10 to i8*
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %40, i8 -86, i64 40, i1 false) #4
  call void @llvm.lifetime.start.p0i8(i64 40, i8* nonnull %46) #4
  %47 = getelementptr inbounds %struct.fe, %struct.fe* %10, i64 0, i32 0, i64 0
  %48 = getelementptr inbounds %struct.fe, %struct.fe* %10, i64 0, i32 0, i64 1
  %49 = getelementptr inbounds %struct.fe, %struct.fe* %10, i64 0, i32 0, i64 2
  %50 = getelementptr inbounds %struct.fe, %struct.fe* %10, i64 0, i32 0, i64 3
  %51 = getelementptr inbounds %struct.fe, %struct.fe* %10, i64 0, i32 0, i64 4
  %52 = bitcast %struct.fe_loose* %11 to i8*
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %46, i8 -86, i64 40, i1 false) #4
  call void @llvm.lifetime.start.p0i8(i64 40, i8* nonnull %52) #4
  %53 = getelementptr inbounds %struct.fe_loose, %struct.fe_loose* %11, i64 0, i32 0, i64 0
  %54 = getelementptr inbounds %struct.fe_loose, %struct.fe_loose* %11, i64 0, i32 0, i64 1
  %55 = getelementptr inbounds %struct.fe_loose, %struct.fe_loose* %11, i64 0, i32 0, i64 2
  %56 = getelementptr inbounds %struct.fe_loose, %struct.fe_loose* %11, i64 0, i32 0, i64 3
  %57 = getelementptr inbounds %struct.fe_loose, %struct.fe_loose* %11, i64 0, i32 0, i64 4
  %58 = bitcast %struct.fe_loose* %12 to i8*
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %52, i8 -86, i64 40, i1 false) #4
  call void @llvm.lifetime.start.p0i8(i64 40, i8* nonnull %58) #4
  %59 = getelementptr inbounds %struct.fe_loose, %struct.fe_loose* %12, i64 0, i32 0, i64 0
  %60 = getelementptr inbounds %struct.fe_loose, %struct.fe_loose* %12, i64 0, i32 0, i64 1
  %61 = getelementptr inbounds %struct.fe_loose, %struct.fe_loose* %12, i64 0, i32 0, i64 2
  %62 = getelementptr inbounds %struct.fe_loose, %struct.fe_loose* %12, i64 0, i32 0, i64 3
  %63 = getelementptr inbounds %struct.fe_loose, %struct.fe_loose* %12, i64 0, i32 0, i64 4
  %64 = bitcast %struct.fe_loose* %13 to i8*
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %58, i8 -86, i64 40, i1 false) #4
  call void @llvm.lifetime.start.p0i8(i64 40, i8* nonnull %64) #4
  %65 = getelementptr inbounds %struct.fe_loose, %struct.fe_loose* %13, i64 0, i32 0, i64 0
  %66 = getelementptr inbounds %struct.fe_loose, %struct.fe_loose* %13, i64 0, i32 0, i64 1
  %67 = getelementptr inbounds %struct.fe_loose, %struct.fe_loose* %13, i64 0, i32 0, i64 2
  %68 = getelementptr inbounds %struct.fe_loose, %struct.fe_loose* %13, i64 0, i32 0, i64 3
  %69 = getelementptr inbounds %struct.fe_loose, %struct.fe_loose* %13, i64 0, i32 0, i64 4
  %70 = bitcast %struct.fe_loose* %14 to i8*
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %64, i8 -86, i64 40, i1 false) #4
  call void @llvm.lifetime.start.p0i8(i64 40, i8* nonnull %70) #4
  %71 = getelementptr inbounds %struct.fe_loose, %struct.fe_loose* %14, i64 0, i32 0, i64 0
  %72 = getelementptr inbounds %struct.fe_loose, %struct.fe_loose* %14, i64 0, i32 0, i64 1
  %73 = getelementptr inbounds %struct.fe_loose, %struct.fe_loose* %14, i64 0, i32 0, i64 2
  %74 = getelementptr inbounds %struct.fe_loose, %struct.fe_loose* %14, i64 0, i32 0, i64 3
  %75 = getelementptr inbounds %struct.fe_loose, %struct.fe_loose* %14, i64 0, i32 0, i64 4
  %76 = getelementptr inbounds [32 x i8], [32 x i8]* %15, i64 0, i64 0
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %70, i8 -86, i64 40, i1 false) #4
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %76) #4
  %77 = getelementptr inbounds [32 x i8], [32 x i8]* %15, i64 0, i64 31
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 16 %76, i8* align 1 %1, i64 32, i1 false) #4
  %78 = load i8, i8* %76, align 16
  %79 = and i8 %78, -8
  store i8 %79, i8* %76, align 16
  %80 = load i8, i8* %77, align 1
  %81 = and i8 %80, 63
  %82 = or i8 %81, 64
  store i8 %82, i8* %77, align 1
  %83 = load i8, i8* %2, align 1
  %84 = getelementptr inbounds i8, i8* %2, i64 1
  %85 = load i8, i8* %84, align 1
  %86 = getelementptr inbounds i8, i8* %2, i64 2
  %87 = load i8, i8* %86, align 1
  %88 = getelementptr inbounds i8, i8* %2, i64 3
  %89 = load i8, i8* %88, align 1
  %90 = getelementptr inbounds i8, i8* %2, i64 4
  %91 = load i8, i8* %90, align 1
  %92 = getelementptr inbounds i8, i8* %2, i64 5
  %93 = load i8, i8* %92, align 1
  %94 = getelementptr inbounds i8, i8* %2, i64 6
  %95 = load i8, i8* %94, align 1
  %96 = getelementptr inbounds i8, i8* %2, i64 7
  %97 = load i8, i8* %96, align 1
  %98 = getelementptr inbounds i8, i8* %2, i64 8
  %99 = load i8, i8* %98, align 1
  %100 = getelementptr inbounds i8, i8* %2, i64 9
  %101 = load i8, i8* %100, align 1
  %102 = getelementptr inbounds i8, i8* %2, i64 10
  %103 = load i8, i8* %102, align 1
  %104 = getelementptr inbounds i8, i8* %2, i64 11
  %105 = load i8, i8* %104, align 1
  %106 = getelementptr inbounds i8, i8* %2, i64 12
  %107 = load i8, i8* %106, align 1
  %108 = getelementptr inbounds i8, i8* %2, i64 13
  %109 = load i8, i8* %108, align 1
  %110 = getelementptr inbounds i8, i8* %2, i64 14
  %111 = load i8, i8* %110, align 1
  %112 = getelementptr inbounds i8, i8* %2, i64 15
  %113 = load i8, i8* %112, align 1
  %114 = getelementptr inbounds i8, i8* %2, i64 16
  %115 = load i8, i8* %114, align 1
  %116 = getelementptr inbounds i8, i8* %2, i64 17
  %117 = load i8, i8* %116, align 1
  %118 = getelementptr inbounds i8, i8* %2, i64 18
  %119 = load i8, i8* %118, align 1
  %120 = getelementptr inbounds i8, i8* %2, i64 19
  %121 = load i8, i8* %120, align 1
  %122 = getelementptr inbounds i8, i8* %2, i64 20
  %123 = load i8, i8* %122, align 1
  %124 = getelementptr inbounds i8, i8* %2, i64 21
  %125 = load i8, i8* %124, align 1
  %126 = getelementptr inbounds i8, i8* %2, i64 22
  %127 = load i8, i8* %126, align 1
  %128 = getelementptr inbounds i8, i8* %2, i64 23
  %129 = load i8, i8* %128, align 1
  %130 = getelementptr inbounds i8, i8* %2, i64 24
  %131 = load i8, i8* %130, align 1
  %132 = getelementptr inbounds i8, i8* %2, i64 25
  %133 = load i8, i8* %132, align 1
  %134 = getelementptr inbounds i8, i8* %2, i64 26
  %135 = load i8, i8* %134, align 1
  %136 = getelementptr inbounds i8, i8* %2, i64 27
  %137 = load i8, i8* %136, align 1
  %138 = getelementptr inbounds i8, i8* %2, i64 28
  %139 = load i8, i8* %138, align 1
  %140 = getelementptr inbounds i8, i8* %2, i64 29
  %141 = load i8, i8* %140, align 1
  %142 = getelementptr inbounds i8, i8* %2, i64 30
  %143 = load i8, i8* %142, align 1
  %144 = getelementptr inbounds i8, i8* %2, i64 31
  %145 = load i8, i8* %144, align 1
  %146 = and i8 %145, 127
  %147 = zext i8 %146 to i64
  %148 = shl nuw nsw i64 %147, 44
  %149 = zext i8 %143 to i64
  %150 = shl nuw nsw i64 %149, 36
  %151 = zext i8 %141 to i64
  %152 = shl nuw nsw i64 %151, 28
  %153 = zext i8 %139 to i64
  %154 = shl nuw nsw i64 %153, 20
  %155 = zext i8 %137 to i64
  %156 = shl nuw nsw i64 %155, 12
  %157 = zext i8 %135 to i64
  %158 = shl nuw nsw i64 %157, 4
  %159 = zext i8 %133 to i64
  %160 = shl nuw nsw i64 %159, 47
  %161 = zext i8 %131 to i64
  %162 = shl nuw nsw i64 %161, 39
  %163 = zext i8 %129 to i64
  %164 = shl nuw nsw i64 %163, 31
  %165 = zext i8 %127 to i64
  %166 = shl nuw nsw i64 %165, 23
  %167 = zext i8 %125 to i64
  %168 = shl nuw nsw i64 %167, 15
  %169 = zext i8 %123 to i64
  %170 = shl nuw nsw i64 %169, 7
  %171 = zext i8 %121 to i64
  %172 = shl nuw nsw i64 %171, 50
  %173 = zext i8 %119 to i64
  %174 = shl nuw nsw i64 %173, 42
  %175 = zext i8 %117 to i64
  %176 = shl nuw nsw i64 %175, 34
  %177 = zext i8 %115 to i64
  %178 = shl nuw nsw i64 %177, 26
  %179 = zext i8 %113 to i64
  %180 = shl nuw nsw i64 %179, 18
  %181 = zext i8 %111 to i64
  %182 = shl nuw nsw i64 %181, 10
  %183 = zext i8 %109 to i64
  %184 = shl nuw nsw i64 %183, 2
  %185 = zext i8 %107 to i64
  %186 = shl nuw nsw i64 %185, 45
  %187 = zext i8 %105 to i64
  %188 = shl nuw nsw i64 %187, 37
  %189 = zext i8 %103 to i64
  %190 = shl nuw nsw i64 %189, 29
  %191 = zext i8 %101 to i64
  %192 = shl nuw nsw i64 %191, 21
  %193 = zext i8 %99 to i64
  %194 = shl nuw nsw i64 %193, 13
  %195 = zext i8 %97 to i64
  %196 = shl nuw nsw i64 %195, 5
  %197 = zext i8 %95 to i64
  %198 = shl nuw nsw i64 %197, 48
  %199 = zext i8 %93 to i64
  %200 = shl nuw nsw i64 %199, 40
  %201 = zext i8 %91 to i64
  %202 = shl nuw nsw i64 %201, 32
  %203 = zext i8 %89 to i64
  %204 = shl nuw nsw i64 %203, 24
  %205 = zext i8 %87 to i64
  %206 = shl nuw nsw i64 %205, 16
  %207 = zext i8 %85 to i64
  %208 = shl nuw nsw i64 %207, 8
  %209 = zext i8 %83 to i64
  %210 = lshr i8 %95, 3
  %211 = and i64 %198, 1970324836974592
  %212 = or i64 %208, %209
  %213 = or i64 %212, %206
  %214 = or i64 %213, %204
  %215 = or i64 %214, %202
  %216 = or i64 %215, %200
  %217 = or i64 %216, %211
  %218 = or i64 %156, %158
  %219 = or i64 %218, %154
  %220 = or i64 %219, %152
  %221 = or i64 %220, %150
  %222 = or i64 %221, %148
  %223 = or i64 %168, %170
  %224 = or i64 %223, %166
  %225 = or i64 %224, %164
  %226 = or i64 %225, %162
  %227 = or i64 %226, %160
  %228 = or i64 %182, %184
  %229 = or i64 %228, %180
  %230 = or i64 %229, %178
  %231 = or i64 %230, %176
  %232 = or i64 %231, %174
  %233 = or i64 %232, %172
  %234 = zext i8 %210 to i64
  %235 = lshr i8 %107, 6
  %236 = and i64 %186, 2216615441596416
  %237 = or i64 %196, %234
  %238 = or i64 %237, %194
  %239 = or i64 %238, %192
  %240 = or i64 %239, %190
  %241 = or i64 %240, %188
  %242 = or i64 %241, %236
  %243 = zext i8 %235 to i64
  %244 = add i64 %233, %243
  %245 = lshr i64 %244, 51
  %246 = and i64 %244, 2251799813685247
  %247 = and i64 %245, 255
  %248 = add i64 %227, %247
  %249 = lshr i64 %248, 51
  %250 = and i64 %248, 2251799813685247
  %251 = and i64 %249, 255
  %252 = add i64 %222, %251
  store i64 %217, i64* %17, align 8
  store i64 %242, i64* %18, align 8
  store i64 %246, i64* %19, align 8
  store i64 %250, i64* %20, align 8
  store i64 %252, i64* %21, align 8
  %253 = bitcast i64* %24 to i8*
  call void @llvm.memset.p0i8.i64(i8* align 8 %253, i8 0, i64 32, i1 false) #4
  store i64 1, i64* %23, align 16
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %28, i8 0, i64 40, i1 false) #4
  %254 = bitcast i64* %36 to i8*
  call void @llvm.memset.p0i8.i64(i8* align 8 %254, i8 0, i64 32, i1 false) #4
  store i64 1, i64* %35, align 16
  %255 = bitcast i64* %32 to <2 x i64>*
  br label %256

256:                                              ; preds = %753, %3
  %257 = phi i64 [ 0, %3 ], [ %769, %753 ]
  %258 = phi i64 [ 0, %3 ], [ %768, %753 ]
  %259 = phi i64 [ 0, %3 ], [ %767, %753 ]
  %260 = phi i64 [ 0, %3 ], [ %766, %753 ]
  %261 = phi i64 [ 0, %3 ], [ %765, %753 ]
  %262 = phi i64 [ 0, %3 ], [ %764, %753 ]
  %263 = phi i64 [ 0, %3 ], [ %763, %753 ]
  %264 = phi i64 [ 0, %3 ], [ %762, %753 ]
  %265 = phi i64 [ 1, %3 ], [ %761, %753 ]
  %266 = phi i64 [ 0, %3 ], [ %760, %753 ]
  %267 = phi i64 [ 0, %3 ], [ %759, %753 ]
  %268 = phi i64 [ 0, %3 ], [ %758, %753 ]
  %269 = phi i64 [ 0, %3 ], [ %757, %753 ]
  %270 = phi i64 [ 0, %3 ], [ %756, %753 ]
  %271 = phi i64 [ 1, %3 ], [ %755, %753 ]
  %272 = phi i32 [ 0, %3 ], [ %286, %753 ]
  %273 = phi i32 [ 254, %3 ], [ %754, %753 ]
  %274 = phi i64 [ %252, %3 ], [ %773, %753 ]
  %275 = phi i64 [ %250, %3 ], [ %772, %753 ]
  %276 = phi i64 [ %246, %3 ], [ %746, %753 ]
  %277 = phi i64 [ %242, %3 ], [ %771, %753 ]
  %278 = phi i64 [ %217, %3 ], [ %770, %753 ]
  %279 = lshr i32 %273, 3
  %280 = zext i32 %279 to i64
  %281 = getelementptr inbounds [32 x i8], [32 x i8]* %15, i64 0, i64 %280
  %282 = load i8, i8* %281, align 1
  %283 = zext i8 %282 to i32
  %284 = and i32 %273, 7
  %285 = lshr i32 %283, %284
  %286 = and i32 %285, 1
  %287 = xor i32 %286, %272
  %288 = zext i32 %287 to i64
  %289 = sub nsw i64 0, %288
  %290 = xor i64 %278, %271
  %291 = and i64 %290, %289
  %292 = xor i64 %291, %271
  store i64 %292, i64* %23, align 16
  %293 = xor i64 %291, %278
  %294 = xor i64 %277, %270
  %295 = and i64 %294, %289
  %296 = xor i64 %295, %270
  store i64 %296, i64* %24, align 8
  %297 = xor i64 %295, %277
  %298 = xor i64 %276, %269
  %299 = and i64 %298, %289
  %300 = xor i64 %299, %269
  store i64 %300, i64* %25, align 16
  %301 = xor i64 %299, %276
  %302 = xor i64 %275, %268
  %303 = and i64 %302, %289
  %304 = xor i64 %303, %268
  store i64 %304, i64* %26, align 8
  %305 = xor i64 %303, %275
  %306 = xor i64 %274, %267
  %307 = and i64 %306, %289
  %308 = xor i64 %307, %267
  store i64 %308, i64* %27, align 16
  %309 = xor i64 %307, %274
  %310 = xor i64 %266, %265
  %311 = and i64 %310, %289
  %312 = xor i64 %311, %266
  store i64 %312, i64* %29, align 16
  %313 = xor i64 %311, %265
  store i64 %313, i64* %35, align 16
  %314 = xor i64 %264, %263
  %315 = and i64 %314, %289
  %316 = xor i64 %315, %264
  store i64 %316, i64* %30, align 8
  %317 = xor i64 %315, %263
  store i64 %317, i64* %36, align 8
  %318 = xor i64 %262, %261
  %319 = and i64 %318, %289
  %320 = xor i64 %319, %262
  store i64 %320, i64* %31, align 16
  %321 = xor i64 %319, %261
  store i64 %321, i64* %37, align 16
  %322 = xor i64 %260, %259
  %323 = and i64 %322, %289
  %324 = xor i64 %323, %260
  store i64 %324, i64* %32, align 8
  %325 = xor i64 %323, %259
  store i64 %325, i64* %38, align 8
  %326 = xor i64 %258, %257
  %327 = and i64 %326, %289
  %328 = xor i64 %327, %258
  store i64 %328, i64* %33, align 16
  %329 = xor i64 %327, %257
  store i64 %329, i64* %39, align 16
  %330 = add i64 %293, 4503599627370458
  %331 = sub i64 %330, %313
  %332 = add i64 %297, 4503599627370494
  %333 = sub i64 %332, %317
  %334 = add i64 %301, 4503599627370494
  %335 = sub i64 %334, %321
  %336 = add i64 %305, 4503599627370494
  %337 = sub i64 %336, %325
  %338 = add i64 %309, 4503599627370494
  %339 = sub i64 %338, %329
  store i64 %331, i64* %65, align 8
  store i64 %333, i64* %66, align 8
  store i64 %335, i64* %67, align 8
  store i64 %337, i64* %68, align 8
  store i64 %339, i64* %69, align 8
  %340 = add i64 %292, 4503599627370458
  %341 = sub i64 %340, %312
  %342 = add i64 %296, 4503599627370494
  %343 = sub i64 %342, %316
  %344 = add i64 %300, 4503599627370494
  %345 = sub i64 %344, %320
  %346 = add i64 %304, 4503599627370494
  %347 = sub i64 %346, %324
  %348 = add i64 %308, 4503599627370494
  %349 = sub i64 %348, %328
  store i64 %341, i64* %71, align 8
  store i64 %343, i64* %72, align 8
  store i64 %345, i64* %73, align 8
  store i64 %347, i64* %74, align 8
  store i64 %349, i64* %75, align 8
  %350 = add i64 %312, %292
  %351 = add i64 %316, %296
  %352 = add i64 %320, %300
  %353 = add i64 %324, %304
  %354 = add i64 %328, %308
  store i64 %350, i64* %53, align 8
  store i64 %351, i64* %54, align 8
  store i64 %352, i64* %55, align 8
  store i64 %353, i64* %56, align 8
  store i64 %354, i64* %57, align 8
  %355 = add i64 %313, %293
  %356 = add i64 %317, %297
  %357 = add i64 %321, %301
  %358 = add i64 %325, %305
  %359 = add i64 %329, %309
  store i64 %355, i64* %59, align 8
  store i64 %356, i64* %60, align 8
  store i64 %357, i64* %61, align 8
  store i64 %358, i64* %62, align 8
  store i64 %359, i64* %63, align 8
  call fastcc void @fe_mul_impl(i64* nonnull %35, i64* nonnull %65, i64* nonnull %53) #4
  call fastcc void @fe_mul_impl(i64* nonnull %29, i64* nonnull %59, i64* nonnull %71) #4
  %360 = mul i64 %349, 19
  %361 = mul i64 %349, 38
  %362 = shl i64 %349, 1
  %363 = mul i64 %347, 19
  %364 = mul i64 %347, 38
  %365 = shl i64 %347, 1
  %366 = shl i64 %345, 1
  %367 = shl i64 %343, 1
  %368 = zext i64 %349 to i128
  %369 = zext i64 %360 to i128
  %370 = mul nuw i128 %369, %368
  %371 = zext i64 %347 to i128
  %372 = zext i64 %361 to i128
  %373 = mul nuw i128 %372, %371
  %374 = zext i64 %363 to i128
  %375 = mul nuw i128 %374, %371
  %376 = zext i64 %345 to i128
  %377 = mul nuw i128 %372, %376
  %378 = zext i64 %364 to i128
  %379 = mul nuw i128 %378, %376
  %380 = mul nuw i128 %376, %376
  %381 = zext i64 %343 to i128
  %382 = mul nuw i128 %372, %381
  %383 = zext i64 %365 to i128
  %384 = mul nuw i128 %383, %381
  %385 = zext i64 %366 to i128
  %386 = mul nuw i128 %385, %381
  %387 = mul nuw i128 %381, %381
  %388 = zext i64 %341 to i128
  %389 = zext i64 %362 to i128
  %390 = mul nuw i128 %389, %388
  %391 = mul nuw i128 %383, %388
  %392 = mul nuw i128 %385, %388
  %393 = zext i64 %367 to i128
  %394 = mul nuw i128 %393, %388
  %395 = mul nuw i128 %388, %388
  %396 = add i128 %379, %395
  %397 = add i128 %396, %382
  %398 = lshr i128 %397, 51
  %399 = trunc i128 %397 to i64
  %400 = and i64 %399, 2251799813685247
  %401 = add i128 %373, %387
  %402 = add i128 %401, %392
  %403 = add i128 %377, %375
  %404 = add i128 %403, %394
  %405 = and i128 %398, 18446744073709551615
  %406 = add i128 %404, %405
  %407 = lshr i128 %406, 51
  %408 = trunc i128 %406 to i64
  %409 = and i64 %408, 2251799813685247
  %410 = and i128 %407, 18446744073709551615
  %411 = add i128 %402, %410
  %412 = lshr i128 %411, 51
  %413 = trunc i128 %411 to i64
  %414 = and i64 %413, 2251799813685247
  %415 = and i128 %412, 18446744073709551615
  %416 = add i128 %391, %370
  %417 = add i128 %416, %386
  %418 = add i128 %417, %415
  %419 = lshr i128 %418, 51
  %420 = trunc i128 %418 to i64
  %421 = and i64 %420, 2251799813685247
  %422 = and i128 %419, 18446744073709551615
  %423 = add i128 %390, %380
  %424 = add i128 %423, %384
  %425 = add i128 %424, %422
  %426 = lshr i128 %425, 51
  %427 = trunc i128 %426 to i64
  %428 = trunc i128 %425 to i64
  %429 = and i64 %428, 2251799813685247
  %430 = mul i64 %427, 19
  %431 = add i64 %430, %400
  %432 = lshr i64 %431, 51
  %433 = and i64 %431, 2251799813685247
  %434 = add nuw nsw i64 %432, %409
  %435 = lshr i64 %434, 51
  %436 = and i64 %434, 2251799813685247
  %437 = add nuw nsw i64 %435, %414
  store i64 %433, i64* %41, align 8
  store i64 %436, i64* %42, align 8
  store i64 %437, i64* %43, align 8
  store i64 %421, i64* %44, align 8
  store i64 %429, i64* %45, align 8
  %438 = mul i64 %354, 19
  %439 = mul i64 %354, 38
  %440 = shl i64 %354, 1
  %441 = mul i64 %353, 19
  %442 = mul i64 %353, 38
  %443 = shl i64 %353, 1
  %444 = shl i64 %352, 1
  %445 = shl i64 %351, 1
  %446 = zext i64 %354 to i128
  %447 = zext i64 %438 to i128
  %448 = mul nuw i128 %447, %446
  %449 = zext i64 %353 to i128
  %450 = zext i64 %439 to i128
  %451 = mul nuw i128 %450, %449
  %452 = zext i64 %441 to i128
  %453 = mul nuw i128 %452, %449
  %454 = zext i64 %352 to i128
  %455 = mul nuw i128 %450, %454
  %456 = zext i64 %442 to i128
  %457 = mul nuw i128 %456, %454
  %458 = mul nuw i128 %454, %454
  %459 = zext i64 %351 to i128
  %460 = mul nuw i128 %450, %459
  %461 = zext i64 %443 to i128
  %462 = mul nuw i128 %461, %459
  %463 = zext i64 %444 to i128
  %464 = mul nuw i128 %463, %459
  %465 = mul nuw i128 %459, %459
  %466 = zext i64 %350 to i128
  %467 = zext i64 %440 to i128
  %468 = mul nuw i128 %467, %466
  %469 = mul nuw i128 %461, %466
  %470 = mul nuw i128 %463, %466
  %471 = zext i64 %445 to i128
  %472 = mul nuw i128 %471, %466
  %473 = mul nuw i128 %466, %466
  %474 = add i128 %457, %473
  %475 = add i128 %474, %460
  %476 = lshr i128 %475, 51
  %477 = trunc i128 %475 to i64
  %478 = and i64 %477, 2251799813685247
  %479 = add i128 %451, %465
  %480 = add i128 %479, %470
  %481 = add i128 %455, %453
  %482 = add i128 %481, %472
  %483 = and i128 %476, 18446744073709551615
  %484 = add i128 %482, %483
  %485 = lshr i128 %484, 51
  %486 = trunc i128 %484 to i64
  %487 = and i64 %486, 2251799813685247
  %488 = and i128 %485, 18446744073709551615
  %489 = add i128 %480, %488
  %490 = lshr i128 %489, 51
  %491 = trunc i128 %489 to i64
  %492 = and i64 %491, 2251799813685247
  %493 = and i128 %490, 18446744073709551615
  %494 = add i128 %469, %448
  %495 = add i128 %494, %464
  %496 = add i128 %495, %493
  %497 = lshr i128 %496, 51
  %498 = trunc i128 %496 to i64
  %499 = and i64 %498, 2251799813685247
  %500 = and i128 %497, 18446744073709551615
  %501 = add i128 %468, %458
  %502 = add i128 %501, %462
  %503 = add i128 %502, %500
  %504 = lshr i128 %503, 51
  %505 = trunc i128 %504 to i64
  %506 = trunc i128 %503 to i64
  %507 = and i64 %506, 2251799813685247
  %508 = mul i64 %505, 19
  %509 = add i64 %508, %478
  %510 = lshr i64 %509, 51
  %511 = and i64 %509, 2251799813685247
  %512 = add nuw nsw i64 %510, %487
  %513 = lshr i64 %512, 51
  %514 = and i64 %512, 2251799813685247
  %515 = add nuw nsw i64 %513, %492
  store i64 %511, i64* %47, align 8
  store i64 %514, i64* %48, align 8
  store i64 %515, i64* %49, align 8
  store i64 %499, i64* %50, align 8
  store i64 %507, i64* %51, align 8
  %516 = load i64, i64* %35, align 16
  %517 = load i64, i64* %29, align 16
  %518 = add i64 %517, %516
  %519 = load i64, i64* %36, align 8
  %520 = load i64, i64* %30, align 8
  %521 = add i64 %520, %519
  %522 = load i64, i64* %37, align 16
  %523 = load i64, i64* %31, align 16
  %524 = add i64 %523, %522
  %525 = load i64, i64* %38, align 8
  %526 = load i64, i64* %32, align 8
  %527 = add i64 %526, %525
  %528 = load i64, i64* %39, align 16
  %529 = load i64, i64* %33, align 16
  %530 = add i64 %529, %528
  %531 = add i64 %516, 4503599627370458
  %532 = sub i64 %531, %517
  %533 = add i64 %519, 4503599627370494
  %534 = sub i64 %533, %520
  %535 = add i64 %522, 4503599627370494
  %536 = sub i64 %535, %523
  %537 = add i64 %525, 4503599627370494
  %538 = sub i64 %537, %526
  %539 = add i64 %528, 4503599627370494
  %540 = sub i64 %539, %529
  store i64 %532, i64* %59, align 8
  store i64 %534, i64* %60, align 8
  store i64 %536, i64* %61, align 8
  store i64 %538, i64* %62, align 8
  store i64 %540, i64* %63, align 8
  call fastcc void @fe_mul_impl(i64* nonnull %23, i64* nonnull %47, i64* nonnull %41) #4
  %541 = sub nuw nsw i64 4503599627370458, %433
  %542 = add nuw nsw i64 %541, %511
  %543 = add nuw nsw i64 %514, 4503599627370494
  %544 = sub nuw nsw i64 %543, %436
  %545 = add nuw nsw i64 %515, 4503599627370494
  %546 = sub nuw nsw i64 %545, %437
  %547 = add nuw nsw i64 %499, 4503599627370494
  %548 = sub nuw nsw i64 %547, %421
  %549 = add nuw nsw i64 %507, 4503599627370494
  %550 = sub nuw nsw i64 %549, %429
  store i64 %542, i64* %71, align 8
  store i64 %544, i64* %72, align 8
  store i64 %546, i64* %73, align 8
  store i64 %548, i64* %74, align 8
  store i64 %550, i64* %75, align 8
  %551 = mul i64 %540, 19
  %552 = mul i64 %540, 38
  %553 = shl i64 %540, 1
  %554 = mul i64 %538, 19
  %555 = mul i64 %538, 38
  %556 = shl i64 %538, 1
  %557 = shl i64 %536, 1
  %558 = shl i64 %534, 1
  %559 = zext i64 %540 to i128
  %560 = zext i64 %551 to i128
  %561 = mul nuw i128 %560, %559
  %562 = zext i64 %538 to i128
  %563 = zext i64 %552 to i128
  %564 = mul nuw i128 %563, %562
  %565 = zext i64 %554 to i128
  %566 = mul nuw i128 %565, %562
  %567 = zext i64 %536 to i128
  %568 = mul nuw i128 %563, %567
  %569 = zext i64 %555 to i128
  %570 = mul nuw i128 %569, %567
  %571 = mul nuw i128 %567, %567
  %572 = zext i64 %534 to i128
  %573 = mul nuw i128 %563, %572
  %574 = zext i64 %556 to i128
  %575 = mul nuw i128 %574, %572
  %576 = zext i64 %557 to i128
  %577 = mul nuw i128 %576, %572
  %578 = mul nuw i128 %572, %572
  %579 = zext i64 %532 to i128
  %580 = zext i64 %553 to i128
  %581 = mul nuw i128 %580, %579
  %582 = mul nuw i128 %574, %579
  %583 = mul nuw i128 %576, %579
  %584 = zext i64 %558 to i128
  %585 = mul nuw i128 %584, %579
  %586 = mul nuw i128 %579, %579
  %587 = add i128 %570, %586
  %588 = add i128 %587, %573
  %589 = lshr i128 %588, 51
  %590 = trunc i128 %588 to i64
  %591 = and i64 %590, 2251799813685247
  %592 = and i128 %589, 18446744073709551615
  %593 = add i128 %566, %585
  %594 = add i128 %593, %568
  %595 = add i128 %594, %592
  %596 = lshr i128 %595, 51
  %597 = trunc i128 %595 to i64
  %598 = and i64 %597, 2251799813685247
  %599 = and i128 %596, 18446744073709551615
  %600 = add i128 %583, %578
  %601 = add i128 %600, %564
  %602 = add i128 %601, %599
  %603 = lshr i128 %602, 51
  %604 = trunc i128 %602 to i64
  %605 = and i64 %604, 2251799813685247
  %606 = and i128 %603, 18446744073709551615
  %607 = add i128 %582, %577
  %608 = add i128 %607, %561
  %609 = add i128 %608, %606
  %610 = lshr i128 %609, 51
  %611 = and i128 %610, 18446744073709551615
  %612 = add i128 %575, %571
  %613 = add i128 %612, %581
  %614 = add i128 %613, %611
  %615 = lshr i128 %614, 51
  %616 = trunc i128 %615 to i64
  %617 = insertelement <2 x i128> undef, i128 %609, i32 0
  %618 = insertelement <2 x i128> %617, i128 %614, i32 1
  %619 = trunc <2 x i128> %618 to <2 x i64>
  %620 = and <2 x i64> %619, <i64 2251799813685247, i64 2251799813685247>
  %621 = mul i64 %616, 19
  %622 = add i64 %621, %591
  %623 = lshr i64 %622, 51
  %624 = and i64 %622, 2251799813685247
  %625 = add nuw nsw i64 %623, %598
  %626 = lshr i64 %625, 51
  %627 = and i64 %625, 2251799813685247
  %628 = add nuw nsw i64 %626, %605
  store i64 %624, i64* %29, align 16
  store i64 %627, i64* %30, align 8
  store i64 %628, i64* %31, align 16
  store <2 x i64> %620, <2 x i64>* %255, align 8
  %629 = zext i64 %550 to i128
  %630 = mul nuw nsw i128 %629, 121666
  %631 = zext i64 %548 to i128
  %632 = mul nuw nsw i128 %631, 121666
  %633 = zext i64 %546 to i128
  %634 = mul nuw nsw i128 %633, 121666
  %635 = zext i64 %544 to i128
  %636 = mul nuw nsw i128 %635, 121666
  %637 = zext i64 %542 to i128
  %638 = mul nuw nsw i128 %637, 121666
  %639 = lshr i128 %638, 51
  %640 = trunc i128 %638 to i64
  %641 = and i64 %640, 2251799813685246
  %642 = add nuw nsw i128 %636, %639
  %643 = lshr i128 %642, 51
  %644 = trunc i128 %642 to i64
  %645 = and i64 %644, 2251799813685247
  %646 = add nuw nsw i128 %643, %634
  %647 = lshr i128 %646, 51
  %648 = trunc i128 %646 to i64
  %649 = and i64 %648, 2251799813685247
  %650 = add nuw nsw i128 %647, %632
  %651 = lshr i128 %650, 51
  %652 = trunc i128 %650 to i64
  %653 = and i64 %652, 2251799813685247
  %654 = add nuw nsw i128 %651, %630
  %655 = lshr i128 %654, 51
  %656 = trunc i128 %655 to i64
  %657 = trunc i128 %654 to i64
  %658 = and i64 %657, 2251799813685247
  %659 = mul nuw nsw i64 %656, 19
  %660 = add nuw nsw i64 %659, %641
  %661 = lshr i64 %660, 51
  %662 = and i64 %660, 2251799813685247
  %663 = and i64 %661, 255
  %664 = add nuw nsw i64 %663, %645
  %665 = lshr i64 %664, 51
  %666 = and i64 %664, 2251799813685247
  %667 = add nuw nsw i64 %665, %649
  store i64 %662, i64* %35, align 16
  store i64 %666, i64* %36, align 8
  store i64 %667, i64* %37, align 16
  store i64 %653, i64* %38, align 8
  store i64 %658, i64* %39, align 16
  %668 = mul i64 %530, 19
  %669 = mul i64 %530, 38
  %670 = shl i64 %530, 1
  %671 = mul i64 %527, 19
  %672 = mul i64 %527, 38
  %673 = shl i64 %527, 1
  %674 = shl i64 %524, 1
  %675 = shl i64 %521, 1
  %676 = zext i64 %530 to i128
  %677 = zext i64 %668 to i128
  %678 = mul nuw i128 %677, %676
  %679 = zext i64 %527 to i128
  %680 = zext i64 %669 to i128
  %681 = mul nuw i128 %680, %679
  %682 = zext i64 %671 to i128
  %683 = mul nuw i128 %682, %679
  %684 = zext i64 %524 to i128
  %685 = mul nuw i128 %680, %684
  %686 = zext i64 %672 to i128
  %687 = mul nuw i128 %686, %684
  %688 = mul nuw i128 %684, %684
  %689 = zext i64 %521 to i128
  %690 = mul nuw i128 %680, %689
  %691 = zext i64 %673 to i128
  %692 = mul nuw i128 %691, %689
  %693 = zext i64 %674 to i128
  %694 = mul nuw i128 %693, %689
  %695 = mul nuw i128 %689, %689
  %696 = zext i64 %518 to i128
  %697 = zext i64 %670 to i128
  %698 = mul nuw i128 %697, %696
  %699 = mul nuw i128 %691, %696
  %700 = mul nuw i128 %693, %696
  %701 = zext i64 %675 to i128
  %702 = mul nuw i128 %701, %696
  %703 = mul nuw i128 %696, %696
  %704 = add i128 %687, %703
  %705 = add i128 %704, %690
  %706 = lshr i128 %705, 51
  %707 = trunc i128 %705 to i64
  %708 = and i64 %707, 2251799813685247
  %709 = add i128 %692, %688
  %710 = add i128 %709, %698
  %711 = and i128 %706, 18446744073709551615
  %712 = add i128 %683, %702
  %713 = add i128 %712, %685
  %714 = add i128 %713, %711
  %715 = lshr i128 %714, 51
  %716 = trunc i128 %714 to i64
  %717 = and i64 %716, 2251799813685247
  %718 = and i128 %715, 18446744073709551615
  %719 = add i128 %700, %695
  %720 = add i128 %719, %681
  %721 = add i128 %720, %718
  %722 = lshr i128 %721, 51
  %723 = trunc i128 %721 to i64
  %724 = and i64 %723, 2251799813685247
  %725 = and i128 %722, 18446744073709551615
  %726 = add i128 %699, %694
  %727 = add i128 %726, %678
  %728 = add i128 %727, %725
  %729 = lshr i128 %728, 51
  %730 = and i128 %729, 18446744073709551615
  %731 = add i128 %710, %730
  %732 = lshr i128 %731, 51
  %733 = trunc i128 %732 to i64
  %734 = insertelement <2 x i128> undef, i128 %728, i32 0
  %735 = insertelement <2 x i128> %734, i128 %731, i32 1
  %736 = trunc <2 x i128> %735 to <2 x i64>
  %737 = and <2 x i64> %736, <i64 2251799813685247, i64 2251799813685247>
  %738 = mul i64 %733, 19
  %739 = add i64 %738, %708
  %740 = lshr i64 %739, 51
  %741 = add nuw nsw i64 %740, %717
  %742 = lshr i64 %741, 51
  %743 = insertelement <2 x i64> undef, i64 %739, i32 0
  %744 = insertelement <2 x i64> %743, i64 %741, i32 1
  %745 = and <2 x i64> %744, <i64 2251799813685247, i64 2251799813685247>
  %746 = add nuw nsw i64 %742, %724
  %747 = add nuw nsw i64 %662, %433
  %748 = add nuw nsw i64 %666, %436
  %749 = add nuw nsw i64 %667, %437
  %750 = add nuw nsw i64 %653, %421
  %751 = add nuw nsw i64 %658, %429
  store i64 %747, i64* %65, align 8
  store i64 %748, i64* %66, align 8
  store i64 %749, i64* %67, align 8
  store i64 %750, i64* %68, align 8
  store i64 %751, i64* %69, align 8
  call fastcc void @fe_mul_impl(i64* nonnull %35, i64* nonnull %17, i64* nonnull %29) #4
  call fastcc void @fe_mul_impl(i64* nonnull %29, i64* nonnull %71, i64* nonnull %65) #4
  %752 = icmp eq i32 %273, 0
  br i1 %752, label %774, label %753

753:                                              ; preds = %256
  %754 = add nsw i32 %273, -1
  %755 = load i64, i64* %23, align 16
  %756 = load i64, i64* %24, align 8
  %757 = load i64, i64* %25, align 16
  %758 = load i64, i64* %26, align 8
  %759 = load i64, i64* %27, align 16
  %760 = load i64, i64* %29, align 16
  %761 = load i64, i64* %35, align 16
  %762 = load i64, i64* %30, align 8
  %763 = load i64, i64* %36, align 8
  %764 = load i64, i64* %31, align 16
  %765 = load i64, i64* %37, align 16
  %766 = load i64, i64* %32, align 8
  %767 = load i64, i64* %38, align 8
  %768 = load i64, i64* %33, align 16
  %769 = load i64, i64* %39, align 16
  %770 = extractelement <2 x i64> %745, i32 0
  %771 = extractelement <2 x i64> %745, i32 1
  %772 = extractelement <2 x i64> %737, i32 0
  %773 = extractelement <2 x i64> %737, i32 1
  br label %256

774:                                              ; preds = %256
  %775 = zext i32 %286 to i64
  %776 = sub nsw i64 0, %775
  %777 = bitcast %struct.fe* %6 to <2 x i64>*
  %778 = load <2 x i64>, <2 x i64>* %777, align 16
  %779 = xor <2 x i64> %778, %745
  %780 = insertelement <2 x i64> undef, i64 %776, i32 0
  %781 = shufflevector <2 x i64> %780, <2 x i64> undef, <2 x i32> zeroinitializer
  %782 = and <2 x i64> %779, %781
  %783 = xor <2 x i64> %782, %778
  %784 = bitcast %struct.fe* %6 to <2 x i64>*
  store <2 x i64> %783, <2 x i64>* %784, align 16
  %785 = load i64, i64* %25, align 16
  %786 = xor i64 %785, %746
  %787 = and i64 %786, %776
  %788 = xor i64 %787, %785
  store i64 %788, i64* %25, align 16
  %789 = bitcast i64* %26 to <2 x i64>*
  %790 = load <2 x i64>, <2 x i64>* %789, align 8
  %791 = xor <2 x i64> %790, %737
  %792 = and <2 x i64> %791, %781
  %793 = xor <2 x i64> %792, %790
  %794 = bitcast i64* %26 to <2 x i64>*
  store <2 x i64> %793, <2 x i64>* %794, align 8
  %795 = bitcast %struct.fe* %7 to <2 x i64>*
  %796 = load <2 x i64>, <2 x i64>* %795, align 16
  %797 = bitcast %struct.fe* %8 to <2 x i64>*
  %798 = load <2 x i64>, <2 x i64>* %797, align 16
  %799 = xor <2 x i64> %798, %796
  %800 = and <2 x i64> %799, %781
  %801 = xor <2 x i64> %800, %796
  %802 = bitcast %struct.fe* %7 to <2 x i64>*
  store <2 x i64> %801, <2 x i64>* %802, align 16
  %803 = xor <2 x i64> %800, %798
  %804 = bitcast %struct.fe* %8 to <2 x i64>*
  store <2 x i64> %803, <2 x i64>* %804, align 16
  %805 = bitcast i64* %31 to <2 x i64>*
  %806 = load <2 x i64>, <2 x i64>* %805, align 16
  %807 = bitcast i64* %37 to <2 x i64>*
  %808 = load <2 x i64>, <2 x i64>* %807, align 16
  %809 = xor <2 x i64> %808, %806
  %810 = and <2 x i64> %809, %781
  %811 = xor <2 x i64> %810, %806
  %812 = bitcast i64* %31 to <2 x i64>*
  store <2 x i64> %811, <2 x i64>* %812, align 16
  %813 = xor <2 x i64> %810, %808
  %814 = bitcast i64* %37 to <2 x i64>*
  store <2 x i64> %813, <2 x i64>* %814, align 16
  %815 = load i64, i64* %33, align 16
  %816 = load i64, i64* %39, align 16
  %817 = xor i64 %816, %815
  %818 = and i64 %817, %776
  %819 = xor i64 %818, %815
  store i64 %819, i64* %33, align 16
  %820 = bitcast %struct.fe_loose* %4 to i8*
  call void @llvm.lifetime.start.p0i8(i64 40, i8* nonnull %820) #4
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 8 %820, i8* nonnull align 16 %28, i64 40, i1 false) #4
  call fastcc void @fe_loose_invert(%struct.fe* nonnull %7, %struct.fe_loose* nonnull %4) #4
  call void @llvm.lifetime.end.p0i8(i64 40, i8* nonnull %820) #4
  call fastcc void @fe_mul_impl(i64* nonnull %23, i64* nonnull %23, i64* nonnull %29) #4
  call fastcc void @fe_tobytes(i8* %0, %struct.fe* nonnull %6) #4
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %76) #4
  call void @llvm.lifetime.end.p0i8(i64 40, i8* nonnull %70) #4
  call void @llvm.lifetime.end.p0i8(i64 40, i8* nonnull %64) #4
  call void @llvm.lifetime.end.p0i8(i64 40, i8* nonnull %58) #4
  call void @llvm.lifetime.end.p0i8(i64 40, i8* nonnull %52) #4
  call void @llvm.lifetime.end.p0i8(i64 40, i8* nonnull %46) #4
  call void @llvm.lifetime.end.p0i8(i64 40, i8* nonnull %40) #4
  call void @llvm.lifetime.end.p0i8(i64 40, i8* nonnull %34) #4
  call void @llvm.lifetime.end.p0i8(i64 40, i8* nonnull %28) #4
  call void @llvm.lifetime.end.p0i8(i64 40, i8* nonnull %22) #4
  call void @llvm.lifetime.end.p0i8(i64 40, i8* nonnull %16) #4
  %821 = tail call i32 @CRYPTO_memcmp(i8* getelementptr inbounds ([32 x i8], [32 x i8]* @X25519.kZeros, i64 0, i64 0), i8* %0, i64 32) #4
  %822 = icmp ne i32 %821, 0
  %823 = zext i1 %822 to i32
  ret i32 %823
}

; Function Attrs: nounwind ssp uwtable
define internal fastcc void @fe_loose_invert(%struct.fe* nocapture, %struct.fe_loose* nocapture readonly) unnamed_addr #0 {
  %3 = alloca %struct.fe, align 8
  %4 = alloca %struct.fe, align 8
  %5 = alloca %struct.fe, align 8
  %6 = alloca %struct.fe, align 8
  %7 = bitcast %struct.fe* %3 to i8*
  call void @llvm.lifetime.start.p0i8(i64 40, i8* nonnull %7) #4
  %8 = getelementptr inbounds %struct.fe, %struct.fe* %3, i64 0, i32 0, i64 0
  %9 = getelementptr inbounds %struct.fe, %struct.fe* %3, i64 0, i32 0, i64 1
  %10 = getelementptr inbounds %struct.fe, %struct.fe* %3, i64 0, i32 0, i64 2
  %11 = getelementptr inbounds %struct.fe, %struct.fe* %3, i64 0, i32 0, i64 3
  %12 = getelementptr inbounds %struct.fe, %struct.fe* %3, i64 0, i32 0, i64 4
  %13 = bitcast %struct.fe* %4 to i8*
  %14 = bitcast %struct.fe* %3 to i8*
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %14, i8 -86, i64 40, i1 false)
  call void @llvm.lifetime.start.p0i8(i64 40, i8* nonnull %13) #4
  %15 = getelementptr inbounds %struct.fe, %struct.fe* %4, i64 0, i32 0, i64 0
  %16 = getelementptr inbounds %struct.fe, %struct.fe* %4, i64 0, i32 0, i64 1
  %17 = getelementptr inbounds %struct.fe, %struct.fe* %4, i64 0, i32 0, i64 2
  %18 = getelementptr inbounds %struct.fe, %struct.fe* %4, i64 0, i32 0, i64 3
  %19 = getelementptr inbounds %struct.fe, %struct.fe* %4, i64 0, i32 0, i64 4
  %20 = bitcast %struct.fe* %5 to i8*
  %21 = bitcast %struct.fe* %4 to i8*
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %21, i8 -86, i64 40, i1 false)
  call void @llvm.lifetime.start.p0i8(i64 40, i8* nonnull %20) #4
  %22 = getelementptr inbounds %struct.fe, %struct.fe* %5, i64 0, i32 0, i64 0
  %23 = getelementptr inbounds %struct.fe, %struct.fe* %5, i64 0, i32 0, i64 1
  %24 = getelementptr inbounds %struct.fe, %struct.fe* %5, i64 0, i32 0, i64 2
  %25 = getelementptr inbounds %struct.fe, %struct.fe* %5, i64 0, i32 0, i64 3
  %26 = getelementptr inbounds %struct.fe, %struct.fe* %5, i64 0, i32 0, i64 4
  %27 = bitcast %struct.fe* %6 to i8*
  %28 = bitcast %struct.fe* %5 to i8*
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %28, i8 -86, i64 40, i1 false)
  call void @llvm.lifetime.start.p0i8(i64 40, i8* nonnull %27) #4
  %29 = getelementptr inbounds %struct.fe, %struct.fe* %6, i64 0, i32 0, i64 0
  %30 = getelementptr inbounds %struct.fe, %struct.fe* %6, i64 0, i32 0, i64 1
  %31 = getelementptr inbounds %struct.fe, %struct.fe* %6, i64 0, i32 0, i64 2
  %32 = getelementptr inbounds %struct.fe, %struct.fe* %6, i64 0, i32 0, i64 3
  %33 = getelementptr inbounds %struct.fe, %struct.fe* %6, i64 0, i32 0, i64 4
  %34 = getelementptr inbounds %struct.fe_loose, %struct.fe_loose* %1, i64 0, i32 0, i64 0
  %35 = getelementptr inbounds %struct.fe_loose, %struct.fe_loose* %1, i64 0, i32 0, i64 4
  %36 = bitcast %struct.fe* %6 to i8*
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %36, i8 -86, i64 40, i1 false)
  %37 = load i64, i64* %35, align 8
  %38 = mul i64 %37, 19
  %39 = mul i64 %37, 38
  %40 = shl i64 %37, 1
  %41 = getelementptr inbounds %struct.fe_loose, %struct.fe_loose* %1, i64 0, i32 0, i64 3
  %42 = load i64, i64* %41, align 8
  %43 = mul i64 %42, 19
  %44 = mul i64 %42, 38
  %45 = shl i64 %42, 1
  %46 = getelementptr inbounds %struct.fe_loose, %struct.fe_loose* %1, i64 0, i32 0, i64 2
  %47 = load i64, i64* %46, align 8
  %48 = shl i64 %47, 1
  %49 = getelementptr inbounds %struct.fe_loose, %struct.fe_loose* %1, i64 0, i32 0, i64 1
  %50 = load i64, i64* %49, align 8
  %51 = shl i64 %50, 1
  %52 = zext i64 %37 to i128
  %53 = zext i64 %38 to i128
  %54 = mul nuw i128 %53, %52
  %55 = zext i64 %42 to i128
  %56 = zext i64 %39 to i128
  %57 = mul nuw i128 %55, %56
  %58 = zext i64 %43 to i128
  %59 = mul nuw i128 %58, %55
  %60 = zext i64 %47 to i128
  %61 = mul nuw i128 %60, %56
  %62 = zext i64 %44 to i128
  %63 = mul nuw i128 %60, %62
  %64 = mul nuw i128 %60, %60
  %65 = zext i64 %50 to i128
  %66 = mul nuw i128 %65, %56
  %67 = zext i64 %45 to i128
  %68 = mul nuw i128 %65, %67
  %69 = zext i64 %48 to i128
  %70 = mul nuw i128 %65, %69
  %71 = mul nuw i128 %65, %65
  %72 = load i64, i64* %34, align 8
  %73 = zext i64 %72 to i128
  %74 = zext i64 %40 to i128
  %75 = mul nuw i128 %73, %74
  %76 = mul nuw i128 %73, %67
  %77 = mul nuw i128 %73, %69
  %78 = zext i64 %51 to i128
  %79 = mul nuw i128 %73, %78
  %80 = mul nuw i128 %73, %73
  %81 = add i128 %66, %63
  %82 = add i128 %81, %80
  %83 = lshr i128 %82, 51
  %84 = trunc i128 %82 to i64
  %85 = and i64 %84, 2251799813685247
  %86 = add i128 %68, %64
  %87 = add i128 %86, %75
  %88 = add i128 %70, %54
  %89 = add i128 %88, %76
  %90 = add i128 %71, %57
  %91 = add i128 %90, %77
  %92 = add i128 %61, %59
  %93 = add i128 %92, %79
  %94 = and i128 %83, 18446744073709551615
  %95 = add i128 %93, %94
  %96 = lshr i128 %95, 51
  %97 = trunc i128 %95 to i64
  %98 = and i64 %97, 2251799813685247
  %99 = and i128 %96, 18446744073709551615
  %100 = add i128 %91, %99
  %101 = lshr i128 %100, 51
  %102 = trunc i128 %100 to i64
  %103 = and i64 %102, 2251799813685247
  %104 = and i128 %101, 18446744073709551615
  %105 = add i128 %89, %104
  %106 = lshr i128 %105, 51
  %107 = trunc i128 %105 to i64
  %108 = and i64 %107, 2251799813685247
  %109 = and i128 %106, 18446744073709551615
  %110 = add i128 %87, %109
  %111 = lshr i128 %110, 51
  %112 = trunc i128 %111 to i64
  %113 = trunc i128 %110 to i64
  %114 = and i64 %113, 2251799813685247
  %115 = mul i64 %112, 19
  %116 = add i64 %115, %85
  %117 = lshr i64 %116, 51
  %118 = and i64 %116, 2251799813685247
  %119 = add nuw nsw i64 %117, %98
  %120 = lshr i64 %119, 51
  %121 = and i64 %119, 2251799813685247
  %122 = add nuw nsw i64 %120, %103
  store i64 %118, i64* %8, align 8
  store i64 %121, i64* %9, align 8
  store i64 %122, i64* %10, align 8
  store i64 %108, i64* %11, align 8
  store i64 %114, i64* %12, align 8
  %123 = mul nuw nsw i64 %114, 19
  %124 = mul nuw nsw i64 %114, 38
  %125 = shl nuw nsw i64 %114, 1
  %126 = mul nuw nsw i64 %108, 19
  %127 = mul nuw nsw i64 %108, 38
  %128 = shl nuw nsw i64 %108, 1
  %129 = shl nuw nsw i64 %122, 1
  %130 = shl nuw nsw i64 %121, 1
  %131 = zext i64 %114 to i128
  %132 = zext i64 %123 to i128
  %133 = mul nuw nsw i128 %132, %131
  %134 = zext i64 %108 to i128
  %135 = zext i64 %124 to i128
  %136 = mul nuw nsw i128 %135, %134
  %137 = zext i64 %126 to i128
  %138 = mul nuw nsw i128 %137, %134
  %139 = zext i64 %122 to i128
  %140 = mul nuw nsw i128 %139, %135
  %141 = zext i64 %127 to i128
  %142 = mul nuw nsw i128 %139, %141
  %143 = mul nuw nsw i128 %139, %139
  %144 = zext i64 %121 to i128
  %145 = mul nuw nsw i128 %144, %135
  %146 = zext i64 %128 to i128
  %147 = mul nuw nsw i128 %144, %146
  %148 = zext i64 %129 to i128
  %149 = mul nuw nsw i128 %148, %144
  %150 = mul nuw nsw i128 %144, %144
  %151 = zext i64 %118 to i128
  %152 = zext i64 %125 to i128
  %153 = mul nuw nsw i128 %151, %152
  %154 = mul nuw nsw i128 %151, %146
  %155 = mul nuw nsw i128 %148, %151
  %156 = zext i64 %130 to i128
  %157 = mul nuw nsw i128 %156, %151
  %158 = mul nuw nsw i128 %151, %151
  %159 = add nuw nsw i128 %145, %158
  %160 = add nuw nsw i128 %159, %142
  %161 = lshr i128 %160, 51
  %162 = trunc i128 %160 to i64
  %163 = and i64 %162, 2251799813685247
  %164 = add nuw nsw i128 %150, %136
  %165 = add nuw nsw i128 %164, %155
  %166 = add nuw nsw i128 %140, %138
  %167 = add nuw nsw i128 %166, %157
  %168 = and i128 %161, 18446744073709551615
  %169 = add nuw nsw i128 %167, %168
  %170 = lshr i128 %169, 51
  %171 = trunc i128 %169 to i64
  %172 = and i64 %171, 2251799813685247
  %173 = and i128 %170, 18446744073709551615
  %174 = add nuw nsw i128 %165, %173
  %175 = lshr i128 %174, 51
  %176 = trunc i128 %174 to i64
  %177 = and i64 %176, 2251799813685247
  %178 = and i128 %175, 18446744073709551615
  %179 = add nuw nsw i128 %154, %133
  %180 = add nuw nsw i128 %179, %149
  %181 = add nuw nsw i128 %180, %178
  %182 = lshr i128 %181, 51
  %183 = and i128 %182, 18446744073709551615
  %184 = add nuw nsw i128 %147, %153
  %185 = add nuw nsw i128 %184, %143
  %186 = add nuw nsw i128 %185, %183
  %187 = lshr i128 %186, 51
  %188 = trunc i128 %187 to i64
  %189 = insertelement <2 x i128> undef, i128 %181, i32 0
  %190 = insertelement <2 x i128> %189, i128 %186, i32 1
  %191 = trunc <2 x i128> %190 to <2 x i64>
  %192 = and <2 x i64> %191, <i64 2251799813685247, i64 2251799813685247>
  %193 = mul i64 %188, 19
  %194 = add nuw nsw i64 %193, %163
  %195 = lshr i64 %194, 51
  %196 = and i64 %194, 2251799813685247
  %197 = add nuw nsw i64 %195, %172
  %198 = lshr i64 %197, 51
  %199 = and i64 %197, 2251799813685247
  %200 = add nuw nsw i64 %198, %177
  store i64 %196, i64* %15, align 8
  store i64 %199, i64* %16, align 8
  store i64 %200, i64* %17, align 8
  %201 = bitcast i64* %18 to <2 x i64>*
  store <2 x i64> %192, <2 x i64>* %201, align 8
  %202 = trunc i128 %111 to i51
  %203 = mul i51 %202, 19
  %204 = trunc i128 %82 to i51
  %205 = add i51 %203, %204
  %206 = trunc i128 %110 to i51
  %207 = mul i51 %205, %206
  %208 = trunc i64 %117 to i51
  %209 = trunc i128 %95 to i51
  %210 = add i51 %208, %209
  %211 = trunc i128 %105 to i51
  %212 = mul i51 %210, %211
  %213 = add i51 %207, %212
  %214 = shl i51 %213, 1
  %215 = trunc i64 %120 to i51
  %216 = trunc i128 %100 to i51
  %217 = add i51 %215, %216
  %218 = mul i51 %217, %217
  %219 = add i51 %214, %218
  %220 = trunc i128 %182 to i51
  %221 = add i51 %219, %220
  %222 = mul i51 %221, %221
  %223 = mul i51 %222, 19
  %224 = mul i51 %206, %206
  %225 = mul i51 %224, 19
  %226 = mul i51 %217, %210
  %227 = shl i51 %226, 1
  %228 = add i51 %225, %227
  %229 = mul i51 %205, %211
  %230 = shl i51 %229, 1
  %231 = add i51 %228, %230
  %232 = trunc i128 %175 to i51
  %233 = add i51 %231, %232
  %234 = trunc i64 %194 to i51
  %235 = mul i51 %233, %234
  %236 = shl i51 %235, 1
  %237 = add i51 %223, %236
  %238 = mul i51 %206, %211
  %239 = mul i51 %238, 38
  %240 = mul i51 %217, %205
  %241 = shl i51 %240, 1
  %242 = add i51 %239, %241
  %243 = mul i51 %210, %210
  %244 = add i51 %242, %243
  %245 = trunc i128 %170 to i51
  %246 = add i51 %244, %245
  %247 = trunc i64 %198 to i51
  %248 = add i51 %246, %247
  %249 = trunc i64 %197 to i51
  %250 = mul i51 %248, %249
  %251 = shl i51 %250, 1
  %252 = add i51 %237, %251
  %253 = zext i51 %221 to i128
  %254 = zext i51 %233 to i128
  %255 = mul nuw nsw i128 %253, %254
  %256 = mul nuw nsw i128 %255, 38
  %257 = zext i64 %200 to i128
  %258 = shl nuw nsw i64 %200, 1
  %259 = zext i64 %258 to i128
  %260 = zext i64 %196 to i128
  %261 = mul nuw nsw i128 %259, %260
  %262 = add nuw nsw i128 %256, %261
  %263 = zext i64 %199 to i128
  %264 = mul nuw nsw i128 %263, %263
  %265 = add nuw nsw i128 %262, %264
  %266 = mul nuw nsw i128 %253, %257
  %267 = mul nuw nsw i128 %266, 38
  %268 = mul nuw nsw i128 %254, %254
  %269 = mul nuw nsw i128 %268, 19
  %270 = add nuw nsw i128 %267, %269
  %271 = shl nuw nsw i64 %199, 1
  %272 = zext i64 %271 to i128
  %273 = mul nuw nsw i128 %260, %272
  %274 = add nuw nsw i128 %270, %273
  %275 = mul nuw nsw i128 %253, %263
  %276 = mul nuw nsw i128 %254, %257
  %277 = add nuw nsw i128 %275, %276
  %278 = mul nuw nsw i128 %277, 38
  %279 = mul nuw nsw i128 %260, %260
  %280 = add nuw nsw i128 %278, %279
  %281 = lshr i128 %280, 51
  %282 = and i128 %281, 18446744073709551615
  %283 = add nuw nsw i128 %274, %282
  %284 = lshr i128 %283, 51
  %285 = and i128 %284, 18446744073709551615
  %286 = add nuw nsw i128 %265, %285
  %287 = lshr i128 %286, 51
  %288 = trunc i128 %287 to i51
  %289 = add i51 %252, %288
  %290 = mul i51 %221, %234
  %291 = mul i51 %233, %249
  %292 = add i51 %290, %291
  %293 = shl i51 %292, 1
  %294 = mul i51 %248, %248
  %295 = add i51 %293, %294
  %296 = mul nuw nsw i128 %253, %253
  %297 = mul nuw nsw i128 %296, 19
  %298 = mul nuw nsw i128 %260, %254
  %299 = shl nuw nsw i128 %298, 1
  %300 = add nuw nsw i128 %297, %299
  %301 = mul nuw nsw i128 %259, %263
  %302 = add nuw nsw i128 %300, %301
  %303 = and i128 %287, 18446744073709551615
  %304 = add nuw nsw i128 %302, %303
  %305 = lshr i128 %304, 51
  %306 = trunc i128 %305 to i51
  %307 = add i51 %295, %306
  %308 = mul nuw nsw i128 %253, %260
  %309 = mul nuw nsw i128 %263, %254
  %310 = add nuw nsw i128 %308, %309
  %311 = shl nuw nsw i128 %310, 1
  %312 = mul nuw nsw i128 %257, %257
  %313 = add nuw nsw i128 %311, %312
  %314 = and i128 %305, 18446744073709551615
  %315 = add nuw nsw i128 %313, %314
  %316 = lshr i128 %315, 51
  %317 = trunc i128 %316 to i51
  %318 = mul i51 %317, 19
  %319 = mul i51 %233, %248
  %320 = mul i51 %221, %249
  %321 = add i51 %319, %320
  %322 = mul i51 %321, 38
  %323 = mul i64 %196, %196
  %324 = trunc i64 %323 to i51
  %325 = add i51 %322, %324
  %326 = add i51 %318, %325
  %327 = mul i51 %221, %248
  %328 = mul i51 %327, 38
  %329 = mul i51 %233, %233
  %330 = mul i51 %329, 19
  %331 = add i51 %328, %330
  %332 = trunc i128 %281 to i51
  %333 = add i51 %331, %332
  %334 = mul i64 %199, %196
  %335 = shl i64 %334, 1
  %336 = trunc i64 %335 to i51
  %337 = add i51 %333, %336
  %338 = trunc i128 %316 to i64
  %339 = mul i64 %338, 19
  %340 = zext i51 %325 to i64
  %341 = add nuw nsw i64 %339, %340
  %342 = lshr i64 %341, 51
  %343 = trunc i64 %342 to i51
  %344 = add i51 %337, %343
  %345 = zext i51 %337 to i64
  %346 = add nuw nsw i64 %342, %345
  %347 = mul i51 %221, %233
  %348 = mul i51 %347, 38
  %349 = mul i51 %248, %234
  %350 = shl i51 %349, 1
  %351 = add i51 %348, %350
  %352 = trunc i128 %284 to i51
  %353 = add i51 %351, %352
  %354 = mul i64 %199, %199
  %355 = trunc i64 %354 to i51
  %356 = add i51 %353, %355
  %357 = zext i51 %356 to i64
  %358 = zext i51 %289 to i64
  %359 = zext i51 %307 to i64
  %360 = zext i51 %326 to i64
  %361 = zext i51 %344 to i64
  %362 = lshr i64 %346, 51
  %363 = add nuw nsw i64 %362, %357
  store i64 %359, i64* %19, align 8
  store i64 %358, i64* %18, align 8
  store i64 %363, i64* %17, align 8
  store i64 %361, i64* %16, align 8
  store i64 %360, i64* %15, align 8
  call fastcc void @fe_mul_impl(i64* nonnull %15, i64* %34, i64* nonnull %15) #4
  call fastcc void @fe_mul_impl(i64* nonnull %8, i64* nonnull %8, i64* nonnull %15) #4
  %364 = load i64, i64* %12, align 8
  %365 = mul i64 %364, 19
  %366 = mul i64 %364, 38
  %367 = shl i64 %364, 1
  %368 = load i64, i64* %11, align 8
  %369 = mul i64 %368, 19
  %370 = mul i64 %368, 38
  %371 = shl i64 %368, 1
  %372 = load i64, i64* %10, align 8
  %373 = shl i64 %372, 1
  %374 = load i64, i64* %9, align 8
  %375 = shl i64 %374, 1
  %376 = zext i64 %364 to i128
  %377 = zext i64 %365 to i128
  %378 = mul nuw i128 %377, %376
  %379 = zext i64 %368 to i128
  %380 = zext i64 %366 to i128
  %381 = mul nuw i128 %379, %380
  %382 = zext i64 %369 to i128
  %383 = mul nuw i128 %382, %379
  %384 = zext i64 %372 to i128
  %385 = mul nuw i128 %384, %380
  %386 = zext i64 %370 to i128
  %387 = mul nuw i128 %384, %386
  %388 = mul nuw i128 %384, %384
  %389 = zext i64 %374 to i128
  %390 = mul nuw i128 %389, %380
  %391 = zext i64 %371 to i128
  %392 = mul nuw i128 %389, %391
  %393 = zext i64 %373 to i128
  %394 = mul nuw i128 %389, %393
  %395 = mul nuw i128 %389, %389
  %396 = load i64, i64* %8, align 8
  %397 = zext i64 %396 to i128
  %398 = zext i64 %367 to i128
  %399 = mul nuw i128 %397, %398
  %400 = mul nuw i128 %397, %391
  %401 = mul nuw i128 %397, %393
  %402 = zext i64 %375 to i128
  %403 = mul nuw i128 %397, %402
  %404 = mul nuw i128 %397, %397
  %405 = add i128 %390, %387
  %406 = add i128 %405, %404
  %407 = lshr i128 %406, 51
  %408 = trunc i128 %406 to i64
  %409 = and i64 %408, 2251799813685247
  %410 = add i128 %392, %388
  %411 = add i128 %410, %399
  %412 = add i128 %394, %378
  %413 = add i128 %412, %400
  %414 = add i128 %395, %381
  %415 = add i128 %414, %401
  %416 = add i128 %385, %383
  %417 = add i128 %416, %403
  %418 = and i128 %407, 18446744073709551615
  %419 = add i128 %417, %418
  %420 = lshr i128 %419, 51
  %421 = trunc i128 %419 to i64
  %422 = and i64 %421, 2251799813685247
  %423 = and i128 %420, 18446744073709551615
  %424 = add i128 %415, %423
  %425 = lshr i128 %424, 51
  %426 = trunc i128 %424 to i64
  %427 = and i64 %426, 2251799813685247
  %428 = and i128 %425, 18446744073709551615
  %429 = add i128 %413, %428
  %430 = lshr i128 %429, 51
  %431 = and i128 %430, 18446744073709551615
  %432 = add i128 %411, %431
  %433 = lshr i128 %432, 51
  %434 = trunc i128 %433 to i64
  %435 = insertelement <2 x i128> undef, i128 %429, i32 0
  %436 = insertelement <2 x i128> %435, i128 %432, i32 1
  %437 = trunc <2 x i128> %436 to <2 x i64>
  %438 = and <2 x i64> %437, <i64 2251799813685247, i64 2251799813685247>
  %439 = mul i64 %434, 19
  %440 = add i64 %439, %409
  %441 = lshr i64 %440, 51
  %442 = and i64 %440, 2251799813685247
  %443 = add nuw nsw i64 %441, %422
  %444 = lshr i64 %443, 51
  %445 = and i64 %443, 2251799813685247
  %446 = add nuw nsw i64 %444, %427
  store i64 %442, i64* %22, align 8
  store i64 %445, i64* %23, align 8
  store i64 %446, i64* %24, align 8
  %447 = bitcast i64* %25 to <2 x i64>*
  store <2 x i64> %438, <2 x i64>* %447, align 8
  call fastcc void @fe_mul_impl(i64* nonnull %15, i64* nonnull %15, i64* nonnull %22) #4
  %448 = load i64, i64* %19, align 8
  %449 = mul i64 %448, 19
  %450 = mul i64 %448, 38
  %451 = shl i64 %448, 1
  %452 = load i64, i64* %18, align 8
  %453 = mul i64 %452, 19
  %454 = mul i64 %452, 38
  %455 = shl i64 %452, 1
  %456 = load i64, i64* %17, align 8
  %457 = shl i64 %456, 1
  %458 = load i64, i64* %16, align 8
  %459 = shl i64 %458, 1
  %460 = zext i64 %448 to i128
  %461 = zext i64 %449 to i128
  %462 = mul nuw i128 %461, %460
  %463 = zext i64 %452 to i128
  %464 = zext i64 %450 to i128
  %465 = mul nuw i128 %463, %464
  %466 = zext i64 %453 to i128
  %467 = mul nuw i128 %466, %463
  %468 = zext i64 %456 to i128
  %469 = mul nuw i128 %468, %464
  %470 = zext i64 %454 to i128
  %471 = mul nuw i128 %468, %470
  %472 = mul nuw i128 %468, %468
  %473 = zext i64 %458 to i128
  %474 = mul nuw i128 %473, %464
  %475 = zext i64 %455 to i128
  %476 = mul nuw i128 %473, %475
  %477 = zext i64 %457 to i128
  %478 = mul nuw i128 %473, %477
  %479 = mul nuw i128 %473, %473
  %480 = load i64, i64* %15, align 8
  %481 = zext i64 %480 to i128
  %482 = zext i64 %451 to i128
  %483 = mul nuw i128 %481, %482
  %484 = mul nuw i128 %481, %475
  %485 = mul nuw i128 %481, %477
  %486 = zext i64 %459 to i128
  %487 = mul nuw i128 %481, %486
  %488 = mul nuw i128 %481, %481
  %489 = add i128 %474, %471
  %490 = add i128 %489, %488
  %491 = lshr i128 %490, 51
  %492 = trunc i128 %490 to i64
  %493 = and i64 %492, 2251799813685247
  %494 = add i128 %476, %472
  %495 = add i128 %494, %483
  %496 = add i128 %478, %462
  %497 = add i128 %496, %484
  %498 = add i128 %479, %465
  %499 = add i128 %498, %485
  %500 = add i128 %469, %467
  %501 = add i128 %500, %487
  %502 = and i128 %491, 18446744073709551615
  %503 = add i128 %501, %502
  %504 = lshr i128 %503, 51
  %505 = trunc i128 %503 to i64
  %506 = and i64 %505, 2251799813685247
  %507 = and i128 %504, 18446744073709551615
  %508 = add i128 %499, %507
  %509 = lshr i128 %508, 51
  %510 = trunc i128 %508 to i64
  %511 = and i64 %510, 2251799813685247
  %512 = and i128 %509, 18446744073709551615
  %513 = add i128 %497, %512
  %514 = lshr i128 %513, 51
  %515 = trunc i128 %513 to i64
  %516 = and i64 %515, 2251799813685247
  %517 = and i128 %514, 18446744073709551615
  %518 = add i128 %495, %517
  %519 = lshr i128 %518, 51
  %520 = trunc i128 %519 to i64
  %521 = trunc i128 %518 to i64
  %522 = and i64 %521, 2251799813685247
  %523 = mul i64 %520, 19
  %524 = add i64 %523, %493
  %525 = lshr i64 %524, 51
  %526 = and i64 %524, 2251799813685247
  %527 = add nuw nsw i64 %525, %506
  %528 = lshr i64 %527, 51
  %529 = and i64 %527, 2251799813685247
  %530 = add nuw nsw i64 %528, %511
  store i64 %526, i64* %22, align 8
  store i64 %529, i64* %23, align 8
  store i64 %530, i64* %24, align 8
  store i64 %516, i64* %25, align 8
  store i64 %522, i64* %26, align 8
  br label %531

531:                                              ; preds = %531, %2
  %532 = phi i64 [ %526, %2 ], [ %611, %531 ]
  %533 = phi i64 [ %529, %2 ], [ %614, %531 ]
  %534 = phi i64 [ %530, %2 ], [ %615, %531 ]
  %535 = phi i64 [ %516, %2 ], [ %601, %531 ]
  %536 = phi i64 [ %522, %2 ], [ %607, %531 ]
  %537 = phi i32 [ 1, %2 ], [ %616, %531 ]
  %538 = mul nuw nsw i64 %536, 19
  %539 = mul nuw nsw i64 %536, 38
  %540 = shl nuw nsw i64 %536, 1
  %541 = mul nuw nsw i64 %535, 19
  %542 = mul nuw nsw i64 %535, 38
  %543 = shl nuw nsw i64 %535, 1
  %544 = shl nsw i64 %534, 1
  %545 = shl nuw nsw i64 %533, 1
  %546 = zext i64 %536 to i128
  %547 = zext i64 %538 to i128
  %548 = mul nuw nsw i128 %547, %546
  %549 = zext i64 %535 to i128
  %550 = zext i64 %539 to i128
  %551 = mul nuw nsw i128 %549, %550
  %552 = zext i64 %541 to i128
  %553 = mul nuw nsw i128 %552, %549
  %554 = zext i64 %534 to i128
  %555 = mul nuw nsw i128 %554, %550
  %556 = zext i64 %542 to i128
  %557 = mul nuw nsw i128 %554, %556
  %558 = mul nuw i128 %554, %554
  %559 = zext i64 %533 to i128
  %560 = mul nuw nsw i128 %559, %550
  %561 = zext i64 %543 to i128
  %562 = mul nuw nsw i128 %559, %561
  %563 = zext i64 %544 to i128
  %564 = mul nuw nsw i128 %559, %563
  %565 = mul nuw nsw i128 %559, %559
  %566 = zext i64 %532 to i128
  %567 = zext i64 %540 to i128
  %568 = mul nuw nsw i128 %566, %567
  %569 = mul nuw nsw i128 %566, %561
  %570 = mul nuw nsw i128 %566, %563
  %571 = zext i64 %545 to i128
  %572 = mul nuw nsw i128 %566, %571
  %573 = mul nuw nsw i128 %566, %566
  %574 = add nuw nsw i128 %560, %557
  %575 = add nuw nsw i128 %574, %573
  %576 = lshr i128 %575, 51
  %577 = trunc i128 %575 to i64
  %578 = and i64 %577, 2251799813685247
  %579 = add i128 %562, %558
  %580 = add i128 %579, %568
  %581 = add nuw nsw i128 %564, %548
  %582 = add nuw nsw i128 %581, %569
  %583 = add nuw nsw i128 %565, %551
  %584 = add nuw nsw i128 %583, %570
  %585 = add nuw nsw i128 %555, %553
  %586 = add nuw nsw i128 %585, %572
  %587 = and i128 %576, 18446744073709551615
  %588 = add nuw i128 %586, %587
  %589 = lshr i128 %588, 51
  %590 = trunc i128 %588 to i64
  %591 = and i64 %590, 2251799813685247
  %592 = and i128 %589, 18446744073709551615
  %593 = add nuw nsw i128 %584, %592
  %594 = lshr i128 %593, 51
  %595 = trunc i128 %593 to i64
  %596 = and i64 %595, 2251799813685247
  %597 = and i128 %594, 18446744073709551615
  %598 = add nuw nsw i128 %582, %597
  %599 = lshr i128 %598, 51
  %600 = trunc i128 %598 to i64
  %601 = and i64 %600, 2251799813685247
  %602 = and i128 %599, 18446744073709551615
  %603 = add i128 %580, %602
  %604 = lshr i128 %603, 51
  %605 = trunc i128 %604 to i64
  %606 = trunc i128 %603 to i64
  %607 = and i64 %606, 2251799813685247
  %608 = mul i64 %605, 19
  %609 = add i64 %608, %578
  %610 = lshr i64 %609, 51
  %611 = and i64 %609, 2251799813685247
  %612 = add nuw nsw i64 %610, %591
  %613 = lshr i64 %612, 51
  %614 = and i64 %612, 2251799813685247
  %615 = add nuw nsw i64 %613, %596
  %616 = add nuw nsw i32 %537, 1
  %617 = icmp eq i32 %616, 5
  br i1 %617, label %618, label %531

618:                                              ; preds = %531
  store i64 %607, i64* %26, align 8
  store i64 %601, i64* %25, align 8
  store i64 %615, i64* %24, align 8
  store i64 %614, i64* %23, align 8
  store i64 %611, i64* %22, align 8
  call fastcc void @fe_mul_impl(i64* nonnull %15, i64* nonnull %22, i64* nonnull %15) #4
  %619 = load i64, i64* %19, align 8
  %620 = mul i64 %619, 19
  %621 = mul i64 %619, 38
  %622 = shl i64 %619, 1
  %623 = load i64, i64* %18, align 8
  %624 = mul i64 %623, 19
  %625 = mul i64 %623, 38
  %626 = shl i64 %623, 1
  %627 = load i64, i64* %17, align 8
  %628 = shl i64 %627, 1
  %629 = load i64, i64* %16, align 8
  %630 = shl i64 %629, 1
  %631 = zext i64 %619 to i128
  %632 = zext i64 %620 to i128
  %633 = mul nuw i128 %632, %631
  %634 = zext i64 %623 to i128
  %635 = zext i64 %621 to i128
  %636 = mul nuw i128 %634, %635
  %637 = zext i64 %624 to i128
  %638 = mul nuw i128 %637, %634
  %639 = zext i64 %627 to i128
  %640 = mul nuw i128 %639, %635
  %641 = zext i64 %625 to i128
  %642 = mul nuw i128 %639, %641
  %643 = mul nuw i128 %639, %639
  %644 = zext i64 %629 to i128
  %645 = mul nuw i128 %644, %635
  %646 = zext i64 %626 to i128
  %647 = mul nuw i128 %644, %646
  %648 = zext i64 %628 to i128
  %649 = mul nuw i128 %644, %648
  %650 = mul nuw i128 %644, %644
  %651 = load i64, i64* %15, align 8
  %652 = zext i64 %651 to i128
  %653 = zext i64 %622 to i128
  %654 = mul nuw i128 %652, %653
  %655 = mul nuw i128 %652, %646
  %656 = mul nuw i128 %652, %648
  %657 = zext i64 %630 to i128
  %658 = mul nuw i128 %652, %657
  %659 = mul nuw i128 %652, %652
  %660 = add i128 %645, %642
  %661 = add i128 %660, %659
  %662 = lshr i128 %661, 51
  %663 = trunc i128 %661 to i64
  %664 = and i64 %663, 2251799813685247
  %665 = add i128 %647, %643
  %666 = add i128 %665, %654
  %667 = add i128 %649, %633
  %668 = add i128 %667, %655
  %669 = add i128 %650, %636
  %670 = add i128 %669, %656
  %671 = add i128 %640, %638
  %672 = add i128 %671, %658
  %673 = and i128 %662, 18446744073709551615
  %674 = add i128 %672, %673
  %675 = lshr i128 %674, 51
  %676 = trunc i128 %674 to i64
  %677 = and i64 %676, 2251799813685247
  %678 = and i128 %675, 18446744073709551615
  %679 = add i128 %670, %678
  %680 = lshr i128 %679, 51
  %681 = trunc i128 %679 to i64
  %682 = and i64 %681, 2251799813685247
  %683 = and i128 %680, 18446744073709551615
  %684 = add i128 %668, %683
  %685 = lshr i128 %684, 51
  %686 = trunc i128 %684 to i64
  %687 = and i64 %686, 2251799813685247
  %688 = and i128 %685, 18446744073709551615
  %689 = add i128 %666, %688
  %690 = lshr i128 %689, 51
  %691 = trunc i128 %690 to i64
  %692 = trunc i128 %689 to i64
  %693 = and i64 %692, 2251799813685247
  %694 = mul i64 %691, 19
  %695 = add i64 %694, %664
  %696 = lshr i64 %695, 51
  %697 = and i64 %695, 2251799813685247
  %698 = add nuw nsw i64 %696, %677
  %699 = lshr i64 %698, 51
  %700 = and i64 %698, 2251799813685247
  %701 = add nuw nsw i64 %699, %682
  store i64 %697, i64* %22, align 8
  store i64 %700, i64* %23, align 8
  store i64 %701, i64* %24, align 8
  store i64 %687, i64* %25, align 8
  store i64 %693, i64* %26, align 8
  br label %702

702:                                              ; preds = %702, %618
  %703 = phi i64 [ %697, %618 ], [ %782, %702 ]
  %704 = phi i64 [ %700, %618 ], [ %785, %702 ]
  %705 = phi i64 [ %701, %618 ], [ %786, %702 ]
  %706 = phi i64 [ %687, %618 ], [ %772, %702 ]
  %707 = phi i64 [ %693, %618 ], [ %778, %702 ]
  %708 = phi i32 [ 1, %618 ], [ %787, %702 ]
  %709 = mul nuw nsw i64 %707, 19
  %710 = mul nuw nsw i64 %707, 38
  %711 = shl nuw nsw i64 %707, 1
  %712 = mul nuw nsw i64 %706, 19
  %713 = mul nuw nsw i64 %706, 38
  %714 = shl nuw nsw i64 %706, 1
  %715 = shl nsw i64 %705, 1
  %716 = shl nuw nsw i64 %704, 1
  %717 = zext i64 %707 to i128
  %718 = zext i64 %709 to i128
  %719 = mul nuw nsw i128 %718, %717
  %720 = zext i64 %706 to i128
  %721 = zext i64 %710 to i128
  %722 = mul nuw nsw i128 %720, %721
  %723 = zext i64 %712 to i128
  %724 = mul nuw nsw i128 %723, %720
  %725 = zext i64 %705 to i128
  %726 = mul nuw nsw i128 %725, %721
  %727 = zext i64 %713 to i128
  %728 = mul nuw nsw i128 %725, %727
  %729 = mul nuw i128 %725, %725
  %730 = zext i64 %704 to i128
  %731 = mul nuw nsw i128 %730, %721
  %732 = zext i64 %714 to i128
  %733 = mul nuw nsw i128 %730, %732
  %734 = zext i64 %715 to i128
  %735 = mul nuw nsw i128 %730, %734
  %736 = mul nuw nsw i128 %730, %730
  %737 = zext i64 %703 to i128
  %738 = zext i64 %711 to i128
  %739 = mul nuw nsw i128 %737, %738
  %740 = mul nuw nsw i128 %737, %732
  %741 = mul nuw nsw i128 %737, %734
  %742 = zext i64 %716 to i128
  %743 = mul nuw nsw i128 %737, %742
  %744 = mul nuw nsw i128 %737, %737
  %745 = add nuw nsw i128 %731, %728
  %746 = add nuw nsw i128 %745, %744
  %747 = lshr i128 %746, 51
  %748 = trunc i128 %746 to i64
  %749 = and i64 %748, 2251799813685247
  %750 = add i128 %733, %729
  %751 = add i128 %750, %739
  %752 = add nuw nsw i128 %735, %719
  %753 = add nuw nsw i128 %752, %740
  %754 = add nuw nsw i128 %736, %722
  %755 = add nuw nsw i128 %754, %741
  %756 = add nuw nsw i128 %726, %724
  %757 = add nuw nsw i128 %756, %743
  %758 = and i128 %747, 18446744073709551615
  %759 = add nuw i128 %757, %758
  %760 = lshr i128 %759, 51
  %761 = trunc i128 %759 to i64
  %762 = and i64 %761, 2251799813685247
  %763 = and i128 %760, 18446744073709551615
  %764 = add nuw nsw i128 %755, %763
  %765 = lshr i128 %764, 51
  %766 = trunc i128 %764 to i64
  %767 = and i64 %766, 2251799813685247
  %768 = and i128 %765, 18446744073709551615
  %769 = add nuw nsw i128 %753, %768
  %770 = lshr i128 %769, 51
  %771 = trunc i128 %769 to i64
  %772 = and i64 %771, 2251799813685247
  %773 = and i128 %770, 18446744073709551615
  %774 = add i128 %751, %773
  %775 = lshr i128 %774, 51
  %776 = trunc i128 %775 to i64
  %777 = trunc i128 %774 to i64
  %778 = and i64 %777, 2251799813685247
  %779 = mul i64 %776, 19
  %780 = add i64 %779, %749
  %781 = lshr i64 %780, 51
  %782 = and i64 %780, 2251799813685247
  %783 = add nuw nsw i64 %781, %762
  %784 = lshr i64 %783, 51
  %785 = and i64 %783, 2251799813685247
  %786 = add nuw nsw i64 %784, %767
  %787 = add nuw nsw i32 %708, 1
  %788 = icmp eq i32 %787, 10
  br i1 %788, label %789, label %702

789:                                              ; preds = %702
  store i64 %778, i64* %26, align 8
  store i64 %772, i64* %25, align 8
  store i64 %786, i64* %24, align 8
  store i64 %785, i64* %23, align 8
  store i64 %782, i64* %22, align 8
  call fastcc void @fe_mul_impl(i64* nonnull %22, i64* nonnull %22, i64* nonnull %15) #4
  %790 = load i64, i64* %26, align 8
  %791 = mul i64 %790, 19
  %792 = mul i64 %790, 38
  %793 = shl i64 %790, 1
  %794 = load i64, i64* %25, align 8
  %795 = mul i64 %794, 19
  %796 = mul i64 %794, 38
  %797 = shl i64 %794, 1
  %798 = load i64, i64* %24, align 8
  %799 = shl i64 %798, 1
  %800 = load i64, i64* %23, align 8
  %801 = shl i64 %800, 1
  %802 = zext i64 %790 to i128
  %803 = zext i64 %791 to i128
  %804 = mul nuw i128 %803, %802
  %805 = zext i64 %794 to i128
  %806 = zext i64 %792 to i128
  %807 = mul nuw i128 %805, %806
  %808 = zext i64 %795 to i128
  %809 = mul nuw i128 %808, %805
  %810 = zext i64 %798 to i128
  %811 = mul nuw i128 %810, %806
  %812 = zext i64 %796 to i128
  %813 = mul nuw i128 %810, %812
  %814 = mul nuw i128 %810, %810
  %815 = zext i64 %800 to i128
  %816 = mul nuw i128 %815, %806
  %817 = zext i64 %797 to i128
  %818 = mul nuw i128 %815, %817
  %819 = zext i64 %799 to i128
  %820 = mul nuw i128 %815, %819
  %821 = mul nuw i128 %815, %815
  %822 = load i64, i64* %22, align 8
  %823 = zext i64 %822 to i128
  %824 = zext i64 %793 to i128
  %825 = mul nuw i128 %823, %824
  %826 = mul nuw i128 %823, %817
  %827 = mul nuw i128 %823, %819
  %828 = zext i64 %801 to i128
  %829 = mul nuw i128 %823, %828
  %830 = mul nuw i128 %823, %823
  %831 = add i128 %816, %813
  %832 = add i128 %831, %830
  %833 = lshr i128 %832, 51
  %834 = trunc i128 %832 to i64
  %835 = and i64 %834, 2251799813685247
  %836 = add i128 %818, %814
  %837 = add i128 %836, %825
  %838 = add i128 %820, %804
  %839 = add i128 %838, %826
  %840 = add i128 %821, %807
  %841 = add i128 %840, %827
  %842 = add i128 %811, %809
  %843 = add i128 %842, %829
  %844 = and i128 %833, 18446744073709551615
  %845 = add i128 %843, %844
  %846 = lshr i128 %845, 51
  %847 = trunc i128 %845 to i64
  %848 = and i64 %847, 2251799813685247
  %849 = and i128 %846, 18446744073709551615
  %850 = add i128 %841, %849
  %851 = lshr i128 %850, 51
  %852 = trunc i128 %850 to i64
  %853 = and i64 %852, 2251799813685247
  %854 = and i128 %851, 18446744073709551615
  %855 = add i128 %839, %854
  %856 = lshr i128 %855, 51
  %857 = trunc i128 %855 to i64
  %858 = and i64 %857, 2251799813685247
  %859 = and i128 %856, 18446744073709551615
  %860 = add i128 %837, %859
  %861 = lshr i128 %860, 51
  %862 = trunc i128 %861 to i64
  %863 = trunc i128 %860 to i64
  %864 = and i64 %863, 2251799813685247
  %865 = mul i64 %862, 19
  %866 = add i64 %865, %835
  %867 = lshr i64 %866, 51
  %868 = and i64 %866, 2251799813685247
  %869 = add nuw nsw i64 %867, %848
  %870 = lshr i64 %869, 51
  %871 = and i64 %869, 2251799813685247
  %872 = add nuw nsw i64 %870, %853
  store i64 %868, i64* %29, align 8
  store i64 %871, i64* %30, align 8
  store i64 %872, i64* %31, align 8
  store i64 %858, i64* %32, align 8
  store i64 %864, i64* %33, align 8
  br label %873

873:                                              ; preds = %873, %789
  %874 = phi i64 [ %868, %789 ], [ %953, %873 ]
  %875 = phi i64 [ %871, %789 ], [ %956, %873 ]
  %876 = phi i64 [ %872, %789 ], [ %957, %873 ]
  %877 = phi i64 [ %858, %789 ], [ %943, %873 ]
  %878 = phi i64 [ %864, %789 ], [ %949, %873 ]
  %879 = phi i32 [ 1, %789 ], [ %958, %873 ]
  %880 = mul nuw nsw i64 %878, 19
  %881 = mul nuw nsw i64 %878, 38
  %882 = shl nuw nsw i64 %878, 1
  %883 = mul nuw nsw i64 %877, 19
  %884 = mul nuw nsw i64 %877, 38
  %885 = shl nuw nsw i64 %877, 1
  %886 = shl nsw i64 %876, 1
  %887 = shl nuw nsw i64 %875, 1
  %888 = zext i64 %878 to i128
  %889 = zext i64 %880 to i128
  %890 = mul nuw nsw i128 %889, %888
  %891 = zext i64 %877 to i128
  %892 = zext i64 %881 to i128
  %893 = mul nuw nsw i128 %891, %892
  %894 = zext i64 %883 to i128
  %895 = mul nuw nsw i128 %894, %891
  %896 = zext i64 %876 to i128
  %897 = mul nuw nsw i128 %896, %892
  %898 = zext i64 %884 to i128
  %899 = mul nuw nsw i128 %896, %898
  %900 = mul nuw i128 %896, %896
  %901 = zext i64 %875 to i128
  %902 = mul nuw nsw i128 %901, %892
  %903 = zext i64 %885 to i128
  %904 = mul nuw nsw i128 %901, %903
  %905 = zext i64 %886 to i128
  %906 = mul nuw nsw i128 %901, %905
  %907 = mul nuw nsw i128 %901, %901
  %908 = zext i64 %874 to i128
  %909 = zext i64 %882 to i128
  %910 = mul nuw nsw i128 %908, %909
  %911 = mul nuw nsw i128 %908, %903
  %912 = mul nuw nsw i128 %908, %905
  %913 = zext i64 %887 to i128
  %914 = mul nuw nsw i128 %908, %913
  %915 = mul nuw nsw i128 %908, %908
  %916 = add nuw nsw i128 %902, %899
  %917 = add nuw nsw i128 %916, %915
  %918 = lshr i128 %917, 51
  %919 = trunc i128 %917 to i64
  %920 = and i64 %919, 2251799813685247
  %921 = add i128 %904, %900
  %922 = add i128 %921, %910
  %923 = add nuw nsw i128 %906, %890
  %924 = add nuw nsw i128 %923, %911
  %925 = add nuw nsw i128 %907, %893
  %926 = add nuw nsw i128 %925, %912
  %927 = add nuw nsw i128 %897, %895
  %928 = add nuw nsw i128 %927, %914
  %929 = and i128 %918, 18446744073709551615
  %930 = add nuw i128 %928, %929
  %931 = lshr i128 %930, 51
  %932 = trunc i128 %930 to i64
  %933 = and i64 %932, 2251799813685247
  %934 = and i128 %931, 18446744073709551615
  %935 = add nuw nsw i128 %926, %934
  %936 = lshr i128 %935, 51
  %937 = trunc i128 %935 to i64
  %938 = and i64 %937, 2251799813685247
  %939 = and i128 %936, 18446744073709551615
  %940 = add nuw nsw i128 %924, %939
  %941 = lshr i128 %940, 51
  %942 = trunc i128 %940 to i64
  %943 = and i64 %942, 2251799813685247
  %944 = and i128 %941, 18446744073709551615
  %945 = add i128 %922, %944
  %946 = lshr i128 %945, 51
  %947 = trunc i128 %946 to i64
  %948 = trunc i128 %945 to i64
  %949 = and i64 %948, 2251799813685247
  %950 = mul i64 %947, 19
  %951 = add i64 %950, %920
  %952 = lshr i64 %951, 51
  %953 = and i64 %951, 2251799813685247
  %954 = add nuw nsw i64 %952, %933
  %955 = lshr i64 %954, 51
  %956 = and i64 %954, 2251799813685247
  %957 = add nuw nsw i64 %955, %938
  %958 = add nuw nsw i32 %879, 1
  %959 = icmp eq i32 %958, 20
  br i1 %959, label %960, label %873

960:                                              ; preds = %873
  store i64 %949, i64* %33, align 8
  store i64 %943, i64* %32, align 8
  store i64 %957, i64* %31, align 8
  store i64 %956, i64* %30, align 8
  store i64 %953, i64* %29, align 8
  call fastcc void @fe_mul_impl(i64* nonnull %22, i64* nonnull %29, i64* nonnull %22) #4
  %961 = load i64, i64* %26, align 8
  %962 = mul i64 %961, 19
  %963 = mul i64 %961, 38
  %964 = shl i64 %961, 1
  %965 = load i64, i64* %25, align 8
  %966 = mul i64 %965, 19
  %967 = mul i64 %965, 38
  %968 = shl i64 %965, 1
  %969 = load i64, i64* %24, align 8
  %970 = shl i64 %969, 1
  %971 = load i64, i64* %23, align 8
  %972 = shl i64 %971, 1
  %973 = zext i64 %961 to i128
  %974 = zext i64 %962 to i128
  %975 = mul nuw i128 %974, %973
  %976 = zext i64 %965 to i128
  %977 = zext i64 %963 to i128
  %978 = mul nuw i128 %976, %977
  %979 = zext i64 %966 to i128
  %980 = mul nuw i128 %979, %976
  %981 = zext i64 %969 to i128
  %982 = mul nuw i128 %981, %977
  %983 = zext i64 %967 to i128
  %984 = mul nuw i128 %981, %983
  %985 = mul nuw i128 %981, %981
  %986 = zext i64 %971 to i128
  %987 = mul nuw i128 %986, %977
  %988 = zext i64 %968 to i128
  %989 = mul nuw i128 %986, %988
  %990 = zext i64 %970 to i128
  %991 = mul nuw i128 %986, %990
  %992 = mul nuw i128 %986, %986
  %993 = load i64, i64* %22, align 8
  %994 = zext i64 %993 to i128
  %995 = zext i64 %964 to i128
  %996 = mul nuw i128 %994, %995
  %997 = mul nuw i128 %994, %988
  %998 = mul nuw i128 %994, %990
  %999 = zext i64 %972 to i128
  %1000 = mul nuw i128 %994, %999
  %1001 = mul nuw i128 %994, %994
  %1002 = add i128 %987, %984
  %1003 = add i128 %1002, %1001
  %1004 = lshr i128 %1003, 51
  %1005 = trunc i128 %1003 to i64
  %1006 = and i64 %1005, 2251799813685247
  %1007 = add i128 %989, %985
  %1008 = add i128 %1007, %996
  %1009 = add i128 %991, %975
  %1010 = add i128 %1009, %997
  %1011 = add i128 %992, %978
  %1012 = add i128 %1011, %998
  %1013 = add i128 %982, %980
  %1014 = add i128 %1013, %1000
  %1015 = and i128 %1004, 18446744073709551615
  %1016 = add i128 %1014, %1015
  %1017 = lshr i128 %1016, 51
  %1018 = trunc i128 %1016 to i64
  %1019 = and i64 %1018, 2251799813685247
  %1020 = and i128 %1017, 18446744073709551615
  %1021 = add i128 %1012, %1020
  %1022 = lshr i128 %1021, 51
  %1023 = trunc i128 %1021 to i64
  %1024 = and i64 %1023, 2251799813685247
  %1025 = and i128 %1022, 18446744073709551615
  %1026 = add i128 %1010, %1025
  %1027 = lshr i128 %1026, 51
  %1028 = trunc i128 %1026 to i64
  %1029 = and i64 %1028, 2251799813685247
  %1030 = and i128 %1027, 18446744073709551615
  %1031 = add i128 %1008, %1030
  %1032 = lshr i128 %1031, 51
  %1033 = trunc i128 %1032 to i64
  %1034 = trunc i128 %1031 to i64
  %1035 = and i64 %1034, 2251799813685247
  %1036 = mul i64 %1033, 19
  %1037 = add i64 %1036, %1006
  %1038 = lshr i64 %1037, 51
  %1039 = and i64 %1037, 2251799813685247
  %1040 = add nuw nsw i64 %1038, %1019
  %1041 = lshr i64 %1040, 51
  %1042 = and i64 %1040, 2251799813685247
  %1043 = add nuw nsw i64 %1041, %1024
  store i64 %1039, i64* %22, align 8
  store i64 %1042, i64* %23, align 8
  store i64 %1043, i64* %24, align 8
  store i64 %1029, i64* %25, align 8
  store i64 %1035, i64* %26, align 8
  br label %1044

1044:                                             ; preds = %1044, %960
  %1045 = phi i64 [ %1039, %960 ], [ %1124, %1044 ]
  %1046 = phi i64 [ %1042, %960 ], [ %1127, %1044 ]
  %1047 = phi i64 [ %1043, %960 ], [ %1128, %1044 ]
  %1048 = phi i64 [ %1029, %960 ], [ %1114, %1044 ]
  %1049 = phi i64 [ %1035, %960 ], [ %1120, %1044 ]
  %1050 = phi i32 [ 1, %960 ], [ %1129, %1044 ]
  %1051 = mul nuw nsw i64 %1049, 19
  %1052 = mul nuw nsw i64 %1049, 38
  %1053 = shl nuw nsw i64 %1049, 1
  %1054 = mul nuw nsw i64 %1048, 19
  %1055 = mul nuw nsw i64 %1048, 38
  %1056 = shl nuw nsw i64 %1048, 1
  %1057 = shl nsw i64 %1047, 1
  %1058 = shl nuw nsw i64 %1046, 1
  %1059 = zext i64 %1049 to i128
  %1060 = zext i64 %1051 to i128
  %1061 = mul nuw nsw i128 %1060, %1059
  %1062 = zext i64 %1048 to i128
  %1063 = zext i64 %1052 to i128
  %1064 = mul nuw nsw i128 %1062, %1063
  %1065 = zext i64 %1054 to i128
  %1066 = mul nuw nsw i128 %1065, %1062
  %1067 = zext i64 %1047 to i128
  %1068 = mul nuw nsw i128 %1067, %1063
  %1069 = zext i64 %1055 to i128
  %1070 = mul nuw nsw i128 %1067, %1069
  %1071 = mul nuw i128 %1067, %1067
  %1072 = zext i64 %1046 to i128
  %1073 = mul nuw nsw i128 %1072, %1063
  %1074 = zext i64 %1056 to i128
  %1075 = mul nuw nsw i128 %1072, %1074
  %1076 = zext i64 %1057 to i128
  %1077 = mul nuw nsw i128 %1072, %1076
  %1078 = mul nuw nsw i128 %1072, %1072
  %1079 = zext i64 %1045 to i128
  %1080 = zext i64 %1053 to i128
  %1081 = mul nuw nsw i128 %1079, %1080
  %1082 = mul nuw nsw i128 %1079, %1074
  %1083 = mul nuw nsw i128 %1079, %1076
  %1084 = zext i64 %1058 to i128
  %1085 = mul nuw nsw i128 %1079, %1084
  %1086 = mul nuw nsw i128 %1079, %1079
  %1087 = add nuw nsw i128 %1073, %1070
  %1088 = add nuw nsw i128 %1087, %1086
  %1089 = lshr i128 %1088, 51
  %1090 = trunc i128 %1088 to i64
  %1091 = and i64 %1090, 2251799813685247
  %1092 = add i128 %1075, %1071
  %1093 = add i128 %1092, %1081
  %1094 = add nuw nsw i128 %1077, %1061
  %1095 = add nuw nsw i128 %1094, %1082
  %1096 = add nuw nsw i128 %1078, %1064
  %1097 = add nuw nsw i128 %1096, %1083
  %1098 = add nuw nsw i128 %1068, %1066
  %1099 = add nuw nsw i128 %1098, %1085
  %1100 = and i128 %1089, 18446744073709551615
  %1101 = add nuw i128 %1099, %1100
  %1102 = lshr i128 %1101, 51
  %1103 = trunc i128 %1101 to i64
  %1104 = and i64 %1103, 2251799813685247
  %1105 = and i128 %1102, 18446744073709551615
  %1106 = add nuw nsw i128 %1097, %1105
  %1107 = lshr i128 %1106, 51
  %1108 = trunc i128 %1106 to i64
  %1109 = and i64 %1108, 2251799813685247
  %1110 = and i128 %1107, 18446744073709551615
  %1111 = add nuw nsw i128 %1095, %1110
  %1112 = lshr i128 %1111, 51
  %1113 = trunc i128 %1111 to i64
  %1114 = and i64 %1113, 2251799813685247
  %1115 = and i128 %1112, 18446744073709551615
  %1116 = add i128 %1093, %1115
  %1117 = lshr i128 %1116, 51
  %1118 = trunc i128 %1117 to i64
  %1119 = trunc i128 %1116 to i64
  %1120 = and i64 %1119, 2251799813685247
  %1121 = mul i64 %1118, 19
  %1122 = add i64 %1121, %1091
  %1123 = lshr i64 %1122, 51
  %1124 = and i64 %1122, 2251799813685247
  %1125 = add nuw nsw i64 %1123, %1104
  %1126 = lshr i64 %1125, 51
  %1127 = and i64 %1125, 2251799813685247
  %1128 = add nuw nsw i64 %1126, %1109
  %1129 = add nuw nsw i32 %1050, 1
  %1130 = icmp eq i32 %1129, 10
  br i1 %1130, label %1131, label %1044

1131:                                             ; preds = %1044
  store i64 %1120, i64* %26, align 8
  store i64 %1114, i64* %25, align 8
  store i64 %1128, i64* %24, align 8
  store i64 %1127, i64* %23, align 8
  store i64 %1124, i64* %22, align 8
  call fastcc void @fe_mul_impl(i64* nonnull %15, i64* nonnull %22, i64* nonnull %15) #4
  %1132 = load i64, i64* %19, align 8
  %1133 = mul i64 %1132, 19
  %1134 = mul i64 %1132, 38
  %1135 = shl i64 %1132, 1
  %1136 = load i64, i64* %18, align 8
  %1137 = mul i64 %1136, 19
  %1138 = mul i64 %1136, 38
  %1139 = shl i64 %1136, 1
  %1140 = load i64, i64* %17, align 8
  %1141 = shl i64 %1140, 1
  %1142 = load i64, i64* %16, align 8
  %1143 = shl i64 %1142, 1
  %1144 = zext i64 %1132 to i128
  %1145 = zext i64 %1133 to i128
  %1146 = mul nuw i128 %1145, %1144
  %1147 = zext i64 %1136 to i128
  %1148 = zext i64 %1134 to i128
  %1149 = mul nuw i128 %1147, %1148
  %1150 = zext i64 %1137 to i128
  %1151 = mul nuw i128 %1150, %1147
  %1152 = zext i64 %1140 to i128
  %1153 = mul nuw i128 %1152, %1148
  %1154 = zext i64 %1138 to i128
  %1155 = mul nuw i128 %1152, %1154
  %1156 = mul nuw i128 %1152, %1152
  %1157 = zext i64 %1142 to i128
  %1158 = mul nuw i128 %1157, %1148
  %1159 = zext i64 %1139 to i128
  %1160 = mul nuw i128 %1157, %1159
  %1161 = zext i64 %1141 to i128
  %1162 = mul nuw i128 %1157, %1161
  %1163 = mul nuw i128 %1157, %1157
  %1164 = load i64, i64* %15, align 8
  %1165 = zext i64 %1164 to i128
  %1166 = zext i64 %1135 to i128
  %1167 = mul nuw i128 %1165, %1166
  %1168 = mul nuw i128 %1165, %1159
  %1169 = mul nuw i128 %1165, %1161
  %1170 = zext i64 %1143 to i128
  %1171 = mul nuw i128 %1165, %1170
  %1172 = mul nuw i128 %1165, %1165
  %1173 = add i128 %1158, %1155
  %1174 = add i128 %1173, %1172
  %1175 = lshr i128 %1174, 51
  %1176 = trunc i128 %1174 to i64
  %1177 = and i64 %1176, 2251799813685247
  %1178 = add i128 %1160, %1156
  %1179 = add i128 %1178, %1167
  %1180 = add i128 %1162, %1146
  %1181 = add i128 %1180, %1168
  %1182 = add i128 %1163, %1149
  %1183 = add i128 %1182, %1169
  %1184 = add i128 %1153, %1151
  %1185 = add i128 %1184, %1171
  %1186 = and i128 %1175, 18446744073709551615
  %1187 = add i128 %1185, %1186
  %1188 = lshr i128 %1187, 51
  %1189 = trunc i128 %1187 to i64
  %1190 = and i64 %1189, 2251799813685247
  %1191 = and i128 %1188, 18446744073709551615
  %1192 = add i128 %1183, %1191
  %1193 = lshr i128 %1192, 51
  %1194 = trunc i128 %1192 to i64
  %1195 = and i64 %1194, 2251799813685247
  %1196 = and i128 %1193, 18446744073709551615
  %1197 = add i128 %1181, %1196
  %1198 = lshr i128 %1197, 51
  %1199 = trunc i128 %1197 to i64
  %1200 = and i64 %1199, 2251799813685247
  %1201 = and i128 %1198, 18446744073709551615
  %1202 = add i128 %1179, %1201
  %1203 = lshr i128 %1202, 51
  %1204 = trunc i128 %1203 to i64
  %1205 = trunc i128 %1202 to i64
  %1206 = and i64 %1205, 2251799813685247
  %1207 = mul i64 %1204, 19
  %1208 = add i64 %1207, %1177
  %1209 = lshr i64 %1208, 51
  %1210 = and i64 %1208, 2251799813685247
  %1211 = add nuw nsw i64 %1209, %1190
  %1212 = lshr i64 %1211, 51
  %1213 = and i64 %1211, 2251799813685247
  %1214 = add nuw nsw i64 %1212, %1195
  store i64 %1210, i64* %22, align 8
  store i64 %1213, i64* %23, align 8
  store i64 %1214, i64* %24, align 8
  store i64 %1200, i64* %25, align 8
  store i64 %1206, i64* %26, align 8
  br label %1215

1215:                                             ; preds = %1215, %1131
  %1216 = phi i64 [ %1210, %1131 ], [ %1295, %1215 ]
  %1217 = phi i64 [ %1213, %1131 ], [ %1298, %1215 ]
  %1218 = phi i64 [ %1214, %1131 ], [ %1299, %1215 ]
  %1219 = phi i64 [ %1200, %1131 ], [ %1285, %1215 ]
  %1220 = phi i64 [ %1206, %1131 ], [ %1291, %1215 ]
  %1221 = phi i32 [ 1, %1131 ], [ %1300, %1215 ]
  %1222 = mul nuw nsw i64 %1220, 19
  %1223 = mul nuw nsw i64 %1220, 38
  %1224 = shl nuw nsw i64 %1220, 1
  %1225 = mul nuw nsw i64 %1219, 19
  %1226 = mul nuw nsw i64 %1219, 38
  %1227 = shl nuw nsw i64 %1219, 1
  %1228 = shl nsw i64 %1218, 1
  %1229 = shl nuw nsw i64 %1217, 1
  %1230 = zext i64 %1220 to i128
  %1231 = zext i64 %1222 to i128
  %1232 = mul nuw nsw i128 %1231, %1230
  %1233 = zext i64 %1219 to i128
  %1234 = zext i64 %1223 to i128
  %1235 = mul nuw nsw i128 %1233, %1234
  %1236 = zext i64 %1225 to i128
  %1237 = mul nuw nsw i128 %1236, %1233
  %1238 = zext i64 %1218 to i128
  %1239 = mul nuw nsw i128 %1238, %1234
  %1240 = zext i64 %1226 to i128
  %1241 = mul nuw nsw i128 %1238, %1240
  %1242 = mul nuw i128 %1238, %1238
  %1243 = zext i64 %1217 to i128
  %1244 = mul nuw nsw i128 %1243, %1234
  %1245 = zext i64 %1227 to i128
  %1246 = mul nuw nsw i128 %1243, %1245
  %1247 = zext i64 %1228 to i128
  %1248 = mul nuw nsw i128 %1243, %1247
  %1249 = mul nuw nsw i128 %1243, %1243
  %1250 = zext i64 %1216 to i128
  %1251 = zext i64 %1224 to i128
  %1252 = mul nuw nsw i128 %1250, %1251
  %1253 = mul nuw nsw i128 %1250, %1245
  %1254 = mul nuw nsw i128 %1250, %1247
  %1255 = zext i64 %1229 to i128
  %1256 = mul nuw nsw i128 %1250, %1255
  %1257 = mul nuw nsw i128 %1250, %1250
  %1258 = add nuw nsw i128 %1244, %1241
  %1259 = add nuw nsw i128 %1258, %1257
  %1260 = lshr i128 %1259, 51
  %1261 = trunc i128 %1259 to i64
  %1262 = and i64 %1261, 2251799813685247
  %1263 = add i128 %1246, %1242
  %1264 = add i128 %1263, %1252
  %1265 = add nuw nsw i128 %1248, %1232
  %1266 = add nuw nsw i128 %1265, %1253
  %1267 = add nuw nsw i128 %1249, %1235
  %1268 = add nuw nsw i128 %1267, %1254
  %1269 = add nuw nsw i128 %1239, %1237
  %1270 = add nuw nsw i128 %1269, %1256
  %1271 = and i128 %1260, 18446744073709551615
  %1272 = add nuw i128 %1270, %1271
  %1273 = lshr i128 %1272, 51
  %1274 = trunc i128 %1272 to i64
  %1275 = and i64 %1274, 2251799813685247
  %1276 = and i128 %1273, 18446744073709551615
  %1277 = add nuw nsw i128 %1268, %1276
  %1278 = lshr i128 %1277, 51
  %1279 = trunc i128 %1277 to i64
  %1280 = and i64 %1279, 2251799813685247
  %1281 = and i128 %1278, 18446744073709551615
  %1282 = add nuw nsw i128 %1266, %1281
  %1283 = lshr i128 %1282, 51
  %1284 = trunc i128 %1282 to i64
  %1285 = and i64 %1284, 2251799813685247
  %1286 = and i128 %1283, 18446744073709551615
  %1287 = add i128 %1264, %1286
  %1288 = lshr i128 %1287, 51
  %1289 = trunc i128 %1288 to i64
  %1290 = trunc i128 %1287 to i64
  %1291 = and i64 %1290, 2251799813685247
  %1292 = mul i64 %1289, 19
  %1293 = add i64 %1292, %1262
  %1294 = lshr i64 %1293, 51
  %1295 = and i64 %1293, 2251799813685247
  %1296 = add nuw nsw i64 %1294, %1275
  %1297 = lshr i64 %1296, 51
  %1298 = and i64 %1296, 2251799813685247
  %1299 = add nuw nsw i64 %1297, %1280
  %1300 = add nuw nsw i32 %1221, 1
  %1301 = icmp eq i32 %1300, 50
  br i1 %1301, label %1302, label %1215

1302:                                             ; preds = %1215
  store i64 %1291, i64* %26, align 8
  store i64 %1285, i64* %25, align 8
  store i64 %1299, i64* %24, align 8
  store i64 %1298, i64* %23, align 8
  store i64 %1295, i64* %22, align 8
  call fastcc void @fe_mul_impl(i64* nonnull %22, i64* nonnull %22, i64* nonnull %15) #4
  %1303 = load i64, i64* %26, align 8
  %1304 = mul i64 %1303, 19
  %1305 = mul i64 %1303, 38
  %1306 = shl i64 %1303, 1
  %1307 = load i64, i64* %25, align 8
  %1308 = mul i64 %1307, 19
  %1309 = mul i64 %1307, 38
  %1310 = shl i64 %1307, 1
  %1311 = load i64, i64* %24, align 8
  %1312 = shl i64 %1311, 1
  %1313 = load i64, i64* %23, align 8
  %1314 = shl i64 %1313, 1
  %1315 = zext i64 %1303 to i128
  %1316 = zext i64 %1304 to i128
  %1317 = mul nuw i128 %1316, %1315
  %1318 = zext i64 %1307 to i128
  %1319 = zext i64 %1305 to i128
  %1320 = mul nuw i128 %1318, %1319
  %1321 = zext i64 %1308 to i128
  %1322 = mul nuw i128 %1321, %1318
  %1323 = zext i64 %1311 to i128
  %1324 = mul nuw i128 %1323, %1319
  %1325 = zext i64 %1309 to i128
  %1326 = mul nuw i128 %1323, %1325
  %1327 = mul nuw i128 %1323, %1323
  %1328 = zext i64 %1313 to i128
  %1329 = mul nuw i128 %1328, %1319
  %1330 = zext i64 %1310 to i128
  %1331 = mul nuw i128 %1328, %1330
  %1332 = zext i64 %1312 to i128
  %1333 = mul nuw i128 %1328, %1332
  %1334 = mul nuw i128 %1328, %1328
  %1335 = load i64, i64* %22, align 8
  %1336 = zext i64 %1335 to i128
  %1337 = zext i64 %1306 to i128
  %1338 = mul nuw i128 %1336, %1337
  %1339 = mul nuw i128 %1336, %1330
  %1340 = mul nuw i128 %1336, %1332
  %1341 = zext i64 %1314 to i128
  %1342 = mul nuw i128 %1336, %1341
  %1343 = mul nuw i128 %1336, %1336
  %1344 = add i128 %1329, %1326
  %1345 = add i128 %1344, %1343
  %1346 = lshr i128 %1345, 51
  %1347 = trunc i128 %1345 to i64
  %1348 = and i64 %1347, 2251799813685247
  %1349 = add i128 %1331, %1327
  %1350 = add i128 %1349, %1338
  %1351 = add i128 %1333, %1317
  %1352 = add i128 %1351, %1339
  %1353 = add i128 %1334, %1320
  %1354 = add i128 %1353, %1340
  %1355 = add i128 %1324, %1322
  %1356 = add i128 %1355, %1342
  %1357 = and i128 %1346, 18446744073709551615
  %1358 = add i128 %1356, %1357
  %1359 = lshr i128 %1358, 51
  %1360 = trunc i128 %1358 to i64
  %1361 = and i64 %1360, 2251799813685247
  %1362 = and i128 %1359, 18446744073709551615
  %1363 = add i128 %1354, %1362
  %1364 = lshr i128 %1363, 51
  %1365 = trunc i128 %1363 to i64
  %1366 = and i64 %1365, 2251799813685247
  %1367 = and i128 %1364, 18446744073709551615
  %1368 = add i128 %1352, %1367
  %1369 = lshr i128 %1368, 51
  %1370 = trunc i128 %1368 to i64
  %1371 = and i64 %1370, 2251799813685247
  %1372 = and i128 %1369, 18446744073709551615
  %1373 = add i128 %1350, %1372
  %1374 = lshr i128 %1373, 51
  %1375 = trunc i128 %1374 to i64
  %1376 = trunc i128 %1373 to i64
  %1377 = and i64 %1376, 2251799813685247
  %1378 = mul i64 %1375, 19
  %1379 = add i64 %1378, %1348
  %1380 = lshr i64 %1379, 51
  %1381 = and i64 %1379, 2251799813685247
  %1382 = add nuw nsw i64 %1380, %1361
  %1383 = lshr i64 %1382, 51
  %1384 = and i64 %1382, 2251799813685247
  %1385 = add nuw nsw i64 %1383, %1366
  store i64 %1381, i64* %29, align 8
  store i64 %1384, i64* %30, align 8
  store i64 %1385, i64* %31, align 8
  store i64 %1371, i64* %32, align 8
  store i64 %1377, i64* %33, align 8
  br label %1386

1386:                                             ; preds = %1386, %1302
  %1387 = phi i64 [ %1381, %1302 ], [ %1466, %1386 ]
  %1388 = phi i64 [ %1384, %1302 ], [ %1469, %1386 ]
  %1389 = phi i64 [ %1385, %1302 ], [ %1470, %1386 ]
  %1390 = phi i64 [ %1371, %1302 ], [ %1456, %1386 ]
  %1391 = phi i64 [ %1377, %1302 ], [ %1462, %1386 ]
  %1392 = phi i32 [ 1, %1302 ], [ %1471, %1386 ]
  %1393 = mul nuw nsw i64 %1391, 19
  %1394 = mul nuw nsw i64 %1391, 38
  %1395 = shl nuw nsw i64 %1391, 1
  %1396 = mul nuw nsw i64 %1390, 19
  %1397 = mul nuw nsw i64 %1390, 38
  %1398 = shl nuw nsw i64 %1390, 1
  %1399 = shl nsw i64 %1389, 1
  %1400 = shl nuw nsw i64 %1388, 1
  %1401 = zext i64 %1391 to i128
  %1402 = zext i64 %1393 to i128
  %1403 = mul nuw nsw i128 %1402, %1401
  %1404 = zext i64 %1390 to i128
  %1405 = zext i64 %1394 to i128
  %1406 = mul nuw nsw i128 %1404, %1405
  %1407 = zext i64 %1396 to i128
  %1408 = mul nuw nsw i128 %1407, %1404
  %1409 = zext i64 %1389 to i128
  %1410 = mul nuw nsw i128 %1409, %1405
  %1411 = zext i64 %1397 to i128
  %1412 = mul nuw nsw i128 %1409, %1411
  %1413 = mul nuw i128 %1409, %1409
  %1414 = zext i64 %1388 to i128
  %1415 = mul nuw nsw i128 %1414, %1405
  %1416 = zext i64 %1398 to i128
  %1417 = mul nuw nsw i128 %1414, %1416
  %1418 = zext i64 %1399 to i128
  %1419 = mul nuw nsw i128 %1414, %1418
  %1420 = mul nuw nsw i128 %1414, %1414
  %1421 = zext i64 %1387 to i128
  %1422 = zext i64 %1395 to i128
  %1423 = mul nuw nsw i128 %1421, %1422
  %1424 = mul nuw nsw i128 %1421, %1416
  %1425 = mul nuw nsw i128 %1421, %1418
  %1426 = zext i64 %1400 to i128
  %1427 = mul nuw nsw i128 %1421, %1426
  %1428 = mul nuw nsw i128 %1421, %1421
  %1429 = add nuw nsw i128 %1415, %1412
  %1430 = add nuw nsw i128 %1429, %1428
  %1431 = lshr i128 %1430, 51
  %1432 = trunc i128 %1430 to i64
  %1433 = and i64 %1432, 2251799813685247
  %1434 = add i128 %1417, %1413
  %1435 = add i128 %1434, %1423
  %1436 = add nuw nsw i128 %1419, %1403
  %1437 = add nuw nsw i128 %1436, %1424
  %1438 = add nuw nsw i128 %1420, %1406
  %1439 = add nuw nsw i128 %1438, %1425
  %1440 = add nuw nsw i128 %1410, %1408
  %1441 = add nuw nsw i128 %1440, %1427
  %1442 = and i128 %1431, 18446744073709551615
  %1443 = add nuw i128 %1441, %1442
  %1444 = lshr i128 %1443, 51
  %1445 = trunc i128 %1443 to i64
  %1446 = and i64 %1445, 2251799813685247
  %1447 = and i128 %1444, 18446744073709551615
  %1448 = add nuw nsw i128 %1439, %1447
  %1449 = lshr i128 %1448, 51
  %1450 = trunc i128 %1448 to i64
  %1451 = and i64 %1450, 2251799813685247
  %1452 = and i128 %1449, 18446744073709551615
  %1453 = add nuw nsw i128 %1437, %1452
  %1454 = lshr i128 %1453, 51
  %1455 = trunc i128 %1453 to i64
  %1456 = and i64 %1455, 2251799813685247
  %1457 = and i128 %1454, 18446744073709551615
  %1458 = add i128 %1435, %1457
  %1459 = lshr i128 %1458, 51
  %1460 = trunc i128 %1459 to i64
  %1461 = trunc i128 %1458 to i64
  %1462 = and i64 %1461, 2251799813685247
  %1463 = mul i64 %1460, 19
  %1464 = add i64 %1463, %1433
  %1465 = lshr i64 %1464, 51
  %1466 = and i64 %1464, 2251799813685247
  %1467 = add nuw nsw i64 %1465, %1446
  %1468 = lshr i64 %1467, 51
  %1469 = and i64 %1467, 2251799813685247
  %1470 = add nuw nsw i64 %1468, %1451
  %1471 = add nuw nsw i32 %1392, 1
  %1472 = icmp eq i32 %1471, 100
  br i1 %1472, label %1473, label %1386

1473:                                             ; preds = %1386
  store i64 %1462, i64* %33, align 8
  store i64 %1456, i64* %32, align 8
  store i64 %1470, i64* %31, align 8
  store i64 %1469, i64* %30, align 8
  store i64 %1466, i64* %29, align 8
  call fastcc void @fe_mul_impl(i64* nonnull %22, i64* nonnull %29, i64* nonnull %22) #4
  %1474 = load i64, i64* %26, align 8
  %1475 = mul i64 %1474, 19
  %1476 = mul i64 %1474, 38
  %1477 = shl i64 %1474, 1
  %1478 = load i64, i64* %25, align 8
  %1479 = mul i64 %1478, 19
  %1480 = mul i64 %1478, 38
  %1481 = shl i64 %1478, 1
  %1482 = load i64, i64* %24, align 8
  %1483 = shl i64 %1482, 1
  %1484 = load i64, i64* %23, align 8
  %1485 = shl i64 %1484, 1
  %1486 = zext i64 %1474 to i128
  %1487 = zext i64 %1475 to i128
  %1488 = mul nuw i128 %1487, %1486
  %1489 = zext i64 %1478 to i128
  %1490 = zext i64 %1476 to i128
  %1491 = mul nuw i128 %1489, %1490
  %1492 = zext i64 %1479 to i128
  %1493 = mul nuw i128 %1492, %1489
  %1494 = zext i64 %1482 to i128
  %1495 = mul nuw i128 %1494, %1490
  %1496 = zext i64 %1480 to i128
  %1497 = mul nuw i128 %1494, %1496
  %1498 = mul nuw i128 %1494, %1494
  %1499 = zext i64 %1484 to i128
  %1500 = mul nuw i128 %1499, %1490
  %1501 = zext i64 %1481 to i128
  %1502 = mul nuw i128 %1499, %1501
  %1503 = zext i64 %1483 to i128
  %1504 = mul nuw i128 %1499, %1503
  %1505 = mul nuw i128 %1499, %1499
  %1506 = load i64, i64* %22, align 8
  %1507 = zext i64 %1506 to i128
  %1508 = zext i64 %1477 to i128
  %1509 = mul nuw i128 %1507, %1508
  %1510 = mul nuw i128 %1507, %1501
  %1511 = mul nuw i128 %1507, %1503
  %1512 = zext i64 %1485 to i128
  %1513 = mul nuw i128 %1507, %1512
  %1514 = mul nuw i128 %1507, %1507
  %1515 = add i128 %1500, %1497
  %1516 = add i128 %1515, %1514
  %1517 = lshr i128 %1516, 51
  %1518 = trunc i128 %1516 to i64
  %1519 = and i64 %1518, 2251799813685247
  %1520 = add i128 %1502, %1498
  %1521 = add i128 %1520, %1509
  %1522 = add i128 %1504, %1488
  %1523 = add i128 %1522, %1510
  %1524 = add i128 %1505, %1491
  %1525 = add i128 %1524, %1511
  %1526 = add i128 %1495, %1493
  %1527 = add i128 %1526, %1513
  %1528 = and i128 %1517, 18446744073709551615
  %1529 = add i128 %1527, %1528
  %1530 = lshr i128 %1529, 51
  %1531 = trunc i128 %1529 to i64
  %1532 = and i64 %1531, 2251799813685247
  %1533 = and i128 %1530, 18446744073709551615
  %1534 = add i128 %1525, %1533
  %1535 = lshr i128 %1534, 51
  %1536 = trunc i128 %1534 to i64
  %1537 = and i64 %1536, 2251799813685247
  %1538 = and i128 %1535, 18446744073709551615
  %1539 = add i128 %1523, %1538
  %1540 = lshr i128 %1539, 51
  %1541 = trunc i128 %1539 to i64
  %1542 = and i64 %1541, 2251799813685247
  %1543 = and i128 %1540, 18446744073709551615
  %1544 = add i128 %1521, %1543
  %1545 = lshr i128 %1544, 51
  %1546 = trunc i128 %1545 to i64
  %1547 = trunc i128 %1544 to i64
  %1548 = and i64 %1547, 2251799813685247
  %1549 = mul i64 %1546, 19
  %1550 = add i64 %1549, %1519
  %1551 = lshr i64 %1550, 51
  %1552 = and i64 %1550, 2251799813685247
  %1553 = add nuw nsw i64 %1551, %1532
  %1554 = lshr i64 %1553, 51
  %1555 = and i64 %1553, 2251799813685247
  %1556 = add nuw nsw i64 %1554, %1537
  store i64 %1552, i64* %22, align 8
  store i64 %1555, i64* %23, align 8
  store i64 %1556, i64* %24, align 8
  store i64 %1542, i64* %25, align 8
  store i64 %1548, i64* %26, align 8
  br label %1557

1557:                                             ; preds = %1557, %1473
  %1558 = phi i64 [ %1552, %1473 ], [ %1637, %1557 ]
  %1559 = phi i64 [ %1555, %1473 ], [ %1640, %1557 ]
  %1560 = phi i64 [ %1556, %1473 ], [ %1641, %1557 ]
  %1561 = phi i64 [ %1542, %1473 ], [ %1627, %1557 ]
  %1562 = phi i64 [ %1548, %1473 ], [ %1633, %1557 ]
  %1563 = phi i32 [ 1, %1473 ], [ %1642, %1557 ]
  %1564 = mul nuw nsw i64 %1562, 19
  %1565 = mul nuw nsw i64 %1562, 38
  %1566 = shl nuw nsw i64 %1562, 1
  %1567 = mul nuw nsw i64 %1561, 19
  %1568 = mul nuw nsw i64 %1561, 38
  %1569 = shl nuw nsw i64 %1561, 1
  %1570 = shl nsw i64 %1560, 1
  %1571 = shl nuw nsw i64 %1559, 1
  %1572 = zext i64 %1562 to i128
  %1573 = zext i64 %1564 to i128
  %1574 = mul nuw nsw i128 %1573, %1572
  %1575 = zext i64 %1561 to i128
  %1576 = zext i64 %1565 to i128
  %1577 = mul nuw nsw i128 %1575, %1576
  %1578 = zext i64 %1567 to i128
  %1579 = mul nuw nsw i128 %1578, %1575
  %1580 = zext i64 %1560 to i128
  %1581 = mul nuw nsw i128 %1580, %1576
  %1582 = zext i64 %1568 to i128
  %1583 = mul nuw nsw i128 %1580, %1582
  %1584 = mul nuw i128 %1580, %1580
  %1585 = zext i64 %1559 to i128
  %1586 = mul nuw nsw i128 %1585, %1576
  %1587 = zext i64 %1569 to i128
  %1588 = mul nuw nsw i128 %1585, %1587
  %1589 = zext i64 %1570 to i128
  %1590 = mul nuw nsw i128 %1585, %1589
  %1591 = mul nuw nsw i128 %1585, %1585
  %1592 = zext i64 %1558 to i128
  %1593 = zext i64 %1566 to i128
  %1594 = mul nuw nsw i128 %1592, %1593
  %1595 = mul nuw nsw i128 %1592, %1587
  %1596 = mul nuw nsw i128 %1592, %1589
  %1597 = zext i64 %1571 to i128
  %1598 = mul nuw nsw i128 %1592, %1597
  %1599 = mul nuw nsw i128 %1592, %1592
  %1600 = add nuw nsw i128 %1586, %1583
  %1601 = add nuw nsw i128 %1600, %1599
  %1602 = lshr i128 %1601, 51
  %1603 = trunc i128 %1601 to i64
  %1604 = and i64 %1603, 2251799813685247
  %1605 = add i128 %1588, %1584
  %1606 = add i128 %1605, %1594
  %1607 = add nuw nsw i128 %1590, %1574
  %1608 = add nuw nsw i128 %1607, %1595
  %1609 = add nuw nsw i128 %1591, %1577
  %1610 = add nuw nsw i128 %1609, %1596
  %1611 = add nuw nsw i128 %1581, %1579
  %1612 = add nuw nsw i128 %1611, %1598
  %1613 = and i128 %1602, 18446744073709551615
  %1614 = add nuw i128 %1612, %1613
  %1615 = lshr i128 %1614, 51
  %1616 = trunc i128 %1614 to i64
  %1617 = and i64 %1616, 2251799813685247
  %1618 = and i128 %1615, 18446744073709551615
  %1619 = add nuw nsw i128 %1610, %1618
  %1620 = lshr i128 %1619, 51
  %1621 = trunc i128 %1619 to i64
  %1622 = and i64 %1621, 2251799813685247
  %1623 = and i128 %1620, 18446744073709551615
  %1624 = add nuw nsw i128 %1608, %1623
  %1625 = lshr i128 %1624, 51
  %1626 = trunc i128 %1624 to i64
  %1627 = and i64 %1626, 2251799813685247
  %1628 = and i128 %1625, 18446744073709551615
  %1629 = add i128 %1606, %1628
  %1630 = lshr i128 %1629, 51
  %1631 = trunc i128 %1630 to i64
  %1632 = trunc i128 %1629 to i64
  %1633 = and i64 %1632, 2251799813685247
  %1634 = mul i64 %1631, 19
  %1635 = add i64 %1634, %1604
  %1636 = lshr i64 %1635, 51
  %1637 = and i64 %1635, 2251799813685247
  %1638 = add nuw nsw i64 %1636, %1617
  %1639 = lshr i64 %1638, 51
  %1640 = and i64 %1638, 2251799813685247
  %1641 = add nuw nsw i64 %1639, %1622
  %1642 = add nuw nsw i32 %1563, 1
  %1643 = icmp eq i32 %1642, 50
  br i1 %1643, label %1644, label %1557

1644:                                             ; preds = %1557
  store i64 %1633, i64* %26, align 8
  store i64 %1627, i64* %25, align 8
  store i64 %1641, i64* %24, align 8
  store i64 %1640, i64* %23, align 8
  store i64 %1637, i64* %22, align 8
  call fastcc void @fe_mul_impl(i64* nonnull %15, i64* nonnull %22, i64* nonnull %15) #4
  %1645 = load i64, i64* %19, align 8
  %1646 = mul i64 %1645, 19
  %1647 = mul i64 %1645, 38
  %1648 = shl i64 %1645, 1
  %1649 = load i64, i64* %18, align 8
  %1650 = mul i64 %1649, 19
  %1651 = mul i64 %1649, 38
  %1652 = shl i64 %1649, 1
  %1653 = load i64, i64* %17, align 8
  %1654 = shl i64 %1653, 1
  %1655 = load i64, i64* %16, align 8
  %1656 = shl i64 %1655, 1
  %1657 = zext i64 %1645 to i128
  %1658 = zext i64 %1646 to i128
  %1659 = mul nuw i128 %1658, %1657
  %1660 = zext i64 %1649 to i128
  %1661 = zext i64 %1647 to i128
  %1662 = mul nuw i128 %1660, %1661
  %1663 = zext i64 %1650 to i128
  %1664 = mul nuw i128 %1663, %1660
  %1665 = zext i64 %1653 to i128
  %1666 = mul nuw i128 %1665, %1661
  %1667 = zext i64 %1651 to i128
  %1668 = mul nuw i128 %1665, %1667
  %1669 = mul nuw i128 %1665, %1665
  %1670 = zext i64 %1655 to i128
  %1671 = mul nuw i128 %1670, %1661
  %1672 = zext i64 %1652 to i128
  %1673 = mul nuw i128 %1670, %1672
  %1674 = zext i64 %1654 to i128
  %1675 = mul nuw i128 %1670, %1674
  %1676 = mul nuw i128 %1670, %1670
  %1677 = load i64, i64* %15, align 8
  %1678 = zext i64 %1677 to i128
  %1679 = zext i64 %1648 to i128
  %1680 = mul nuw i128 %1678, %1679
  %1681 = mul nuw i128 %1678, %1672
  %1682 = mul nuw i128 %1678, %1674
  %1683 = zext i64 %1656 to i128
  %1684 = mul nuw i128 %1678, %1683
  %1685 = mul nuw i128 %1678, %1678
  %1686 = add i128 %1671, %1668
  %1687 = add i128 %1686, %1685
  %1688 = lshr i128 %1687, 51
  %1689 = trunc i128 %1687 to i64
  %1690 = and i64 %1689, 2251799813685247
  %1691 = add i128 %1673, %1669
  %1692 = add i128 %1691, %1680
  %1693 = add i128 %1675, %1659
  %1694 = add i128 %1693, %1681
  %1695 = add i128 %1676, %1662
  %1696 = add i128 %1695, %1682
  %1697 = add i128 %1666, %1664
  %1698 = add i128 %1697, %1684
  %1699 = and i128 %1688, 18446744073709551615
  %1700 = add i128 %1698, %1699
  %1701 = lshr i128 %1700, 51
  %1702 = trunc i128 %1700 to i64
  %1703 = and i64 %1702, 2251799813685247
  %1704 = and i128 %1701, 18446744073709551615
  %1705 = add i128 %1696, %1704
  %1706 = lshr i128 %1705, 51
  %1707 = trunc i128 %1705 to i64
  %1708 = and i64 %1707, 2251799813685247
  %1709 = and i128 %1706, 18446744073709551615
  %1710 = add i128 %1694, %1709
  %1711 = lshr i128 %1710, 51
  %1712 = trunc i128 %1710 to i64
  %1713 = and i64 %1712, 2251799813685247
  %1714 = and i128 %1711, 18446744073709551615
  %1715 = add i128 %1692, %1714
  %1716 = lshr i128 %1715, 51
  %1717 = trunc i128 %1716 to i64
  %1718 = trunc i128 %1715 to i64
  %1719 = and i64 %1718, 2251799813685247
  %1720 = mul i64 %1717, 19
  %1721 = add i64 %1720, %1690
  %1722 = lshr i64 %1721, 51
  %1723 = and i64 %1721, 2251799813685247
  %1724 = add nuw nsw i64 %1722, %1703
  %1725 = lshr i64 %1724, 51
  %1726 = and i64 %1724, 2251799813685247
  %1727 = add nuw nsw i64 %1725, %1708
  store i64 %1723, i64* %15, align 8
  store i64 %1726, i64* %16, align 8
  store i64 %1727, i64* %17, align 8
  store i64 %1713, i64* %18, align 8
  store i64 %1719, i64* %19, align 8
  br label %1728

1728:                                             ; preds = %1728, %1644
  %1729 = phi i64 [ %1723, %1644 ], [ %1808, %1728 ]
  %1730 = phi i64 [ %1726, %1644 ], [ %1811, %1728 ]
  %1731 = phi i64 [ %1727, %1644 ], [ %1812, %1728 ]
  %1732 = phi i64 [ %1713, %1644 ], [ %1798, %1728 ]
  %1733 = phi i64 [ %1719, %1644 ], [ %1804, %1728 ]
  %1734 = phi i32 [ 1, %1644 ], [ %1813, %1728 ]
  %1735 = mul nuw nsw i64 %1733, 19
  %1736 = mul nuw nsw i64 %1733, 38
  %1737 = shl nuw nsw i64 %1733, 1
  %1738 = mul nuw nsw i64 %1732, 19
  %1739 = mul nuw nsw i64 %1732, 38
  %1740 = shl nuw nsw i64 %1732, 1
  %1741 = shl nsw i64 %1731, 1
  %1742 = shl nuw nsw i64 %1730, 1
  %1743 = zext i64 %1733 to i128
  %1744 = zext i64 %1735 to i128
  %1745 = mul nuw nsw i128 %1744, %1743
  %1746 = zext i64 %1732 to i128
  %1747 = zext i64 %1736 to i128
  %1748 = mul nuw nsw i128 %1746, %1747
  %1749 = zext i64 %1738 to i128
  %1750 = mul nuw nsw i128 %1749, %1746
  %1751 = zext i64 %1731 to i128
  %1752 = mul nuw nsw i128 %1751, %1747
  %1753 = zext i64 %1739 to i128
  %1754 = mul nuw nsw i128 %1751, %1753
  %1755 = mul nuw i128 %1751, %1751
  %1756 = zext i64 %1730 to i128
  %1757 = mul nuw nsw i128 %1756, %1747
  %1758 = zext i64 %1740 to i128
  %1759 = mul nuw nsw i128 %1756, %1758
  %1760 = zext i64 %1741 to i128
  %1761 = mul nuw nsw i128 %1756, %1760
  %1762 = mul nuw nsw i128 %1756, %1756
  %1763 = zext i64 %1729 to i128
  %1764 = zext i64 %1737 to i128
  %1765 = mul nuw nsw i128 %1763, %1764
  %1766 = mul nuw nsw i128 %1763, %1758
  %1767 = mul nuw nsw i128 %1763, %1760
  %1768 = zext i64 %1742 to i128
  %1769 = mul nuw nsw i128 %1763, %1768
  %1770 = mul nuw nsw i128 %1763, %1763
  %1771 = add nuw nsw i128 %1757, %1754
  %1772 = add nuw nsw i128 %1771, %1770
  %1773 = lshr i128 %1772, 51
  %1774 = trunc i128 %1772 to i64
  %1775 = and i64 %1774, 2251799813685247
  %1776 = add i128 %1759, %1755
  %1777 = add i128 %1776, %1765
  %1778 = add nuw nsw i128 %1761, %1745
  %1779 = add nuw nsw i128 %1778, %1766
  %1780 = add nuw nsw i128 %1762, %1748
  %1781 = add nuw nsw i128 %1780, %1767
  %1782 = add nuw nsw i128 %1752, %1750
  %1783 = add nuw nsw i128 %1782, %1769
  %1784 = and i128 %1773, 18446744073709551615
  %1785 = add nuw i128 %1783, %1784
  %1786 = lshr i128 %1785, 51
  %1787 = trunc i128 %1785 to i64
  %1788 = and i64 %1787, 2251799813685247
  %1789 = and i128 %1786, 18446744073709551615
  %1790 = add nuw nsw i128 %1781, %1789
  %1791 = lshr i128 %1790, 51
  %1792 = trunc i128 %1790 to i64
  %1793 = and i64 %1792, 2251799813685247
  %1794 = and i128 %1791, 18446744073709551615
  %1795 = add nuw nsw i128 %1779, %1794
  %1796 = lshr i128 %1795, 51
  %1797 = trunc i128 %1795 to i64
  %1798 = and i64 %1797, 2251799813685247
  %1799 = and i128 %1796, 18446744073709551615
  %1800 = add i128 %1777, %1799
  %1801 = lshr i128 %1800, 51
  %1802 = trunc i128 %1801 to i64
  %1803 = trunc i128 %1800 to i64
  %1804 = and i64 %1803, 2251799813685247
  %1805 = mul i64 %1802, 19
  %1806 = add i64 %1805, %1775
  %1807 = lshr i64 %1806, 51
  %1808 = and i64 %1806, 2251799813685247
  %1809 = add nuw nsw i64 %1807, %1788
  %1810 = lshr i64 %1809, 51
  %1811 = and i64 %1809, 2251799813685247
  %1812 = add nuw nsw i64 %1810, %1793
  %1813 = add nuw nsw i32 %1734, 1
  %1814 = icmp eq i32 %1813, 5
  br i1 %1814, label %1815, label %1728

1815:                                             ; preds = %1728
  store i64 %1804, i64* %19, align 8
  store i64 %1798, i64* %18, align 8
  store i64 %1812, i64* %17, align 8
  store i64 %1811, i64* %16, align 8
  store i64 %1808, i64* %15, align 8
  %1816 = getelementptr inbounds %struct.fe, %struct.fe* %0, i64 0, i32 0, i64 0
  call fastcc void @fe_mul_impl(i64* %1816, i64* nonnull %15, i64* nonnull %8) #4
  call void @llvm.lifetime.end.p0i8(i64 40, i8* nonnull %27) #4
  call void @llvm.lifetime.end.p0i8(i64 40, i8* nonnull %20) #4
  call void @llvm.lifetime.end.p0i8(i64 40, i8* nonnull %13) #4
  call void @llvm.lifetime.end.p0i8(i64 40, i8* nonnull %7) #4
  ret void
}

; Function Attrs: nofree norecurse nounwind ssp uwtable
define internal fastcc void @fe_mul_impl(i64* nocapture, i64* nocapture readonly, i64* nocapture readonly) unnamed_addr #2 {
  %4 = getelementptr inbounds i64, i64* %1, i64 4
  %5 = load i64, i64* %4, align 8
  %6 = zext i64 %5 to i128
  %7 = getelementptr inbounds i64, i64* %2, i64 4
  %8 = load i64, i64* %7, align 8
  %9 = mul i64 %8, 19
  %10 = zext i64 %9 to i128
  %11 = mul nuw i128 %10, %6
  %12 = getelementptr inbounds i64, i64* %2, i64 3
  %13 = load i64, i64* %12, align 8
  %14 = mul i64 %13, 19
  %15 = zext i64 %14 to i128
  %16 = mul nuw i128 %15, %6
  %17 = getelementptr inbounds i64, i64* %2, i64 2
  %18 = load i64, i64* %17, align 8
  %19 = mul i64 %18, 19
  %20 = zext i64 %19 to i128
  %21 = mul nuw i128 %20, %6
  %22 = getelementptr inbounds i64, i64* %2, i64 1
  %23 = load i64, i64* %22, align 8
  %24 = mul i64 %23, 19
  %25 = zext i64 %24 to i128
  %26 = mul nuw i128 %25, %6
  %27 = getelementptr inbounds i64, i64* %1, i64 3
  %28 = load i64, i64* %27, align 8
  %29 = zext i64 %28 to i128
  %30 = mul nuw i128 %29, %10
  %31 = mul nuw i128 %29, %15
  %32 = mul nuw i128 %29, %20
  %33 = getelementptr inbounds i64, i64* %1, i64 2
  %34 = load i64, i64* %33, align 8
  %35 = zext i64 %34 to i128
  %36 = mul nuw i128 %35, %10
  %37 = mul nuw i128 %35, %15
  %38 = getelementptr inbounds i64, i64* %1, i64 1
  %39 = load i64, i64* %38, align 8
  %40 = zext i64 %39 to i128
  %41 = mul nuw i128 %40, %10
  %42 = load i64, i64* %2, align 8
  %43 = zext i64 %42 to i128
  %44 = mul nuw i128 %43, %6
  %45 = zext i64 %23 to i128
  %46 = mul nuw i128 %29, %45
  %47 = mul nuw i128 %43, %29
  %48 = zext i64 %18 to i128
  %49 = mul nuw i128 %35, %48
  %50 = mul nuw i128 %35, %45
  %51 = mul nuw i128 %43, %35
  %52 = zext i64 %13 to i128
  %53 = mul nuw i128 %40, %52
  %54 = mul nuw i128 %40, %48
  %55 = mul nuw i128 %40, %45
  %56 = mul nuw i128 %43, %40
  %57 = load i64, i64* %1, align 8
  %58 = zext i64 %57 to i128
  %59 = zext i64 %8 to i128
  %60 = mul nuw i128 %58, %59
  %61 = mul nuw i128 %58, %52
  %62 = mul nuw i128 %58, %48
  %63 = mul nuw i128 %58, %45
  %64 = mul nuw i128 %58, %43
  %65 = add i128 %32, %26
  %66 = add i128 %65, %37
  %67 = add i128 %66, %41
  %68 = add i128 %67, %64
  %69 = lshr i128 %68, 51
  %70 = trunc i128 %68 to i64
  %71 = and i64 %70, 2251799813685247
  %72 = add i128 %30, %16
  %73 = add i128 %31, %21
  %74 = add i128 %73, %36
  %75 = add i128 %74, %56
  %76 = add i128 %75, %63
  %77 = and i128 %69, 18446744073709551615
  %78 = add i128 %76, %77
  %79 = lshr i128 %78, 51
  %80 = trunc i128 %78 to i64
  %81 = and i64 %80, 2251799813685247
  %82 = and i128 %79, 18446744073709551615
  %83 = add i128 %72, %55
  %84 = add i128 %83, %51
  %85 = add i128 %84, %62
  %86 = add i128 %85, %82
  %87 = lshr i128 %86, 51
  %88 = trunc i128 %86 to i64
  %89 = and i64 %88, 2251799813685247
  %90 = and i128 %87, 18446744073709551615
  %91 = add i128 %50, %11
  %92 = add i128 %91, %54
  %93 = add i128 %92, %47
  %94 = add i128 %93, %61
  %95 = add i128 %94, %90
  %96 = lshr i128 %95, 51
  %97 = and i128 %96, 18446744073709551615
  %98 = add i128 %49, %46
  %99 = add i128 %98, %53
  %100 = add i128 %99, %44
  %101 = add i128 %100, %60
  %102 = add i128 %101, %97
  %103 = lshr i128 %102, 51
  %104 = trunc i128 %103 to i64
  %105 = insertelement <2 x i128> undef, i128 %95, i32 0
  %106 = insertelement <2 x i128> %105, i128 %102, i32 1
  %107 = trunc <2 x i128> %106 to <2 x i64>
  %108 = and <2 x i64> %107, <i64 2251799813685247, i64 2251799813685247>
  %109 = mul i64 %104, 19
  %110 = add i64 %109, %71
  %111 = lshr i64 %110, 51
  %112 = and i64 %110, 2251799813685247
  %113 = add nuw nsw i64 %111, %81
  %114 = lshr i64 %113, 51
  %115 = and i64 %113, 2251799813685247
  %116 = add nuw nsw i64 %114, %89
  store i64 %112, i64* %0, align 8
  %117 = getelementptr inbounds i64, i64* %0, i64 1
  store i64 %115, i64* %117, align 8
  %118 = getelementptr inbounds i64, i64* %0, i64 2
  store i64 %116, i64* %118, align 8
  %119 = getelementptr inbounds i64, i64* %0, i64 3
  %120 = bitcast i64* %119 to <2 x i64>*
  store <2 x i64> %108, <2 x i64>* %120, align 8
  ret void
}

; Function Attrs: argmemonly nounwind
declare void @llvm.memmove.p0i8.p0i8.i64(i8* nocapture, i8* nocapture readonly, i64, i1 immarg) #1

; Function Attrs: argmemonly nounwind
declare void @llvm.memcpy.p0i8.p0i8.i64(i8* nocapture writeonly, i8* nocapture readonly, i64, i1 immarg) #1

attributes #0 = { nounwind ssp uwtable "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "min-legal-vector-width"="0" "no-frame-pointer-elim"="false" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "null-pointer-is-valid"="true" "stack-protector-buffer-size"="4" "target-cpu"="x86-64" "target-features"="+cx8,+fxsr,+mmx,+sse,+sse2,+sse3,+x87" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #1 = { argmemonly nounwind }
attributes #2 = { nofree norecurse nounwind ssp uwtable "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "min-legal-vector-width"="0" "no-frame-pointer-elim"="false" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "null-pointer-is-valid"="true" "stack-protector-buffer-size"="4" "target-cpu"="x86-64" "target-features"="+cx8,+fxsr,+mmx,+sse,+sse2,+sse3,+x87" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #3 = { "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "no-frame-pointer-elim"="false" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "null-pointer-is-valid"="true" "stack-protector-buffer-size"="4" "target-cpu"="x86-64" "target-features"="+cx8,+fxsr,+mmx,+sse,+sse2,+sse3,+x87" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #4 = { nounwind }
attributes #5 = { nounwind readnone }

!llvm.module.flags = !{!0, !1}

!0 = !{i32 1, !"wchar_size", i32 4}
!1 = !{i32 7, !"PIC Level", i32 2}
!2 = !{i32 431894}
