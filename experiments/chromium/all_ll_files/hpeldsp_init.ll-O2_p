; ModuleID = '../../third_party/ffmpeg/libavcodec/x86/hpeldsp_init.c'
source_filename = "../../third_party/ffmpeg/libavcodec/x86/hpeldsp_init.c"
target datalayout = "e-m:e-i64:64-f80:128-n8:16:32:64-S128"
target triple = "x86_64-unknown-linux-gnu"

module asm ".symver exp, exp@GLIBC_2.2.5"
module asm ".symver exp2, exp2@GLIBC_2.2.5"
module asm ".symver exp2f, exp2f@GLIBC_2.2.5"
module asm ".symver expf, expf@GLIBC_2.2.5"
module asm ".symver lgamma, lgamma@GLIBC_2.2.5"
module asm ".symver lgammaf, lgammaf@GLIBC_2.2.5"
module asm ".symver lgammal, lgammal@GLIBC_2.2.5"
module asm ".symver log, log@GLIBC_2.2.5"
module asm ".symver log2, log2@GLIBC_2.2.5"
module asm ".symver log2f, log2f@GLIBC_2.2.5"
module asm ".symver logf, logf@GLIBC_2.2.5"
module asm ".symver pow, pow@GLIBC_2.2.5"
module asm ".symver powf, powf@GLIBC_2.2.5"
module asm ".symver exp, exp@GLIBC_2.2.5"
module asm ".symver exp2, exp2@GLIBC_2.2.5"
module asm ".symver exp2f, exp2f@GLIBC_2.2.5"
module asm ".symver expf, expf@GLIBC_2.2.5"
module asm ".symver lgamma, lgamma@GLIBC_2.2.5"
module asm ".symver lgammaf, lgammaf@GLIBC_2.2.5"
module asm ".symver lgammal, lgammal@GLIBC_2.2.5"
module asm ".symver log, log@GLIBC_2.2.5"
module asm ".symver log2, log2@GLIBC_2.2.5"
module asm ".symver log2f, log2f@GLIBC_2.2.5"
module asm ".symver logf, logf@GLIBC_2.2.5"
module asm ".symver pow, pow@GLIBC_2.2.5"
module asm ".symver powf, powf@GLIBC_2.2.5"
module asm ".symver exp, exp@GLIBC_2.2.5"
module asm ".symver exp2, exp2@GLIBC_2.2.5"
module asm ".symver exp2f, exp2f@GLIBC_2.2.5"
module asm ".symver expf, expf@GLIBC_2.2.5"
module asm ".symver lgamma, lgamma@GLIBC_2.2.5"
module asm ".symver lgammaf, lgammaf@GLIBC_2.2.5"
module asm ".symver lgammal, lgammal@GLIBC_2.2.5"
module asm ".symver log, log@GLIBC_2.2.5"
module asm ".symver log2, log2@GLIBC_2.2.5"
module asm ".symver log2f, log2f@GLIBC_2.2.5"
module asm ".symver logf, logf@GLIBC_2.2.5"
module asm ".symver pow, pow@GLIBC_2.2.5"
module asm ".symver powf, powf@GLIBC_2.2.5"

%struct.HpelDSPContext = type { [4 x [4 x void (i8*, i8*, i64, i32)*]], [4 x [4 x void (i8*, i8*, i64, i32)*]], [4 x [4 x void (i8*, i8*, i64, i32)*]], [4 x void (i8*, i8*, i64, i32)*] }

; Function Attrs: nounwind ssp uwtable
define hidden void @ff_put_pixels8_xy2_mmx(i8*, i8*, i64, i32) #0 {
  %5 = alloca i32, align 4
  store i32 %3, i32* %5, align 4
  tail call void asm sideeffect "pxor %mm7, %mm7", "~{dirflag},~{fpsr},~{flags}"() #4, !srcloc !2
  tail call void asm sideeffect "pcmpeqd %mm6, %mm6   \0A\09psrlw         $$15, %mm6   \0A\09psllw          $$1, %mm6   \0A\09", "~{dirflag},~{fpsr},~{flags}"() #4, !srcloc !3
  %6 = call i8* asm sideeffect "movq   ($1), %mm0             \0A\09movq   1($1), %mm4            \0A\09movq   %mm0, %mm1            \0A\09movq   %mm4, %mm5            \0A\09punpcklbw %mm7, %mm0         \0A\09punpcklbw %mm7, %mm4         \0A\09punpckhbw %mm7, %mm1         \0A\09punpckhbw %mm7, %mm5         \0A\09paddusw %mm0, %mm4           \0A\09paddusw %mm1, %mm5           \0A\09xor    %rax, %rax \0A\09add    $3, $1                  \0A\09.p2align 3                     \0A\091:                             \0A\09movq   ($1, %rax), %mm0  \0A\09movq   1($1, %rax), %mm2 \0A\09movq   %mm0, %mm1            \0A\09movq   %mm2, %mm3            \0A\09punpcklbw %mm7, %mm0         \0A\09punpcklbw %mm7, %mm2         \0A\09punpckhbw %mm7, %mm1         \0A\09punpckhbw %mm7, %mm3         \0A\09paddusw %mm2, %mm0           \0A\09paddusw %mm3, %mm1           \0A\09paddusw %mm6, %mm4           \0A\09paddusw %mm6, %mm5           \0A\09paddusw %mm0, %mm4           \0A\09paddusw %mm1, %mm5           \0A\09psrlw  $$2, %mm4               \0A\09psrlw  $$2, %mm5               \0A\09packuswb  %mm5, %mm4         \0A\09movq   %mm4, ($2, %rax)  \0A\09add    $3, %rax           \0A\09movq   ($1, %rax), %mm2  \0A\09movq   1($1, %rax), %mm4 \0A\09movq   %mm2, %mm3            \0A\09movq   %mm4, %mm5            \0A\09punpcklbw %mm7, %mm2         \0A\09punpcklbw %mm7, %mm4         \0A\09punpckhbw %mm7, %mm3         \0A\09punpckhbw %mm7, %mm5         \0A\09paddusw %mm2, %mm4           \0A\09paddusw %mm3, %mm5           \0A\09paddusw %mm6, %mm0           \0A\09paddusw %mm6, %mm1           \0A\09paddusw %mm4, %mm0           \0A\09paddusw %mm5, %mm1           \0A\09psrlw  $$2, %mm0               \0A\09psrlw  $$2, %mm1               \0A\09packuswb  %mm1, %mm0         \0A\09movq   %mm0, ($2, %rax)  \0A\09add    $3, %rax        \0A\09subl   $$2, $0                  \0A\09jnz    1b                      \0A\09", "=*imr,={si},{di},r,0,1,~{rax},~{memory},~{dirflag},~{fpsr},~{flags}"(i32* nonnull %5, i8* %0, i64 %2, i32 %3, i8* %1) #4, !srcloc !4
  ret void
}

; Function Attrs: nounwind ssp uwtable
define hidden void @ff_avg_pixels8_xy2_mmx(i8*, i8*, i64, i32) #0 {
  %5 = alloca i32, align 4
  store i32 %3, i32* %5, align 4
  tail call void asm sideeffect "pxor %mm7, %mm7", "~{dirflag},~{fpsr},~{flags}"() #4, !srcloc !5
  tail call void asm sideeffect "pcmpeqd %mm6, %mm6   \0A\09psrlw         $$15, %mm6   \0A\09psllw          $$1, %mm6   \0A\09", "~{dirflag},~{fpsr},~{flags}"() #4, !srcloc !6
  %6 = call i8* asm sideeffect "movq   ($1), %mm0             \0A\09movq   1($1), %mm4            \0A\09movq   %mm0, %mm1            \0A\09movq   %mm4, %mm5            \0A\09punpcklbw %mm7, %mm0         \0A\09punpcklbw %mm7, %mm4         \0A\09punpckhbw %mm7, %mm1         \0A\09punpckhbw %mm7, %mm5         \0A\09paddusw %mm0, %mm4           \0A\09paddusw %mm1, %mm5           \0A\09xor    %rax, %rax \0A\09add    $3, $1                  \0A\09.p2align 3                     \0A\091:                             \0A\09movq   ($1, %rax), %mm0  \0A\09movq   1($1, %rax), %mm2 \0A\09movq   %mm0, %mm1            \0A\09movq   %mm2, %mm3            \0A\09punpcklbw %mm7, %mm0         \0A\09punpcklbw %mm7, %mm2         \0A\09punpckhbw %mm7, %mm1         \0A\09punpckhbw %mm7, %mm3         \0A\09paddusw %mm2, %mm0           \0A\09paddusw %mm3, %mm1           \0A\09paddusw %mm6, %mm4           \0A\09paddusw %mm6, %mm5           \0A\09paddusw %mm0, %mm4           \0A\09paddusw %mm1, %mm5           \0A\09psrlw  $$2, %mm4               \0A\09psrlw  $$2, %mm5               \0A\09movq   ($2, %rax), %mm3  \0A\09packuswb  %mm5, %mm4         \0A\09pcmpeqd %mm2, %mm2   \0A\09paddb %mm2, %mm2     \0A\09movq   %mm3, %mm5            \0A\09por    %mm4, %mm5            \0A\09pxor   %mm3, %mm4            \0A\09pand  %mm2, %mm4            \0A\09psrlq       $$1, %mm4            \0A\09psubb  %mm4, %mm5            \0A\09movq   %mm5, ($2, %rax)  \0A\09add    $3, %rax        \0A\09movq   ($1, %rax), %mm2  \0A\09movq   1($1, %rax), %mm4 \0A\09movq   %mm2, %mm3            \0A\09movq   %mm4, %mm5            \0A\09punpcklbw %mm7, %mm2         \0A\09punpcklbw %mm7, %mm4         \0A\09punpckhbw %mm7, %mm3         \0A\09punpckhbw %mm7, %mm5         \0A\09paddusw %mm2, %mm4           \0A\09paddusw %mm3, %mm5           \0A\09paddusw %mm6, %mm0           \0A\09paddusw %mm6, %mm1           \0A\09paddusw %mm4, %mm0           \0A\09paddusw %mm5, %mm1           \0A\09psrlw  $$2, %mm0               \0A\09psrlw  $$2, %mm1               \0A\09movq   ($2, %rax), %mm3  \0A\09packuswb  %mm1, %mm0         \0A\09pcmpeqd %mm2, %mm2   \0A\09paddb %mm2, %mm2     \0A\09movq   %mm3, %mm1            \0A\09por    %mm0, %mm1            \0A\09pxor   %mm3, %mm0            \0A\09pand  %mm2, %mm0            \0A\09psrlq       $$1, %mm0            \0A\09psubb  %mm0, %mm1            \0A\09movq   %mm1, ($2, %rax)  \0A\09add    $3, %rax           \0A\09subl   $$2, $0                  \0A\09jnz    1b                      \0A\09", "=*imr,={si},{di},r,0,1,~{rax},~{memory},~{dirflag},~{fpsr},~{flags}"(i32* nonnull %5, i8* %0, i64 %2, i32 %3, i8* %1) #4, !srcloc !7
  ret void
}

; Function Attrs: nounwind ssp uwtable
define hidden void @ff_avg_pixels16_xy2_mmx(i8*, i8*, i64, i32) #0 {
  %5 = alloca i32, align 4
  %6 = alloca i32, align 4
  %7 = bitcast i32* %6 to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %7)
  store i32 %3, i32* %6, align 4
  tail call void asm sideeffect "pxor %mm7, %mm7", "~{dirflag},~{fpsr},~{flags}"() #4, !srcloc !5
  tail call void asm sideeffect "pcmpeqd %mm6, %mm6   \0A\09psrlw         $$15, %mm6   \0A\09psllw          $$1, %mm6   \0A\09", "~{dirflag},~{fpsr},~{flags}"() #4, !srcloc !6
  %8 = call i8* asm sideeffect "movq   ($1), %mm0             \0A\09movq   1($1), %mm4            \0A\09movq   %mm0, %mm1            \0A\09movq   %mm4, %mm5            \0A\09punpcklbw %mm7, %mm0         \0A\09punpcklbw %mm7, %mm4         \0A\09punpckhbw %mm7, %mm1         \0A\09punpckhbw %mm7, %mm5         \0A\09paddusw %mm0, %mm4           \0A\09paddusw %mm1, %mm5           \0A\09xor    %rax, %rax \0A\09add    $3, $1                  \0A\09.p2align 3                     \0A\091:                             \0A\09movq   ($1, %rax), %mm0  \0A\09movq   1($1, %rax), %mm2 \0A\09movq   %mm0, %mm1            \0A\09movq   %mm2, %mm3            \0A\09punpcklbw %mm7, %mm0         \0A\09punpcklbw %mm7, %mm2         \0A\09punpckhbw %mm7, %mm1         \0A\09punpckhbw %mm7, %mm3         \0A\09paddusw %mm2, %mm0           \0A\09paddusw %mm3, %mm1           \0A\09paddusw %mm6, %mm4           \0A\09paddusw %mm6, %mm5           \0A\09paddusw %mm0, %mm4           \0A\09paddusw %mm1, %mm5           \0A\09psrlw  $$2, %mm4               \0A\09psrlw  $$2, %mm5               \0A\09movq   ($2, %rax), %mm3  \0A\09packuswb  %mm5, %mm4         \0A\09pcmpeqd %mm2, %mm2   \0A\09paddb %mm2, %mm2     \0A\09movq   %mm3, %mm5            \0A\09por    %mm4, %mm5            \0A\09pxor   %mm3, %mm4            \0A\09pand  %mm2, %mm4            \0A\09psrlq       $$1, %mm4            \0A\09psubb  %mm4, %mm5            \0A\09movq   %mm5, ($2, %rax)  \0A\09add    $3, %rax        \0A\09movq   ($1, %rax), %mm2  \0A\09movq   1($1, %rax), %mm4 \0A\09movq   %mm2, %mm3            \0A\09movq   %mm4, %mm5            \0A\09punpcklbw %mm7, %mm2         \0A\09punpcklbw %mm7, %mm4         \0A\09punpckhbw %mm7, %mm3         \0A\09punpckhbw %mm7, %mm5         \0A\09paddusw %mm2, %mm4           \0A\09paddusw %mm3, %mm5           \0A\09paddusw %mm6, %mm0           \0A\09paddusw %mm6, %mm1           \0A\09paddusw %mm4, %mm0           \0A\09paddusw %mm5, %mm1           \0A\09psrlw  $$2, %mm0               \0A\09psrlw  $$2, %mm1               \0A\09movq   ($2, %rax), %mm3  \0A\09packuswb  %mm1, %mm0         \0A\09pcmpeqd %mm2, %mm2   \0A\09paddb %mm2, %mm2     \0A\09movq   %mm3, %mm1            \0A\09por    %mm0, %mm1            \0A\09pxor   %mm3, %mm0            \0A\09pand  %mm2, %mm0            \0A\09psrlq       $$1, %mm0            \0A\09psubb  %mm0, %mm1            \0A\09movq   %mm1, ($2, %rax)  \0A\09add    $3, %rax           \0A\09subl   $$2, $0                  \0A\09jnz    1b                      \0A\09", "=*imr,={si},{di},r,0,1,~{rax},~{memory},~{dirflag},~{fpsr},~{flags}"(i32* nonnull %6, i8* %0, i64 %2, i32 %3, i8* %1) #4, !srcloc !7
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %7)
  %9 = getelementptr inbounds i8, i8* %0, i64 8
  %10 = getelementptr inbounds i8, i8* %1, i64 8
  %11 = bitcast i32* %5 to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %11)
  store i32 %3, i32* %5, align 4
  call void asm sideeffect "pxor %mm7, %mm7", "~{dirflag},~{fpsr},~{flags}"() #4, !srcloc !5
  call void asm sideeffect "pcmpeqd %mm6, %mm6   \0A\09psrlw         $$15, %mm6   \0A\09psllw          $$1, %mm6   \0A\09", "~{dirflag},~{fpsr},~{flags}"() #4, !srcloc !6
  %12 = call i8* asm sideeffect "movq   ($1), %mm0             \0A\09movq   1($1), %mm4            \0A\09movq   %mm0, %mm1            \0A\09movq   %mm4, %mm5            \0A\09punpcklbw %mm7, %mm0         \0A\09punpcklbw %mm7, %mm4         \0A\09punpckhbw %mm7, %mm1         \0A\09punpckhbw %mm7, %mm5         \0A\09paddusw %mm0, %mm4           \0A\09paddusw %mm1, %mm5           \0A\09xor    %rax, %rax \0A\09add    $3, $1                  \0A\09.p2align 3                     \0A\091:                             \0A\09movq   ($1, %rax), %mm0  \0A\09movq   1($1, %rax), %mm2 \0A\09movq   %mm0, %mm1            \0A\09movq   %mm2, %mm3            \0A\09punpcklbw %mm7, %mm0         \0A\09punpcklbw %mm7, %mm2         \0A\09punpckhbw %mm7, %mm1         \0A\09punpckhbw %mm7, %mm3         \0A\09paddusw %mm2, %mm0           \0A\09paddusw %mm3, %mm1           \0A\09paddusw %mm6, %mm4           \0A\09paddusw %mm6, %mm5           \0A\09paddusw %mm0, %mm4           \0A\09paddusw %mm1, %mm5           \0A\09psrlw  $$2, %mm4               \0A\09psrlw  $$2, %mm5               \0A\09movq   ($2, %rax), %mm3  \0A\09packuswb  %mm5, %mm4         \0A\09pcmpeqd %mm2, %mm2   \0A\09paddb %mm2, %mm2     \0A\09movq   %mm3, %mm5            \0A\09por    %mm4, %mm5            \0A\09pxor   %mm3, %mm4            \0A\09pand  %mm2, %mm4            \0A\09psrlq       $$1, %mm4            \0A\09psubb  %mm4, %mm5            \0A\09movq   %mm5, ($2, %rax)  \0A\09add    $3, %rax        \0A\09movq   ($1, %rax), %mm2  \0A\09movq   1($1, %rax), %mm4 \0A\09movq   %mm2, %mm3            \0A\09movq   %mm4, %mm5            \0A\09punpcklbw %mm7, %mm2         \0A\09punpcklbw %mm7, %mm4         \0A\09punpckhbw %mm7, %mm3         \0A\09punpckhbw %mm7, %mm5         \0A\09paddusw %mm2, %mm4           \0A\09paddusw %mm3, %mm5           \0A\09paddusw %mm6, %mm0           \0A\09paddusw %mm6, %mm1           \0A\09paddusw %mm4, %mm0           \0A\09paddusw %mm5, %mm1           \0A\09psrlw  $$2, %mm0               \0A\09psrlw  $$2, %mm1               \0A\09movq   ($2, %rax), %mm3  \0A\09packuswb  %mm1, %mm0         \0A\09pcmpeqd %mm2, %mm2   \0A\09paddb %mm2, %mm2     \0A\09movq   %mm3, %mm1            \0A\09por    %mm0, %mm1            \0A\09pxor   %mm3, %mm0            \0A\09pand  %mm2, %mm0            \0A\09psrlq       $$1, %mm0            \0A\09psubb  %mm0, %mm1            \0A\09movq   %mm1, ($2, %rax)  \0A\09add    $3, %rax           \0A\09subl   $$2, $0                  \0A\09jnz    1b                      \0A\09", "=*imr,={si},{di},r,0,1,~{rax},~{memory},~{dirflag},~{fpsr},~{flags}"(i32* nonnull %5, i8* %9, i64 %2, i32 %3, i8* %10) #4, !srcloc !7
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %11)
  ret void
}

; Function Attrs: nounwind ssp uwtable
define hidden void @ff_put_pixels16_xy2_mmx(i8*, i8*, i64, i32) #0 {
  %5 = alloca i32, align 4
  %6 = alloca i32, align 4
  %7 = bitcast i32* %6 to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %7)
  store i32 %3, i32* %6, align 4
  tail call void asm sideeffect "pxor %mm7, %mm7", "~{dirflag},~{fpsr},~{flags}"() #4, !srcloc !2
  tail call void asm sideeffect "pcmpeqd %mm6, %mm6   \0A\09psrlw         $$15, %mm6   \0A\09psllw          $$1, %mm6   \0A\09", "~{dirflag},~{fpsr},~{flags}"() #4, !srcloc !3
  %8 = call i8* asm sideeffect "movq   ($1), %mm0             \0A\09movq   1($1), %mm4            \0A\09movq   %mm0, %mm1            \0A\09movq   %mm4, %mm5            \0A\09punpcklbw %mm7, %mm0         \0A\09punpcklbw %mm7, %mm4         \0A\09punpckhbw %mm7, %mm1         \0A\09punpckhbw %mm7, %mm5         \0A\09paddusw %mm0, %mm4           \0A\09paddusw %mm1, %mm5           \0A\09xor    %rax, %rax \0A\09add    $3, $1                  \0A\09.p2align 3                     \0A\091:                             \0A\09movq   ($1, %rax), %mm0  \0A\09movq   1($1, %rax), %mm2 \0A\09movq   %mm0, %mm1            \0A\09movq   %mm2, %mm3            \0A\09punpcklbw %mm7, %mm0         \0A\09punpcklbw %mm7, %mm2         \0A\09punpckhbw %mm7, %mm1         \0A\09punpckhbw %mm7, %mm3         \0A\09paddusw %mm2, %mm0           \0A\09paddusw %mm3, %mm1           \0A\09paddusw %mm6, %mm4           \0A\09paddusw %mm6, %mm5           \0A\09paddusw %mm0, %mm4           \0A\09paddusw %mm1, %mm5           \0A\09psrlw  $$2, %mm4               \0A\09psrlw  $$2, %mm5               \0A\09packuswb  %mm5, %mm4         \0A\09movq   %mm4, ($2, %rax)  \0A\09add    $3, %rax           \0A\09movq   ($1, %rax), %mm2  \0A\09movq   1($1, %rax), %mm4 \0A\09movq   %mm2, %mm3            \0A\09movq   %mm4, %mm5            \0A\09punpcklbw %mm7, %mm2         \0A\09punpcklbw %mm7, %mm4         \0A\09punpckhbw %mm7, %mm3         \0A\09punpckhbw %mm7, %mm5         \0A\09paddusw %mm2, %mm4           \0A\09paddusw %mm3, %mm5           \0A\09paddusw %mm6, %mm0           \0A\09paddusw %mm6, %mm1           \0A\09paddusw %mm4, %mm0           \0A\09paddusw %mm5, %mm1           \0A\09psrlw  $$2, %mm0               \0A\09psrlw  $$2, %mm1               \0A\09packuswb  %mm1, %mm0         \0A\09movq   %mm0, ($2, %rax)  \0A\09add    $3, %rax        \0A\09subl   $$2, $0                  \0A\09jnz    1b                      \0A\09", "=*imr,={si},{di},r,0,1,~{rax},~{memory},~{dirflag},~{fpsr},~{flags}"(i32* nonnull %6, i8* %0, i64 %2, i32 %3, i8* %1) #4, !srcloc !4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %7)
  %9 = getelementptr inbounds i8, i8* %0, i64 8
  %10 = getelementptr inbounds i8, i8* %1, i64 8
  %11 = bitcast i32* %5 to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %11)
  store i32 %3, i32* %5, align 4
  call void asm sideeffect "pxor %mm7, %mm7", "~{dirflag},~{fpsr},~{flags}"() #4, !srcloc !2
  call void asm sideeffect "pcmpeqd %mm6, %mm6   \0A\09psrlw         $$15, %mm6   \0A\09psllw          $$1, %mm6   \0A\09", "~{dirflag},~{fpsr},~{flags}"() #4, !srcloc !3
  %12 = call i8* asm sideeffect "movq   ($1), %mm0             \0A\09movq   1($1), %mm4            \0A\09movq   %mm0, %mm1            \0A\09movq   %mm4, %mm5            \0A\09punpcklbw %mm7, %mm0         \0A\09punpcklbw %mm7, %mm4         \0A\09punpckhbw %mm7, %mm1         \0A\09punpckhbw %mm7, %mm5         \0A\09paddusw %mm0, %mm4           \0A\09paddusw %mm1, %mm5           \0A\09xor    %rax, %rax \0A\09add    $3, $1                  \0A\09.p2align 3                     \0A\091:                             \0A\09movq   ($1, %rax), %mm0  \0A\09movq   1($1, %rax), %mm2 \0A\09movq   %mm0, %mm1            \0A\09movq   %mm2, %mm3            \0A\09punpcklbw %mm7, %mm0         \0A\09punpcklbw %mm7, %mm2         \0A\09punpckhbw %mm7, %mm1         \0A\09punpckhbw %mm7, %mm3         \0A\09paddusw %mm2, %mm0           \0A\09paddusw %mm3, %mm1           \0A\09paddusw %mm6, %mm4           \0A\09paddusw %mm6, %mm5           \0A\09paddusw %mm0, %mm4           \0A\09paddusw %mm1, %mm5           \0A\09psrlw  $$2, %mm4               \0A\09psrlw  $$2, %mm5               \0A\09packuswb  %mm5, %mm4         \0A\09movq   %mm4, ($2, %rax)  \0A\09add    $3, %rax           \0A\09movq   ($1, %rax), %mm2  \0A\09movq   1($1, %rax), %mm4 \0A\09movq   %mm2, %mm3            \0A\09movq   %mm4, %mm5            \0A\09punpcklbw %mm7, %mm2         \0A\09punpcklbw %mm7, %mm4         \0A\09punpckhbw %mm7, %mm3         \0A\09punpckhbw %mm7, %mm5         \0A\09paddusw %mm2, %mm4           \0A\09paddusw %mm3, %mm5           \0A\09paddusw %mm6, %mm0           \0A\09paddusw %mm6, %mm1           \0A\09paddusw %mm4, %mm0           \0A\09paddusw %mm5, %mm1           \0A\09psrlw  $$2, %mm0               \0A\09psrlw  $$2, %mm1               \0A\09packuswb  %mm1, %mm0         \0A\09movq   %mm0, ($2, %rax)  \0A\09add    $3, %rax        \0A\09subl   $$2, $0                  \0A\09jnz    1b                      \0A\09", "=*imr,={si},{di},r,0,1,~{rax},~{memory},~{dirflag},~{fpsr},~{flags}"(i32* nonnull %5, i8* %9, i64 %2, i32 %3, i8* %10) #4, !srcloc !4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %11)
  ret void
}

; Function Attrs: cold nounwind optsize ssp uwtable
define hidden void @ff_hpeldsp_init_x86(%struct.HpelDSPContext*, i32) local_unnamed_addr #1 {
  %3 = tail call i32 @av_get_cpu_flags() #4
  %4 = and i32 %3, 1
  %5 = icmp eq i32 %4, 0
  br i1 %5, label %34, label %6

6:                                                ; preds = %2
  %7 = bitcast %struct.HpelDSPContext* %0 to <2 x void (i8*, i8*, i64, i32)*>*
  store <2 x void (i8*, i8*, i64, i32)*> <void (i8*, i8*, i64, i32)* @ff_put_pixels16_mmx, void (i8*, i8*, i64, i32)* @put_pixels16_x2_mmx>, <2 x void (i8*, i8*, i64, i32)*>* %7, align 8
  %8 = getelementptr inbounds %struct.HpelDSPContext, %struct.HpelDSPContext* %0, i64 0, i32 0, i64 0, i64 2
  %9 = bitcast void (i8*, i8*, i64, i32)** %8 to <2 x void (i8*, i8*, i64, i32)*>*
  store <2 x void (i8*, i8*, i64, i32)*> <void (i8*, i8*, i64, i32)* @put_pixels16_y2_mmx, void (i8*, i8*, i64, i32)* @ff_put_pixels16_xy2_mmx>, <2 x void (i8*, i8*, i64, i32)*>* %9, align 8
  %10 = getelementptr inbounds %struct.HpelDSPContext, %struct.HpelDSPContext* %0, i64 0, i32 2, i64 0, i64 0
  %11 = bitcast void (i8*, i8*, i64, i32)** %10 to <2 x void (i8*, i8*, i64, i32)*>*
  store <2 x void (i8*, i8*, i64, i32)*> <void (i8*, i8*, i64, i32)* @ff_put_pixels16_mmx, void (i8*, i8*, i64, i32)* @put_no_rnd_pixels16_x2_mmx>, <2 x void (i8*, i8*, i64, i32)*>* %11, align 8
  %12 = getelementptr inbounds %struct.HpelDSPContext, %struct.HpelDSPContext* %0, i64 0, i32 2, i64 0, i64 2
  %13 = bitcast void (i8*, i8*, i64, i32)** %12 to <2 x void (i8*, i8*, i64, i32)*>*
  store <2 x void (i8*, i8*, i64, i32)*> <void (i8*, i8*, i64, i32)* @put_no_rnd_pixels16_y2_mmx, void (i8*, i8*, i64, i32)* @put_no_rnd_pixels16_xy2_mmx>, <2 x void (i8*, i8*, i64, i32)*>* %13, align 8
  %14 = getelementptr inbounds %struct.HpelDSPContext, %struct.HpelDSPContext* %0, i64 0, i32 1, i64 0, i64 0
  %15 = bitcast void (i8*, i8*, i64, i32)** %14 to <2 x void (i8*, i8*, i64, i32)*>*
  store <2 x void (i8*, i8*, i64, i32)*> <void (i8*, i8*, i64, i32)* @ff_avg_pixels16_mmx, void (i8*, i8*, i64, i32)* @avg_pixels16_x2_mmx>, <2 x void (i8*, i8*, i64, i32)*>* %15, align 8
  %16 = getelementptr inbounds %struct.HpelDSPContext, %struct.HpelDSPContext* %0, i64 0, i32 1, i64 0, i64 2
  %17 = bitcast void (i8*, i8*, i64, i32)** %16 to <2 x void (i8*, i8*, i64, i32)*>*
  store <2 x void (i8*, i8*, i64, i32)*> <void (i8*, i8*, i64, i32)* @avg_pixels16_y2_mmx, void (i8*, i8*, i64, i32)* @ff_avg_pixels16_xy2_mmx>, <2 x void (i8*, i8*, i64, i32)*>* %17, align 8
  %18 = getelementptr inbounds %struct.HpelDSPContext, %struct.HpelDSPContext* %0, i64 0, i32 3, i64 0
  %19 = bitcast void (i8*, i8*, i64, i32)** %18 to <2 x void (i8*, i8*, i64, i32)*>*
  store <2 x void (i8*, i8*, i64, i32)*> <void (i8*, i8*, i64, i32)* @ff_avg_pixels16_mmx, void (i8*, i8*, i64, i32)* @avg_no_rnd_pixels16_x2_mmx>, <2 x void (i8*, i8*, i64, i32)*>* %19, align 8
  %20 = getelementptr inbounds %struct.HpelDSPContext, %struct.HpelDSPContext* %0, i64 0, i32 3, i64 2
  %21 = bitcast void (i8*, i8*, i64, i32)** %20 to <2 x void (i8*, i8*, i64, i32)*>*
  store <2 x void (i8*, i8*, i64, i32)*> <void (i8*, i8*, i64, i32)* @avg_no_rnd_pixels16_y2_mmx, void (i8*, i8*, i64, i32)* @avg_no_rnd_pixels16_xy2_mmx>, <2 x void (i8*, i8*, i64, i32)*>* %21, align 8
  %22 = getelementptr inbounds %struct.HpelDSPContext, %struct.HpelDSPContext* %0, i64 0, i32 0, i64 1, i64 0
  %23 = bitcast void (i8*, i8*, i64, i32)** %22 to <2 x void (i8*, i8*, i64, i32)*>*
  store <2 x void (i8*, i8*, i64, i32)*> <void (i8*, i8*, i64, i32)* @ff_put_pixels8_mmx, void (i8*, i8*, i64, i32)* @put_pixels8_x2_mmx>, <2 x void (i8*, i8*, i64, i32)*>* %23, align 8
  %24 = getelementptr inbounds %struct.HpelDSPContext, %struct.HpelDSPContext* %0, i64 0, i32 0, i64 1, i64 2
  %25 = bitcast void (i8*, i8*, i64, i32)** %24 to <2 x void (i8*, i8*, i64, i32)*>*
  store <2 x void (i8*, i8*, i64, i32)*> <void (i8*, i8*, i64, i32)* @put_pixels8_y2_mmx, void (i8*, i8*, i64, i32)* @ff_put_pixels8_xy2_mmx>, <2 x void (i8*, i8*, i64, i32)*>* %25, align 8
  %26 = getelementptr inbounds %struct.HpelDSPContext, %struct.HpelDSPContext* %0, i64 0, i32 2, i64 1, i64 0
  %27 = bitcast void (i8*, i8*, i64, i32)** %26 to <2 x void (i8*, i8*, i64, i32)*>*
  store <2 x void (i8*, i8*, i64, i32)*> <void (i8*, i8*, i64, i32)* @ff_put_pixels8_mmx, void (i8*, i8*, i64, i32)* @put_no_rnd_pixels8_x2_mmx>, <2 x void (i8*, i8*, i64, i32)*>* %27, align 8
  %28 = getelementptr inbounds %struct.HpelDSPContext, %struct.HpelDSPContext* %0, i64 0, i32 2, i64 1, i64 2
  %29 = bitcast void (i8*, i8*, i64, i32)** %28 to <2 x void (i8*, i8*, i64, i32)*>*
  store <2 x void (i8*, i8*, i64, i32)*> <void (i8*, i8*, i64, i32)* @put_no_rnd_pixels8_y2_mmx, void (i8*, i8*, i64, i32)* @put_no_rnd_pixels8_xy2_mmx>, <2 x void (i8*, i8*, i64, i32)*>* %29, align 8
  %30 = getelementptr inbounds %struct.HpelDSPContext, %struct.HpelDSPContext* %0, i64 0, i32 1, i64 1, i64 0
  %31 = bitcast void (i8*, i8*, i64, i32)** %30 to <2 x void (i8*, i8*, i64, i32)*>*
  store <2 x void (i8*, i8*, i64, i32)*> <void (i8*, i8*, i64, i32)* @ff_avg_pixels8_mmx, void (i8*, i8*, i64, i32)* @ff_avg_pixels8_x2_mmx>, <2 x void (i8*, i8*, i64, i32)*>* %31, align 8
  %32 = getelementptr inbounds %struct.HpelDSPContext, %struct.HpelDSPContext* %0, i64 0, i32 1, i64 1, i64 2
  %33 = bitcast void (i8*, i8*, i64, i32)** %32 to <2 x void (i8*, i8*, i64, i32)*>*
  store <2 x void (i8*, i8*, i64, i32)*> <void (i8*, i8*, i64, i32)* @avg_pixels8_y2_mmx, void (i8*, i8*, i64, i32)* @ff_avg_pixels8_xy2_mmx>, <2 x void (i8*, i8*, i64, i32)*>* %33, align 8
  br label %34

34:                                               ; preds = %2, %6
  %35 = and i32 %3, 4
  %36 = icmp eq i32 %35, 0
  br i1 %36, label %59, label %37

37:                                               ; preds = %34
  %38 = getelementptr inbounds %struct.HpelDSPContext, %struct.HpelDSPContext* %0, i64 0, i32 0, i64 0, i64 1
  %39 = bitcast void (i8*, i8*, i64, i32)** %38 to <2 x void (i8*, i8*, i64, i32)*>*
  store <2 x void (i8*, i8*, i64, i32)*> <void (i8*, i8*, i64, i32)* @ff_put_pixels16_x2_3dnow, void (i8*, i8*, i64, i32)* @put_pixels16_y2_3dnow>, <2 x void (i8*, i8*, i64, i32)*>* %39, align 8
  %40 = getelementptr inbounds %struct.HpelDSPContext, %struct.HpelDSPContext* %0, i64 0, i32 1, i64 0, i64 0
  %41 = bitcast void (i8*, i8*, i64, i32)** %40 to <2 x void (i8*, i8*, i64, i32)*>*
  store <2 x void (i8*, i8*, i64, i32)*> <void (i8*, i8*, i64, i32)* @avg_pixels16_3dnow, void (i8*, i8*, i64, i32)* @avg_pixels16_x2_3dnow>, <2 x void (i8*, i8*, i64, i32)*>* %41, align 8
  %42 = getelementptr inbounds %struct.HpelDSPContext, %struct.HpelDSPContext* %0, i64 0, i32 1, i64 0, i64 2
  %43 = bitcast void (i8*, i8*, i64, i32)** %42 to <2 x void (i8*, i8*, i64, i32)*>*
  store <2 x void (i8*, i8*, i64, i32)*> <void (i8*, i8*, i64, i32)* @avg_pixels16_y2_3dnow, void (i8*, i8*, i64, i32)* @avg_pixels16_xy2_3dnow>, <2 x void (i8*, i8*, i64, i32)*>* %43, align 8
  %44 = getelementptr inbounds %struct.HpelDSPContext, %struct.HpelDSPContext* %0, i64 0, i32 0, i64 1, i64 1
  %45 = bitcast void (i8*, i8*, i64, i32)** %44 to <2 x void (i8*, i8*, i64, i32)*>*
  store <2 x void (i8*, i8*, i64, i32)*> <void (i8*, i8*, i64, i32)* @ff_put_pixels8_x2_3dnow, void (i8*, i8*, i64, i32)* @ff_put_pixels8_y2_3dnow>, <2 x void (i8*, i8*, i64, i32)*>* %45, align 8
  %46 = getelementptr inbounds %struct.HpelDSPContext, %struct.HpelDSPContext* %0, i64 0, i32 1, i64 1, i64 0
  %47 = bitcast void (i8*, i8*, i64, i32)** %46 to <2 x void (i8*, i8*, i64, i32)*>*
  store <2 x void (i8*, i8*, i64, i32)*> <void (i8*, i8*, i64, i32)* @ff_avg_pixels8_3dnow, void (i8*, i8*, i64, i32)* @ff_avg_pixels8_x2_3dnow>, <2 x void (i8*, i8*, i64, i32)*>* %47, align 8
  %48 = getelementptr inbounds %struct.HpelDSPContext, %struct.HpelDSPContext* %0, i64 0, i32 1, i64 1, i64 2
  %49 = bitcast void (i8*, i8*, i64, i32)** %48 to <2 x void (i8*, i8*, i64, i32)*>*
  store <2 x void (i8*, i8*, i64, i32)*> <void (i8*, i8*, i64, i32)* @ff_avg_pixels8_y2_3dnow, void (i8*, i8*, i64, i32)* @ff_avg_pixels8_xy2_3dnow>, <2 x void (i8*, i8*, i64, i32)*>* %49, align 8
  %50 = and i32 %1, 8388608
  %51 = icmp eq i32 %50, 0
  br i1 %51, label %52, label %59

52:                                               ; preds = %37
  %53 = getelementptr inbounds %struct.HpelDSPContext, %struct.HpelDSPContext* %0, i64 0, i32 1, i64 1, i64 3
  %54 = getelementptr inbounds %struct.HpelDSPContext, %struct.HpelDSPContext* %0, i64 0, i32 1, i64 0, i64 3
  %55 = getelementptr inbounds %struct.HpelDSPContext, %struct.HpelDSPContext* %0, i64 0, i32 2, i64 0, i64 1
  %56 = bitcast void (i8*, i8*, i64, i32)** %55 to <2 x void (i8*, i8*, i64, i32)*>*
  store <2 x void (i8*, i8*, i64, i32)*> <void (i8*, i8*, i64, i32)* @put_no_rnd_pixels16_x2_3dnow, void (i8*, i8*, i64, i32)* @put_no_rnd_pixels16_y2_3dnow>, <2 x void (i8*, i8*, i64, i32)*>* %56, align 8
  %57 = getelementptr inbounds %struct.HpelDSPContext, %struct.HpelDSPContext* %0, i64 0, i32 2, i64 1, i64 1
  %58 = bitcast void (i8*, i8*, i64, i32)** %57 to <2 x void (i8*, i8*, i64, i32)*>*
  store <2 x void (i8*, i8*, i64, i32)*> <void (i8*, i8*, i64, i32)* @ff_put_no_rnd_pixels8_x2_3dnow, void (i8*, i8*, i64, i32)* @ff_put_no_rnd_pixels8_y2_3dnow>, <2 x void (i8*, i8*, i64, i32)*>* %58, align 8
  store void (i8*, i8*, i64, i32)* @avg_approx_pixels16_xy2_3dnow, void (i8*, i8*, i64, i32)** %54, align 8
  store void (i8*, i8*, i64, i32)* @ff_avg_approx_pixels8_xy2_3dnow, void (i8*, i8*, i64, i32)** %53, align 8
  br label %59

59:                                               ; preds = %52, %37, %34
  %60 = and i32 %3, 2
  %61 = icmp eq i32 %60, 0
  br i1 %61, label %84, label %62

62:                                               ; preds = %59
  %63 = getelementptr inbounds %struct.HpelDSPContext, %struct.HpelDSPContext* %0, i64 0, i32 0, i64 0, i64 1
  %64 = bitcast void (i8*, i8*, i64, i32)** %63 to <2 x void (i8*, i8*, i64, i32)*>*
  store <2 x void (i8*, i8*, i64, i32)*> <void (i8*, i8*, i64, i32)* @ff_put_pixels16_x2_mmxext, void (i8*, i8*, i64, i32)* @put_pixels16_y2_mmxext>, <2 x void (i8*, i8*, i64, i32)*>* %64, align 8
  %65 = getelementptr inbounds %struct.HpelDSPContext, %struct.HpelDSPContext* %0, i64 0, i32 1, i64 0, i64 0
  %66 = bitcast void (i8*, i8*, i64, i32)** %65 to <2 x void (i8*, i8*, i64, i32)*>*
  store <2 x void (i8*, i8*, i64, i32)*> <void (i8*, i8*, i64, i32)* @avg_pixels16_mmxext, void (i8*, i8*, i64, i32)* @avg_pixels16_x2_mmxext>, <2 x void (i8*, i8*, i64, i32)*>* %66, align 8
  %67 = getelementptr inbounds %struct.HpelDSPContext, %struct.HpelDSPContext* %0, i64 0, i32 1, i64 0, i64 2
  %68 = bitcast void (i8*, i8*, i64, i32)** %67 to <2 x void (i8*, i8*, i64, i32)*>*
  store <2 x void (i8*, i8*, i64, i32)*> <void (i8*, i8*, i64, i32)* @avg_pixels16_y2_mmxext, void (i8*, i8*, i64, i32)* @avg_pixels16_xy2_mmxext>, <2 x void (i8*, i8*, i64, i32)*>* %68, align 8
  %69 = getelementptr inbounds %struct.HpelDSPContext, %struct.HpelDSPContext* %0, i64 0, i32 0, i64 1, i64 1
  %70 = bitcast void (i8*, i8*, i64, i32)** %69 to <2 x void (i8*, i8*, i64, i32)*>*
  store <2 x void (i8*, i8*, i64, i32)*> <void (i8*, i8*, i64, i32)* @ff_put_pixels8_x2_mmxext, void (i8*, i8*, i64, i32)* @ff_put_pixels8_y2_mmxext>, <2 x void (i8*, i8*, i64, i32)*>* %70, align 8
  %71 = getelementptr inbounds %struct.HpelDSPContext, %struct.HpelDSPContext* %0, i64 0, i32 1, i64 1, i64 0
  %72 = bitcast void (i8*, i8*, i64, i32)** %71 to <2 x void (i8*, i8*, i64, i32)*>*
  store <2 x void (i8*, i8*, i64, i32)*> <void (i8*, i8*, i64, i32)* @ff_avg_pixels8_mmxext, void (i8*, i8*, i64, i32)* @ff_avg_pixels8_x2_mmxext>, <2 x void (i8*, i8*, i64, i32)*>* %72, align 8
  %73 = getelementptr inbounds %struct.HpelDSPContext, %struct.HpelDSPContext* %0, i64 0, i32 1, i64 1, i64 2
  %74 = bitcast void (i8*, i8*, i64, i32)** %73 to <2 x void (i8*, i8*, i64, i32)*>*
  store <2 x void (i8*, i8*, i64, i32)*> <void (i8*, i8*, i64, i32)* @ff_avg_pixels8_y2_mmxext, void (i8*, i8*, i64, i32)* @ff_avg_pixels8_xy2_mmxext>, <2 x void (i8*, i8*, i64, i32)*>* %74, align 8
  %75 = and i32 %1, 8388608
  %76 = icmp eq i32 %75, 0
  br i1 %76, label %77, label %84

77:                                               ; preds = %62
  %78 = getelementptr inbounds %struct.HpelDSPContext, %struct.HpelDSPContext* %0, i64 0, i32 1, i64 1, i64 3
  %79 = getelementptr inbounds %struct.HpelDSPContext, %struct.HpelDSPContext* %0, i64 0, i32 1, i64 0, i64 3
  %80 = getelementptr inbounds %struct.HpelDSPContext, %struct.HpelDSPContext* %0, i64 0, i32 2, i64 0, i64 1
  %81 = bitcast void (i8*, i8*, i64, i32)** %80 to <2 x void (i8*, i8*, i64, i32)*>*
  store <2 x void (i8*, i8*, i64, i32)*> <void (i8*, i8*, i64, i32)* @put_no_rnd_pixels16_x2_mmxext, void (i8*, i8*, i64, i32)* @put_no_rnd_pixels16_y2_mmxext>, <2 x void (i8*, i8*, i64, i32)*>* %81, align 8
  %82 = getelementptr inbounds %struct.HpelDSPContext, %struct.HpelDSPContext* %0, i64 0, i32 2, i64 1, i64 1
  %83 = bitcast void (i8*, i8*, i64, i32)** %82 to <2 x void (i8*, i8*, i64, i32)*>*
  store <2 x void (i8*, i8*, i64, i32)*> <void (i8*, i8*, i64, i32)* @ff_put_no_rnd_pixels8_x2_mmxext, void (i8*, i8*, i64, i32)* @ff_put_no_rnd_pixels8_y2_mmxext>, <2 x void (i8*, i8*, i64, i32)*>* %83, align 8
  store void (i8*, i8*, i64, i32)* @avg_approx_pixels16_xy2_mmxext, void (i8*, i8*, i64, i32)** %79, align 8
  store void (i8*, i8*, i64, i32)* @ff_avg_approx_pixels8_xy2_mmxext, void (i8*, i8*, i64, i32)** %78, align 8
  br label %84

84:                                               ; preds = %77, %62, %59
  %85 = and i32 %3, 1073741840
  %86 = icmp eq i32 %85, 16
  br i1 %86, label %87, label %96

87:                                               ; preds = %84
  %88 = getelementptr inbounds %struct.HpelDSPContext, %struct.HpelDSPContext* %0, i64 0, i32 2, i64 0, i64 0
  store void (i8*, i8*, i64, i32)* @ff_put_pixels16_sse2, void (i8*, i8*, i64, i32)** %88, align 8
  %89 = bitcast %struct.HpelDSPContext* %0 to <2 x void (i8*, i8*, i64, i32)*>*
  store <2 x void (i8*, i8*, i64, i32)*> <void (i8*, i8*, i64, i32)* @ff_put_pixels16_sse2, void (i8*, i8*, i64, i32)* @ff_put_pixels16_x2_sse2>, <2 x void (i8*, i8*, i64, i32)*>* %89, align 8
  %90 = getelementptr inbounds %struct.HpelDSPContext, %struct.HpelDSPContext* %0, i64 0, i32 0, i64 0, i64 2
  %91 = bitcast void (i8*, i8*, i64, i32)** %90 to <2 x void (i8*, i8*, i64, i32)*>*
  store <2 x void (i8*, i8*, i64, i32)*> <void (i8*, i8*, i64, i32)* @ff_put_pixels16_y2_sse2, void (i8*, i8*, i64, i32)* @ff_put_pixels16_xy2_sse2>, <2 x void (i8*, i8*, i64, i32)*>* %91, align 8
  %92 = getelementptr inbounds %struct.HpelDSPContext, %struct.HpelDSPContext* %0, i64 0, i32 1, i64 0, i64 0
  %93 = bitcast void (i8*, i8*, i64, i32)** %92 to <2 x void (i8*, i8*, i64, i32)*>*
  store <2 x void (i8*, i8*, i64, i32)*> <void (i8*, i8*, i64, i32)* @ff_avg_pixels16_sse2, void (i8*, i8*, i64, i32)* @ff_avg_pixels16_x2_sse2>, <2 x void (i8*, i8*, i64, i32)*>* %93, align 8
  %94 = getelementptr inbounds %struct.HpelDSPContext, %struct.HpelDSPContext* %0, i64 0, i32 1, i64 0, i64 2
  %95 = bitcast void (i8*, i8*, i64, i32)** %94 to <2 x void (i8*, i8*, i64, i32)*>*
  store <2 x void (i8*, i8*, i64, i32)*> <void (i8*, i8*, i64, i32)* @ff_avg_pixels16_y2_sse2, void (i8*, i8*, i64, i32)* @ff_avg_pixels16_xy2_sse2>, <2 x void (i8*, i8*, i64, i32)*>* %95, align 8
  br label %96

96:                                               ; preds = %84, %87
  %97 = trunc i32 %3 to i8
  %98 = icmp slt i8 %97, 0
  br i1 %98, label %99, label %104

99:                                               ; preds = %96
  %100 = getelementptr inbounds %struct.HpelDSPContext, %struct.HpelDSPContext* %0, i64 0, i32 0, i64 0, i64 3
  store void (i8*, i8*, i64, i32)* @ff_put_pixels16_xy2_ssse3, void (i8*, i8*, i64, i32)** %100, align 8
  %101 = getelementptr inbounds %struct.HpelDSPContext, %struct.HpelDSPContext* %0, i64 0, i32 1, i64 0, i64 3
  store void (i8*, i8*, i64, i32)* @ff_avg_pixels16_xy2_ssse3, void (i8*, i8*, i64, i32)** %101, align 8
  %102 = getelementptr inbounds %struct.HpelDSPContext, %struct.HpelDSPContext* %0, i64 0, i32 0, i64 1, i64 3
  store void (i8*, i8*, i64, i32)* @ff_put_pixels8_xy2_ssse3, void (i8*, i8*, i64, i32)** %102, align 8
  %103 = getelementptr inbounds %struct.HpelDSPContext, %struct.HpelDSPContext* %0, i64 0, i32 1, i64 1, i64 3
  store void (i8*, i8*, i64, i32)* @ff_avg_pixels8_xy2_ssse3, void (i8*, i8*, i64, i32)** %103, align 8
  br label %104

104:                                              ; preds = %99, %96
  tail call void @ff_hpeldsp_vp3_init_x86(%struct.HpelDSPContext* %0, i32 %3, i32 %1) #4
  ret void
}

declare i32 @av_get_cpu_flags() local_unnamed_addr #2

declare void @ff_hpeldsp_vp3_init_x86(%struct.HpelDSPContext*, i32, i32) local_unnamed_addr #2

declare void @ff_put_pixels16_mmx(i8*, i8*, i64, i32) #2

; Function Attrs: nounwind ssp uwtable
define internal void @put_pixels16_x2_mmx(i8*, i8*, i64, i32) #0 {
  %5 = alloca i32, align 4
  store i32 %3, i32* %5, align 4
  tail call void asm sideeffect "pcmpeqd %mm6, %mm6   \0A\09paddb   %mm6, %mm6   \0A\09", "~{dirflag},~{fpsr},~{flags}"() #4, !srcloc !8
  %6 = call { i8*, i8* } asm sideeffect "lea    ($3, $3), %rax  \0A\09.p2align 3                     \0A\091:                             \0A\09movq   ($1), %mm0             \0A\09movq   1($1), %mm1            \0A\09movq   ($1, $3), %mm2         \0A\09movq   1($1, $3), %mm3        \0A\09movq  %mm0, %mm4             \0A\09movq  %mm2, %mm5             \0A\09por   %mm1, %mm4             \0A\09por   %mm3, %mm5             \0A\09pxor  %mm0, %mm1             \0A\09pxor  %mm2, %mm3             \0A\09pand    %mm6, %mm1             \0A\09pand    %mm6, %mm3             \0A\09psrlq      $$1, %mm3             \0A\09psrlq      $$1, %mm1             \0A\09psubb %mm1, %mm4             \0A\09psubb %mm3, %mm5             \0A\09movq   %mm4, ($2)             \0A\09movq   %mm5, ($2, $3)         \0A\09movq   8($1), %mm0            \0A\09movq   9($1), %mm1            \0A\09movq   8($1, $3), %mm2        \0A\09movq   9($1, $3), %mm3        \0A\09movq  %mm0, %mm4             \0A\09movq  %mm2, %mm5             \0A\09por   %mm1, %mm4             \0A\09por   %mm3, %mm5             \0A\09pxor  %mm0, %mm1             \0A\09pxor  %mm2, %mm3             \0A\09pand    %mm6, %mm1             \0A\09pand    %mm6, %mm3             \0A\09psrlq      $$1, %mm3             \0A\09psrlq      $$1, %mm1             \0A\09psubb %mm1, %mm4             \0A\09psubb %mm3, %mm5             \0A\09movq   %mm4, 8($2)            \0A\09movq   %mm5, 8($2, $3)        \0A\09add    %rax, $1        \0A\09add    %rax, $2        \0A\09movq   ($1), %mm0             \0A\09movq   1($1), %mm1            \0A\09movq   ($1, $3), %mm2         \0A\09movq   1($1, $3), %mm3        \0A\09movq  %mm0, %mm4             \0A\09movq  %mm2, %mm5             \0A\09por   %mm1, %mm4             \0A\09por   %mm3, %mm5             \0A\09pxor  %mm0, %mm1             \0A\09pxor  %mm2, %mm3             \0A\09pand    %mm6, %mm1             \0A\09pand    %mm6, %mm3             \0A\09psrlq      $$1, %mm3             \0A\09psrlq      $$1, %mm1             \0A\09psubb %mm1, %mm4             \0A\09psubb %mm3, %mm5             \0A\09movq   %mm4, ($2)             \0A\09movq   %mm5, ($2, $3)         \0A\09movq   8($1), %mm0            \0A\09movq   9($1), %mm1            \0A\09movq   8($1, $3), %mm2        \0A\09movq   9($1, $3), %mm3        \0A\09movq  %mm0, %mm4             \0A\09movq  %mm2, %mm5             \0A\09por   %mm1, %mm4             \0A\09por   %mm3, %mm5             \0A\09pxor  %mm0, %mm1             \0A\09pxor  %mm2, %mm3             \0A\09pand    %mm6, %mm1             \0A\09pand    %mm6, %mm3             \0A\09psrlq      $$1, %mm3             \0A\09psrlq      $$1, %mm1             \0A\09psubb %mm1, %mm4             \0A\09psubb %mm3, %mm5             \0A\09movq   %mm4, 8($2)            \0A\09movq   %mm5, 8($2, $3)        \0A\09add    %rax, $1        \0A\09add    %rax, $2        \0A\09subl   $$4, $0                  \0A\09jnz    1b                      \0A\09", "=*imr,={si},={di},r,0,1,2,~{rax},~{memory},~{dirflag},~{fpsr},~{flags}"(i32* nonnull %5, i64 %2, i32 %3, i8* %1, i8* %0) #4, !srcloc !9
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal void @put_pixels16_y2_mmx(i8*, i8*, i64, i32) #0 {
  %5 = alloca i32, align 4
  %6 = alloca i32, align 4
  %7 = bitcast i32* %6 to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %7)
  store i32 %3, i32* %6, align 4
  tail call void asm sideeffect "pcmpeqd %mm6, %mm6   \0A\09paddb   %mm6, %mm6   \0A\09", "~{dirflag},~{fpsr},~{flags}"() #4, !srcloc !10
  %8 = call { i8*, i8* } asm sideeffect "lea ($3, $3), %rax     \0A\09movq ($1), %mm0               \0A\09.p2align 3                     \0A\091:                             \0A\09movq   ($1, $3), %mm1         \0A\09movq   ($1, %rax),%mm2\0A\09movq  %mm1, %mm4             \0A\09movq  %mm2, %mm5             \0A\09por   %mm0, %mm4             \0A\09por   %mm1, %mm5             \0A\09pxor  %mm1, %mm0             \0A\09pxor  %mm2, %mm1             \0A\09pand    %mm6, %mm0             \0A\09pand    %mm6, %mm1             \0A\09psrlq      $$1, %mm1             \0A\09psrlq      $$1, %mm0             \0A\09psubb %mm0, %mm4             \0A\09psubb %mm1, %mm5             \0A\09movq   %mm4, ($2)             \0A\09movq   %mm5, ($2, $3)         \0A\09add    %rax, $1        \0A\09add    %rax, $2        \0A\09movq   ($1, $3), %mm1         \0A\09movq   ($1, %rax),%mm0\0A\09movq  %mm1, %mm4             \0A\09movq  %mm0, %mm5             \0A\09por   %mm2, %mm4             \0A\09por   %mm1, %mm5             \0A\09pxor  %mm1, %mm2             \0A\09pxor  %mm0, %mm1             \0A\09pand    %mm6, %mm2             \0A\09pand    %mm6, %mm1             \0A\09psrlq      $$1, %mm1             \0A\09psrlq      $$1, %mm2             \0A\09psubb %mm2, %mm4             \0A\09psubb %mm1, %mm5             \0A\09movq   %mm4, ($2)             \0A\09movq   %mm5, ($2, $3)         \0A\09add    %rax, $1        \0A\09add    %rax, $2        \0A\09subl   $$4, $0                  \0A\09jnz    1b                      \0A\09", "=*imr,={si},={di},r,0,1,2,~{rax},~{memory},~{dirflag},~{fpsr},~{flags}"(i32* nonnull %6, i64 %2, i32 %3, i8* %1, i8* %0) #4, !srcloc !11
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %7)
  %9 = getelementptr inbounds i8, i8* %0, i64 8
  %10 = getelementptr inbounds i8, i8* %1, i64 8
  %11 = bitcast i32* %5 to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %11)
  store i32 %3, i32* %5, align 4
  call void asm sideeffect "pcmpeqd %mm6, %mm6   \0A\09paddb   %mm6, %mm6   \0A\09", "~{dirflag},~{fpsr},~{flags}"() #4, !srcloc !10
  %12 = call { i8*, i8* } asm sideeffect "lea ($3, $3), %rax     \0A\09movq ($1), %mm0               \0A\09.p2align 3                     \0A\091:                             \0A\09movq   ($1, $3), %mm1         \0A\09movq   ($1, %rax),%mm2\0A\09movq  %mm1, %mm4             \0A\09movq  %mm2, %mm5             \0A\09por   %mm0, %mm4             \0A\09por   %mm1, %mm5             \0A\09pxor  %mm1, %mm0             \0A\09pxor  %mm2, %mm1             \0A\09pand    %mm6, %mm0             \0A\09pand    %mm6, %mm1             \0A\09psrlq      $$1, %mm1             \0A\09psrlq      $$1, %mm0             \0A\09psubb %mm0, %mm4             \0A\09psubb %mm1, %mm5             \0A\09movq   %mm4, ($2)             \0A\09movq   %mm5, ($2, $3)         \0A\09add    %rax, $1        \0A\09add    %rax, $2        \0A\09movq   ($1, $3), %mm1         \0A\09movq   ($1, %rax),%mm0\0A\09movq  %mm1, %mm4             \0A\09movq  %mm0, %mm5             \0A\09por   %mm2, %mm4             \0A\09por   %mm1, %mm5             \0A\09pxor  %mm1, %mm2             \0A\09pxor  %mm0, %mm1             \0A\09pand    %mm6, %mm2             \0A\09pand    %mm6, %mm1             \0A\09psrlq      $$1, %mm1             \0A\09psrlq      $$1, %mm2             \0A\09psubb %mm2, %mm4             \0A\09psubb %mm1, %mm5             \0A\09movq   %mm4, ($2)             \0A\09movq   %mm5, ($2, $3)         \0A\09add    %rax, $1        \0A\09add    %rax, $2        \0A\09subl   $$4, $0                  \0A\09jnz    1b                      \0A\09", "=*imr,={si},={di},r,0,1,2,~{rax},~{memory},~{dirflag},~{fpsr},~{flags}"(i32* nonnull %5, i64 %2, i32 %3, i8* %10, i8* %9) #4, !srcloc !11
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %11)
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal void @put_no_rnd_pixels16_x2_mmx(i8*, i8*, i64, i32) #0 {
  %5 = alloca i32, align 4
  store i32 %3, i32* %5, align 4
  tail call void asm sideeffect "pcmpeqd %mm6, %mm6   \0A\09paddb   %mm6, %mm6   \0A\09", "~{dirflag},~{fpsr},~{flags}"() #4, !srcloc !12
  %6 = call { i8*, i8* } asm sideeffect "lea    ($3, $3), %rax  \0A\09.p2align 3                     \0A\091:                             \0A\09movq   ($1), %mm0             \0A\09movq   1($1), %mm1            \0A\09movq   ($1, $3), %mm2         \0A\09movq   1($1, $3), %mm3        \0A\09movq  %mm0, %mm4             \0A\09movq  %mm2, %mm5             \0A\09pand  %mm1, %mm4             \0A\09pand  %mm3, %mm5             \0A\09pxor  %mm0, %mm1             \0A\09pxor  %mm2, %mm3             \0A\09pand    %mm6, %mm1             \0A\09pand    %mm6, %mm3             \0A\09psrlq      $$1, %mm1             \0A\09psrlq      $$1, %mm3             \0A\09paddb %mm1, %mm4             \0A\09paddb %mm3, %mm5             \0A\09movq   %mm4, ($2)             \0A\09movq   %mm5, ($2, $3)         \0A\09movq   8($1), %mm0            \0A\09movq   9($1), %mm1            \0A\09movq   8($1, $3), %mm2        \0A\09movq   9($1, $3), %mm3        \0A\09movq  %mm0, %mm4             \0A\09movq  %mm2, %mm5             \0A\09pand  %mm1, %mm4             \0A\09pand  %mm3, %mm5             \0A\09pxor  %mm0, %mm1             \0A\09pxor  %mm2, %mm3             \0A\09pand    %mm6, %mm1             \0A\09pand    %mm6, %mm3             \0A\09psrlq      $$1, %mm1             \0A\09psrlq      $$1, %mm3             \0A\09paddb %mm1, %mm4             \0A\09paddb %mm3, %mm5             \0A\09movq   %mm4, 8($2)            \0A\09movq   %mm5, 8($2, $3)        \0A\09add    %rax, $1        \0A\09add    %rax, $2        \0A\09movq   ($1), %mm0             \0A\09movq   1($1), %mm1            \0A\09movq   ($1, $3), %mm2         \0A\09movq   1($1, $3), %mm3        \0A\09movq  %mm0, %mm4             \0A\09movq  %mm2, %mm5             \0A\09pand  %mm1, %mm4             \0A\09pand  %mm3, %mm5             \0A\09pxor  %mm0, %mm1             \0A\09pxor  %mm2, %mm3             \0A\09pand    %mm6, %mm1             \0A\09pand    %mm6, %mm3             \0A\09psrlq      $$1, %mm1             \0A\09psrlq      $$1, %mm3             \0A\09paddb %mm1, %mm4             \0A\09paddb %mm3, %mm5             \0A\09movq   %mm4, ($2)             \0A\09movq   %mm5, ($2, $3)         \0A\09movq   8($1), %mm0            \0A\09movq   9($1), %mm1            \0A\09movq   8($1, $3), %mm2        \0A\09movq   9($1, $3), %mm3        \0A\09movq  %mm0, %mm4             \0A\09movq  %mm2, %mm5             \0A\09pand  %mm1, %mm4             \0A\09pand  %mm3, %mm5             \0A\09pxor  %mm0, %mm1             \0A\09pxor  %mm2, %mm3             \0A\09pand    %mm6, %mm1             \0A\09pand    %mm6, %mm3             \0A\09psrlq      $$1, %mm1             \0A\09psrlq      $$1, %mm3             \0A\09paddb %mm1, %mm4             \0A\09paddb %mm3, %mm5             \0A\09movq   %mm4, 8($2)            \0A\09movq   %mm5, 8($2, $3)        \0A\09add    %rax, $1        \0A\09add    %rax, $2        \0A\09subl   $$4, $0                  \0A\09jnz    1b                      \0A\09", "=*imr,={si},={di},r,0,1,2,~{rax},~{memory},~{dirflag},~{fpsr},~{flags}"(i32* nonnull %5, i64 %2, i32 %3, i8* %1, i8* %0) #4, !srcloc !13
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal void @put_no_rnd_pixels16_y2_mmx(i8*, i8*, i64, i32) #0 {
  %5 = alloca i32, align 4
  %6 = alloca i32, align 4
  %7 = bitcast i32* %6 to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %7)
  store i32 %3, i32* %6, align 4
  tail call void asm sideeffect "pcmpeqd %mm6, %mm6   \0A\09paddb   %mm6, %mm6   \0A\09", "~{dirflag},~{fpsr},~{flags}"() #4, !srcloc !14
  %8 = call { i8*, i8* } asm sideeffect "lea ($3, $3), %rax     \0A\09movq ($1), %mm0               \0A\09.p2align 3                     \0A\091:                             \0A\09movq   ($1, $3), %mm1         \0A\09movq   ($1, %rax),%mm2\0A\09movq  %mm1, %mm4             \0A\09movq  %mm2, %mm5             \0A\09pand  %mm0, %mm4             \0A\09pand  %mm1, %mm5             \0A\09pxor  %mm1, %mm0             \0A\09pxor  %mm2, %mm1             \0A\09pand    %mm6, %mm0             \0A\09pand    %mm6, %mm1             \0A\09psrlq      $$1, %mm0             \0A\09psrlq      $$1, %mm1             \0A\09paddb %mm0, %mm4             \0A\09paddb %mm1, %mm5             \0A\09movq   %mm4, ($2)             \0A\09movq   %mm5, ($2, $3)         \0A\09add    %rax, $1        \0A\09add    %rax, $2        \0A\09movq   ($1, $3), %mm1         \0A\09movq   ($1, %rax),%mm0\0A\09movq  %mm1, %mm4             \0A\09movq  %mm0, %mm5             \0A\09pand  %mm2, %mm4             \0A\09pand  %mm1, %mm5             \0A\09pxor  %mm1, %mm2             \0A\09pxor  %mm0, %mm1             \0A\09pand    %mm6, %mm2             \0A\09pand    %mm6, %mm1             \0A\09psrlq      $$1, %mm2             \0A\09psrlq      $$1, %mm1             \0A\09paddb %mm2, %mm4             \0A\09paddb %mm1, %mm5             \0A\09movq   %mm4, ($2)             \0A\09movq   %mm5, ($2, $3)         \0A\09add    %rax, $1        \0A\09add    %rax, $2        \0A\09subl   $$4, $0                  \0A\09jnz    1b                      \0A\09", "=*imr,={si},={di},r,0,1,2,~{rax},~{memory},~{dirflag},~{fpsr},~{flags}"(i32* nonnull %6, i64 %2, i32 %3, i8* %1, i8* %0) #4, !srcloc !15
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %7)
  %9 = getelementptr inbounds i8, i8* %0, i64 8
  %10 = getelementptr inbounds i8, i8* %1, i64 8
  %11 = bitcast i32* %5 to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %11)
  store i32 %3, i32* %5, align 4
  call void asm sideeffect "pcmpeqd %mm6, %mm6   \0A\09paddb   %mm6, %mm6   \0A\09", "~{dirflag},~{fpsr},~{flags}"() #4, !srcloc !14
  %12 = call { i8*, i8* } asm sideeffect "lea ($3, $3), %rax     \0A\09movq ($1), %mm0               \0A\09.p2align 3                     \0A\091:                             \0A\09movq   ($1, $3), %mm1         \0A\09movq   ($1, %rax),%mm2\0A\09movq  %mm1, %mm4             \0A\09movq  %mm2, %mm5             \0A\09pand  %mm0, %mm4             \0A\09pand  %mm1, %mm5             \0A\09pxor  %mm1, %mm0             \0A\09pxor  %mm2, %mm1             \0A\09pand    %mm6, %mm0             \0A\09pand    %mm6, %mm1             \0A\09psrlq      $$1, %mm0             \0A\09psrlq      $$1, %mm1             \0A\09paddb %mm0, %mm4             \0A\09paddb %mm1, %mm5             \0A\09movq   %mm4, ($2)             \0A\09movq   %mm5, ($2, $3)         \0A\09add    %rax, $1        \0A\09add    %rax, $2        \0A\09movq   ($1, $3), %mm1         \0A\09movq   ($1, %rax),%mm0\0A\09movq  %mm1, %mm4             \0A\09movq  %mm0, %mm5             \0A\09pand  %mm2, %mm4             \0A\09pand  %mm1, %mm5             \0A\09pxor  %mm1, %mm2             \0A\09pxor  %mm0, %mm1             \0A\09pand    %mm6, %mm2             \0A\09pand    %mm6, %mm1             \0A\09psrlq      $$1, %mm2             \0A\09psrlq      $$1, %mm1             \0A\09paddb %mm2, %mm4             \0A\09paddb %mm1, %mm5             \0A\09movq   %mm4, ($2)             \0A\09movq   %mm5, ($2, $3)         \0A\09add    %rax, $1        \0A\09add    %rax, $2        \0A\09subl   $$4, $0                  \0A\09jnz    1b                      \0A\09", "=*imr,={si},={di},r,0,1,2,~{rax},~{memory},~{dirflag},~{fpsr},~{flags}"(i32* nonnull %5, i64 %2, i32 %3, i8* %10, i8* %9) #4, !srcloc !15
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %11)
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal void @put_no_rnd_pixels16_xy2_mmx(i8*, i8*, i64, i32) #0 {
  %5 = alloca i32, align 4
  %6 = alloca i32, align 4
  %7 = bitcast i32* %6 to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %7)
  store i32 %3, i32* %6, align 4
  tail call void asm sideeffect "pxor %mm7, %mm7", "~{dirflag},~{fpsr},~{flags}"() #4, !srcloc !16
  tail call void asm sideeffect "pcmpeqd %mm6, %mm6 \0A\09psrlw $$15, %mm6", "~{dirflag},~{fpsr},~{flags}"() #4, !srcloc !17
  %8 = call i8* asm sideeffect "movq   ($1), %mm0             \0A\09movq   1($1), %mm4            \0A\09movq   %mm0, %mm1            \0A\09movq   %mm4, %mm5            \0A\09punpcklbw %mm7, %mm0         \0A\09punpcklbw %mm7, %mm4         \0A\09punpckhbw %mm7, %mm1         \0A\09punpckhbw %mm7, %mm5         \0A\09paddusw %mm0, %mm4           \0A\09paddusw %mm1, %mm5           \0A\09xor    %rax, %rax \0A\09add    $3, $1                  \0A\09.p2align 3                     \0A\091:                             \0A\09movq   ($1, %rax), %mm0  \0A\09movq   1($1, %rax), %mm2 \0A\09movq   %mm0, %mm1            \0A\09movq   %mm2, %mm3            \0A\09punpcklbw %mm7, %mm0         \0A\09punpcklbw %mm7, %mm2         \0A\09punpckhbw %mm7, %mm1         \0A\09punpckhbw %mm7, %mm3         \0A\09paddusw %mm2, %mm0           \0A\09paddusw %mm3, %mm1           \0A\09paddusw %mm6, %mm4           \0A\09paddusw %mm6, %mm5           \0A\09paddusw %mm0, %mm4           \0A\09paddusw %mm1, %mm5           \0A\09psrlw  $$2, %mm4               \0A\09psrlw  $$2, %mm5               \0A\09packuswb  %mm5, %mm4         \0A\09movq   %mm4, ($2, %rax)  \0A\09add    $3, %rax           \0A\09movq   ($1, %rax), %mm2  \0A\09movq   1($1, %rax), %mm4 \0A\09movq   %mm2, %mm3            \0A\09movq   %mm4, %mm5            \0A\09punpcklbw %mm7, %mm2         \0A\09punpcklbw %mm7, %mm4         \0A\09punpckhbw %mm7, %mm3         \0A\09punpckhbw %mm7, %mm5         \0A\09paddusw %mm2, %mm4           \0A\09paddusw %mm3, %mm5           \0A\09paddusw %mm6, %mm0           \0A\09paddusw %mm6, %mm1           \0A\09paddusw %mm4, %mm0           \0A\09paddusw %mm5, %mm1           \0A\09psrlw  $$2, %mm0               \0A\09psrlw  $$2, %mm1               \0A\09packuswb  %mm1, %mm0         \0A\09movq   %mm0, ($2, %rax)  \0A\09add    $3, %rax        \0A\09subl   $$2, $0                  \0A\09jnz    1b                      \0A\09", "=*imr,={si},{di},r,0,1,~{rax},~{memory},~{dirflag},~{fpsr},~{flags}"(i32* nonnull %6, i8* %0, i64 %2, i32 %3, i8* %1) #4, !srcloc !18
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %7)
  %9 = getelementptr inbounds i8, i8* %0, i64 8
  %10 = getelementptr inbounds i8, i8* %1, i64 8
  %11 = bitcast i32* %5 to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %11)
  store i32 %3, i32* %5, align 4
  call void asm sideeffect "pxor %mm7, %mm7", "~{dirflag},~{fpsr},~{flags}"() #4, !srcloc !16
  call void asm sideeffect "pcmpeqd %mm6, %mm6 \0A\09psrlw $$15, %mm6", "~{dirflag},~{fpsr},~{flags}"() #4, !srcloc !17
  %12 = call i8* asm sideeffect "movq   ($1), %mm0             \0A\09movq   1($1), %mm4            \0A\09movq   %mm0, %mm1            \0A\09movq   %mm4, %mm5            \0A\09punpcklbw %mm7, %mm0         \0A\09punpcklbw %mm7, %mm4         \0A\09punpckhbw %mm7, %mm1         \0A\09punpckhbw %mm7, %mm5         \0A\09paddusw %mm0, %mm4           \0A\09paddusw %mm1, %mm5           \0A\09xor    %rax, %rax \0A\09add    $3, $1                  \0A\09.p2align 3                     \0A\091:                             \0A\09movq   ($1, %rax), %mm0  \0A\09movq   1($1, %rax), %mm2 \0A\09movq   %mm0, %mm1            \0A\09movq   %mm2, %mm3            \0A\09punpcklbw %mm7, %mm0         \0A\09punpcklbw %mm7, %mm2         \0A\09punpckhbw %mm7, %mm1         \0A\09punpckhbw %mm7, %mm3         \0A\09paddusw %mm2, %mm0           \0A\09paddusw %mm3, %mm1           \0A\09paddusw %mm6, %mm4           \0A\09paddusw %mm6, %mm5           \0A\09paddusw %mm0, %mm4           \0A\09paddusw %mm1, %mm5           \0A\09psrlw  $$2, %mm4               \0A\09psrlw  $$2, %mm5               \0A\09packuswb  %mm5, %mm4         \0A\09movq   %mm4, ($2, %rax)  \0A\09add    $3, %rax           \0A\09movq   ($1, %rax), %mm2  \0A\09movq   1($1, %rax), %mm4 \0A\09movq   %mm2, %mm3            \0A\09movq   %mm4, %mm5            \0A\09punpcklbw %mm7, %mm2         \0A\09punpcklbw %mm7, %mm4         \0A\09punpckhbw %mm7, %mm3         \0A\09punpckhbw %mm7, %mm5         \0A\09paddusw %mm2, %mm4           \0A\09paddusw %mm3, %mm5           \0A\09paddusw %mm6, %mm0           \0A\09paddusw %mm6, %mm1           \0A\09paddusw %mm4, %mm0           \0A\09paddusw %mm5, %mm1           \0A\09psrlw  $$2, %mm0               \0A\09psrlw  $$2, %mm1               \0A\09packuswb  %mm1, %mm0         \0A\09movq   %mm0, ($2, %rax)  \0A\09add    $3, %rax        \0A\09subl   $$2, $0                  \0A\09jnz    1b                      \0A\09", "=*imr,={si},{di},r,0,1,~{rax},~{memory},~{dirflag},~{fpsr},~{flags}"(i32* nonnull %5, i8* %9, i64 %2, i32 %3, i8* %10) #4, !srcloc !18
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %11)
  ret void
}

declare void @ff_avg_pixels16_mmx(i8*, i8*, i64, i32) #2

; Function Attrs: nounwind ssp uwtable
define internal void @avg_pixels16_x2_mmx(i8*, i8*, i64, i32) #0 {
  %5 = alloca i32, align 4
  store i32 %3, i32* %5, align 4
  tail call void asm sideeffect "pcmpeqd %mm6, %mm6   \0A\09paddb   %mm6, %mm6   \0A\09", "~{dirflag},~{fpsr},~{flags}"() #4, !srcloc !19
  %6 = call { i8*, i8* } asm sideeffect ".p2align 3                 \0A\091:                         \0A\09movq  ($1), %mm0          \0A\09movq  1($1), %mm1         \0A\09movq  ($2), %mm3          \0A\09movq   %mm0, %mm2            \0A\09por    %mm1, %mm2            \0A\09pxor   %mm0, %mm1            \0A\09pand  %mm6, %mm1            \0A\09psrlq       $$1, %mm1            \0A\09psubb  %mm1, %mm2            \0A\09movq   %mm3, %mm0            \0A\09por    %mm2, %mm0            \0A\09pxor   %mm3, %mm2            \0A\09pand  %mm6, %mm2            \0A\09psrlq       $$1, %mm2            \0A\09psubb  %mm2, %mm0            \0A\09movq  %mm0, ($2)          \0A\09movq  8($1), %mm0         \0A\09movq  9($1), %mm1         \0A\09movq  8($2), %mm3         \0A\09movq   %mm0, %mm2            \0A\09por    %mm1, %mm2            \0A\09pxor   %mm0, %mm1            \0A\09pand  %mm6, %mm1            \0A\09psrlq       $$1, %mm1            \0A\09psubb  %mm1, %mm2            \0A\09movq   %mm3, %mm0            \0A\09por    %mm2, %mm0            \0A\09pxor   %mm3, %mm2            \0A\09pand  %mm6, %mm2            \0A\09psrlq       $$1, %mm2            \0A\09psubb  %mm2, %mm0            \0A\09movq  %mm0, 8($2)         \0A\09add    $3, $1              \0A\09add    $3, $2              \0A\09subl   $$1, $0              \0A\09jnz    1b                  \0A\09", "=*imr,={si},={di},r,0,1,2,~{memory},~{dirflag},~{fpsr},~{flags}"(i32* nonnull %5, i64 %2, i32 %3, i8* %1, i8* %0) #4, !srcloc !20
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal void @avg_pixels16_y2_mmx(i8*, i8*, i64, i32) #0 {
  %5 = alloca i32, align 4
  %6 = alloca i32, align 4
  %7 = bitcast i32* %6 to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %7)
  store i32 %3, i32* %6, align 4
  tail call void asm sideeffect "pcmpeqd %mm6, %mm6   \0A\09paddb   %mm6, %mm6   \0A\09", "~{dirflag},~{fpsr},~{flags}"() #4, !srcloc !21
  %8 = call { i8*, i8* } asm sideeffect "lea    ($3, $3), %rax  \0A\09movq   ($1), %mm0             \0A\09.p2align 3                     \0A\091:                             \0A\09movq   ($1, $3), %mm1         \0A\09movq   ($1, %rax), %mm2 \0A\09movq  %mm1, %mm4             \0A\09movq  %mm2, %mm5             \0A\09por   %mm0, %mm4             \0A\09por   %mm1, %mm5             \0A\09pxor  %mm1, %mm0             \0A\09pxor  %mm2, %mm1             \0A\09pand    %mm6, %mm0             \0A\09pand    %mm6, %mm1             \0A\09psrlq      $$1, %mm1             \0A\09psrlq      $$1, %mm0             \0A\09psubb %mm0, %mm4             \0A\09psubb %mm1, %mm5             \0A\09movq   ($2), %mm3             \0A\09movq   %mm3, %mm0            \0A\09por    %mm4, %mm0            \0A\09pxor   %mm3, %mm4            \0A\09pand  %mm6, %mm4            \0A\09psrlq       $$1, %mm4            \0A\09psubb  %mm4, %mm0            \0A\09movq   ($2, $3), %mm3         \0A\09movq   %mm3, %mm1            \0A\09por    %mm5, %mm1            \0A\09pxor   %mm3, %mm5            \0A\09pand  %mm6, %mm5            \0A\09psrlq       $$1, %mm5            \0A\09psubb  %mm5, %mm1            \0A\09movq   %mm0, ($2)             \0A\09movq   %mm1, ($2, $3)         \0A\09add    %rax, $1        \0A\09add    %rax, $2        \0A\09movq   ($1, $3), %mm1         \0A\09movq   ($1, %rax), %mm0 \0A\09movq  %mm1, %mm4             \0A\09movq  %mm0, %mm5             \0A\09por   %mm2, %mm4             \0A\09por   %mm1, %mm5             \0A\09pxor  %mm1, %mm2             \0A\09pxor  %mm0, %mm1             \0A\09pand    %mm6, %mm2             \0A\09pand    %mm6, %mm1             \0A\09psrlq      $$1, %mm1             \0A\09psrlq      $$1, %mm2             \0A\09psubb %mm2, %mm4             \0A\09psubb %mm1, %mm5             \0A\09movq   ($2), %mm3             \0A\09movq   %mm3, %mm2            \0A\09por    %mm4, %mm2            \0A\09pxor   %mm3, %mm4            \0A\09pand  %mm6, %mm4            \0A\09psrlq       $$1, %mm4            \0A\09psubb  %mm4, %mm2            \0A\09movq   ($2, $3), %mm3         \0A\09movq   %mm3, %mm1            \0A\09por    %mm5, %mm1            \0A\09pxor   %mm3, %mm5            \0A\09pand  %mm6, %mm5            \0A\09psrlq       $$1, %mm5            \0A\09psubb  %mm5, %mm1            \0A\09movq   %mm2, ($2)             \0A\09movq   %mm1, ($2, $3)         \0A\09add    %rax, $1        \0A\09add    %rax, $2        \0A\09subl   $$4, $0                  \0A\09jnz    1b                      \0A\09", "=*imr,={si},={di},r,0,1,2,~{rax},~{memory},~{dirflag},~{fpsr},~{flags}"(i32* nonnull %6, i64 %2, i32 %3, i8* %1, i8* %0) #4, !srcloc !22
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %7)
  %9 = getelementptr inbounds i8, i8* %0, i64 8
  %10 = getelementptr inbounds i8, i8* %1, i64 8
  %11 = bitcast i32* %5 to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %11)
  store i32 %3, i32* %5, align 4
  call void asm sideeffect "pcmpeqd %mm6, %mm6   \0A\09paddb   %mm6, %mm6   \0A\09", "~{dirflag},~{fpsr},~{flags}"() #4, !srcloc !21
  %12 = call { i8*, i8* } asm sideeffect "lea    ($3, $3), %rax  \0A\09movq   ($1), %mm0             \0A\09.p2align 3                     \0A\091:                             \0A\09movq   ($1, $3), %mm1         \0A\09movq   ($1, %rax), %mm2 \0A\09movq  %mm1, %mm4             \0A\09movq  %mm2, %mm5             \0A\09por   %mm0, %mm4             \0A\09por   %mm1, %mm5             \0A\09pxor  %mm1, %mm0             \0A\09pxor  %mm2, %mm1             \0A\09pand    %mm6, %mm0             \0A\09pand    %mm6, %mm1             \0A\09psrlq      $$1, %mm1             \0A\09psrlq      $$1, %mm0             \0A\09psubb %mm0, %mm4             \0A\09psubb %mm1, %mm5             \0A\09movq   ($2), %mm3             \0A\09movq   %mm3, %mm0            \0A\09por    %mm4, %mm0            \0A\09pxor   %mm3, %mm4            \0A\09pand  %mm6, %mm4            \0A\09psrlq       $$1, %mm4            \0A\09psubb  %mm4, %mm0            \0A\09movq   ($2, $3), %mm3         \0A\09movq   %mm3, %mm1            \0A\09por    %mm5, %mm1            \0A\09pxor   %mm3, %mm5            \0A\09pand  %mm6, %mm5            \0A\09psrlq       $$1, %mm5            \0A\09psubb  %mm5, %mm1            \0A\09movq   %mm0, ($2)             \0A\09movq   %mm1, ($2, $3)         \0A\09add    %rax, $1        \0A\09add    %rax, $2        \0A\09movq   ($1, $3), %mm1         \0A\09movq   ($1, %rax), %mm0 \0A\09movq  %mm1, %mm4             \0A\09movq  %mm0, %mm5             \0A\09por   %mm2, %mm4             \0A\09por   %mm1, %mm5             \0A\09pxor  %mm1, %mm2             \0A\09pxor  %mm0, %mm1             \0A\09pand    %mm6, %mm2             \0A\09pand    %mm6, %mm1             \0A\09psrlq      $$1, %mm1             \0A\09psrlq      $$1, %mm2             \0A\09psubb %mm2, %mm4             \0A\09psubb %mm1, %mm5             \0A\09movq   ($2), %mm3             \0A\09movq   %mm3, %mm2            \0A\09por    %mm4, %mm2            \0A\09pxor   %mm3, %mm4            \0A\09pand  %mm6, %mm4            \0A\09psrlq       $$1, %mm4            \0A\09psubb  %mm4, %mm2            \0A\09movq   ($2, $3), %mm3         \0A\09movq   %mm3, %mm1            \0A\09por    %mm5, %mm1            \0A\09pxor   %mm3, %mm5            \0A\09pand  %mm6, %mm5            \0A\09psrlq       $$1, %mm5            \0A\09psubb  %mm5, %mm1            \0A\09movq   %mm2, ($2)             \0A\09movq   %mm1, ($2, $3)         \0A\09add    %rax, $1        \0A\09add    %rax, $2        \0A\09subl   $$4, $0                  \0A\09jnz    1b                      \0A\09", "=*imr,={si},={di},r,0,1,2,~{rax},~{memory},~{dirflag},~{fpsr},~{flags}"(i32* nonnull %5, i64 %2, i32 %3, i8* %10, i8* %9) #4, !srcloc !22
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %11)
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal void @avg_no_rnd_pixels16_x2_mmx(i8*, i8*, i64, i32) #0 {
  %5 = alloca i32, align 4
  store i32 %3, i32* %5, align 4
  tail call void asm sideeffect "pcmpeqd %mm6, %mm6   \0A\09paddb   %mm6, %mm6   \0A\09", "~{dirflag},~{fpsr},~{flags}"() #4, !srcloc !23
  %6 = call { i8*, i8* } asm sideeffect ".p2align 3                 \0A\091:                         \0A\09movq  ($1), %mm0          \0A\09movq  1($1), %mm1         \0A\09movq  ($2), %mm3          \0A\09movq   %mm0, %mm2            \0A\09pand   %mm1, %mm2            \0A\09pxor   %mm0, %mm1            \0A\09pand  %mm6, %mm1            \0A\09psrlq       $$1, %mm1            \0A\09paddb  %mm1, %mm2            \0A\09movq   %mm3, %mm0            \0A\09por    %mm2, %mm0            \0A\09pxor   %mm3, %mm2            \0A\09pand  %mm6, %mm2            \0A\09psrlq       $$1, %mm2            \0A\09psubb  %mm2, %mm0            \0A\09movq  %mm0, ($2)          \0A\09movq  8($1), %mm0         \0A\09movq  9($1), %mm1         \0A\09movq  8($2), %mm3         \0A\09movq   %mm0, %mm2            \0A\09pand   %mm1, %mm2            \0A\09pxor   %mm0, %mm1            \0A\09pand  %mm6, %mm1            \0A\09psrlq       $$1, %mm1            \0A\09paddb  %mm1, %mm2            \0A\09movq   %mm3, %mm0            \0A\09por    %mm2, %mm0            \0A\09pxor   %mm3, %mm2            \0A\09pand  %mm6, %mm2            \0A\09psrlq       $$1, %mm2            \0A\09psubb  %mm2, %mm0            \0A\09movq  %mm0, 8($2)         \0A\09add    $3, $1              \0A\09add    $3, $2              \0A\09subl   $$1, $0              \0A\09jnz    1b                  \0A\09", "=*imr,={si},={di},r,0,1,2,~{memory},~{dirflag},~{fpsr},~{flags}"(i32* nonnull %5, i64 %2, i32 %3, i8* %1, i8* %0) #4, !srcloc !24
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal void @avg_no_rnd_pixels16_y2_mmx(i8*, i8*, i64, i32) #0 {
  %5 = alloca i32, align 4
  %6 = alloca i32, align 4
  %7 = bitcast i32* %6 to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %7)
  store i32 %3, i32* %6, align 4
  tail call void asm sideeffect "pcmpeqd %mm6, %mm6   \0A\09paddb   %mm6, %mm6   \0A\09", "~{dirflag},~{fpsr},~{flags}"() #4, !srcloc !25
  %8 = call { i8*, i8* } asm sideeffect "lea    ($3, $3), %rax  \0A\09movq   ($1), %mm0             \0A\09.p2align 3                     \0A\091:                             \0A\09movq   ($1, $3), %mm1         \0A\09movq   ($1, %rax), %mm2 \0A\09movq  %mm1, %mm4             \0A\09movq  %mm2, %mm5             \0A\09pand  %mm0, %mm4             \0A\09pand  %mm1, %mm5             \0A\09pxor  %mm1, %mm0             \0A\09pxor  %mm2, %mm1             \0A\09pand    %mm6, %mm0             \0A\09pand    %mm6, %mm1             \0A\09psrlq      $$1, %mm0             \0A\09psrlq      $$1, %mm1             \0A\09paddb %mm0, %mm4             \0A\09paddb %mm1, %mm5             \0A\09movq   ($2), %mm3             \0A\09movq   %mm3, %mm0            \0A\09por    %mm4, %mm0            \0A\09pxor   %mm3, %mm4            \0A\09pand  %mm6, %mm4            \0A\09psrlq       $$1, %mm4            \0A\09psubb  %mm4, %mm0            \0A\09movq   ($2, $3), %mm3         \0A\09movq   %mm3, %mm1            \0A\09por    %mm5, %mm1            \0A\09pxor   %mm3, %mm5            \0A\09pand  %mm6, %mm5            \0A\09psrlq       $$1, %mm5            \0A\09psubb  %mm5, %mm1            \0A\09movq   %mm0, ($2)             \0A\09movq   %mm1, ($2, $3)         \0A\09add    %rax, $1        \0A\09add    %rax, $2        \0A\09movq   ($1, $3), %mm1         \0A\09movq   ($1, %rax), %mm0 \0A\09movq  %mm1, %mm4             \0A\09movq  %mm0, %mm5             \0A\09pand  %mm2, %mm4             \0A\09pand  %mm1, %mm5             \0A\09pxor  %mm1, %mm2             \0A\09pxor  %mm0, %mm1             \0A\09pand    %mm6, %mm2             \0A\09pand    %mm6, %mm1             \0A\09psrlq      $$1, %mm2             \0A\09psrlq      $$1, %mm1             \0A\09paddb %mm2, %mm4             \0A\09paddb %mm1, %mm5             \0A\09movq   ($2), %mm3             \0A\09movq   %mm3, %mm2            \0A\09por    %mm4, %mm2            \0A\09pxor   %mm3, %mm4            \0A\09pand  %mm6, %mm4            \0A\09psrlq       $$1, %mm4            \0A\09psubb  %mm4, %mm2            \0A\09movq   ($2, $3), %mm3         \0A\09movq   %mm3, %mm1            \0A\09por    %mm5, %mm1            \0A\09pxor   %mm3, %mm5            \0A\09pand  %mm6, %mm5            \0A\09psrlq       $$1, %mm5            \0A\09psubb  %mm5, %mm1            \0A\09movq   %mm2, ($2)             \0A\09movq   %mm1, ($2, $3)         \0A\09add    %rax, $1        \0A\09add    %rax, $2        \0A\09subl   $$4, $0                  \0A\09jnz    1b                      \0A\09", "=*imr,={si},={di},r,0,1,2,~{rax},~{memory},~{dirflag},~{fpsr},~{flags}"(i32* nonnull %6, i64 %2, i32 %3, i8* %1, i8* %0) #4, !srcloc !26
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %7)
  %9 = getelementptr inbounds i8, i8* %0, i64 8
  %10 = getelementptr inbounds i8, i8* %1, i64 8
  %11 = bitcast i32* %5 to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %11)
  store i32 %3, i32* %5, align 4
  call void asm sideeffect "pcmpeqd %mm6, %mm6   \0A\09paddb   %mm6, %mm6   \0A\09", "~{dirflag},~{fpsr},~{flags}"() #4, !srcloc !25
  %12 = call { i8*, i8* } asm sideeffect "lea    ($3, $3), %rax  \0A\09movq   ($1), %mm0             \0A\09.p2align 3                     \0A\091:                             \0A\09movq   ($1, $3), %mm1         \0A\09movq   ($1, %rax), %mm2 \0A\09movq  %mm1, %mm4             \0A\09movq  %mm2, %mm5             \0A\09pand  %mm0, %mm4             \0A\09pand  %mm1, %mm5             \0A\09pxor  %mm1, %mm0             \0A\09pxor  %mm2, %mm1             \0A\09pand    %mm6, %mm0             \0A\09pand    %mm6, %mm1             \0A\09psrlq      $$1, %mm0             \0A\09psrlq      $$1, %mm1             \0A\09paddb %mm0, %mm4             \0A\09paddb %mm1, %mm5             \0A\09movq   ($2), %mm3             \0A\09movq   %mm3, %mm0            \0A\09por    %mm4, %mm0            \0A\09pxor   %mm3, %mm4            \0A\09pand  %mm6, %mm4            \0A\09psrlq       $$1, %mm4            \0A\09psubb  %mm4, %mm0            \0A\09movq   ($2, $3), %mm3         \0A\09movq   %mm3, %mm1            \0A\09por    %mm5, %mm1            \0A\09pxor   %mm3, %mm5            \0A\09pand  %mm6, %mm5            \0A\09psrlq       $$1, %mm5            \0A\09psubb  %mm5, %mm1            \0A\09movq   %mm0, ($2)             \0A\09movq   %mm1, ($2, $3)         \0A\09add    %rax, $1        \0A\09add    %rax, $2        \0A\09movq   ($1, $3), %mm1         \0A\09movq   ($1, %rax), %mm0 \0A\09movq  %mm1, %mm4             \0A\09movq  %mm0, %mm5             \0A\09pand  %mm2, %mm4             \0A\09pand  %mm1, %mm5             \0A\09pxor  %mm1, %mm2             \0A\09pxor  %mm0, %mm1             \0A\09pand    %mm6, %mm2             \0A\09pand    %mm6, %mm1             \0A\09psrlq      $$1, %mm2             \0A\09psrlq      $$1, %mm1             \0A\09paddb %mm2, %mm4             \0A\09paddb %mm1, %mm5             \0A\09movq   ($2), %mm3             \0A\09movq   %mm3, %mm2            \0A\09por    %mm4, %mm2            \0A\09pxor   %mm3, %mm4            \0A\09pand  %mm6, %mm4            \0A\09psrlq       $$1, %mm4            \0A\09psubb  %mm4, %mm2            \0A\09movq   ($2, $3), %mm3         \0A\09movq   %mm3, %mm1            \0A\09por    %mm5, %mm1            \0A\09pxor   %mm3, %mm5            \0A\09pand  %mm6, %mm5            \0A\09psrlq       $$1, %mm5            \0A\09psubb  %mm5, %mm1            \0A\09movq   %mm2, ($2)             \0A\09movq   %mm1, ($2, $3)         \0A\09add    %rax, $1        \0A\09add    %rax, $2        \0A\09subl   $$4, $0                  \0A\09jnz    1b                      \0A\09", "=*imr,={si},={di},r,0,1,2,~{rax},~{memory},~{dirflag},~{fpsr},~{flags}"(i32* nonnull %5, i64 %2, i32 %3, i8* %10, i8* %9) #4, !srcloc !26
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %11)
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal void @avg_no_rnd_pixels16_xy2_mmx(i8*, i8*, i64, i32) #0 {
  %5 = alloca i32, align 4
  %6 = alloca i32, align 4
  %7 = bitcast i32* %6 to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %7)
  store i32 %3, i32* %6, align 4
  tail call void asm sideeffect "pxor %mm7, %mm7", "~{dirflag},~{fpsr},~{flags}"() #4, !srcloc !27
  tail call void asm sideeffect "pcmpeqd %mm6, %mm6 \0A\09psrlw $$15, %mm6", "~{dirflag},~{fpsr},~{flags}"() #4, !srcloc !28
  %8 = call i8* asm sideeffect "movq   ($1), %mm0             \0A\09movq   1($1), %mm4            \0A\09movq   %mm0, %mm1            \0A\09movq   %mm4, %mm5            \0A\09punpcklbw %mm7, %mm0         \0A\09punpcklbw %mm7, %mm4         \0A\09punpckhbw %mm7, %mm1         \0A\09punpckhbw %mm7, %mm5         \0A\09paddusw %mm0, %mm4           \0A\09paddusw %mm1, %mm5           \0A\09xor    %rax, %rax \0A\09add    $3, $1                  \0A\09.p2align 3                     \0A\091:                             \0A\09movq   ($1, %rax), %mm0  \0A\09movq   1($1, %rax), %mm2 \0A\09movq   %mm0, %mm1            \0A\09movq   %mm2, %mm3            \0A\09punpcklbw %mm7, %mm0         \0A\09punpcklbw %mm7, %mm2         \0A\09punpckhbw %mm7, %mm1         \0A\09punpckhbw %mm7, %mm3         \0A\09paddusw %mm2, %mm0           \0A\09paddusw %mm3, %mm1           \0A\09paddusw %mm6, %mm4           \0A\09paddusw %mm6, %mm5           \0A\09paddusw %mm0, %mm4           \0A\09paddusw %mm1, %mm5           \0A\09psrlw  $$2, %mm4               \0A\09psrlw  $$2, %mm5               \0A\09movq   ($2, %rax), %mm3  \0A\09packuswb  %mm5, %mm4         \0A\09pcmpeqd %mm2, %mm2   \0A\09paddb %mm2, %mm2     \0A\09movq   %mm3, %mm5            \0A\09por    %mm4, %mm5            \0A\09pxor   %mm3, %mm4            \0A\09pand  %mm2, %mm4            \0A\09psrlq       $$1, %mm4            \0A\09psubb  %mm4, %mm5            \0A\09movq   %mm5, ($2, %rax)  \0A\09add    $3, %rax        \0A\09movq   ($1, %rax), %mm2  \0A\09movq   1($1, %rax), %mm4 \0A\09movq   %mm2, %mm3            \0A\09movq   %mm4, %mm5            \0A\09punpcklbw %mm7, %mm2         \0A\09punpcklbw %mm7, %mm4         \0A\09punpckhbw %mm7, %mm3         \0A\09punpckhbw %mm7, %mm5         \0A\09paddusw %mm2, %mm4           \0A\09paddusw %mm3, %mm5           \0A\09paddusw %mm6, %mm0           \0A\09paddusw %mm6, %mm1           \0A\09paddusw %mm4, %mm0           \0A\09paddusw %mm5, %mm1           \0A\09psrlw  $$2, %mm0               \0A\09psrlw  $$2, %mm1               \0A\09movq   ($2, %rax), %mm3  \0A\09packuswb  %mm1, %mm0         \0A\09pcmpeqd %mm2, %mm2   \0A\09paddb %mm2, %mm2     \0A\09movq   %mm3, %mm1            \0A\09por    %mm0, %mm1            \0A\09pxor   %mm3, %mm0            \0A\09pand  %mm2, %mm0            \0A\09psrlq       $$1, %mm0            \0A\09psubb  %mm0, %mm1            \0A\09movq   %mm1, ($2, %rax)  \0A\09add    $3, %rax           \0A\09subl   $$2, $0                  \0A\09jnz    1b                      \0A\09", "=*imr,={si},{di},r,0,1,~{rax},~{memory},~{dirflag},~{fpsr},~{flags}"(i32* nonnull %6, i8* %0, i64 %2, i32 %3, i8* %1) #4, !srcloc !29
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %7)
  %9 = getelementptr inbounds i8, i8* %0, i64 8
  %10 = getelementptr inbounds i8, i8* %1, i64 8
  %11 = bitcast i32* %5 to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %11)
  store i32 %3, i32* %5, align 4
  call void asm sideeffect "pxor %mm7, %mm7", "~{dirflag},~{fpsr},~{flags}"() #4, !srcloc !27
  call void asm sideeffect "pcmpeqd %mm6, %mm6 \0A\09psrlw $$15, %mm6", "~{dirflag},~{fpsr},~{flags}"() #4, !srcloc !28
  %12 = call i8* asm sideeffect "movq   ($1), %mm0             \0A\09movq   1($1), %mm4            \0A\09movq   %mm0, %mm1            \0A\09movq   %mm4, %mm5            \0A\09punpcklbw %mm7, %mm0         \0A\09punpcklbw %mm7, %mm4         \0A\09punpckhbw %mm7, %mm1         \0A\09punpckhbw %mm7, %mm5         \0A\09paddusw %mm0, %mm4           \0A\09paddusw %mm1, %mm5           \0A\09xor    %rax, %rax \0A\09add    $3, $1                  \0A\09.p2align 3                     \0A\091:                             \0A\09movq   ($1, %rax), %mm0  \0A\09movq   1($1, %rax), %mm2 \0A\09movq   %mm0, %mm1            \0A\09movq   %mm2, %mm3            \0A\09punpcklbw %mm7, %mm0         \0A\09punpcklbw %mm7, %mm2         \0A\09punpckhbw %mm7, %mm1         \0A\09punpckhbw %mm7, %mm3         \0A\09paddusw %mm2, %mm0           \0A\09paddusw %mm3, %mm1           \0A\09paddusw %mm6, %mm4           \0A\09paddusw %mm6, %mm5           \0A\09paddusw %mm0, %mm4           \0A\09paddusw %mm1, %mm5           \0A\09psrlw  $$2, %mm4               \0A\09psrlw  $$2, %mm5               \0A\09movq   ($2, %rax), %mm3  \0A\09packuswb  %mm5, %mm4         \0A\09pcmpeqd %mm2, %mm2   \0A\09paddb %mm2, %mm2     \0A\09movq   %mm3, %mm5            \0A\09por    %mm4, %mm5            \0A\09pxor   %mm3, %mm4            \0A\09pand  %mm2, %mm4            \0A\09psrlq       $$1, %mm4            \0A\09psubb  %mm4, %mm5            \0A\09movq   %mm5, ($2, %rax)  \0A\09add    $3, %rax        \0A\09movq   ($1, %rax), %mm2  \0A\09movq   1($1, %rax), %mm4 \0A\09movq   %mm2, %mm3            \0A\09movq   %mm4, %mm5            \0A\09punpcklbw %mm7, %mm2         \0A\09punpcklbw %mm7, %mm4         \0A\09punpckhbw %mm7, %mm3         \0A\09punpckhbw %mm7, %mm5         \0A\09paddusw %mm2, %mm4           \0A\09paddusw %mm3, %mm5           \0A\09paddusw %mm6, %mm0           \0A\09paddusw %mm6, %mm1           \0A\09paddusw %mm4, %mm0           \0A\09paddusw %mm5, %mm1           \0A\09psrlw  $$2, %mm0               \0A\09psrlw  $$2, %mm1               \0A\09movq   ($2, %rax), %mm3  \0A\09packuswb  %mm1, %mm0         \0A\09pcmpeqd %mm2, %mm2   \0A\09paddb %mm2, %mm2     \0A\09movq   %mm3, %mm1            \0A\09por    %mm0, %mm1            \0A\09pxor   %mm3, %mm0            \0A\09pand  %mm2, %mm0            \0A\09psrlq       $$1, %mm0            \0A\09psubb  %mm0, %mm1            \0A\09movq   %mm1, ($2, %rax)  \0A\09add    $3, %rax           \0A\09subl   $$2, $0                  \0A\09jnz    1b                      \0A\09", "=*imr,={si},{di},r,0,1,~{rax},~{memory},~{dirflag},~{fpsr},~{flags}"(i32* nonnull %5, i8* %9, i64 %2, i32 %3, i8* %10) #4, !srcloc !29
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %11)
  ret void
}

declare void @ff_put_pixels8_mmx(i8*, i8*, i64, i32) #2

; Function Attrs: nounwind ssp uwtable
define internal void @put_pixels8_x2_mmx(i8*, i8*, i64, i32) #0 {
  %5 = alloca i32, align 4
  store i32 %3, i32* %5, align 4
  tail call void asm sideeffect "pcmpeqd %mm6, %mm6   \0A\09paddb   %mm6, %mm6   \0A\09", "~{dirflag},~{fpsr},~{flags}"() #4, !srcloc !30
  %6 = call { i8*, i8* } asm sideeffect "lea    ($3, $3), %rax  \0A\09.p2align 3                     \0A\091:                             \0A\09movq   ($1), %mm0             \0A\09movq   1($1), %mm1            \0A\09movq   ($1, $3), %mm2         \0A\09movq   1($1, $3), %mm3        \0A\09movq  %mm0, %mm4             \0A\09movq  %mm2, %mm5             \0A\09por   %mm1, %mm4             \0A\09por   %mm3, %mm5             \0A\09pxor  %mm0, %mm1             \0A\09pxor  %mm2, %mm3             \0A\09pand    %mm6, %mm1             \0A\09pand    %mm6, %mm3             \0A\09psrlq      $$1, %mm3             \0A\09psrlq      $$1, %mm1             \0A\09psubb %mm1, %mm4             \0A\09psubb %mm3, %mm5             \0A\09movq   %mm4, ($2)             \0A\09movq   %mm5, ($2, $3)         \0A\09add    %rax, $1        \0A\09add    %rax, $2        \0A\09movq   ($1), %mm0             \0A\09movq   1($1), %mm1            \0A\09movq   ($1, $3), %mm2         \0A\09movq   1($1, $3), %mm3        \0A\09movq  %mm0, %mm4             \0A\09movq  %mm2, %mm5             \0A\09por   %mm1, %mm4             \0A\09por   %mm3, %mm5             \0A\09pxor  %mm0, %mm1             \0A\09pxor  %mm2, %mm3             \0A\09pand    %mm6, %mm1             \0A\09pand    %mm6, %mm3             \0A\09psrlq      $$1, %mm3             \0A\09psrlq      $$1, %mm1             \0A\09psubb %mm1, %mm4             \0A\09psubb %mm3, %mm5             \0A\09movq   %mm4, ($2)             \0A\09movq   %mm5, ($2, $3)         \0A\09add    %rax, $1        \0A\09add    %rax, $2        \0A\09subl   $$4, $0                  \0A\09jnz    1b                      \0A\09", "=*imr,={si},={di},r,0,1,2,~{rax},~{memory},~{dirflag},~{fpsr},~{flags}"(i32* nonnull %5, i64 %2, i32 %3, i8* %1, i8* %0) #4, !srcloc !31
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal void @put_pixels8_y2_mmx(i8*, i8*, i64, i32) #0 {
  %5 = alloca i32, align 4
  store i32 %3, i32* %5, align 4
  tail call void asm sideeffect "pcmpeqd %mm6, %mm6   \0A\09paddb   %mm6, %mm6   \0A\09", "~{dirflag},~{fpsr},~{flags}"() #4, !srcloc !10
  %6 = call { i8*, i8* } asm sideeffect "lea ($3, $3), %rax     \0A\09movq ($1), %mm0               \0A\09.p2align 3                     \0A\091:                             \0A\09movq   ($1, $3), %mm1         \0A\09movq   ($1, %rax),%mm2\0A\09movq  %mm1, %mm4             \0A\09movq  %mm2, %mm5             \0A\09por   %mm0, %mm4             \0A\09por   %mm1, %mm5             \0A\09pxor  %mm1, %mm0             \0A\09pxor  %mm2, %mm1             \0A\09pand    %mm6, %mm0             \0A\09pand    %mm6, %mm1             \0A\09psrlq      $$1, %mm1             \0A\09psrlq      $$1, %mm0             \0A\09psubb %mm0, %mm4             \0A\09psubb %mm1, %mm5             \0A\09movq   %mm4, ($2)             \0A\09movq   %mm5, ($2, $3)         \0A\09add    %rax, $1        \0A\09add    %rax, $2        \0A\09movq   ($1, $3), %mm1         \0A\09movq   ($1, %rax),%mm0\0A\09movq  %mm1, %mm4             \0A\09movq  %mm0, %mm5             \0A\09por   %mm2, %mm4             \0A\09por   %mm1, %mm5             \0A\09pxor  %mm1, %mm2             \0A\09pxor  %mm0, %mm1             \0A\09pand    %mm6, %mm2             \0A\09pand    %mm6, %mm1             \0A\09psrlq      $$1, %mm1             \0A\09psrlq      $$1, %mm2             \0A\09psubb %mm2, %mm4             \0A\09psubb %mm1, %mm5             \0A\09movq   %mm4, ($2)             \0A\09movq   %mm5, ($2, $3)         \0A\09add    %rax, $1        \0A\09add    %rax, $2        \0A\09subl   $$4, $0                  \0A\09jnz    1b                      \0A\09", "=*imr,={si},={di},r,0,1,2,~{rax},~{memory},~{dirflag},~{fpsr},~{flags}"(i32* nonnull %5, i64 %2, i32 %3, i8* %1, i8* %0) #4, !srcloc !11
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal void @put_no_rnd_pixels8_x2_mmx(i8*, i8*, i64, i32) #0 {
  %5 = alloca i32, align 4
  store i32 %3, i32* %5, align 4
  tail call void asm sideeffect "pcmpeqd %mm6, %mm6   \0A\09paddb   %mm6, %mm6   \0A\09", "~{dirflag},~{fpsr},~{flags}"() #4, !srcloc !32
  %6 = call { i8*, i8* } asm sideeffect "lea    ($3, $3), %rax  \0A\09.p2align 3                     \0A\091:                             \0A\09movq   ($1), %mm0             \0A\09movq   1($1), %mm1            \0A\09movq   ($1, $3), %mm2         \0A\09movq   1($1, $3), %mm3        \0A\09movq  %mm0, %mm4             \0A\09movq  %mm2, %mm5             \0A\09pand  %mm1, %mm4             \0A\09pand  %mm3, %mm5             \0A\09pxor  %mm0, %mm1             \0A\09pxor  %mm2, %mm3             \0A\09pand    %mm6, %mm1             \0A\09pand    %mm6, %mm3             \0A\09psrlq      $$1, %mm1             \0A\09psrlq      $$1, %mm3             \0A\09paddb %mm1, %mm4             \0A\09paddb %mm3, %mm5             \0A\09movq   %mm4, ($2)             \0A\09movq   %mm5, ($2, $3)         \0A\09add    %rax, $1        \0A\09add    %rax, $2        \0A\09movq   ($1), %mm0             \0A\09movq   1($1), %mm1            \0A\09movq   ($1, $3), %mm2         \0A\09movq   1($1, $3), %mm3        \0A\09movq  %mm0, %mm4             \0A\09movq  %mm2, %mm5             \0A\09pand  %mm1, %mm4             \0A\09pand  %mm3, %mm5             \0A\09pxor  %mm0, %mm1             \0A\09pxor  %mm2, %mm3             \0A\09pand    %mm6, %mm1             \0A\09pand    %mm6, %mm3             \0A\09psrlq      $$1, %mm1             \0A\09psrlq      $$1, %mm3             \0A\09paddb %mm1, %mm4             \0A\09paddb %mm3, %mm5             \0A\09movq   %mm4, ($2)             \0A\09movq   %mm5, ($2, $3)         \0A\09add    %rax, $1        \0A\09add    %rax, $2        \0A\09subl   $$4, $0                  \0A\09jnz    1b                      \0A\09", "=*imr,={si},={di},r,0,1,2,~{rax},~{memory},~{dirflag},~{fpsr},~{flags}"(i32* nonnull %5, i64 %2, i32 %3, i8* %1, i8* %0) #4, !srcloc !33
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal void @put_no_rnd_pixels8_y2_mmx(i8*, i8*, i64, i32) #0 {
  %5 = alloca i32, align 4
  store i32 %3, i32* %5, align 4
  tail call void asm sideeffect "pcmpeqd %mm6, %mm6   \0A\09paddb   %mm6, %mm6   \0A\09", "~{dirflag},~{fpsr},~{flags}"() #4, !srcloc !14
  %6 = call { i8*, i8* } asm sideeffect "lea ($3, $3), %rax     \0A\09movq ($1), %mm0               \0A\09.p2align 3                     \0A\091:                             \0A\09movq   ($1, $3), %mm1         \0A\09movq   ($1, %rax),%mm2\0A\09movq  %mm1, %mm4             \0A\09movq  %mm2, %mm5             \0A\09pand  %mm0, %mm4             \0A\09pand  %mm1, %mm5             \0A\09pxor  %mm1, %mm0             \0A\09pxor  %mm2, %mm1             \0A\09pand    %mm6, %mm0             \0A\09pand    %mm6, %mm1             \0A\09psrlq      $$1, %mm0             \0A\09psrlq      $$1, %mm1             \0A\09paddb %mm0, %mm4             \0A\09paddb %mm1, %mm5             \0A\09movq   %mm4, ($2)             \0A\09movq   %mm5, ($2, $3)         \0A\09add    %rax, $1        \0A\09add    %rax, $2        \0A\09movq   ($1, $3), %mm1         \0A\09movq   ($1, %rax),%mm0\0A\09movq  %mm1, %mm4             \0A\09movq  %mm0, %mm5             \0A\09pand  %mm2, %mm4             \0A\09pand  %mm1, %mm5             \0A\09pxor  %mm1, %mm2             \0A\09pxor  %mm0, %mm1             \0A\09pand    %mm6, %mm2             \0A\09pand    %mm6, %mm1             \0A\09psrlq      $$1, %mm2             \0A\09psrlq      $$1, %mm1             \0A\09paddb %mm2, %mm4             \0A\09paddb %mm1, %mm5             \0A\09movq   %mm4, ($2)             \0A\09movq   %mm5, ($2, $3)         \0A\09add    %rax, $1        \0A\09add    %rax, $2        \0A\09subl   $$4, $0                  \0A\09jnz    1b                      \0A\09", "=*imr,={si},={di},r,0,1,2,~{rax},~{memory},~{dirflag},~{fpsr},~{flags}"(i32* nonnull %5, i64 %2, i32 %3, i8* %1, i8* %0) #4, !srcloc !15
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal void @put_no_rnd_pixels8_xy2_mmx(i8*, i8*, i64, i32) #0 {
  %5 = alloca i32, align 4
  store i32 %3, i32* %5, align 4
  tail call void asm sideeffect "pxor %mm7, %mm7", "~{dirflag},~{fpsr},~{flags}"() #4, !srcloc !16
  tail call void asm sideeffect "pcmpeqd %mm6, %mm6 \0A\09psrlw $$15, %mm6", "~{dirflag},~{fpsr},~{flags}"() #4, !srcloc !17
  %6 = call i8* asm sideeffect "movq   ($1), %mm0             \0A\09movq   1($1), %mm4            \0A\09movq   %mm0, %mm1            \0A\09movq   %mm4, %mm5            \0A\09punpcklbw %mm7, %mm0         \0A\09punpcklbw %mm7, %mm4         \0A\09punpckhbw %mm7, %mm1         \0A\09punpckhbw %mm7, %mm5         \0A\09paddusw %mm0, %mm4           \0A\09paddusw %mm1, %mm5           \0A\09xor    %rax, %rax \0A\09add    $3, $1                  \0A\09.p2align 3                     \0A\091:                             \0A\09movq   ($1, %rax), %mm0  \0A\09movq   1($1, %rax), %mm2 \0A\09movq   %mm0, %mm1            \0A\09movq   %mm2, %mm3            \0A\09punpcklbw %mm7, %mm0         \0A\09punpcklbw %mm7, %mm2         \0A\09punpckhbw %mm7, %mm1         \0A\09punpckhbw %mm7, %mm3         \0A\09paddusw %mm2, %mm0           \0A\09paddusw %mm3, %mm1           \0A\09paddusw %mm6, %mm4           \0A\09paddusw %mm6, %mm5           \0A\09paddusw %mm0, %mm4           \0A\09paddusw %mm1, %mm5           \0A\09psrlw  $$2, %mm4               \0A\09psrlw  $$2, %mm5               \0A\09packuswb  %mm5, %mm4         \0A\09movq   %mm4, ($2, %rax)  \0A\09add    $3, %rax           \0A\09movq   ($1, %rax), %mm2  \0A\09movq   1($1, %rax), %mm4 \0A\09movq   %mm2, %mm3            \0A\09movq   %mm4, %mm5            \0A\09punpcklbw %mm7, %mm2         \0A\09punpcklbw %mm7, %mm4         \0A\09punpckhbw %mm7, %mm3         \0A\09punpckhbw %mm7, %mm5         \0A\09paddusw %mm2, %mm4           \0A\09paddusw %mm3, %mm5           \0A\09paddusw %mm6, %mm0           \0A\09paddusw %mm6, %mm1           \0A\09paddusw %mm4, %mm0           \0A\09paddusw %mm5, %mm1           \0A\09psrlw  $$2, %mm0               \0A\09psrlw  $$2, %mm1               \0A\09packuswb  %mm1, %mm0         \0A\09movq   %mm0, ($2, %rax)  \0A\09add    $3, %rax        \0A\09subl   $$2, $0                  \0A\09jnz    1b                      \0A\09", "=*imr,={si},{di},r,0,1,~{rax},~{memory},~{dirflag},~{fpsr},~{flags}"(i32* nonnull %5, i8* %0, i64 %2, i32 %3, i8* %1) #4, !srcloc !18
  ret void
}

declare void @ff_avg_pixels8_mmx(i8*, i8*, i64, i32) #2

declare void @ff_avg_pixels8_x2_mmx(i8*, i8*, i64, i32) #2

; Function Attrs: nounwind ssp uwtable
define internal void @avg_pixels8_y2_mmx(i8*, i8*, i64, i32) #0 {
  %5 = alloca i32, align 4
  store i32 %3, i32* %5, align 4
  tail call void asm sideeffect "pcmpeqd %mm6, %mm6   \0A\09paddb   %mm6, %mm6   \0A\09", "~{dirflag},~{fpsr},~{flags}"() #4, !srcloc !21
  %6 = call { i8*, i8* } asm sideeffect "lea    ($3, $3), %rax  \0A\09movq   ($1), %mm0             \0A\09.p2align 3                     \0A\091:                             \0A\09movq   ($1, $3), %mm1         \0A\09movq   ($1, %rax), %mm2 \0A\09movq  %mm1, %mm4             \0A\09movq  %mm2, %mm5             \0A\09por   %mm0, %mm4             \0A\09por   %mm1, %mm5             \0A\09pxor  %mm1, %mm0             \0A\09pxor  %mm2, %mm1             \0A\09pand    %mm6, %mm0             \0A\09pand    %mm6, %mm1             \0A\09psrlq      $$1, %mm1             \0A\09psrlq      $$1, %mm0             \0A\09psubb %mm0, %mm4             \0A\09psubb %mm1, %mm5             \0A\09movq   ($2), %mm3             \0A\09movq   %mm3, %mm0            \0A\09por    %mm4, %mm0            \0A\09pxor   %mm3, %mm4            \0A\09pand  %mm6, %mm4            \0A\09psrlq       $$1, %mm4            \0A\09psubb  %mm4, %mm0            \0A\09movq   ($2, $3), %mm3         \0A\09movq   %mm3, %mm1            \0A\09por    %mm5, %mm1            \0A\09pxor   %mm3, %mm5            \0A\09pand  %mm6, %mm5            \0A\09psrlq       $$1, %mm5            \0A\09psubb  %mm5, %mm1            \0A\09movq   %mm0, ($2)             \0A\09movq   %mm1, ($2, $3)         \0A\09add    %rax, $1        \0A\09add    %rax, $2        \0A\09movq   ($1, $3), %mm1         \0A\09movq   ($1, %rax), %mm0 \0A\09movq  %mm1, %mm4             \0A\09movq  %mm0, %mm5             \0A\09por   %mm2, %mm4             \0A\09por   %mm1, %mm5             \0A\09pxor  %mm1, %mm2             \0A\09pxor  %mm0, %mm1             \0A\09pand    %mm6, %mm2             \0A\09pand    %mm6, %mm1             \0A\09psrlq      $$1, %mm1             \0A\09psrlq      $$1, %mm2             \0A\09psubb %mm2, %mm4             \0A\09psubb %mm1, %mm5             \0A\09movq   ($2), %mm3             \0A\09movq   %mm3, %mm2            \0A\09por    %mm4, %mm2            \0A\09pxor   %mm3, %mm4            \0A\09pand  %mm6, %mm4            \0A\09psrlq       $$1, %mm4            \0A\09psubb  %mm4, %mm2            \0A\09movq   ($2, $3), %mm3         \0A\09movq   %mm3, %mm1            \0A\09por    %mm5, %mm1            \0A\09pxor   %mm3, %mm5            \0A\09pand  %mm6, %mm5            \0A\09psrlq       $$1, %mm5            \0A\09psubb  %mm5, %mm1            \0A\09movq   %mm2, ($2)             \0A\09movq   %mm1, ($2, $3)         \0A\09add    %rax, $1        \0A\09add    %rax, $2        \0A\09subl   $$4, $0                  \0A\09jnz    1b                      \0A\09", "=*imr,={si},={di},r,0,1,2,~{rax},~{memory},~{dirflag},~{fpsr},~{flags}"(i32* nonnull %5, i64 %2, i32 %3, i8* %1, i8* %0) #4, !srcloc !22
  ret void
}

declare void @ff_put_pixels16_x2_3dnow(i8*, i8*, i64, i32) #2

; Function Attrs: nounwind ssp uwtable
define internal void @put_pixels16_y2_3dnow(i8*, i8*, i64, i32) #0 {
  tail call void @ff_put_pixels8_y2_3dnow(i8* %0, i8* %1, i64 %2, i32 %3) #4
  %5 = getelementptr inbounds i8, i8* %0, i64 8
  %6 = getelementptr inbounds i8, i8* %1, i64 8
  tail call void @ff_put_pixels8_y2_3dnow(i8* %5, i8* %6, i64 %2, i32 %3) #4
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal void @avg_pixels16_3dnow(i8*, i8*, i64, i32) #0 {
  tail call void @ff_avg_pixels8_3dnow(i8* %0, i8* %1, i64 %2, i32 %3) #4
  %5 = getelementptr inbounds i8, i8* %0, i64 8
  %6 = getelementptr inbounds i8, i8* %1, i64 8
  tail call void @ff_avg_pixels8_3dnow(i8* %5, i8* %6, i64 %2, i32 %3) #4
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal void @avg_pixels16_x2_3dnow(i8*, i8*, i64, i32) #0 {
  tail call void @ff_avg_pixels8_x2_3dnow(i8* %0, i8* %1, i64 %2, i32 %3) #4
  %5 = getelementptr inbounds i8, i8* %0, i64 8
  %6 = getelementptr inbounds i8, i8* %1, i64 8
  tail call void @ff_avg_pixels8_x2_3dnow(i8* %5, i8* %6, i64 %2, i32 %3) #4
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal void @avg_pixels16_y2_3dnow(i8*, i8*, i64, i32) #0 {
  tail call void @ff_avg_pixels8_y2_3dnow(i8* %0, i8* %1, i64 %2, i32 %3) #4
  %5 = getelementptr inbounds i8, i8* %0, i64 8
  %6 = getelementptr inbounds i8, i8* %1, i64 8
  tail call void @ff_avg_pixels8_y2_3dnow(i8* %5, i8* %6, i64 %2, i32 %3) #4
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal void @avg_pixels16_xy2_3dnow(i8*, i8*, i64, i32) #0 {
  tail call void @ff_avg_pixels8_xy2_3dnow(i8* %0, i8* %1, i64 %2, i32 %3) #4
  %5 = getelementptr inbounds i8, i8* %0, i64 8
  %6 = getelementptr inbounds i8, i8* %1, i64 8
  tail call void @ff_avg_pixels8_xy2_3dnow(i8* %5, i8* %6, i64 %2, i32 %3) #4
  ret void
}

declare void @ff_put_pixels8_x2_3dnow(i8*, i8*, i64, i32) #2

declare void @ff_put_pixels8_y2_3dnow(i8*, i8*, i64, i32) #2

declare void @ff_avg_pixels8_3dnow(i8*, i8*, i64, i32) #2

declare void @ff_avg_pixels8_x2_3dnow(i8*, i8*, i64, i32) #2

declare void @ff_avg_pixels8_y2_3dnow(i8*, i8*, i64, i32) #2

declare void @ff_avg_pixels8_xy2_3dnow(i8*, i8*, i64, i32) #2

; Function Attrs: nounwind ssp uwtable
define internal void @put_no_rnd_pixels16_x2_3dnow(i8*, i8*, i64, i32) #0 {
  tail call void @ff_put_no_rnd_pixels8_x2_3dnow(i8* %0, i8* %1, i64 %2, i32 %3) #4
  %5 = getelementptr inbounds i8, i8* %0, i64 8
  %6 = getelementptr inbounds i8, i8* %1, i64 8
  tail call void @ff_put_no_rnd_pixels8_x2_3dnow(i8* %5, i8* %6, i64 %2, i32 %3) #4
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal void @put_no_rnd_pixels16_y2_3dnow(i8*, i8*, i64, i32) #0 {
  tail call void @ff_put_no_rnd_pixels8_y2_3dnow(i8* %0, i8* %1, i64 %2, i32 %3) #4
  %5 = getelementptr inbounds i8, i8* %0, i64 8
  %6 = getelementptr inbounds i8, i8* %1, i64 8
  tail call void @ff_put_no_rnd_pixels8_y2_3dnow(i8* %5, i8* %6, i64 %2, i32 %3) #4
  ret void
}

declare void @ff_put_no_rnd_pixels8_x2_3dnow(i8*, i8*, i64, i32) #2

declare void @ff_put_no_rnd_pixels8_y2_3dnow(i8*, i8*, i64, i32) #2

; Function Attrs: nounwind ssp uwtable
define internal void @avg_approx_pixels16_xy2_3dnow(i8*, i8*, i64, i32) #0 {
  tail call void @ff_avg_approx_pixels8_xy2_3dnow(i8* %0, i8* %1, i64 %2, i32 %3) #4
  %5 = getelementptr inbounds i8, i8* %0, i64 8
  %6 = getelementptr inbounds i8, i8* %1, i64 8
  tail call void @ff_avg_approx_pixels8_xy2_3dnow(i8* %5, i8* %6, i64 %2, i32 %3) #4
  ret void
}

declare void @ff_avg_approx_pixels8_xy2_3dnow(i8*, i8*, i64, i32) #2

declare void @ff_put_pixels16_x2_mmxext(i8*, i8*, i64, i32) #2

; Function Attrs: nounwind ssp uwtable
define internal void @put_pixels16_y2_mmxext(i8*, i8*, i64, i32) #0 {
  tail call void @ff_put_pixels8_y2_mmxext(i8* %0, i8* %1, i64 %2, i32 %3) #4
  %5 = getelementptr inbounds i8, i8* %0, i64 8
  %6 = getelementptr inbounds i8, i8* %1, i64 8
  tail call void @ff_put_pixels8_y2_mmxext(i8* %5, i8* %6, i64 %2, i32 %3) #4
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal void @avg_pixels16_mmxext(i8*, i8*, i64, i32) #0 {
  tail call void @ff_avg_pixels8_mmxext(i8* %0, i8* %1, i64 %2, i32 %3) #4
  %5 = getelementptr inbounds i8, i8* %0, i64 8
  %6 = getelementptr inbounds i8, i8* %1, i64 8
  tail call void @ff_avg_pixels8_mmxext(i8* %5, i8* %6, i64 %2, i32 %3) #4
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal void @avg_pixels16_x2_mmxext(i8*, i8*, i64, i32) #0 {
  tail call void @ff_avg_pixels8_x2_mmxext(i8* %0, i8* %1, i64 %2, i32 %3) #4
  %5 = getelementptr inbounds i8, i8* %0, i64 8
  %6 = getelementptr inbounds i8, i8* %1, i64 8
  tail call void @ff_avg_pixels8_x2_mmxext(i8* %5, i8* %6, i64 %2, i32 %3) #4
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal void @avg_pixels16_y2_mmxext(i8*, i8*, i64, i32) #0 {
  tail call void @ff_avg_pixels8_y2_mmxext(i8* %0, i8* %1, i64 %2, i32 %3) #4
  %5 = getelementptr inbounds i8, i8* %0, i64 8
  %6 = getelementptr inbounds i8, i8* %1, i64 8
  tail call void @ff_avg_pixels8_y2_mmxext(i8* %5, i8* %6, i64 %2, i32 %3) #4
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal void @avg_pixels16_xy2_mmxext(i8*, i8*, i64, i32) #0 {
  tail call void @ff_avg_pixels8_xy2_mmxext(i8* %0, i8* %1, i64 %2, i32 %3) #4
  %5 = getelementptr inbounds i8, i8* %0, i64 8
  %6 = getelementptr inbounds i8, i8* %1, i64 8
  tail call void @ff_avg_pixels8_xy2_mmxext(i8* %5, i8* %6, i64 %2, i32 %3) #4
  ret void
}

declare void @ff_put_pixels8_x2_mmxext(i8*, i8*, i64, i32) #2

declare void @ff_put_pixels8_y2_mmxext(i8*, i8*, i64, i32) #2

declare void @ff_avg_pixels8_mmxext(i8*, i8*, i64, i32) #2

declare void @ff_avg_pixels8_x2_mmxext(i8*, i8*, i64, i32) #2

declare void @ff_avg_pixels8_y2_mmxext(i8*, i8*, i64, i32) #2

declare void @ff_avg_pixels8_xy2_mmxext(i8*, i8*, i64, i32) #2

; Function Attrs: nounwind ssp uwtable
define internal void @put_no_rnd_pixels16_x2_mmxext(i8*, i8*, i64, i32) #0 {
  tail call void @ff_put_no_rnd_pixels8_x2_mmxext(i8* %0, i8* %1, i64 %2, i32 %3) #4
  %5 = getelementptr inbounds i8, i8* %0, i64 8
  %6 = getelementptr inbounds i8, i8* %1, i64 8
  tail call void @ff_put_no_rnd_pixels8_x2_mmxext(i8* %5, i8* %6, i64 %2, i32 %3) #4
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal void @put_no_rnd_pixels16_y2_mmxext(i8*, i8*, i64, i32) #0 {
  tail call void @ff_put_no_rnd_pixels8_y2_mmxext(i8* %0, i8* %1, i64 %2, i32 %3) #4
  %5 = getelementptr inbounds i8, i8* %0, i64 8
  %6 = getelementptr inbounds i8, i8* %1, i64 8
  tail call void @ff_put_no_rnd_pixels8_y2_mmxext(i8* %5, i8* %6, i64 %2, i32 %3) #4
  ret void
}

declare void @ff_put_no_rnd_pixels8_x2_mmxext(i8*, i8*, i64, i32) #2

declare void @ff_put_no_rnd_pixels8_y2_mmxext(i8*, i8*, i64, i32) #2

; Function Attrs: nounwind ssp uwtable
define internal void @avg_approx_pixels16_xy2_mmxext(i8*, i8*, i64, i32) #0 {
  tail call void @ff_avg_approx_pixels8_xy2_mmxext(i8* %0, i8* %1, i64 %2, i32 %3) #4
  %5 = getelementptr inbounds i8, i8* %0, i64 8
  %6 = getelementptr inbounds i8, i8* %1, i64 8
  tail call void @ff_avg_approx_pixels8_xy2_mmxext(i8* %5, i8* %6, i64 %2, i32 %3) #4
  ret void
}

declare void @ff_avg_approx_pixels8_xy2_mmxext(i8*, i8*, i64, i32) #2

declare void @ff_put_pixels16_sse2(i8*, i8*, i64, i32) #2

declare void @ff_put_pixels16_x2_sse2(i8*, i8*, i64, i32) #2

declare void @ff_put_pixels16_y2_sse2(i8*, i8*, i64, i32) #2

declare void @ff_put_pixels16_xy2_sse2(i8*, i8*, i64, i32) #2

declare void @ff_avg_pixels16_sse2(i8*, i8*, i64, i32) #2

declare void @ff_avg_pixels16_x2_sse2(i8*, i8*, i64, i32) #2

declare void @ff_avg_pixels16_y2_sse2(i8*, i8*, i64, i32) #2

declare void @ff_avg_pixels16_xy2_sse2(i8*, i8*, i64, i32) #2

declare void @ff_put_pixels16_xy2_ssse3(i8*, i8*, i64, i32) #2

declare void @ff_avg_pixels16_xy2_ssse3(i8*, i8*, i64, i32) #2

declare void @ff_put_pixels8_xy2_ssse3(i8*, i8*, i64, i32) #2

declare void @ff_avg_pixels8_xy2_ssse3(i8*, i8*, i64, i32) #2

; Function Attrs: argmemonly nounwind
declare void @llvm.lifetime.start.p0i8(i64 immarg, i8* nocapture) #3

; Function Attrs: argmemonly nounwind
declare void @llvm.lifetime.end.p0i8(i64 immarg, i8* nocapture) #3

attributes #0 = { nounwind ssp uwtable "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "min-legal-vector-width"="0" "no-frame-pointer-elim"="false" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="true" "no-trapping-math"="false" "null-pointer-is-valid"="true" "stack-protector-buffer-size"="4" "target-cpu"="x86-64" "target-features"="+cx8,+fxsr,+mmx,+sse,+sse2,+sse3,+x87" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #1 = { cold nounwind optsize ssp uwtable "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "min-legal-vector-width"="0" "no-frame-pointer-elim"="false" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="true" "no-trapping-math"="false" "null-pointer-is-valid"="true" "stack-protector-buffer-size"="4" "target-cpu"="x86-64" "target-features"="+cx8,+fxsr,+mmx,+sse,+sse2,+sse3,+x87" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #2 = { "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "no-frame-pointer-elim"="false" "no-infs-fp-math"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="true" "no-trapping-math"="false" "null-pointer-is-valid"="true" "stack-protector-buffer-size"="4" "target-cpu"="x86-64" "target-features"="+cx8,+fxsr,+mmx,+sse,+sse2,+sse3,+x87" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #3 = { argmemonly nounwind }
attributes #4 = { nounwind }

!llvm.module.flags = !{!0, !1}

!0 = !{i32 1, !"wchar_size", i32 4}
!1 = !{i32 7, !"PIC Level", i32 2}
!2 = !{i32 -2145622622}
!3 = !{i32 -2145622514, i32 -2145622480, i32 -2145622422, i32 -2145622364}
!4 = !{i32 1851312, i32 1851346, i32 1851392, i32 1851438, i32 1851484, i32 1851530, i32 1851576, i32 1851622, i32 1851668, i32 1851714, i32 1851760, i32 1851809, i32 1851855, i32 1851901, i32 1851947, i32 1851996, i32 1852045, i32 1852091, i32 1852137, i32 1852183, i32 1852229, i32 1852275, i32 1852321, i32 1852367, i32 1852413, i32 1852459, i32 1852505, i32 1852551, i32 1852597, i32 1852643, i32 1852689, i32 1852735, i32 1852784, i32 1852833, i32 1852883, i32 1852953, i32 1852999, i32 1853045, i32 1853091, i32 1853137, i32 1853183, i32 1853229, i32 1853275, i32 1853321, i32 1853367, i32 1853413, i32 1853459, i32 1853505, i32 1853551, i32 1853597, i32 1853643, i32 1853692, i32 1853738, i32 1853785, i32 1853831}
!5 = !{i32 -2145622177}
!6 = !{i32 -2145622069, i32 -2145622035, i32 -2145621977, i32 -2145621919}
!7 = !{i32 1854274, i32 1854308, i32 1854354, i32 1854400, i32 1854446, i32 1854492, i32 1854538, i32 1854584, i32 1854630, i32 1854676, i32 1854722, i32 1854771, i32 1854817, i32 1854863, i32 1854909, i32 1854958, i32 1855007, i32 1855053, i32 1855099, i32 1855145, i32 1855191, i32 1855237, i32 1855283, i32 1855329, i32 1855375, i32 1855421, i32 1855467, i32 1855513, i32 1855559, i32 1855605, i32 1855651, i32 1855708, i32 1855754, i32 1855800, i32 1855846, i32 -2145621838, i32 -2145621771, i32 -2145621704, i32 -2145621637, i32 -2145621570, i32 -2145621503, i32 1855957, i32 1856003, i32 1856053, i32 1856123, i32 1856169, i32 1856215, i32 1856261, i32 1856307, i32 1856353, i32 1856399, i32 1856445, i32 1856491, i32 1856537, i32 1856583, i32 1856629, i32 1856675, i32 1856721, i32 1856767, i32 1856824, i32 1856870, i32 1856916, i32 1856962, i32 -2145621399, i32 -2145621332, i32 -2145621265, i32 -2145621198, i32 -2145621131, i32 -2145621064, i32 1857073, i32 1857122, i32 1857169, i32 1857215}
!8 = !{i32 -2145649334, i32 -2145649300, i32 -2145649242}
!9 = !{i32 1822762, i32 1822796, i32 1822842, i32 1822888, i32 1822934, i32 1822980, i32 1823026, i32 1823072, i32 -2145649119, i32 -2145649052, i32 -2145648985, i32 -2145648918, i32 -2145648851, i32 -2145648784, i32 -2145648717, i32 -2145648650, i32 -2145648583, i32 -2145648516, i32 -2145648449, i32 -2145648382, i32 1823177, i32 1823223, i32 1823269, i32 1823315, i32 1823361, i32 1823407, i32 -2145648227, i32 -2145648160, i32 -2145648093, i32 -2145648026, i32 -2145647959, i32 -2145647892, i32 -2145647825, i32 -2145647758, i32 -2145647691, i32 -2145647624, i32 -2145647557, i32 -2145647490, i32 1823512, i32 1823558, i32 1823604, i32 1823650, i32 1823696, i32 1823742, i32 1823788, i32 1823834, i32 -2145647323, i32 -2145647256, i32 -2145647189, i32 -2145647122, i32 -2145647055, i32 -2145646988, i32 -2145646921, i32 -2145646854, i32 -2145646787, i32 -2145646720, i32 -2145646653, i32 -2145646586, i32 1823939, i32 1823985, i32 1824031, i32 1824077, i32 1824123, i32 1824169, i32 -2145646431, i32 -2145646364, i32 -2145646297, i32 -2145646230, i32 -2145646163, i32 -2145646096, i32 -2145646029, i32 -2145645962, i32 -2145645895, i32 -2145645828, i32 -2145645761, i32 -2145645694, i32 1824274, i32 1824320, i32 1824366, i32 1824412, i32 1824458, i32 1824504}
!10 = !{i32 -2145645484, i32 -2145645450, i32 -2145645392}
!11 = !{i32 1824779, i32 1824813, i32 1824859, i32 1824905, i32 1824951, i32 1824997, i32 1825043, i32 -2145645263, i32 -2145645196, i32 -2145645129, i32 -2145645062, i32 -2145644995, i32 -2145644928, i32 -2145644861, i32 -2145644794, i32 -2145644727, i32 -2145644660, i32 -2145644593, i32 -2145644526, i32 1825148, i32 1825194, i32 1825240, i32 1825286, i32 1825332, i32 1825378, i32 -2145644353, i32 -2145644286, i32 -2145644219, i32 -2145644152, i32 -2145644085, i32 -2145644018, i32 -2145643951, i32 -2145643884, i32 -2145643817, i32 -2145643750, i32 -2145643683, i32 -2145643616, i32 1825483, i32 1825529, i32 1825575, i32 1825621, i32 1825667, i32 1825713}
!12 = !{i32 -2145677022, i32 -2145676988, i32 -2145676930}
!13 = !{i32 1795032, i32 1795066, i32 1795112, i32 1795158, i32 1795204, i32 1795250, i32 1795296, i32 1795342, i32 -2145676800, i32 -2145676733, i32 -2145676666, i32 -2145676599, i32 -2145676532, i32 -2145676465, i32 -2145676398, i32 -2145676331, i32 -2145676264, i32 -2145676197, i32 -2145676130, i32 -2145676063, i32 1795447, i32 1795493, i32 1795539, i32 1795585, i32 1795631, i32 1795677, i32 -2145675901, i32 -2145675834, i32 -2145675767, i32 -2145675700, i32 -2145675633, i32 -2145675566, i32 -2145675499, i32 -2145675432, i32 -2145675365, i32 -2145675298, i32 -2145675231, i32 -2145675164, i32 1795782, i32 1795828, i32 1795874, i32 1795920, i32 1795966, i32 1796012, i32 1796058, i32 1796104, i32 -2145674990, i32 -2145674923, i32 -2145674856, i32 -2145674789, i32 -2145674722, i32 -2145674655, i32 -2145674588, i32 -2145674521, i32 -2145674454, i32 -2145674387, i32 -2145674320, i32 -2145674253, i32 1796209, i32 1796255, i32 1796301, i32 1796347, i32 1796393, i32 1796439, i32 -2145674091, i32 -2145674024, i32 -2145673957, i32 -2145673890, i32 -2145673823, i32 -2145673756, i32 -2145673689, i32 -2145673622, i32 -2145673555, i32 -2145673488, i32 -2145673421, i32 -2145673354, i32 1796544, i32 1796590, i32 1796636, i32 1796682, i32 1796728, i32 1796774}
!14 = !{i32 -2145673130, i32 -2145673096, i32 -2145673038}
!15 = !{i32 1797049, i32 1797083, i32 1797129, i32 1797175, i32 1797221, i32 1797267, i32 1797313, i32 -2145672902, i32 -2145672835, i32 -2145672768, i32 -2145672701, i32 -2145672634, i32 -2145672567, i32 -2145672500, i32 -2145672433, i32 -2145672366, i32 -2145672299, i32 -2145672232, i32 -2145672165, i32 1797418, i32 1797464, i32 1797510, i32 1797556, i32 1797602, i32 1797648, i32 -2145671985, i32 -2145671918, i32 -2145671851, i32 -2145671784, i32 -2145671717, i32 -2145671650, i32 -2145671583, i32 -2145671516, i32 -2145671449, i32 -2145671382, i32 -2145671315, i32 -2145671248, i32 1797753, i32 1797799, i32 1797845, i32 1797891, i32 1797937, i32 1797983}
!16 = !{i32 -2145692681}
!17 = !{i32 -2145692610, i32 -2145692574}
!18 = !{i32 1768128, i32 1768162, i32 1768208, i32 1768254, i32 1768300, i32 1768346, i32 1768392, i32 1768438, i32 1768484, i32 1768530, i32 1768576, i32 1768625, i32 1768671, i32 1768717, i32 1768763, i32 1768812, i32 1768861, i32 1768907, i32 1768953, i32 1768999, i32 1769045, i32 1769091, i32 1769137, i32 1769183, i32 1769229, i32 1769275, i32 1769321, i32 1769367, i32 1769413, i32 1769459, i32 1769505, i32 1769551, i32 1769600, i32 1769649, i32 1769699, i32 1769769, i32 1769815, i32 1769861, i32 1769907, i32 1769953, i32 1769999, i32 1770045, i32 1770091, i32 1770137, i32 1770183, i32 1770229, i32 1770275, i32 1770321, i32 1770367, i32 1770413, i32 1770459, i32 1770508, i32 1770554, i32 1770601, i32 1770647}
!19 = !{i32 -2145639343, i32 -2145639309, i32 -2145639251}
!20 = !{i32 1825997, i32 1826027, i32 1826073, i32 1826119, i32 1826165, i32 1826211, i32 -2145639153, i32 -2145639086, i32 -2145639019, i32 -2145638952, i32 -2145638885, i32 -2145638818, i32 -2145638744, i32 -2145638677, i32 -2145638610, i32 -2145638543, i32 -2145638476, i32 -2145638409, i32 1826353, i32 1826399, i32 1826445, i32 1826491, i32 -2145638289, i32 -2145638222, i32 -2145638155, i32 -2145638088, i32 -2145638021, i32 -2145637954, i32 -2145637880, i32 -2145637813, i32 -2145637746, i32 -2145637679, i32 -2145637612, i32 -2145637545, i32 1826633, i32 1826679, i32 1826725, i32 1826771, i32 1826817}
!21 = !{i32 -2145637369, i32 -2145637335, i32 -2145637277}
!22 = !{i32 1827094, i32 1827128, i32 1827174, i32 1827220, i32 1827266, i32 1827312, i32 1827360, i32 -2145637148, i32 -2145637081, i32 -2145637014, i32 -2145636947, i32 -2145636880, i32 -2145636813, i32 -2145636746, i32 -2145636679, i32 -2145636612, i32 -2145636545, i32 -2145636478, i32 -2145636411, i32 1827465, i32 -2145636321, i32 -2145636254, i32 -2145636187, i32 -2145636120, i32 -2145636053, i32 -2145635986, i32 1827557, i32 -2145635912, i32 -2145635845, i32 -2145635778, i32 -2145635711, i32 -2145635644, i32 -2145635577, i32 1827649, i32 1827695, i32 1827741, i32 1827787, i32 1827834, i32 1827882, i32 -2145635420, i32 -2145635353, i32 -2145635286, i32 -2145635219, i32 -2145635152, i32 -2145635085, i32 -2145635018, i32 -2145634951, i32 -2145634884, i32 -2145634817, i32 -2145634750, i32 -2145634683, i32 1827987, i32 -2145634593, i32 -2145634526, i32 -2145634459, i32 -2145634392, i32 -2145634325, i32 -2145634258, i32 1828079, i32 -2145634184, i32 -2145634117, i32 -2145634050, i32 -2145633983, i32 -2145633916, i32 -2145633849, i32 1828171, i32 1828217, i32 1828263, i32 1828309, i32 1828356, i32 1828402}
!23 = !{i32 -2145671022, i32 -2145670988, i32 -2145670930}
!24 = !{i32 1798267, i32 1798297, i32 1798343, i32 1798389, i32 1798435, i32 1798481, i32 -2145670825, i32 -2145670758, i32 -2145670691, i32 -2145670624, i32 -2145670557, i32 -2145670490, i32 -2145670416, i32 -2145670349, i32 -2145670282, i32 -2145670215, i32 -2145670148, i32 -2145670081, i32 1798623, i32 1798669, i32 1798715, i32 1798761, i32 -2145669954, i32 -2145669887, i32 -2145669820, i32 -2145669753, i32 -2145669686, i32 -2145669619, i32 -2145669545, i32 -2145669478, i32 -2145669411, i32 -2145669344, i32 -2145669277, i32 -2145669210, i32 1798903, i32 1798949, i32 1798995, i32 1799041, i32 1799087}
!25 = !{i32 -2145669020, i32 -2145668986, i32 -2145668928}
!26 = !{i32 1799364, i32 1799398, i32 1799444, i32 1799490, i32 1799536, i32 1799582, i32 1799630, i32 -2145668792, i32 -2145668725, i32 -2145668658, i32 -2145668591, i32 -2145668524, i32 -2145668457, i32 -2145668390, i32 -2145668323, i32 -2145668256, i32 -2145668189, i32 -2145668122, i32 -2145668055, i32 1799735, i32 -2145667965, i32 -2145667898, i32 -2145667831, i32 -2145667764, i32 -2145667697, i32 -2145667630, i32 1799827, i32 -2145667556, i32 -2145667489, i32 -2145667422, i32 -2145667355, i32 -2145667288, i32 -2145667221, i32 1799919, i32 1799965, i32 1800011, i32 1800057, i32 1800104, i32 1800152, i32 -2145667057, i32 -2145666990, i32 -2145666923, i32 -2145666856, i32 -2145666789, i32 -2145666722, i32 -2145666655, i32 -2145666588, i32 -2145666521, i32 -2145666454, i32 -2145666387, i32 -2145666320, i32 1800257, i32 -2145666230, i32 -2145666163, i32 -2145666096, i32 -2145666029, i32 -2145665962, i32 -2145665895, i32 1800349, i32 -2145665821, i32 -2145665754, i32 -2145665687, i32 -2145665620, i32 -2145665553, i32 -2145665486, i32 1800441, i32 1800487, i32 1800533, i32 1800579, i32 1800626, i32 1800672}
!27 = !{i32 -2145692347}
!28 = !{i32 -2145692276, i32 -2145692240}
!29 = !{i32 1771090, i32 1771124, i32 1771170, i32 1771216, i32 1771262, i32 1771308, i32 1771354, i32 1771400, i32 1771446, i32 1771492, i32 1771538, i32 1771587, i32 1771633, i32 1771679, i32 1771725, i32 1771774, i32 1771823, i32 1771869, i32 1771915, i32 1771961, i32 1772007, i32 1772053, i32 1772099, i32 1772145, i32 1772191, i32 1772237, i32 1772283, i32 1772329, i32 1772375, i32 1772421, i32 1772467, i32 1772524, i32 1772570, i32 1772616, i32 1772662, i32 -2145692130, i32 -2145692063, i32 -2145691996, i32 -2145691929, i32 -2145691862, i32 -2145691795, i32 1772773, i32 1772819, i32 1772869, i32 1772939, i32 1772985, i32 1773031, i32 1773077, i32 1773123, i32 1773169, i32 1773215, i32 1773261, i32 1773307, i32 1773353, i32 1773399, i32 1773445, i32 1773491, i32 1773537, i32 1773583, i32 1773640, i32 1773686, i32 1773732, i32 1773778, i32 -2145691691, i32 -2145691624, i32 -2145691557, i32 -2145691490, i32 -2145691423, i32 -2145691356, i32 1773889, i32 1773938, i32 1773985, i32 1774031}
!30 = !{i32 -2145651402, i32 -2145651368, i32 -2145651310}
!31 = !{i32 1821414, i32 1821448, i32 1821494, i32 1821540, i32 1821586, i32 1821632, i32 1821678, i32 1821724, i32 -2145651187, i32 -2145651120, i32 -2145651053, i32 -2145650986, i32 -2145650919, i32 -2145650852, i32 -2145650785, i32 -2145650718, i32 -2145650651, i32 -2145650584, i32 -2145650517, i32 -2145650450, i32 1821829, i32 1821875, i32 1821921, i32 1821967, i32 1822013, i32 1822059, i32 1822105, i32 1822151, i32 -2145650283, i32 -2145650216, i32 -2145650149, i32 -2145650082, i32 -2145650015, i32 -2145649948, i32 -2145649881, i32 -2145649814, i32 -2145649747, i32 -2145649680, i32 -2145649613, i32 -2145649546, i32 1822256, i32 1822302, i32 1822348, i32 1822394, i32 1822440, i32 1822486}
!32 = !{i32 -2145679118, i32 -2145679084, i32 -2145679026}
!33 = !{i32 1793684, i32 1793718, i32 1793764, i32 1793810, i32 1793856, i32 1793902, i32 1793948, i32 1793994, i32 -2145678896, i32 -2145678829, i32 -2145678762, i32 -2145678695, i32 -2145678628, i32 -2145678561, i32 -2145678494, i32 -2145678427, i32 -2145678360, i32 -2145678293, i32 -2145678226, i32 -2145678159, i32 1794099, i32 1794145, i32 1794191, i32 1794237, i32 1794283, i32 1794329, i32 1794375, i32 1794421, i32 -2145677985, i32 -2145677918, i32 -2145677851, i32 -2145677784, i32 -2145677717, i32 -2145677650, i32 -2145677583, i32 -2145677516, i32 -2145677449, i32 -2145677382, i32 -2145677315, i32 -2145677248, i32 1794526, i32 1794572, i32 1794618, i32 1794664, i32 1794710, i32 1794756}
