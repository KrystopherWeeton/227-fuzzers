; ModuleID = '../../third_party/pdfium/core/fdrm/fx_crypt_sha.cpp'
source_filename = "../../third_party/pdfium/core/fdrm/fx_crypt_sha.cpp"
target datalayout = "e-m:e-i64:64-f80:128-n8:16:32:64-S128"
target triple = "x86_64-unknown-linux-gnu"

module asm ".symver exp, exp@GLIBC_2.2.5"
module asm ".symver exp2, exp2@GLIBC_2.2.5"
module asm ".symver exp2f, exp2f@GLIBC_2.2.5"
module asm ".symver expf, expf@GLIBC_2.2.5"
module asm ".symver lgamma, lgamma@GLIBC_2.2.5"
module asm ".symver lgammaf, lgammaf@GLIBC_2.2.5"
module asm ".symver lgammal, lgammal@GLIBC_2.2.5"
module asm ".symver log, log@GLIBC_2.2.5"
module asm ".symver log2, log2@GLIBC_2.2.5"
module asm ".symver log2f, log2f@GLIBC_2.2.5"
module asm ".symver logf, logf@GLIBC_2.2.5"
module asm ".symver pow, pow@GLIBC_2.2.5"
module asm ".symver powf, powf@GLIBC_2.2.5"

%struct.CRYPT_sha1_context = type { i64, i32, [5 x i32], [64 x i8] }
%struct.CRYPT_sha2_context = type { i64, [8 x i64], [128 x i8] }

@_ZN12_GLOBAL__N_114sha256_paddingE = internal constant <{ i8, [63 x i8] }> <{ i8 -128, [63 x i8] zeroinitializer }>, align 16
@_ZN12_GLOBAL__N_19constantsE = internal unnamed_addr constant [80 x i64] [i64 4794697086780616226, i64 8158064640168781261, i64 -5349999486874862801, i64 -1606136188198331460, i64 4131703408338449720, i64 6480981068601479193, i64 -7908458776815382629, i64 -6116909921290321640, i64 -2880145864133508542, i64 1334009975649890238, i64 2608012711638119052, i64 6128411473006802146, i64 8268148722764581231, i64 -9160688886553864527, i64 -7215885187991268811, i64 -4495734319001033068, i64 -1973867731355612462, i64 -1171420211273849373, i64 1135362057144423861, i64 2597628984639134821, i64 3308224258029322869, i64 5365058923640841347, i64 6679025012923562964, i64 8573033837759648693, i64 -7476448914759557205, i64 -6327057829258317296, i64 -5763719355590565569, i64 -4658551843659510044, i64 -4116276920077217854, i64 -3051310485924567259, i64 489312712824947311, i64 1452737877330783856, i64 2861767655752347644, i64 3322285676063803686, i64 5560940570517711597, i64 5996557281743188959, i64 7280758554555802590, i64 8532644243296465576, i64 -9096487096722542874, i64 -7894198246740708037, i64 -6719396339535248540, i64 -6333637450476146687, i64 -4446306890439682159, i64 -4076793802049405392, i64 -3345356375505022440, i64 -2983346525034927856, i64 -860691631967231958, i64 1182934255886127544, i64 1847814050463011016, i64 2177327727835720531, i64 2830643537854262169, i64 3796741975233480872, i64 4115178125766777443, i64 5681478168544905931, i64 6601373596472566643, i64 7507060721942968483, i64 8399075790359081724, i64 8693463985226723168, i64 -8878714635349349518, i64 -8302665154208450068, i64 -8016688836872298968, i64 -6606660893046293015, i64 -4685533653050689259, i64 -4147400797238176981, i64 -3880063495543823972, i64 -3348786107499101689, i64 -1523767162380948706, i64 -757361751448694408, i64 500013540394364858, i64 748580250866718886, i64 1242879168328830382, i64 1977374033974150939, i64 2944078676154940804, i64 3659926193048069267, i64 4368137639120453308, i64 4836135668995329356, i64 5532061633213252278, i64 6448918945643986474, i64 6902733635092675308, i64 7801388544844847127], align 16
@_ZN12_GLOBAL__N_114sha384_paddingE = internal constant <{ i8, [127 x i8] }> <{ i8 -128, [127 x i8] zeroinitializer }>, align 16

; Function Attrs: nofree norecurse nounwind ssp uwtable writeonly
define hidden void @_Z15CRYPT_SHA1StartP18CRYPT_sha1_context(%struct.CRYPT_sha1_context* nocapture) local_unnamed_addr #0 {
  %2 = getelementptr inbounds %struct.CRYPT_sha1_context, %struct.CRYPT_sha1_context* %0, i64 0, i32 2, i64 3
  store i32 271733878, i32* %2, align 4
  %3 = getelementptr inbounds %struct.CRYPT_sha1_context, %struct.CRYPT_sha1_context* %0, i64 0, i32 2, i64 4
  store i32 -1009589776, i32* %3, align 4
  %4 = getelementptr inbounds %struct.CRYPT_sha1_context, %struct.CRYPT_sha1_context* %0, i64 0, i32 0
  store i64 0, i64* %4, align 8
  %5 = getelementptr inbounds %struct.CRYPT_sha1_context, %struct.CRYPT_sha1_context* %0, i64 0, i32 1
  %6 = bitcast i32* %5 to <4 x i32>*
  store <4 x i32> <i32 0, i32 1732584193, i32 -271733879, i32 -1732584194>, <4 x i32>* %6, align 8
  ret void
}

; Function Attrs: nounwind ssp uwtable
define hidden void @_Z16CRYPT_SHA1UpdateP18CRYPT_sha1_contextPKhj(%struct.CRYPT_sha1_context* nocapture, i8* nocapture readonly, i32) local_unnamed_addr #1 {
  %4 = alloca [80 x i32], align 16
  %5 = alloca [16 x i32], align 16
  %6 = zext i32 %2 to i64
  %7 = getelementptr inbounds %struct.CRYPT_sha1_context, %struct.CRYPT_sha1_context* %0, i64 0, i32 0
  %8 = load i64, i64* %7, align 8
  %9 = add i64 %8, %6
  store i64 %9, i64* %7, align 8
  %10 = getelementptr inbounds %struct.CRYPT_sha1_context, %struct.CRYPT_sha1_context* %0, i64 0, i32 1
  %11 = load i32, i32* %10, align 8
  %12 = icmp ne i32 %11, 0
  %13 = sub i32 64, %11
  %14 = icmp ugt i32 %13, %2
  %15 = and i1 %12, %14
  br i1 %15, label %16, label %20

16:                                               ; preds = %3
  %17 = zext i32 %11 to i64
  %18 = getelementptr inbounds %struct.CRYPT_sha1_context, %struct.CRYPT_sha1_context* %0, i64 0, i32 3, i64 %17
  tail call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 1 %18, i8* align 1 %1, i64 %6, i1 false)
  %19 = add i32 %11, %2
  store i32 %19, i32* %10, align 8
  br label %256

20:                                               ; preds = %3
  %21 = bitcast [16 x i32]* %5 to i8*
  call void @llvm.lifetime.start.p0i8(i64 64, i8* nonnull %21) #4
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %21, i8 -86, i64 64, i1 false)
  br i1 %14, label %251, label %22

22:                                               ; preds = %20
  %23 = getelementptr inbounds %struct.CRYPT_sha1_context, %struct.CRYPT_sha1_context* %0, i64 0, i32 2, i64 0
  %24 = bitcast [80 x i32]* %4 to i8*
  %25 = getelementptr inbounds [80 x i32], [80 x i32]* %4, i64 0, i64 16
  %26 = bitcast i32* %25 to i8*
  %27 = getelementptr inbounds %struct.CRYPT_sha1_context, %struct.CRYPT_sha1_context* %0, i64 0, i32 2, i64 4
  %28 = zext i32 %11 to i64
  %29 = getelementptr inbounds %struct.CRYPT_sha1_context, %struct.CRYPT_sha1_context* %0, i64 0, i32 3, i64 0
  %30 = bitcast i8* %29 to <16 x i8>*
  %31 = bitcast [16 x i32]* %5 to <4 x i32>*
  %32 = getelementptr inbounds %struct.CRYPT_sha1_context, %struct.CRYPT_sha1_context* %0, i64 0, i32 3, i64 16
  %33 = bitcast i8* %32 to <16 x i8>*
  %34 = getelementptr inbounds [16 x i32], [16 x i32]* %5, i64 0, i64 4
  %35 = bitcast i32* %34 to <4 x i32>*
  %36 = getelementptr inbounds %struct.CRYPT_sha1_context, %struct.CRYPT_sha1_context* %0, i64 0, i32 3, i64 32
  %37 = bitcast i8* %36 to <16 x i8>*
  %38 = getelementptr inbounds [16 x i32], [16 x i32]* %5, i64 0, i64 8
  %39 = bitcast i32* %38 to <4 x i32>*
  %40 = getelementptr inbounds %struct.CRYPT_sha1_context, %struct.CRYPT_sha1_context* %0, i64 0, i32 3, i64 48
  %41 = bitcast i8* %40 to <16 x i8>*
  %42 = getelementptr inbounds [16 x i32], [16 x i32]* %5, i64 0, i64 12
  %43 = bitcast i32* %42 to <4 x i32>*
  %44 = bitcast i32* %23 to <4 x i32>*
  %45 = bitcast i32* %23 to <4 x i32>*
  br label %46

46:                                               ; preds = %22, %240
  %47 = phi i32 [ %13, %22 ], [ 64, %240 ]
  %48 = phi i64 [ %28, %22 ], [ 0, %240 ]
  %49 = phi i32 [ %2, %22 ], [ %241, %240 ]
  %50 = phi i8* [ %1, %22 ], [ %53, %240 ]
  %51 = getelementptr inbounds %struct.CRYPT_sha1_context, %struct.CRYPT_sha1_context* %0, i64 0, i32 3, i64 %48
  %52 = zext i32 %47 to i64
  tail call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 1 %51, i8* align 1 %50, i64 %52, i1 false)
  %53 = getelementptr inbounds i8, i8* %50, i64 %52
  %54 = load <16 x i8>, <16 x i8>* %30, align 1
  %55 = shufflevector <16 x i8> %54, <16 x i8> undef, <4 x i32> <i32 0, i32 4, i32 8, i32 12>
  %56 = shufflevector <16 x i8> %54, <16 x i8> undef, <4 x i32> <i32 1, i32 5, i32 9, i32 13>
  %57 = shufflevector <16 x i8> %54, <16 x i8> undef, <4 x i32> <i32 2, i32 6, i32 10, i32 14>
  %58 = shufflevector <16 x i8> %54, <16 x i8> undef, <4 x i32> <i32 3, i32 7, i32 11, i32 15>
  %59 = zext <4 x i8> %55 to <4 x i32>
  %60 = shl nuw <4 x i32> %59, <i32 24, i32 24, i32 24, i32 24>
  %61 = zext <4 x i8> %56 to <4 x i32>
  %62 = shl nuw nsw <4 x i32> %61, <i32 16, i32 16, i32 16, i32 16>
  %63 = or <4 x i32> %62, %60
  %64 = zext <4 x i8> %57 to <4 x i32>
  %65 = shl nuw nsw <4 x i32> %64, <i32 8, i32 8, i32 8, i32 8>
  %66 = or <4 x i32> %63, %65
  %67 = zext <4 x i8> %58 to <4 x i32>
  %68 = or <4 x i32> %66, %67
  store <4 x i32> %68, <4 x i32>* %31, align 16
  %69 = load <16 x i8>, <16 x i8>* %33, align 1
  %70 = shufflevector <16 x i8> %69, <16 x i8> undef, <4 x i32> <i32 0, i32 4, i32 8, i32 12>
  %71 = shufflevector <16 x i8> %69, <16 x i8> undef, <4 x i32> <i32 1, i32 5, i32 9, i32 13>
  %72 = shufflevector <16 x i8> %69, <16 x i8> undef, <4 x i32> <i32 2, i32 6, i32 10, i32 14>
  %73 = shufflevector <16 x i8> %69, <16 x i8> undef, <4 x i32> <i32 3, i32 7, i32 11, i32 15>
  %74 = zext <4 x i8> %70 to <4 x i32>
  %75 = shl nuw <4 x i32> %74, <i32 24, i32 24, i32 24, i32 24>
  %76 = zext <4 x i8> %71 to <4 x i32>
  %77 = shl nuw nsw <4 x i32> %76, <i32 16, i32 16, i32 16, i32 16>
  %78 = or <4 x i32> %77, %75
  %79 = zext <4 x i8> %72 to <4 x i32>
  %80 = shl nuw nsw <4 x i32> %79, <i32 8, i32 8, i32 8, i32 8>
  %81 = or <4 x i32> %78, %80
  %82 = zext <4 x i8> %73 to <4 x i32>
  %83 = or <4 x i32> %81, %82
  store <4 x i32> %83, <4 x i32>* %35, align 16
  %84 = load <16 x i8>, <16 x i8>* %37, align 1
  %85 = shufflevector <16 x i8> %84, <16 x i8> undef, <4 x i32> <i32 0, i32 4, i32 8, i32 12>
  %86 = shufflevector <16 x i8> %84, <16 x i8> undef, <4 x i32> <i32 1, i32 5, i32 9, i32 13>
  %87 = shufflevector <16 x i8> %84, <16 x i8> undef, <4 x i32> <i32 2, i32 6, i32 10, i32 14>
  %88 = shufflevector <16 x i8> %84, <16 x i8> undef, <4 x i32> <i32 3, i32 7, i32 11, i32 15>
  %89 = zext <4 x i8> %85 to <4 x i32>
  %90 = shl nuw <4 x i32> %89, <i32 24, i32 24, i32 24, i32 24>
  %91 = zext <4 x i8> %86 to <4 x i32>
  %92 = shl nuw nsw <4 x i32> %91, <i32 16, i32 16, i32 16, i32 16>
  %93 = or <4 x i32> %92, %90
  %94 = zext <4 x i8> %87 to <4 x i32>
  %95 = shl nuw nsw <4 x i32> %94, <i32 8, i32 8, i32 8, i32 8>
  %96 = or <4 x i32> %93, %95
  %97 = zext <4 x i8> %88 to <4 x i32>
  %98 = or <4 x i32> %96, %97
  store <4 x i32> %98, <4 x i32>* %39, align 16
  %99 = load <16 x i8>, <16 x i8>* %41, align 1
  %100 = shufflevector <16 x i8> %99, <16 x i8> undef, <4 x i32> <i32 0, i32 4, i32 8, i32 12>
  %101 = shufflevector <16 x i8> %99, <16 x i8> undef, <4 x i32> <i32 1, i32 5, i32 9, i32 13>
  %102 = shufflevector <16 x i8> %99, <16 x i8> undef, <4 x i32> <i32 2, i32 6, i32 10, i32 14>
  %103 = shufflevector <16 x i8> %99, <16 x i8> undef, <4 x i32> <i32 3, i32 7, i32 11, i32 15>
  %104 = zext <4 x i8> %100 to <4 x i32>
  %105 = shl nuw <4 x i32> %104, <i32 24, i32 24, i32 24, i32 24>
  %106 = zext <4 x i8> %101 to <4 x i32>
  %107 = shl nuw nsw <4 x i32> %106, <i32 16, i32 16, i32 16, i32 16>
  %108 = or <4 x i32> %107, %105
  %109 = zext <4 x i8> %102 to <4 x i32>
  %110 = shl nuw nsw <4 x i32> %109, <i32 8, i32 8, i32 8, i32 8>
  %111 = or <4 x i32> %108, %110
  %112 = zext <4 x i8> %103 to <4 x i32>
  %113 = or <4 x i32> %111, %112
  store <4 x i32> %113, <4 x i32>* %43, align 16
  call void @llvm.lifetime.start.p0i8(i64 320, i8* nonnull %24) #4
  call void @llvm.memset.p0i8.i64(i8* align 16 %26, i8 -86, i64 256, i1 false) #4
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 16 %24, i8* nonnull align 16 %21, i64 64, i1 false) #4
  br label %114

114:                                              ; preds = %114, %46
  %115 = phi i64 [ 16, %46 ], [ %135, %114 ]
  %116 = add nsw i64 %115, -3
  %117 = getelementptr inbounds [80 x i32], [80 x i32]* %4, i64 0, i64 %116
  %118 = load i32, i32* %117, align 4
  %119 = add nsw i64 %115, -8
  %120 = getelementptr inbounds [80 x i32], [80 x i32]* %4, i64 0, i64 %119
  %121 = load i32, i32* %120, align 4
  %122 = xor i32 %121, %118
  %123 = add nsw i64 %115, -14
  %124 = getelementptr inbounds [80 x i32], [80 x i32]* %4, i64 0, i64 %123
  %125 = load i32, i32* %124, align 4
  %126 = xor i32 %122, %125
  %127 = add nsw i64 %115, -16
  %128 = getelementptr inbounds [80 x i32], [80 x i32]* %4, i64 0, i64 %127
  %129 = load i32, i32* %128, align 4
  %130 = xor i32 %126, %129
  %131 = shl i32 %130, 1
  %132 = lshr i32 %130, 31
  %133 = or i32 %131, %132
  %134 = getelementptr inbounds [80 x i32], [80 x i32]* %4, i64 0, i64 %115
  store i32 %133, i32* %134, align 4
  %135 = add nuw nsw i64 %115, 1
  %136 = icmp eq i64 %135, 80
  br i1 %136, label %137, label %114

137:                                              ; preds = %114
  %138 = load <4 x i32>, <4 x i32>* %44, align 4
  %139 = load i32, i32* %27, align 4
  %140 = extractelement <4 x i32> %138, i32 0
  %141 = extractelement <4 x i32> %138, i32 1
  %142 = extractelement <4 x i32> %138, i32 2
  %143 = extractelement <4 x i32> %138, i32 3
  br label %144

144:                                              ; preds = %144, %137
  %145 = phi i64 [ 0, %137 ], [ %167, %144 ]
  %146 = phi i32 [ %140, %137 ], [ %163, %144 ]
  %147 = phi i32 [ %141, %137 ], [ %146, %144 ]
  %148 = phi i32 [ %142, %137 ], [ %166, %144 ]
  %149 = phi i32 [ %139, %137 ], [ %150, %144 ]
  %150 = phi i32 [ %143, %137 ], [ %148, %144 ]
  %151 = shl i32 %146, 5
  %152 = lshr i32 %146, 27
  %153 = or i32 %151, %152
  %154 = and i32 %148, %147
  %155 = xor i32 %147, -1
  %156 = and i32 %150, %155
  %157 = or i32 %156, %154
  %158 = getelementptr inbounds [80 x i32], [80 x i32]* %4, i64 0, i64 %145
  %159 = load i32, i32* %158, align 4
  %160 = add i32 %153, 1518500249
  %161 = add i32 %160, %149
  %162 = add i32 %161, %159
  %163 = add i32 %162, %157
  %164 = shl i32 %147, 30
  %165 = lshr i32 %147, 2
  %166 = or i32 %164, %165
  %167 = add nuw nsw i64 %145, 1
  %168 = icmp eq i64 %167, 20
  br i1 %168, label %169, label %144

169:                                              ; preds = %144, %169
  %170 = phi i64 [ %190, %169 ], [ 20, %144 ]
  %171 = phi i32 [ %186, %169 ], [ %163, %144 ]
  %172 = phi i32 [ %171, %169 ], [ %146, %144 ]
  %173 = phi i32 [ %189, %169 ], [ %166, %144 ]
  %174 = phi i32 [ %175, %169 ], [ %150, %144 ]
  %175 = phi i32 [ %173, %169 ], [ %148, %144 ]
  %176 = shl i32 %171, 5
  %177 = lshr i32 %171, 27
  %178 = or i32 %176, %177
  %179 = xor i32 %173, %172
  %180 = xor i32 %179, %175
  %181 = getelementptr inbounds [80 x i32], [80 x i32]* %4, i64 0, i64 %170
  %182 = load i32, i32* %181, align 4
  %183 = add i32 %178, 1859775393
  %184 = add i32 %183, %174
  %185 = add i32 %184, %182
  %186 = add i32 %185, %180
  %187 = shl i32 %172, 30
  %188 = lshr i32 %172, 2
  %189 = or i32 %187, %188
  %190 = add nuw nsw i64 %170, 1
  %191 = icmp eq i64 %190, 40
  br i1 %191, label %192, label %169

192:                                              ; preds = %169, %192
  %193 = phi i64 [ %215, %192 ], [ 40, %169 ]
  %194 = phi i32 [ %211, %192 ], [ %186, %169 ]
  %195 = phi i32 [ %194, %192 ], [ %171, %169 ]
  %196 = phi i32 [ %214, %192 ], [ %189, %169 ]
  %197 = phi i32 [ %198, %192 ], [ %175, %169 ]
  %198 = phi i32 [ %196, %192 ], [ %173, %169 ]
  %199 = shl i32 %194, 5
  %200 = lshr i32 %194, 27
  %201 = or i32 %199, %200
  %202 = or i32 %198, %196
  %203 = and i32 %202, %195
  %204 = and i32 %198, %196
  %205 = or i32 %203, %204
  %206 = getelementptr inbounds [80 x i32], [80 x i32]* %4, i64 0, i64 %193
  %207 = load i32, i32* %206, align 4
  %208 = add i32 %201, -1894007588
  %209 = add i32 %208, %197
  %210 = add i32 %209, %207
  %211 = add i32 %210, %205
  %212 = shl i32 %195, 30
  %213 = lshr i32 %195, 2
  %214 = or i32 %212, %213
  %215 = add nuw nsw i64 %193, 1
  %216 = icmp eq i64 %215, 60
  br i1 %216, label %217, label %192

217:                                              ; preds = %192, %217
  %218 = phi i64 [ %238, %217 ], [ 60, %192 ]
  %219 = phi i32 [ %234, %217 ], [ %211, %192 ]
  %220 = phi i32 [ %219, %217 ], [ %194, %192 ]
  %221 = phi i32 [ %237, %217 ], [ %214, %192 ]
  %222 = phi i32 [ %223, %217 ], [ %198, %192 ]
  %223 = phi i32 [ %221, %217 ], [ %196, %192 ]
  %224 = shl i32 %219, 5
  %225 = lshr i32 %219, 27
  %226 = or i32 %224, %225
  %227 = xor i32 %221, %220
  %228 = xor i32 %227, %223
  %229 = getelementptr inbounds [80 x i32], [80 x i32]* %4, i64 0, i64 %218
  %230 = load i32, i32* %229, align 4
  %231 = add i32 %226, -899497514
  %232 = add i32 %231, %222
  %233 = add i32 %232, %230
  %234 = add i32 %233, %228
  %235 = shl i32 %220, 30
  %236 = lshr i32 %220, 2
  %237 = or i32 %235, %236
  %238 = add nuw nsw i64 %218, 1
  %239 = icmp eq i64 %238, 80
  br i1 %239, label %240, label %217

240:                                              ; preds = %217
  %241 = sub i32 %49, %47
  %242 = insertelement <4 x i32> undef, i32 %234, i32 0
  %243 = insertelement <4 x i32> %242, i32 %219, i32 1
  %244 = insertelement <4 x i32> %243, i32 %237, i32 2
  %245 = insertelement <4 x i32> %244, i32 %221, i32 3
  %246 = add <4 x i32> %245, %138
  store <4 x i32> %246, <4 x i32>* %45, align 4
  %247 = add i32 %223, %139
  store i32 %247, i32* %27, align 4
  call void @llvm.lifetime.end.p0i8(i64 320, i8* nonnull %24) #4
  store i32 0, i32* %10, align 8
  %248 = icmp ult i32 %241, 64
  br i1 %248, label %249, label %46

249:                                              ; preds = %240
  %250 = zext i32 %241 to i64
  br label %251

251:                                              ; preds = %249, %20
  %252 = phi i64 [ %250, %249 ], [ %6, %20 ]
  %253 = phi i8* [ %53, %249 ], [ %1, %20 ]
  %254 = phi i32 [ %241, %249 ], [ %2, %20 ]
  %255 = getelementptr inbounds %struct.CRYPT_sha1_context, %struct.CRYPT_sha1_context* %0, i64 0, i32 3, i64 0
  tail call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 8 %255, i8* align 1 %253, i64 %252, i1 false)
  store i32 %254, i32* %10, align 8
  call void @llvm.lifetime.end.p0i8(i64 64, i8* nonnull %21) #4
  br label %256

256:                                              ; preds = %251, %16
  ret void
}

; Function Attrs: argmemonly nounwind
declare void @llvm.memcpy.p0i8.p0i8.i64(i8* nocapture writeonly, i8* nocapture readonly, i64, i1 immarg) #2

; Function Attrs: argmemonly nounwind
declare void @llvm.lifetime.start.p0i8(i64 immarg, i8* nocapture) #2

; Function Attrs: argmemonly nounwind
declare void @llvm.memset.p0i8.i64(i8* nocapture writeonly, i8, i64, i1 immarg) #2

; Function Attrs: argmemonly nounwind
declare void @llvm.lifetime.end.p0i8(i64 immarg, i8* nocapture) #2

; Function Attrs: nounwind ssp uwtable
define hidden void @_Z16CRYPT_SHA1FinishP18CRYPT_sha1_contextPh(%struct.CRYPT_sha1_context* nocapture, i8* nocapture) local_unnamed_addr #1 {
  %3 = alloca [64 x i8], align 16
  %4 = getelementptr inbounds %struct.CRYPT_sha1_context, %struct.CRYPT_sha1_context* %0, i64 0, i32 0
  %5 = load i64, i64* %4, align 8
  %6 = getelementptr inbounds [64 x i8], [64 x i8]* %3, i64 0, i64 0
  call void @llvm.lifetime.start.p0i8(i64 64, i8* nonnull %6) #4
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %6, i8 -86, i64 64, i1 false)
  %7 = getelementptr inbounds %struct.CRYPT_sha1_context, %struct.CRYPT_sha1_context* %0, i64 0, i32 1
  %8 = load i32, i32* %7, align 8
  %9 = icmp ugt i32 %8, 55
  %10 = trunc i32 %8 to i8
  %11 = select i1 %9, i8 120, i8 56
  %12 = sub i8 %11, %10
  %13 = zext i8 %12 to i64
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %6, i8 0, i64 %13, i1 false)
  store i8 -128, i8* %6, align 16
  %14 = zext i8 %12 to i32
  call void @_Z16CRYPT_SHA1UpdateP18CRYPT_sha1_contextPKhj(%struct.CRYPT_sha1_context* %0, i8* nonnull %6, i32 %14)
  %15 = lshr i64 %5, 53
  %16 = trunc i64 %15 to i8
  store i8 %16, i8* %6, align 16
  %17 = lshr i64 %5, 45
  %18 = trunc i64 %17 to i8
  %19 = getelementptr inbounds [64 x i8], [64 x i8]* %3, i64 0, i64 1
  store i8 %18, i8* %19, align 1
  %20 = lshr i64 %5, 37
  %21 = trunc i64 %20 to i8
  %22 = getelementptr inbounds [64 x i8], [64 x i8]* %3, i64 0, i64 2
  store i8 %21, i8* %22, align 2
  %23 = lshr i64 %5, 29
  %24 = trunc i64 %23 to i8
  %25 = getelementptr inbounds [64 x i8], [64 x i8]* %3, i64 0, i64 3
  store i8 %24, i8* %25, align 1
  %26 = lshr i64 %5, 21
  %27 = trunc i64 %26 to i8
  %28 = getelementptr inbounds [64 x i8], [64 x i8]* %3, i64 0, i64 4
  store i8 %27, i8* %28, align 4
  %29 = lshr i64 %5, 13
  %30 = trunc i64 %29 to i8
  %31 = getelementptr inbounds [64 x i8], [64 x i8]* %3, i64 0, i64 5
  store i8 %30, i8* %31, align 1
  %32 = lshr i64 %5, 5
  %33 = trunc i64 %32 to i8
  %34 = getelementptr inbounds [64 x i8], [64 x i8]* %3, i64 0, i64 6
  store i8 %33, i8* %34, align 2
  %35 = trunc i64 %5 to i8
  %36 = shl i8 %35, 3
  %37 = getelementptr inbounds [64 x i8], [64 x i8]* %3, i64 0, i64 7
  store i8 %36, i8* %37, align 1
  call void @_Z16CRYPT_SHA1UpdateP18CRYPT_sha1_contextPKhj(%struct.CRYPT_sha1_context* %0, i8* nonnull %6, i32 8)
  %38 = getelementptr inbounds %struct.CRYPT_sha1_context, %struct.CRYPT_sha1_context* %0, i64 0, i32 2, i64 0
  %39 = load i32, i32* %38, align 4
  %40 = lshr i32 %39, 24
  %41 = trunc i32 %40 to i8
  store i8 %41, i8* %1, align 1
  %42 = load i32, i32* %38, align 4
  %43 = lshr i32 %42, 16
  %44 = trunc i32 %43 to i8
  %45 = getelementptr inbounds i8, i8* %1, i64 1
  store i8 %44, i8* %45, align 1
  %46 = load i32, i32* %38, align 4
  %47 = lshr i32 %46, 8
  %48 = trunc i32 %47 to i8
  %49 = getelementptr inbounds i8, i8* %1, i64 2
  store i8 %48, i8* %49, align 1
  %50 = load i32, i32* %38, align 4
  %51 = trunc i32 %50 to i8
  %52 = getelementptr inbounds i8, i8* %1, i64 3
  store i8 %51, i8* %52, align 1
  %53 = getelementptr inbounds %struct.CRYPT_sha1_context, %struct.CRYPT_sha1_context* %0, i64 0, i32 2, i64 1
  %54 = load i32, i32* %53, align 4
  %55 = lshr i32 %54, 24
  %56 = trunc i32 %55 to i8
  %57 = getelementptr inbounds i8, i8* %1, i64 4
  store i8 %56, i8* %57, align 1
  %58 = load i32, i32* %53, align 4
  %59 = lshr i32 %58, 16
  %60 = trunc i32 %59 to i8
  %61 = getelementptr inbounds i8, i8* %1, i64 5
  store i8 %60, i8* %61, align 1
  %62 = load i32, i32* %53, align 4
  %63 = lshr i32 %62, 8
  %64 = trunc i32 %63 to i8
  %65 = getelementptr inbounds i8, i8* %1, i64 6
  store i8 %64, i8* %65, align 1
  %66 = load i32, i32* %53, align 4
  %67 = trunc i32 %66 to i8
  %68 = getelementptr inbounds i8, i8* %1, i64 7
  store i8 %67, i8* %68, align 1
  %69 = getelementptr inbounds %struct.CRYPT_sha1_context, %struct.CRYPT_sha1_context* %0, i64 0, i32 2, i64 2
  %70 = load i32, i32* %69, align 4
  %71 = lshr i32 %70, 24
  %72 = trunc i32 %71 to i8
  %73 = getelementptr inbounds i8, i8* %1, i64 8
  store i8 %72, i8* %73, align 1
  %74 = load i32, i32* %69, align 4
  %75 = lshr i32 %74, 16
  %76 = trunc i32 %75 to i8
  %77 = getelementptr inbounds i8, i8* %1, i64 9
  store i8 %76, i8* %77, align 1
  %78 = load i32, i32* %69, align 4
  %79 = lshr i32 %78, 8
  %80 = trunc i32 %79 to i8
  %81 = getelementptr inbounds i8, i8* %1, i64 10
  store i8 %80, i8* %81, align 1
  %82 = load i32, i32* %69, align 4
  %83 = trunc i32 %82 to i8
  %84 = getelementptr inbounds i8, i8* %1, i64 11
  store i8 %83, i8* %84, align 1
  %85 = getelementptr inbounds %struct.CRYPT_sha1_context, %struct.CRYPT_sha1_context* %0, i64 0, i32 2, i64 3
  %86 = load i32, i32* %85, align 4
  %87 = lshr i32 %86, 24
  %88 = trunc i32 %87 to i8
  %89 = getelementptr inbounds i8, i8* %1, i64 12
  store i8 %88, i8* %89, align 1
  %90 = load i32, i32* %85, align 4
  %91 = lshr i32 %90, 16
  %92 = trunc i32 %91 to i8
  %93 = getelementptr inbounds i8, i8* %1, i64 13
  store i8 %92, i8* %93, align 1
  %94 = load i32, i32* %85, align 4
  %95 = lshr i32 %94, 8
  %96 = trunc i32 %95 to i8
  %97 = getelementptr inbounds i8, i8* %1, i64 14
  store i8 %96, i8* %97, align 1
  %98 = load i32, i32* %85, align 4
  %99 = trunc i32 %98 to i8
  %100 = getelementptr inbounds i8, i8* %1, i64 15
  store i8 %99, i8* %100, align 1
  %101 = getelementptr inbounds %struct.CRYPT_sha1_context, %struct.CRYPT_sha1_context* %0, i64 0, i32 2, i64 4
  %102 = load i32, i32* %101, align 4
  %103 = lshr i32 %102, 24
  %104 = trunc i32 %103 to i8
  %105 = getelementptr inbounds i8, i8* %1, i64 16
  store i8 %104, i8* %105, align 1
  %106 = load i32, i32* %101, align 4
  %107 = lshr i32 %106, 16
  %108 = trunc i32 %107 to i8
  %109 = getelementptr inbounds i8, i8* %1, i64 17
  store i8 %108, i8* %109, align 1
  %110 = load i32, i32* %101, align 4
  %111 = lshr i32 %110, 8
  %112 = trunc i32 %111 to i8
  %113 = getelementptr inbounds i8, i8* %1, i64 18
  store i8 %112, i8* %113, align 1
  %114 = load i32, i32* %101, align 4
  %115 = trunc i32 %114 to i8
  %116 = getelementptr inbounds i8, i8* %1, i64 19
  store i8 %115, i8* %116, align 1
  call void @llvm.lifetime.end.p0i8(i64 64, i8* nonnull %6) #4
  ret void
}

; Function Attrs: nounwind ssp uwtable
define hidden void @_Z18CRYPT_SHA1GeneratePKhjPh(i8* nocapture readonly, i32, i8* nocapture) local_unnamed_addr #1 {
  %4 = alloca %struct.CRYPT_sha1_context, align 8
  %5 = bitcast %struct.CRYPT_sha1_context* %4 to i8*
  call void @llvm.lifetime.start.p0i8(i64 96, i8* nonnull %5) #4
  %6 = getelementptr inbounds %struct.CRYPT_sha1_context, %struct.CRYPT_sha1_context* %4, i64 0, i32 3, i64 0
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %6, i8 -86, i64 64, i1 false)
  %7 = getelementptr inbounds %struct.CRYPT_sha1_context, %struct.CRYPT_sha1_context* %4, i64 0, i32 2, i64 3
  store i32 271733878, i32* %7, align 4
  %8 = getelementptr inbounds %struct.CRYPT_sha1_context, %struct.CRYPT_sha1_context* %4, i64 0, i32 2, i64 4
  store i32 -1009589776, i32* %8, align 4
  %9 = getelementptr inbounds %struct.CRYPT_sha1_context, %struct.CRYPT_sha1_context* %4, i64 0, i32 0
  store i64 0, i64* %9, align 8
  %10 = getelementptr inbounds %struct.CRYPT_sha1_context, %struct.CRYPT_sha1_context* %4, i64 0, i32 1
  %11 = bitcast i32* %10 to <4 x i32>*
  store <4 x i32> <i32 0, i32 1732584193, i32 -271733879, i32 -1732584194>, <4 x i32>* %11, align 8
  call void @_Z16CRYPT_SHA1UpdateP18CRYPT_sha1_contextPKhj(%struct.CRYPT_sha1_context* nonnull %4, i8* %0, i32 %1)
  call void @_Z16CRYPT_SHA1FinishP18CRYPT_sha1_contextPh(%struct.CRYPT_sha1_context* nonnull %4, i8* %2)
  call void @llvm.lifetime.end.p0i8(i64 96, i8* nonnull %5) #4
  ret void
}

; Function Attrs: nounwind ssp uwtable
define hidden void @_Z17CRYPT_SHA256StartP18CRYPT_sha2_context(%struct.CRYPT_sha2_context* nocapture) local_unnamed_addr #1 {
  %2 = bitcast %struct.CRYPT_sha2_context* %0 to <2 x i64>*
  store <2 x i64> <i64 0, i64 1779033703>, <2 x i64>* %2, align 8
  %3 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %0, i64 0, i32 1, i64 1
  %4 = bitcast i64* %3 to <2 x i64>*
  store <2 x i64> <i64 3144134277, i64 1013904242>, <2 x i64>* %4, align 8
  %5 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %0, i64 0, i32 1, i64 3
  %6 = bitcast i64* %5 to <2 x i64>*
  store <2 x i64> <i64 2773480762, i64 1359893119>, <2 x i64>* %6, align 8
  %7 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %0, i64 0, i32 1, i64 5
  %8 = bitcast i64* %7 to <2 x i64>*
  store <2 x i64> <i64 2600822924, i64 528734635>, <2 x i64>* %8, align 8
  %9 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %0, i64 0, i32 1, i64 7
  store i64 1541459225, i64* %9, align 8
  %10 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %0, i64 0, i32 2, i64 0
  tail call void @llvm.memset.p0i8.i64(i8* align 8 %10, i8 0, i64 128, i1 false)
  ret void
}

; Function Attrs: nounwind ssp uwtable
define hidden void @_Z18CRYPT_SHA256UpdateP18CRYPT_sha2_contextPKhj(%struct.CRYPT_sha2_context*, i8* readonly, i32) local_unnamed_addr #1 {
  %4 = icmp eq i32 %2, 0
  br i1 %4, label %44, label %5

5:                                                ; preds = %3
  %6 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %0, i64 0, i32 0
  %7 = load i64, i64* %6, align 8
  %8 = trunc i64 %7 to i32
  %9 = and i32 %8, 63
  %10 = sub nuw nsw i32 64, %9
  %11 = zext i32 %2 to i64
  %12 = add i64 %7, %11
  store i64 %12, i64* %6, align 8
  %13 = icmp eq i32 %9, 0
  br i1 %13, label %23, label %14

14:                                               ; preds = %5
  %15 = icmp ugt i32 %10, %2
  br i1 %15, label %23, label %16

16:                                               ; preds = %14
  %17 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %0, i64 0, i32 2, i64 0
  %18 = zext i32 %9 to i64
  %19 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %0, i64 0, i32 2, i64 %18
  %20 = zext i32 %10 to i64
  tail call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 1 %19, i8* align 1 %1, i64 %20, i1 false)
  tail call fastcc void @_ZN12_GLOBAL__N_114sha256_processEP18CRYPT_sha2_contextPKh(%struct.CRYPT_sha2_context* %0, i8* %17)
  %21 = sub i32 %2, %10
  %22 = getelementptr inbounds i8, i8* %1, i64 %20
  br label %23

23:                                               ; preds = %14, %5, %16
  %24 = phi i8* [ %22, %16 ], [ %1, %14 ], [ %1, %5 ]
  %25 = phi i32 [ %21, %16 ], [ %2, %14 ], [ %2, %5 ]
  %26 = phi i32 [ 0, %16 ], [ %9, %14 ], [ 0, %5 ]
  %27 = icmp ugt i32 %25, 63
  br i1 %27, label %28, label %36

28:                                               ; preds = %23, %28
  %29 = phi i32 [ %31, %28 ], [ %25, %23 ]
  %30 = phi i8* [ %32, %28 ], [ %24, %23 ]
  tail call fastcc void @_ZN12_GLOBAL__N_114sha256_processEP18CRYPT_sha2_contextPKh(%struct.CRYPT_sha2_context* %0, i8* %30)
  %31 = add i32 %29, -64
  %32 = getelementptr inbounds i8, i8* %30, i64 64
  %33 = icmp ugt i32 %31, 63
  br i1 %33, label %28, label %34

34:                                               ; preds = %28
  %35 = and i32 %25, 63
  br label %36

36:                                               ; preds = %34, %23
  %37 = phi i8* [ %24, %23 ], [ %32, %34 ]
  %38 = phi i32 [ %25, %23 ], [ %35, %34 ]
  %39 = icmp eq i32 %38, 0
  br i1 %39, label %44, label %40

40:                                               ; preds = %36
  %41 = zext i32 %26 to i64
  %42 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %0, i64 0, i32 2, i64 %41
  %43 = zext i32 %38 to i64
  tail call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 1 %42, i8* align 1 %37, i64 %43, i1 false)
  br label %44

44:                                               ; preds = %40, %36, %3
  ret void
}

; Function Attrs: nofree norecurse nounwind ssp uwtable
define internal fastcc void @_ZN12_GLOBAL__N_114sha256_processEP18CRYPT_sha2_contextPKh(%struct.CRYPT_sha2_context* nocapture, i8* readonly) unnamed_addr #3 {
  %3 = load i8, i8* %1, align 1
  %4 = zext i8 %3 to i32
  %5 = shl nuw i32 %4, 24
  %6 = getelementptr inbounds i8, i8* %1, i64 1
  %7 = load i8, i8* %6, align 1
  %8 = zext i8 %7 to i32
  %9 = shl nuw nsw i32 %8, 16
  %10 = or i32 %9, %5
  %11 = getelementptr inbounds i8, i8* %1, i64 2
  %12 = load i8, i8* %11, align 1
  %13 = zext i8 %12 to i32
  %14 = shl nuw nsw i32 %13, 8
  %15 = or i32 %10, %14
  %16 = getelementptr inbounds i8, i8* %1, i64 3
  %17 = load i8, i8* %16, align 1
  %18 = zext i8 %17 to i32
  %19 = or i32 %15, %18
  %20 = getelementptr inbounds i8, i8* %1, i64 4
  %21 = load i8, i8* %20, align 1
  %22 = zext i8 %21 to i32
  %23 = shl nuw i32 %22, 24
  %24 = getelementptr inbounds i8, i8* %1, i64 5
  %25 = load i8, i8* %24, align 1
  %26 = zext i8 %25 to i32
  %27 = shl nuw nsw i32 %26, 16
  %28 = or i32 %27, %23
  %29 = getelementptr inbounds i8, i8* %1, i64 6
  %30 = load i8, i8* %29, align 1
  %31 = zext i8 %30 to i32
  %32 = shl nuw nsw i32 %31, 8
  %33 = or i32 %28, %32
  %34 = getelementptr inbounds i8, i8* %1, i64 7
  %35 = load i8, i8* %34, align 1
  %36 = zext i8 %35 to i32
  %37 = or i32 %33, %36
  %38 = getelementptr inbounds i8, i8* %1, i64 8
  %39 = load i8, i8* %38, align 1
  %40 = zext i8 %39 to i32
  %41 = shl nuw i32 %40, 24
  %42 = getelementptr inbounds i8, i8* %1, i64 9
  %43 = load i8, i8* %42, align 1
  %44 = zext i8 %43 to i32
  %45 = shl nuw nsw i32 %44, 16
  %46 = or i32 %45, %41
  %47 = getelementptr inbounds i8, i8* %1, i64 10
  %48 = load i8, i8* %47, align 1
  %49 = zext i8 %48 to i32
  %50 = shl nuw nsw i32 %49, 8
  %51 = or i32 %46, %50
  %52 = getelementptr inbounds i8, i8* %1, i64 11
  %53 = load i8, i8* %52, align 1
  %54 = zext i8 %53 to i32
  %55 = or i32 %51, %54
  %56 = getelementptr inbounds i8, i8* %1, i64 12
  %57 = load i8, i8* %56, align 1
  %58 = zext i8 %57 to i32
  %59 = shl nuw i32 %58, 24
  %60 = getelementptr inbounds i8, i8* %1, i64 13
  %61 = load i8, i8* %60, align 1
  %62 = zext i8 %61 to i32
  %63 = shl nuw nsw i32 %62, 16
  %64 = or i32 %63, %59
  %65 = getelementptr inbounds i8, i8* %1, i64 14
  %66 = load i8, i8* %65, align 1
  %67 = zext i8 %66 to i32
  %68 = shl nuw nsw i32 %67, 8
  %69 = or i32 %64, %68
  %70 = getelementptr inbounds i8, i8* %1, i64 15
  %71 = load i8, i8* %70, align 1
  %72 = zext i8 %71 to i32
  %73 = or i32 %69, %72
  %74 = getelementptr inbounds i8, i8* %1, i64 16
  %75 = load i8, i8* %74, align 1
  %76 = zext i8 %75 to i32
  %77 = shl nuw i32 %76, 24
  %78 = getelementptr inbounds i8, i8* %1, i64 17
  %79 = load i8, i8* %78, align 1
  %80 = zext i8 %79 to i32
  %81 = shl nuw nsw i32 %80, 16
  %82 = or i32 %81, %77
  %83 = getelementptr inbounds i8, i8* %1, i64 18
  %84 = load i8, i8* %83, align 1
  %85 = zext i8 %84 to i32
  %86 = shl nuw nsw i32 %85, 8
  %87 = or i32 %82, %86
  %88 = getelementptr inbounds i8, i8* %1, i64 19
  %89 = load i8, i8* %88, align 1
  %90 = zext i8 %89 to i32
  %91 = or i32 %87, %90
  %92 = getelementptr inbounds i8, i8* %1, i64 20
  %93 = load i8, i8* %92, align 1
  %94 = zext i8 %93 to i32
  %95 = shl nuw i32 %94, 24
  %96 = getelementptr inbounds i8, i8* %1, i64 21
  %97 = load i8, i8* %96, align 1
  %98 = zext i8 %97 to i32
  %99 = shl nuw nsw i32 %98, 16
  %100 = or i32 %99, %95
  %101 = getelementptr inbounds i8, i8* %1, i64 22
  %102 = load i8, i8* %101, align 1
  %103 = zext i8 %102 to i32
  %104 = shl nuw nsw i32 %103, 8
  %105 = or i32 %100, %104
  %106 = getelementptr inbounds i8, i8* %1, i64 23
  %107 = load i8, i8* %106, align 1
  %108 = zext i8 %107 to i32
  %109 = or i32 %105, %108
  %110 = getelementptr inbounds i8, i8* %1, i64 24
  %111 = load i8, i8* %110, align 1
  %112 = zext i8 %111 to i32
  %113 = shl nuw i32 %112, 24
  %114 = getelementptr inbounds i8, i8* %1, i64 25
  %115 = load i8, i8* %114, align 1
  %116 = zext i8 %115 to i32
  %117 = shl nuw nsw i32 %116, 16
  %118 = or i32 %117, %113
  %119 = getelementptr inbounds i8, i8* %1, i64 26
  %120 = load i8, i8* %119, align 1
  %121 = zext i8 %120 to i32
  %122 = shl nuw nsw i32 %121, 8
  %123 = or i32 %118, %122
  %124 = getelementptr inbounds i8, i8* %1, i64 27
  %125 = load i8, i8* %124, align 1
  %126 = zext i8 %125 to i32
  %127 = or i32 %123, %126
  %128 = getelementptr inbounds i8, i8* %1, i64 28
  %129 = load i8, i8* %128, align 1
  %130 = zext i8 %129 to i32
  %131 = shl nuw i32 %130, 24
  %132 = getelementptr inbounds i8, i8* %1, i64 29
  %133 = load i8, i8* %132, align 1
  %134 = zext i8 %133 to i32
  %135 = shl nuw nsw i32 %134, 16
  %136 = or i32 %135, %131
  %137 = getelementptr inbounds i8, i8* %1, i64 30
  %138 = load i8, i8* %137, align 1
  %139 = zext i8 %138 to i32
  %140 = shl nuw nsw i32 %139, 8
  %141 = or i32 %136, %140
  %142 = getelementptr inbounds i8, i8* %1, i64 31
  %143 = load i8, i8* %142, align 1
  %144 = zext i8 %143 to i32
  %145 = or i32 %141, %144
  %146 = getelementptr inbounds i8, i8* %1, i64 32
  %147 = load i8, i8* %146, align 1
  %148 = zext i8 %147 to i32
  %149 = shl nuw i32 %148, 24
  %150 = getelementptr inbounds i8, i8* %1, i64 33
  %151 = load i8, i8* %150, align 1
  %152 = zext i8 %151 to i32
  %153 = shl nuw nsw i32 %152, 16
  %154 = or i32 %153, %149
  %155 = getelementptr inbounds i8, i8* %1, i64 34
  %156 = load i8, i8* %155, align 1
  %157 = zext i8 %156 to i32
  %158 = shl nuw nsw i32 %157, 8
  %159 = or i32 %154, %158
  %160 = getelementptr inbounds i8, i8* %1, i64 35
  %161 = load i8, i8* %160, align 1
  %162 = zext i8 %161 to i32
  %163 = or i32 %159, %162
  %164 = getelementptr inbounds i8, i8* %1, i64 36
  %165 = load i8, i8* %164, align 1
  %166 = zext i8 %165 to i32
  %167 = shl nuw i32 %166, 24
  %168 = getelementptr inbounds i8, i8* %1, i64 37
  %169 = load i8, i8* %168, align 1
  %170 = zext i8 %169 to i32
  %171 = shl nuw nsw i32 %170, 16
  %172 = or i32 %171, %167
  %173 = getelementptr inbounds i8, i8* %1, i64 38
  %174 = load i8, i8* %173, align 1
  %175 = zext i8 %174 to i32
  %176 = shl nuw nsw i32 %175, 8
  %177 = or i32 %172, %176
  %178 = getelementptr inbounds i8, i8* %1, i64 39
  %179 = load i8, i8* %178, align 1
  %180 = zext i8 %179 to i32
  %181 = or i32 %177, %180
  %182 = getelementptr inbounds i8, i8* %1, i64 40
  %183 = load i8, i8* %182, align 1
  %184 = zext i8 %183 to i32
  %185 = shl nuw i32 %184, 24
  %186 = getelementptr inbounds i8, i8* %1, i64 41
  %187 = load i8, i8* %186, align 1
  %188 = zext i8 %187 to i32
  %189 = shl nuw nsw i32 %188, 16
  %190 = or i32 %189, %185
  %191 = getelementptr inbounds i8, i8* %1, i64 42
  %192 = load i8, i8* %191, align 1
  %193 = zext i8 %192 to i32
  %194 = shl nuw nsw i32 %193, 8
  %195 = or i32 %190, %194
  %196 = getelementptr inbounds i8, i8* %1, i64 43
  %197 = load i8, i8* %196, align 1
  %198 = zext i8 %197 to i32
  %199 = or i32 %195, %198
  %200 = getelementptr inbounds i8, i8* %1, i64 44
  %201 = load i8, i8* %200, align 1
  %202 = zext i8 %201 to i32
  %203 = shl nuw i32 %202, 24
  %204 = getelementptr inbounds i8, i8* %1, i64 45
  %205 = load i8, i8* %204, align 1
  %206 = zext i8 %205 to i32
  %207 = shl nuw nsw i32 %206, 16
  %208 = or i32 %207, %203
  %209 = getelementptr inbounds i8, i8* %1, i64 46
  %210 = load i8, i8* %209, align 1
  %211 = zext i8 %210 to i32
  %212 = shl nuw nsw i32 %211, 8
  %213 = or i32 %208, %212
  %214 = getelementptr inbounds i8, i8* %1, i64 47
  %215 = load i8, i8* %214, align 1
  %216 = zext i8 %215 to i32
  %217 = or i32 %213, %216
  %218 = getelementptr inbounds i8, i8* %1, i64 48
  %219 = load i8, i8* %218, align 1
  %220 = zext i8 %219 to i32
  %221 = shl nuw i32 %220, 24
  %222 = getelementptr inbounds i8, i8* %1, i64 49
  %223 = load i8, i8* %222, align 1
  %224 = zext i8 %223 to i32
  %225 = shl nuw nsw i32 %224, 16
  %226 = or i32 %225, %221
  %227 = getelementptr inbounds i8, i8* %1, i64 50
  %228 = load i8, i8* %227, align 1
  %229 = zext i8 %228 to i32
  %230 = shl nuw nsw i32 %229, 8
  %231 = or i32 %226, %230
  %232 = getelementptr inbounds i8, i8* %1, i64 51
  %233 = load i8, i8* %232, align 1
  %234 = zext i8 %233 to i32
  %235 = or i32 %231, %234
  %236 = getelementptr inbounds i8, i8* %1, i64 52
  %237 = load i8, i8* %236, align 1
  %238 = zext i8 %237 to i32
  %239 = shl nuw i32 %238, 24
  %240 = getelementptr inbounds i8, i8* %1, i64 53
  %241 = load i8, i8* %240, align 1
  %242 = zext i8 %241 to i32
  %243 = shl nuw nsw i32 %242, 16
  %244 = or i32 %243, %239
  %245 = getelementptr inbounds i8, i8* %1, i64 54
  %246 = load i8, i8* %245, align 1
  %247 = zext i8 %246 to i32
  %248 = shl nuw nsw i32 %247, 8
  %249 = or i32 %244, %248
  %250 = getelementptr inbounds i8, i8* %1, i64 55
  %251 = load i8, i8* %250, align 1
  %252 = zext i8 %251 to i32
  %253 = or i32 %249, %252
  %254 = getelementptr inbounds i8, i8* %1, i64 56
  %255 = load i8, i8* %254, align 1
  %256 = zext i8 %255 to i32
  %257 = shl nuw i32 %256, 24
  %258 = getelementptr inbounds i8, i8* %1, i64 57
  %259 = load i8, i8* %258, align 1
  %260 = zext i8 %259 to i32
  %261 = shl nuw nsw i32 %260, 16
  %262 = or i32 %261, %257
  %263 = getelementptr inbounds i8, i8* %1, i64 58
  %264 = load i8, i8* %263, align 1
  %265 = zext i8 %264 to i32
  %266 = shl nuw nsw i32 %265, 8
  %267 = or i32 %262, %266
  %268 = getelementptr inbounds i8, i8* %1, i64 59
  %269 = load i8, i8* %268, align 1
  %270 = zext i8 %269 to i32
  %271 = or i32 %267, %270
  %272 = getelementptr inbounds i8, i8* %1, i64 60
  %273 = load i8, i8* %272, align 1
  %274 = zext i8 %273 to i32
  %275 = shl nuw i32 %274, 24
  %276 = getelementptr inbounds i8, i8* %1, i64 61
  %277 = load i8, i8* %276, align 1
  %278 = zext i8 %277 to i32
  %279 = shl nuw nsw i32 %278, 16
  %280 = or i32 %279, %275
  %281 = getelementptr inbounds i8, i8* %1, i64 62
  %282 = load i8, i8* %281, align 1
  %283 = zext i8 %282 to i32
  %284 = shl nuw nsw i32 %283, 8
  %285 = or i32 %280, %284
  %286 = getelementptr inbounds i8, i8* %1, i64 63
  %287 = load i8, i8* %286, align 1
  %288 = zext i8 %287 to i32
  %289 = or i32 %285, %288
  %290 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %0, i64 0, i32 1, i64 0
  %291 = load i64, i64* %290, align 8
  %292 = trunc i64 %291 to i32
  %293 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %0, i64 0, i32 1, i64 1
  %294 = load i64, i64* %293, align 8
  %295 = trunc i64 %294 to i32
  %296 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %0, i64 0, i32 1, i64 2
  %297 = load i64, i64* %296, align 8
  %298 = trunc i64 %297 to i32
  %299 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %0, i64 0, i32 1, i64 3
  %300 = load i64, i64* %299, align 8
  %301 = trunc i64 %300 to i32
  %302 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %0, i64 0, i32 1, i64 4
  %303 = load i64, i64* %302, align 8
  %304 = trunc i64 %303 to i32
  %305 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %0, i64 0, i32 1, i64 5
  %306 = load i64, i64* %305, align 8
  %307 = trunc i64 %306 to i32
  %308 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %0, i64 0, i32 1, i64 6
  %309 = load i64, i64* %308, align 8
  %310 = trunc i64 %309 to i32
  %311 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %0, i64 0, i32 1, i64 7
  %312 = load i64, i64* %311, align 8
  %313 = trunc i64 %312 to i32
  %314 = lshr i32 %304, 6
  %315 = shl i32 %304, 26
  %316 = or i32 %314, %315
  %317 = lshr i32 %304, 11
  %318 = shl i32 %304, 21
  %319 = or i32 %317, %318
  %320 = xor i32 %316, %319
  %321 = lshr i32 %304, 25
  %322 = shl i32 %304, 7
  %323 = or i32 %321, %322
  %324 = xor i32 %320, %323
  %325 = xor i64 %309, %306
  %326 = and i64 %325, %303
  %327 = xor i64 %326, %309
  %328 = trunc i64 %327 to i32
  %329 = add i32 %19, 1116352408
  %330 = add i32 %329, %313
  %331 = add i32 %330, %324
  %332 = add i32 %331, %328
  %333 = lshr i32 %292, 2
  %334 = shl i32 %292, 30
  %335 = or i32 %333, %334
  %336 = lshr i32 %292, 13
  %337 = shl i32 %292, 19
  %338 = or i32 %336, %337
  %339 = xor i32 %335, %338
  %340 = lshr i32 %292, 22
  %341 = shl i32 %292, 10
  %342 = or i32 %340, %341
  %343 = xor i32 %339, %342
  %344 = and i64 %294, %291
  %345 = or i64 %294, %291
  %346 = and i64 %297, %345
  %347 = or i64 %346, %344
  %348 = trunc i64 %347 to i32
  %349 = add i32 %343, %348
  %350 = add i32 %332, %301
  %351 = add i32 %349, %332
  %352 = lshr i32 %350, 6
  %353 = shl i32 %350, 26
  %354 = or i32 %352, %353
  %355 = lshr i32 %350, 11
  %356 = shl i32 %350, 21
  %357 = or i32 %355, %356
  %358 = xor i32 %354, %357
  %359 = lshr i32 %350, 25
  %360 = shl i32 %350, 7
  %361 = or i32 %359, %360
  %362 = xor i32 %358, %361
  %363 = xor i64 %306, %303
  %364 = trunc i64 %363 to i32
  %365 = and i32 %350, %364
  %366 = xor i32 %365, %307
  %367 = add i32 %37, 1899447441
  %368 = add i32 %367, %310
  %369 = add i32 %368, %366
  %370 = add i32 %369, %362
  %371 = lshr i32 %351, 2
  %372 = shl i32 %351, 30
  %373 = or i32 %371, %372
  %374 = lshr i32 %351, 13
  %375 = shl i32 %351, 19
  %376 = or i32 %374, %375
  %377 = xor i32 %373, %376
  %378 = lshr i32 %351, 22
  %379 = shl i32 %351, 10
  %380 = or i32 %378, %379
  %381 = xor i32 %377, %380
  %382 = and i32 %351, %292
  %383 = or i32 %351, %292
  %384 = and i32 %383, %295
  %385 = or i32 %384, %382
  %386 = add i32 %381, %385
  %387 = add i32 %370, %298
  %388 = add i32 %386, %370
  %389 = lshr i32 %387, 6
  %390 = shl i32 %387, 26
  %391 = or i32 %389, %390
  %392 = lshr i32 %387, 11
  %393 = shl i32 %387, 21
  %394 = or i32 %392, %393
  %395 = xor i32 %391, %394
  %396 = lshr i32 %387, 25
  %397 = shl i32 %387, 7
  %398 = or i32 %396, %397
  %399 = xor i32 %395, %398
  %400 = xor i32 %350, %304
  %401 = and i32 %387, %400
  %402 = xor i32 %401, %304
  %403 = add i32 %55, -1245643825
  %404 = add i32 %403, %307
  %405 = add i32 %404, %402
  %406 = add i32 %405, %399
  %407 = lshr i32 %388, 2
  %408 = shl i32 %388, 30
  %409 = or i32 %407, %408
  %410 = lshr i32 %388, 13
  %411 = shl i32 %388, 19
  %412 = or i32 %410, %411
  %413 = xor i32 %409, %412
  %414 = lshr i32 %388, 22
  %415 = shl i32 %388, 10
  %416 = or i32 %414, %415
  %417 = xor i32 %413, %416
  %418 = and i32 %388, %351
  %419 = or i32 %388, %351
  %420 = and i32 %419, %292
  %421 = or i32 %420, %418
  %422 = add i32 %417, %421
  %423 = add i32 %406, %295
  %424 = add i32 %422, %406
  %425 = lshr i32 %423, 6
  %426 = shl i32 %423, 26
  %427 = or i32 %425, %426
  %428 = lshr i32 %423, 11
  %429 = shl i32 %423, 21
  %430 = or i32 %428, %429
  %431 = xor i32 %427, %430
  %432 = lshr i32 %423, 25
  %433 = shl i32 %423, 7
  %434 = or i32 %432, %433
  %435 = xor i32 %431, %434
  %436 = xor i32 %387, %350
  %437 = and i32 %423, %436
  %438 = xor i32 %437, %350
  %439 = add i32 %73, -373957723
  %440 = add i32 %439, %304
  %441 = add i32 %440, %438
  %442 = add i32 %441, %435
  %443 = lshr i32 %424, 2
  %444 = shl i32 %424, 30
  %445 = or i32 %443, %444
  %446 = lshr i32 %424, 13
  %447 = shl i32 %424, 19
  %448 = or i32 %446, %447
  %449 = xor i32 %445, %448
  %450 = lshr i32 %424, 22
  %451 = shl i32 %424, 10
  %452 = or i32 %450, %451
  %453 = xor i32 %449, %452
  %454 = and i32 %424, %388
  %455 = or i32 %424, %388
  %456 = and i32 %455, %351
  %457 = or i32 %456, %454
  %458 = add i32 %453, %457
  %459 = add i32 %442, %292
  %460 = add i32 %458, %442
  %461 = lshr i32 %459, 6
  %462 = shl i32 %459, 26
  %463 = or i32 %461, %462
  %464 = lshr i32 %459, 11
  %465 = shl i32 %459, 21
  %466 = or i32 %464, %465
  %467 = xor i32 %463, %466
  %468 = lshr i32 %459, 25
  %469 = shl i32 %459, 7
  %470 = or i32 %468, %469
  %471 = xor i32 %467, %470
  %472 = xor i32 %423, %387
  %473 = and i32 %459, %472
  %474 = xor i32 %473, %387
  %475 = add i32 %91, 961987163
  %476 = add i32 %475, %350
  %477 = add i32 %476, %474
  %478 = add i32 %477, %471
  %479 = lshr i32 %460, 2
  %480 = shl i32 %460, 30
  %481 = or i32 %479, %480
  %482 = lshr i32 %460, 13
  %483 = shl i32 %460, 19
  %484 = or i32 %482, %483
  %485 = xor i32 %481, %484
  %486 = lshr i32 %460, 22
  %487 = shl i32 %460, 10
  %488 = or i32 %486, %487
  %489 = xor i32 %485, %488
  %490 = and i32 %460, %424
  %491 = or i32 %460, %424
  %492 = and i32 %491, %388
  %493 = or i32 %492, %490
  %494 = add i32 %489, %493
  %495 = add i32 %478, %351
  %496 = add i32 %494, %478
  %497 = lshr i32 %495, 6
  %498 = shl i32 %495, 26
  %499 = or i32 %497, %498
  %500 = lshr i32 %495, 11
  %501 = shl i32 %495, 21
  %502 = or i32 %500, %501
  %503 = xor i32 %499, %502
  %504 = lshr i32 %495, 25
  %505 = shl i32 %495, 7
  %506 = or i32 %504, %505
  %507 = xor i32 %503, %506
  %508 = xor i32 %459, %423
  %509 = and i32 %495, %508
  %510 = xor i32 %509, %423
  %511 = add i32 %109, 1508970993
  %512 = add i32 %511, %387
  %513 = add i32 %512, %510
  %514 = add i32 %513, %507
  %515 = lshr i32 %496, 2
  %516 = shl i32 %496, 30
  %517 = or i32 %515, %516
  %518 = lshr i32 %496, 13
  %519 = shl i32 %496, 19
  %520 = or i32 %518, %519
  %521 = xor i32 %517, %520
  %522 = lshr i32 %496, 22
  %523 = shl i32 %496, 10
  %524 = or i32 %522, %523
  %525 = xor i32 %521, %524
  %526 = and i32 %496, %460
  %527 = or i32 %496, %460
  %528 = and i32 %527, %424
  %529 = or i32 %528, %526
  %530 = add i32 %525, %529
  %531 = add i32 %514, %388
  %532 = add i32 %530, %514
  %533 = lshr i32 %531, 6
  %534 = shl i32 %531, 26
  %535 = or i32 %533, %534
  %536 = lshr i32 %531, 11
  %537 = shl i32 %531, 21
  %538 = or i32 %536, %537
  %539 = xor i32 %535, %538
  %540 = lshr i32 %531, 25
  %541 = shl i32 %531, 7
  %542 = or i32 %540, %541
  %543 = xor i32 %539, %542
  %544 = xor i32 %495, %459
  %545 = and i32 %531, %544
  %546 = xor i32 %545, %459
  %547 = add i32 %127, -1841331548
  %548 = add i32 %547, %423
  %549 = add i32 %548, %546
  %550 = add i32 %549, %543
  %551 = lshr i32 %532, 2
  %552 = shl i32 %532, 30
  %553 = or i32 %551, %552
  %554 = lshr i32 %532, 13
  %555 = shl i32 %532, 19
  %556 = or i32 %554, %555
  %557 = xor i32 %553, %556
  %558 = lshr i32 %532, 22
  %559 = shl i32 %532, 10
  %560 = or i32 %558, %559
  %561 = xor i32 %557, %560
  %562 = and i32 %532, %496
  %563 = or i32 %532, %496
  %564 = and i32 %563, %460
  %565 = or i32 %564, %562
  %566 = add i32 %561, %565
  %567 = add i32 %550, %424
  %568 = add i32 %566, %550
  %569 = lshr i32 %567, 6
  %570 = shl i32 %567, 26
  %571 = or i32 %569, %570
  %572 = lshr i32 %567, 11
  %573 = shl i32 %567, 21
  %574 = or i32 %572, %573
  %575 = xor i32 %571, %574
  %576 = lshr i32 %567, 25
  %577 = shl i32 %567, 7
  %578 = or i32 %576, %577
  %579 = xor i32 %575, %578
  %580 = xor i32 %531, %495
  %581 = and i32 %567, %580
  %582 = xor i32 %581, %495
  %583 = add i32 %145, -1424204075
  %584 = add i32 %583, %459
  %585 = add i32 %584, %582
  %586 = add i32 %585, %579
  %587 = lshr i32 %568, 2
  %588 = shl i32 %568, 30
  %589 = or i32 %587, %588
  %590 = lshr i32 %568, 13
  %591 = shl i32 %568, 19
  %592 = or i32 %590, %591
  %593 = xor i32 %589, %592
  %594 = lshr i32 %568, 22
  %595 = shl i32 %568, 10
  %596 = or i32 %594, %595
  %597 = xor i32 %593, %596
  %598 = and i32 %568, %532
  %599 = or i32 %568, %532
  %600 = and i32 %599, %496
  %601 = or i32 %600, %598
  %602 = add i32 %597, %601
  %603 = add i32 %586, %460
  %604 = add i32 %602, %586
  %605 = lshr i32 %603, 6
  %606 = shl i32 %603, 26
  %607 = or i32 %605, %606
  %608 = lshr i32 %603, 11
  %609 = shl i32 %603, 21
  %610 = or i32 %608, %609
  %611 = xor i32 %607, %610
  %612 = lshr i32 %603, 25
  %613 = shl i32 %603, 7
  %614 = or i32 %612, %613
  %615 = xor i32 %611, %614
  %616 = xor i32 %567, %531
  %617 = and i32 %603, %616
  %618 = xor i32 %617, %531
  %619 = add i32 %163, -670586216
  %620 = add i32 %619, %495
  %621 = add i32 %620, %618
  %622 = add i32 %621, %615
  %623 = lshr i32 %604, 2
  %624 = shl i32 %604, 30
  %625 = or i32 %623, %624
  %626 = lshr i32 %604, 13
  %627 = shl i32 %604, 19
  %628 = or i32 %626, %627
  %629 = xor i32 %625, %628
  %630 = lshr i32 %604, 22
  %631 = shl i32 %604, 10
  %632 = or i32 %630, %631
  %633 = xor i32 %629, %632
  %634 = and i32 %604, %568
  %635 = or i32 %604, %568
  %636 = and i32 %635, %532
  %637 = or i32 %636, %634
  %638 = add i32 %633, %637
  %639 = add i32 %622, %496
  %640 = add i32 %638, %622
  %641 = lshr i32 %639, 6
  %642 = shl i32 %639, 26
  %643 = or i32 %641, %642
  %644 = lshr i32 %639, 11
  %645 = shl i32 %639, 21
  %646 = or i32 %644, %645
  %647 = xor i32 %643, %646
  %648 = lshr i32 %639, 25
  %649 = shl i32 %639, 7
  %650 = or i32 %648, %649
  %651 = xor i32 %647, %650
  %652 = xor i32 %603, %567
  %653 = and i32 %639, %652
  %654 = xor i32 %653, %567
  %655 = add i32 %181, 310598401
  %656 = add i32 %655, %531
  %657 = add i32 %656, %654
  %658 = add i32 %657, %651
  %659 = lshr i32 %640, 2
  %660 = shl i32 %640, 30
  %661 = or i32 %659, %660
  %662 = lshr i32 %640, 13
  %663 = shl i32 %640, 19
  %664 = or i32 %662, %663
  %665 = xor i32 %661, %664
  %666 = lshr i32 %640, 22
  %667 = shl i32 %640, 10
  %668 = or i32 %666, %667
  %669 = xor i32 %665, %668
  %670 = and i32 %640, %604
  %671 = or i32 %640, %604
  %672 = and i32 %671, %568
  %673 = or i32 %672, %670
  %674 = add i32 %669, %673
  %675 = add i32 %658, %532
  %676 = add i32 %674, %658
  %677 = lshr i32 %675, 6
  %678 = shl i32 %675, 26
  %679 = or i32 %677, %678
  %680 = lshr i32 %675, 11
  %681 = shl i32 %675, 21
  %682 = or i32 %680, %681
  %683 = xor i32 %679, %682
  %684 = lshr i32 %675, 25
  %685 = shl i32 %675, 7
  %686 = or i32 %684, %685
  %687 = xor i32 %683, %686
  %688 = xor i32 %639, %603
  %689 = and i32 %675, %688
  %690 = xor i32 %689, %603
  %691 = add i32 %199, 607225278
  %692 = add i32 %691, %567
  %693 = add i32 %692, %690
  %694 = add i32 %693, %687
  %695 = lshr i32 %676, 2
  %696 = shl i32 %676, 30
  %697 = or i32 %695, %696
  %698 = lshr i32 %676, 13
  %699 = shl i32 %676, 19
  %700 = or i32 %698, %699
  %701 = xor i32 %697, %700
  %702 = lshr i32 %676, 22
  %703 = shl i32 %676, 10
  %704 = or i32 %702, %703
  %705 = xor i32 %701, %704
  %706 = and i32 %676, %640
  %707 = or i32 %676, %640
  %708 = and i32 %707, %604
  %709 = or i32 %708, %706
  %710 = add i32 %705, %709
  %711 = add i32 %694, %568
  %712 = add i32 %710, %694
  %713 = lshr i32 %711, 6
  %714 = shl i32 %711, 26
  %715 = or i32 %713, %714
  %716 = lshr i32 %711, 11
  %717 = shl i32 %711, 21
  %718 = or i32 %716, %717
  %719 = xor i32 %715, %718
  %720 = lshr i32 %711, 25
  %721 = shl i32 %711, 7
  %722 = or i32 %720, %721
  %723 = xor i32 %719, %722
  %724 = xor i32 %675, %639
  %725 = and i32 %711, %724
  %726 = xor i32 %725, %639
  %727 = add i32 %217, 1426881987
  %728 = add i32 %727, %603
  %729 = add i32 %728, %726
  %730 = add i32 %729, %723
  %731 = lshr i32 %712, 2
  %732 = shl i32 %712, 30
  %733 = or i32 %731, %732
  %734 = lshr i32 %712, 13
  %735 = shl i32 %712, 19
  %736 = or i32 %734, %735
  %737 = xor i32 %733, %736
  %738 = lshr i32 %712, 22
  %739 = shl i32 %712, 10
  %740 = or i32 %738, %739
  %741 = xor i32 %737, %740
  %742 = and i32 %712, %676
  %743 = or i32 %712, %676
  %744 = and i32 %743, %640
  %745 = or i32 %744, %742
  %746 = add i32 %741, %745
  %747 = add i32 %730, %604
  %748 = add i32 %746, %730
  %749 = lshr i32 %747, 6
  %750 = shl i32 %747, 26
  %751 = or i32 %749, %750
  %752 = lshr i32 %747, 11
  %753 = shl i32 %747, 21
  %754 = or i32 %752, %753
  %755 = xor i32 %751, %754
  %756 = lshr i32 %747, 25
  %757 = shl i32 %747, 7
  %758 = or i32 %756, %757
  %759 = xor i32 %755, %758
  %760 = xor i32 %711, %675
  %761 = and i32 %747, %760
  %762 = xor i32 %761, %675
  %763 = add i32 %235, 1925078388
  %764 = add i32 %763, %639
  %765 = add i32 %764, %762
  %766 = add i32 %765, %759
  %767 = lshr i32 %748, 2
  %768 = shl i32 %748, 30
  %769 = or i32 %767, %768
  %770 = lshr i32 %748, 13
  %771 = shl i32 %748, 19
  %772 = or i32 %770, %771
  %773 = xor i32 %769, %772
  %774 = lshr i32 %748, 22
  %775 = shl i32 %748, 10
  %776 = or i32 %774, %775
  %777 = xor i32 %773, %776
  %778 = and i32 %748, %712
  %779 = or i32 %748, %712
  %780 = and i32 %779, %676
  %781 = or i32 %780, %778
  %782 = add i32 %777, %781
  %783 = add i32 %766, %640
  %784 = add i32 %782, %766
  %785 = lshr i32 %783, 6
  %786 = shl i32 %783, 26
  %787 = or i32 %785, %786
  %788 = lshr i32 %783, 11
  %789 = shl i32 %783, 21
  %790 = or i32 %788, %789
  %791 = xor i32 %787, %790
  %792 = lshr i32 %783, 25
  %793 = shl i32 %783, 7
  %794 = or i32 %792, %793
  %795 = xor i32 %791, %794
  %796 = xor i32 %747, %711
  %797 = and i32 %783, %796
  %798 = xor i32 %797, %711
  %799 = add i32 %253, -2132889090
  %800 = add i32 %799, %675
  %801 = add i32 %800, %798
  %802 = add i32 %801, %795
  %803 = lshr i32 %784, 2
  %804 = shl i32 %784, 30
  %805 = or i32 %803, %804
  %806 = lshr i32 %784, 13
  %807 = shl i32 %784, 19
  %808 = or i32 %806, %807
  %809 = xor i32 %805, %808
  %810 = lshr i32 %784, 22
  %811 = shl i32 %784, 10
  %812 = or i32 %810, %811
  %813 = xor i32 %809, %812
  %814 = and i32 %784, %748
  %815 = or i32 %784, %748
  %816 = and i32 %815, %712
  %817 = or i32 %816, %814
  %818 = add i32 %813, %817
  %819 = add i32 %802, %676
  %820 = add i32 %818, %802
  %821 = lshr i32 %819, 6
  %822 = shl i32 %819, 26
  %823 = or i32 %821, %822
  %824 = lshr i32 %819, 11
  %825 = shl i32 %819, 21
  %826 = or i32 %824, %825
  %827 = xor i32 %823, %826
  %828 = lshr i32 %819, 25
  %829 = shl i32 %819, 7
  %830 = or i32 %828, %829
  %831 = xor i32 %827, %830
  %832 = xor i32 %783, %747
  %833 = and i32 %819, %832
  %834 = xor i32 %833, %747
  %835 = add i32 %271, -1680079193
  %836 = add i32 %835, %711
  %837 = add i32 %836, %834
  %838 = add i32 %837, %831
  %839 = lshr i32 %820, 2
  %840 = shl i32 %820, 30
  %841 = or i32 %839, %840
  %842 = lshr i32 %820, 13
  %843 = shl i32 %820, 19
  %844 = or i32 %842, %843
  %845 = xor i32 %841, %844
  %846 = lshr i32 %820, 22
  %847 = shl i32 %820, 10
  %848 = or i32 %846, %847
  %849 = xor i32 %845, %848
  %850 = and i32 %820, %784
  %851 = or i32 %820, %784
  %852 = and i32 %851, %748
  %853 = or i32 %852, %850
  %854 = add i32 %849, %853
  %855 = add i32 %838, %712
  %856 = add i32 %854, %838
  %857 = lshr i32 %855, 6
  %858 = shl i32 %855, 26
  %859 = or i32 %857, %858
  %860 = lshr i32 %855, 11
  %861 = shl i32 %855, 21
  %862 = or i32 %860, %861
  %863 = xor i32 %859, %862
  %864 = lshr i32 %855, 25
  %865 = shl i32 %855, 7
  %866 = or i32 %864, %865
  %867 = xor i32 %863, %866
  %868 = xor i32 %819, %783
  %869 = and i32 %855, %868
  %870 = xor i32 %869, %783
  %871 = add i32 %289, -1046744716
  %872 = add i32 %871, %747
  %873 = add i32 %872, %870
  %874 = add i32 %873, %867
  %875 = lshr i32 %856, 2
  %876 = shl i32 %856, 30
  %877 = or i32 %875, %876
  %878 = lshr i32 %856, 13
  %879 = shl i32 %856, 19
  %880 = or i32 %878, %879
  %881 = xor i32 %877, %880
  %882 = lshr i32 %856, 22
  %883 = shl i32 %856, 10
  %884 = or i32 %882, %883
  %885 = xor i32 %881, %884
  %886 = and i32 %856, %820
  %887 = or i32 %856, %820
  %888 = and i32 %887, %784
  %889 = or i32 %888, %886
  %890 = add i32 %885, %889
  %891 = add i32 %874, %748
  %892 = add i32 %890, %874
  %893 = lshr i32 %891, 6
  %894 = shl i32 %891, 26
  %895 = or i32 %893, %894
  %896 = lshr i32 %891, 11
  %897 = shl i32 %891, 21
  %898 = or i32 %896, %897
  %899 = xor i32 %895, %898
  %900 = lshr i32 %891, 25
  %901 = shl i32 %891, 7
  %902 = or i32 %900, %901
  %903 = xor i32 %899, %902
  %904 = xor i32 %855, %819
  %905 = and i32 %891, %904
  %906 = xor i32 %905, %819
  %907 = lshr i32 %262, 17
  %908 = shl i32 %271, 15
  %909 = or i32 %908, %907
  %910 = lshr i32 %262, 19
  %911 = shl i32 %271, 13
  %912 = or i32 %911, %910
  %913 = lshr i32 %267, 10
  %914 = xor i32 %912, %913
  %915 = xor i32 %914, %909
  %916 = lshr i32 %37, 7
  %917 = shl i32 %36, 25
  %918 = or i32 %916, %917
  %919 = lshr i32 %28, 18
  %920 = shl i32 %37, 14
  %921 = or i32 %920, %919
  %922 = lshr i32 %37, 3
  %923 = xor i32 %921, %922
  %924 = xor i32 %923, %918
  %925 = add i32 %924, %19
  %926 = add i32 %925, %181
  %927 = add i32 %926, %915
  %928 = add i32 %927, -459576895
  %929 = add i32 %928, %783
  %930 = add i32 %929, %906
  %931 = add i32 %930, %903
  %932 = lshr i32 %892, 2
  %933 = shl i32 %892, 30
  %934 = or i32 %932, %933
  %935 = lshr i32 %892, 13
  %936 = shl i32 %892, 19
  %937 = or i32 %935, %936
  %938 = xor i32 %934, %937
  %939 = lshr i32 %892, 22
  %940 = shl i32 %892, 10
  %941 = or i32 %939, %940
  %942 = xor i32 %938, %941
  %943 = and i32 %892, %856
  %944 = or i32 %892, %856
  %945 = and i32 %944, %820
  %946 = or i32 %945, %943
  %947 = add i32 %942, %946
  %948 = add i32 %931, %784
  %949 = add i32 %947, %931
  %950 = lshr i32 %948, 6
  %951 = shl i32 %948, 26
  %952 = or i32 %950, %951
  %953 = lshr i32 %948, 11
  %954 = shl i32 %948, 21
  %955 = or i32 %953, %954
  %956 = xor i32 %952, %955
  %957 = lshr i32 %948, 25
  %958 = shl i32 %948, 7
  %959 = or i32 %957, %958
  %960 = xor i32 %956, %959
  %961 = xor i32 %891, %855
  %962 = and i32 %948, %961
  %963 = xor i32 %962, %855
  %964 = lshr i32 %280, 17
  %965 = shl i32 %289, 15
  %966 = or i32 %965, %964
  %967 = lshr i32 %280, 19
  %968 = shl i32 %289, 13
  %969 = or i32 %968, %967
  %970 = lshr i32 %285, 10
  %971 = xor i32 %969, %970
  %972 = xor i32 %971, %966
  %973 = lshr i32 %55, 7
  %974 = shl i32 %54, 25
  %975 = or i32 %973, %974
  %976 = lshr i32 %46, 18
  %977 = shl i32 %55, 14
  %978 = or i32 %977, %976
  %979 = lshr i32 %55, 3
  %980 = xor i32 %978, %979
  %981 = xor i32 %980, %975
  %982 = add i32 %981, %37
  %983 = add i32 %982, %199
  %984 = add i32 %983, %972
  %985 = add i32 %984, -272742522
  %986 = add i32 %985, %819
  %987 = add i32 %986, %963
  %988 = add i32 %987, %960
  %989 = lshr i32 %949, 2
  %990 = shl i32 %949, 30
  %991 = or i32 %989, %990
  %992 = lshr i32 %949, 13
  %993 = shl i32 %949, 19
  %994 = or i32 %992, %993
  %995 = xor i32 %991, %994
  %996 = lshr i32 %949, 22
  %997 = shl i32 %949, 10
  %998 = or i32 %996, %997
  %999 = xor i32 %995, %998
  %1000 = and i32 %949, %892
  %1001 = or i32 %949, %892
  %1002 = and i32 %1001, %856
  %1003 = or i32 %1002, %1000
  %1004 = add i32 %999, %1003
  %1005 = add i32 %988, %820
  %1006 = add i32 %1004, %988
  %1007 = lshr i32 %1005, 6
  %1008 = shl i32 %1005, 26
  %1009 = or i32 %1007, %1008
  %1010 = lshr i32 %1005, 11
  %1011 = shl i32 %1005, 21
  %1012 = or i32 %1010, %1011
  %1013 = xor i32 %1009, %1012
  %1014 = lshr i32 %1005, 25
  %1015 = shl i32 %1005, 7
  %1016 = or i32 %1014, %1015
  %1017 = xor i32 %1013, %1016
  %1018 = xor i32 %948, %891
  %1019 = and i32 %1005, %1018
  %1020 = xor i32 %1019, %891
  %1021 = lshr i32 %927, 17
  %1022 = shl i32 %927, 15
  %1023 = or i32 %1021, %1022
  %1024 = lshr i32 %927, 19
  %1025 = shl i32 %927, 13
  %1026 = or i32 %1024, %1025
  %1027 = lshr i32 %927, 10
  %1028 = xor i32 %1026, %1027
  %1029 = xor i32 %1028, %1023
  %1030 = lshr i32 %73, 7
  %1031 = shl i32 %72, 25
  %1032 = or i32 %1030, %1031
  %1033 = lshr i32 %64, 18
  %1034 = shl i32 %73, 14
  %1035 = or i32 %1034, %1033
  %1036 = lshr i32 %73, 3
  %1037 = xor i32 %1035, %1036
  %1038 = xor i32 %1037, %1032
  %1039 = add i32 %1038, %55
  %1040 = add i32 %1039, %217
  %1041 = add i32 %1040, %1029
  %1042 = add i32 %1041, 264347078
  %1043 = add i32 %1042, %855
  %1044 = add i32 %1043, %1020
  %1045 = add i32 %1044, %1017
  %1046 = lshr i32 %1006, 2
  %1047 = shl i32 %1006, 30
  %1048 = or i32 %1046, %1047
  %1049 = lshr i32 %1006, 13
  %1050 = shl i32 %1006, 19
  %1051 = or i32 %1049, %1050
  %1052 = xor i32 %1048, %1051
  %1053 = lshr i32 %1006, 22
  %1054 = shl i32 %1006, 10
  %1055 = or i32 %1053, %1054
  %1056 = xor i32 %1052, %1055
  %1057 = and i32 %1006, %949
  %1058 = or i32 %1006, %949
  %1059 = and i32 %1058, %892
  %1060 = or i32 %1059, %1057
  %1061 = add i32 %1056, %1060
  %1062 = add i32 %1045, %856
  %1063 = add i32 %1061, %1045
  %1064 = lshr i32 %1062, 6
  %1065 = shl i32 %1062, 26
  %1066 = or i32 %1064, %1065
  %1067 = lshr i32 %1062, 11
  %1068 = shl i32 %1062, 21
  %1069 = or i32 %1067, %1068
  %1070 = xor i32 %1066, %1069
  %1071 = lshr i32 %1062, 25
  %1072 = shl i32 %1062, 7
  %1073 = or i32 %1071, %1072
  %1074 = xor i32 %1070, %1073
  %1075 = xor i32 %1005, %948
  %1076 = and i32 %1062, %1075
  %1077 = xor i32 %1076, %948
  %1078 = lshr i32 %984, 17
  %1079 = shl i32 %984, 15
  %1080 = or i32 %1078, %1079
  %1081 = lshr i32 %984, 19
  %1082 = shl i32 %984, 13
  %1083 = or i32 %1081, %1082
  %1084 = lshr i32 %984, 10
  %1085 = xor i32 %1083, %1084
  %1086 = xor i32 %1085, %1080
  %1087 = lshr i32 %91, 7
  %1088 = shl i32 %90, 25
  %1089 = or i32 %1087, %1088
  %1090 = lshr i32 %82, 18
  %1091 = shl i32 %91, 14
  %1092 = or i32 %1091, %1090
  %1093 = lshr i32 %91, 3
  %1094 = xor i32 %1092, %1093
  %1095 = xor i32 %1094, %1089
  %1096 = add i32 %1095, %73
  %1097 = add i32 %1096, %235
  %1098 = add i32 %1097, %1086
  %1099 = add i32 %1098, 604807628
  %1100 = add i32 %1099, %891
  %1101 = add i32 %1100, %1077
  %1102 = add i32 %1101, %1074
  %1103 = lshr i32 %1063, 2
  %1104 = shl i32 %1063, 30
  %1105 = or i32 %1103, %1104
  %1106 = lshr i32 %1063, 13
  %1107 = shl i32 %1063, 19
  %1108 = or i32 %1106, %1107
  %1109 = xor i32 %1105, %1108
  %1110 = lshr i32 %1063, 22
  %1111 = shl i32 %1063, 10
  %1112 = or i32 %1110, %1111
  %1113 = xor i32 %1109, %1112
  %1114 = and i32 %1063, %1006
  %1115 = or i32 %1063, %1006
  %1116 = and i32 %1115, %949
  %1117 = or i32 %1116, %1114
  %1118 = add i32 %1113, %1117
  %1119 = add i32 %1102, %892
  %1120 = add i32 %1118, %1102
  %1121 = lshr i32 %1119, 6
  %1122 = shl i32 %1119, 26
  %1123 = or i32 %1121, %1122
  %1124 = lshr i32 %1119, 11
  %1125 = shl i32 %1119, 21
  %1126 = or i32 %1124, %1125
  %1127 = xor i32 %1123, %1126
  %1128 = lshr i32 %1119, 25
  %1129 = shl i32 %1119, 7
  %1130 = or i32 %1128, %1129
  %1131 = xor i32 %1127, %1130
  %1132 = xor i32 %1062, %1005
  %1133 = and i32 %1119, %1132
  %1134 = xor i32 %1133, %1005
  %1135 = lshr i32 %1041, 17
  %1136 = shl i32 %1041, 15
  %1137 = or i32 %1135, %1136
  %1138 = lshr i32 %1041, 19
  %1139 = shl i32 %1041, 13
  %1140 = or i32 %1138, %1139
  %1141 = lshr i32 %1041, 10
  %1142 = xor i32 %1140, %1141
  %1143 = xor i32 %1142, %1137
  %1144 = lshr i32 %109, 7
  %1145 = shl i32 %108, 25
  %1146 = or i32 %1144, %1145
  %1147 = lshr i32 %100, 18
  %1148 = shl i32 %109, 14
  %1149 = or i32 %1148, %1147
  %1150 = lshr i32 %109, 3
  %1151 = xor i32 %1149, %1150
  %1152 = xor i32 %1151, %1146
  %1153 = add i32 %1152, %91
  %1154 = add i32 %1153, %253
  %1155 = add i32 %1154, %1143
  %1156 = add i32 %1155, 770255983
  %1157 = add i32 %1156, %948
  %1158 = add i32 %1157, %1134
  %1159 = add i32 %1158, %1131
  %1160 = lshr i32 %1120, 2
  %1161 = shl i32 %1120, 30
  %1162 = or i32 %1160, %1161
  %1163 = lshr i32 %1120, 13
  %1164 = shl i32 %1120, 19
  %1165 = or i32 %1163, %1164
  %1166 = xor i32 %1162, %1165
  %1167 = lshr i32 %1120, 22
  %1168 = shl i32 %1120, 10
  %1169 = or i32 %1167, %1168
  %1170 = xor i32 %1166, %1169
  %1171 = and i32 %1120, %1063
  %1172 = or i32 %1120, %1063
  %1173 = and i32 %1172, %1006
  %1174 = or i32 %1173, %1171
  %1175 = add i32 %1170, %1174
  %1176 = add i32 %1159, %949
  %1177 = add i32 %1175, %1159
  %1178 = lshr i32 %1176, 6
  %1179 = shl i32 %1176, 26
  %1180 = or i32 %1178, %1179
  %1181 = lshr i32 %1176, 11
  %1182 = shl i32 %1176, 21
  %1183 = or i32 %1181, %1182
  %1184 = xor i32 %1180, %1183
  %1185 = lshr i32 %1176, 25
  %1186 = shl i32 %1176, 7
  %1187 = or i32 %1185, %1186
  %1188 = xor i32 %1184, %1187
  %1189 = xor i32 %1119, %1062
  %1190 = and i32 %1176, %1189
  %1191 = xor i32 %1190, %1062
  %1192 = lshr i32 %1098, 17
  %1193 = shl i32 %1098, 15
  %1194 = or i32 %1192, %1193
  %1195 = lshr i32 %1098, 19
  %1196 = shl i32 %1098, 13
  %1197 = or i32 %1195, %1196
  %1198 = lshr i32 %1098, 10
  %1199 = xor i32 %1197, %1198
  %1200 = xor i32 %1199, %1194
  %1201 = lshr i32 %127, 7
  %1202 = shl i32 %126, 25
  %1203 = or i32 %1201, %1202
  %1204 = lshr i32 %118, 18
  %1205 = shl i32 %127, 14
  %1206 = or i32 %1205, %1204
  %1207 = lshr i32 %127, 3
  %1208 = xor i32 %1206, %1207
  %1209 = xor i32 %1208, %1203
  %1210 = add i32 %1209, %109
  %1211 = add i32 %1210, %271
  %1212 = add i32 %1211, %1200
  %1213 = add i32 %1212, 1249150122
  %1214 = add i32 %1213, %1005
  %1215 = add i32 %1214, %1191
  %1216 = add i32 %1215, %1188
  %1217 = lshr i32 %1177, 2
  %1218 = shl i32 %1177, 30
  %1219 = or i32 %1217, %1218
  %1220 = lshr i32 %1177, 13
  %1221 = shl i32 %1177, 19
  %1222 = or i32 %1220, %1221
  %1223 = xor i32 %1219, %1222
  %1224 = lshr i32 %1177, 22
  %1225 = shl i32 %1177, 10
  %1226 = or i32 %1224, %1225
  %1227 = xor i32 %1223, %1226
  %1228 = and i32 %1177, %1120
  %1229 = or i32 %1177, %1120
  %1230 = and i32 %1229, %1063
  %1231 = or i32 %1230, %1228
  %1232 = add i32 %1227, %1231
  %1233 = add i32 %1216, %1006
  %1234 = add i32 %1232, %1216
  %1235 = lshr i32 %1233, 6
  %1236 = shl i32 %1233, 26
  %1237 = or i32 %1235, %1236
  %1238 = lshr i32 %1233, 11
  %1239 = shl i32 %1233, 21
  %1240 = or i32 %1238, %1239
  %1241 = xor i32 %1237, %1240
  %1242 = lshr i32 %1233, 25
  %1243 = shl i32 %1233, 7
  %1244 = or i32 %1242, %1243
  %1245 = xor i32 %1241, %1244
  %1246 = xor i32 %1176, %1119
  %1247 = and i32 %1233, %1246
  %1248 = xor i32 %1247, %1119
  %1249 = lshr i32 %1155, 17
  %1250 = shl i32 %1155, 15
  %1251 = or i32 %1249, %1250
  %1252 = lshr i32 %1155, 19
  %1253 = shl i32 %1155, 13
  %1254 = or i32 %1252, %1253
  %1255 = lshr i32 %1155, 10
  %1256 = xor i32 %1254, %1255
  %1257 = xor i32 %1256, %1251
  %1258 = lshr i32 %145, 7
  %1259 = shl i32 %144, 25
  %1260 = or i32 %1258, %1259
  %1261 = lshr i32 %136, 18
  %1262 = shl i32 %145, 14
  %1263 = or i32 %1262, %1261
  %1264 = lshr i32 %145, 3
  %1265 = xor i32 %1263, %1264
  %1266 = xor i32 %1265, %1260
  %1267 = add i32 %1266, %127
  %1268 = add i32 %1267, %289
  %1269 = add i32 %1268, %1257
  %1270 = add i32 %1269, 1555081692
  %1271 = add i32 %1270, %1062
  %1272 = add i32 %1271, %1248
  %1273 = add i32 %1272, %1245
  %1274 = lshr i32 %1234, 2
  %1275 = shl i32 %1234, 30
  %1276 = or i32 %1274, %1275
  %1277 = lshr i32 %1234, 13
  %1278 = shl i32 %1234, 19
  %1279 = or i32 %1277, %1278
  %1280 = xor i32 %1276, %1279
  %1281 = lshr i32 %1234, 22
  %1282 = shl i32 %1234, 10
  %1283 = or i32 %1281, %1282
  %1284 = xor i32 %1280, %1283
  %1285 = and i32 %1234, %1177
  %1286 = or i32 %1234, %1177
  %1287 = and i32 %1286, %1120
  %1288 = or i32 %1287, %1285
  %1289 = add i32 %1284, %1288
  %1290 = add i32 %1273, %1063
  %1291 = add i32 %1289, %1273
  %1292 = lshr i32 %1290, 6
  %1293 = shl i32 %1290, 26
  %1294 = or i32 %1292, %1293
  %1295 = lshr i32 %1290, 11
  %1296 = shl i32 %1290, 21
  %1297 = or i32 %1295, %1296
  %1298 = xor i32 %1294, %1297
  %1299 = lshr i32 %1290, 25
  %1300 = shl i32 %1290, 7
  %1301 = or i32 %1299, %1300
  %1302 = xor i32 %1298, %1301
  %1303 = xor i32 %1233, %1176
  %1304 = and i32 %1290, %1303
  %1305 = xor i32 %1304, %1176
  %1306 = lshr i32 %1212, 17
  %1307 = shl i32 %1212, 15
  %1308 = or i32 %1306, %1307
  %1309 = lshr i32 %1212, 19
  %1310 = shl i32 %1212, 13
  %1311 = or i32 %1309, %1310
  %1312 = lshr i32 %1212, 10
  %1313 = xor i32 %1311, %1312
  %1314 = xor i32 %1313, %1308
  %1315 = lshr i32 %163, 7
  %1316 = shl i32 %162, 25
  %1317 = or i32 %1315, %1316
  %1318 = lshr i32 %154, 18
  %1319 = shl i32 %163, 14
  %1320 = or i32 %1319, %1318
  %1321 = lshr i32 %163, 3
  %1322 = xor i32 %1320, %1321
  %1323 = xor i32 %1322, %1317
  %1324 = add i32 %1323, %145
  %1325 = add i32 %1324, %927
  %1326 = add i32 %1325, %1314
  %1327 = add i32 %1326, 1996064986
  %1328 = add i32 %1327, %1119
  %1329 = add i32 %1328, %1305
  %1330 = add i32 %1329, %1302
  %1331 = lshr i32 %1291, 2
  %1332 = shl i32 %1291, 30
  %1333 = or i32 %1331, %1332
  %1334 = lshr i32 %1291, 13
  %1335 = shl i32 %1291, 19
  %1336 = or i32 %1334, %1335
  %1337 = xor i32 %1333, %1336
  %1338 = lshr i32 %1291, 22
  %1339 = shl i32 %1291, 10
  %1340 = or i32 %1338, %1339
  %1341 = xor i32 %1337, %1340
  %1342 = and i32 %1291, %1234
  %1343 = or i32 %1291, %1234
  %1344 = and i32 %1343, %1177
  %1345 = or i32 %1344, %1342
  %1346 = add i32 %1341, %1345
  %1347 = add i32 %1330, %1120
  %1348 = add i32 %1346, %1330
  %1349 = lshr i32 %1347, 6
  %1350 = shl i32 %1347, 26
  %1351 = or i32 %1349, %1350
  %1352 = lshr i32 %1347, 11
  %1353 = shl i32 %1347, 21
  %1354 = or i32 %1352, %1353
  %1355 = xor i32 %1351, %1354
  %1356 = lshr i32 %1347, 25
  %1357 = shl i32 %1347, 7
  %1358 = or i32 %1356, %1357
  %1359 = xor i32 %1355, %1358
  %1360 = xor i32 %1290, %1233
  %1361 = and i32 %1347, %1360
  %1362 = xor i32 %1361, %1233
  %1363 = lshr i32 %1269, 17
  %1364 = shl i32 %1269, 15
  %1365 = or i32 %1363, %1364
  %1366 = lshr i32 %1269, 19
  %1367 = shl i32 %1269, 13
  %1368 = or i32 %1366, %1367
  %1369 = lshr i32 %1269, 10
  %1370 = xor i32 %1368, %1369
  %1371 = xor i32 %1370, %1365
  %1372 = lshr i32 %181, 7
  %1373 = shl i32 %180, 25
  %1374 = or i32 %1372, %1373
  %1375 = lshr i32 %172, 18
  %1376 = shl i32 %181, 14
  %1377 = or i32 %1376, %1375
  %1378 = lshr i32 %181, 3
  %1379 = xor i32 %1377, %1378
  %1380 = xor i32 %1379, %1374
  %1381 = add i32 %1380, %163
  %1382 = add i32 %1381, %984
  %1383 = add i32 %1382, %1371
  %1384 = add i32 %1383, -1740746414
  %1385 = add i32 %1384, %1176
  %1386 = add i32 %1385, %1362
  %1387 = add i32 %1386, %1359
  %1388 = lshr i32 %1348, 2
  %1389 = shl i32 %1348, 30
  %1390 = or i32 %1388, %1389
  %1391 = lshr i32 %1348, 13
  %1392 = shl i32 %1348, 19
  %1393 = or i32 %1391, %1392
  %1394 = xor i32 %1390, %1393
  %1395 = lshr i32 %1348, 22
  %1396 = shl i32 %1348, 10
  %1397 = or i32 %1395, %1396
  %1398 = xor i32 %1394, %1397
  %1399 = and i32 %1348, %1291
  %1400 = or i32 %1348, %1291
  %1401 = and i32 %1400, %1234
  %1402 = or i32 %1401, %1399
  %1403 = add i32 %1398, %1402
  %1404 = add i32 %1387, %1177
  %1405 = add i32 %1403, %1387
  %1406 = lshr i32 %1404, 6
  %1407 = shl i32 %1404, 26
  %1408 = or i32 %1406, %1407
  %1409 = lshr i32 %1404, 11
  %1410 = shl i32 %1404, 21
  %1411 = or i32 %1409, %1410
  %1412 = xor i32 %1408, %1411
  %1413 = lshr i32 %1404, 25
  %1414 = shl i32 %1404, 7
  %1415 = or i32 %1413, %1414
  %1416 = xor i32 %1412, %1415
  %1417 = xor i32 %1347, %1290
  %1418 = and i32 %1404, %1417
  %1419 = xor i32 %1418, %1290
  %1420 = lshr i32 %1326, 17
  %1421 = shl i32 %1326, 15
  %1422 = or i32 %1420, %1421
  %1423 = lshr i32 %1326, 19
  %1424 = shl i32 %1326, 13
  %1425 = or i32 %1423, %1424
  %1426 = lshr i32 %1326, 10
  %1427 = xor i32 %1425, %1426
  %1428 = xor i32 %1427, %1422
  %1429 = lshr i32 %199, 7
  %1430 = shl i32 %198, 25
  %1431 = or i32 %1429, %1430
  %1432 = lshr i32 %190, 18
  %1433 = shl i32 %199, 14
  %1434 = or i32 %1433, %1432
  %1435 = lshr i32 %199, 3
  %1436 = xor i32 %1434, %1435
  %1437 = xor i32 %1436, %1431
  %1438 = add i32 %1437, %181
  %1439 = add i32 %1438, %1041
  %1440 = add i32 %1439, %1428
  %1441 = add i32 %1440, -1473132947
  %1442 = add i32 %1441, %1233
  %1443 = add i32 %1442, %1419
  %1444 = add i32 %1443, %1416
  %1445 = lshr i32 %1405, 2
  %1446 = shl i32 %1405, 30
  %1447 = or i32 %1445, %1446
  %1448 = lshr i32 %1405, 13
  %1449 = shl i32 %1405, 19
  %1450 = or i32 %1448, %1449
  %1451 = xor i32 %1447, %1450
  %1452 = lshr i32 %1405, 22
  %1453 = shl i32 %1405, 10
  %1454 = or i32 %1452, %1453
  %1455 = xor i32 %1451, %1454
  %1456 = and i32 %1405, %1348
  %1457 = or i32 %1405, %1348
  %1458 = and i32 %1457, %1291
  %1459 = or i32 %1458, %1456
  %1460 = add i32 %1455, %1459
  %1461 = add i32 %1444, %1234
  %1462 = add i32 %1460, %1444
  %1463 = lshr i32 %1461, 6
  %1464 = shl i32 %1461, 26
  %1465 = or i32 %1463, %1464
  %1466 = lshr i32 %1461, 11
  %1467 = shl i32 %1461, 21
  %1468 = or i32 %1466, %1467
  %1469 = xor i32 %1465, %1468
  %1470 = lshr i32 %1461, 25
  %1471 = shl i32 %1461, 7
  %1472 = or i32 %1470, %1471
  %1473 = xor i32 %1469, %1472
  %1474 = xor i32 %1404, %1347
  %1475 = and i32 %1461, %1474
  %1476 = xor i32 %1475, %1347
  %1477 = lshr i32 %1383, 17
  %1478 = shl i32 %1383, 15
  %1479 = or i32 %1477, %1478
  %1480 = lshr i32 %1383, 19
  %1481 = shl i32 %1383, 13
  %1482 = or i32 %1480, %1481
  %1483 = lshr i32 %1383, 10
  %1484 = xor i32 %1482, %1483
  %1485 = xor i32 %1484, %1479
  %1486 = lshr i32 %217, 7
  %1487 = shl i32 %216, 25
  %1488 = or i32 %1486, %1487
  %1489 = lshr i32 %208, 18
  %1490 = shl i32 %217, 14
  %1491 = or i32 %1490, %1489
  %1492 = lshr i32 %217, 3
  %1493 = xor i32 %1491, %1492
  %1494 = xor i32 %1493, %1488
  %1495 = add i32 %1494, %199
  %1496 = add i32 %1495, %1098
  %1497 = add i32 %1496, %1485
  %1498 = add i32 %1497, -1341970488
  %1499 = add i32 %1498, %1290
  %1500 = add i32 %1499, %1476
  %1501 = add i32 %1500, %1473
  %1502 = lshr i32 %1462, 2
  %1503 = shl i32 %1462, 30
  %1504 = or i32 %1502, %1503
  %1505 = lshr i32 %1462, 13
  %1506 = shl i32 %1462, 19
  %1507 = or i32 %1505, %1506
  %1508 = xor i32 %1504, %1507
  %1509 = lshr i32 %1462, 22
  %1510 = shl i32 %1462, 10
  %1511 = or i32 %1509, %1510
  %1512 = xor i32 %1508, %1511
  %1513 = and i32 %1462, %1405
  %1514 = or i32 %1462, %1405
  %1515 = and i32 %1514, %1348
  %1516 = or i32 %1515, %1513
  %1517 = add i32 %1512, %1516
  %1518 = add i32 %1501, %1291
  %1519 = add i32 %1517, %1501
  %1520 = lshr i32 %1518, 6
  %1521 = shl i32 %1518, 26
  %1522 = or i32 %1520, %1521
  %1523 = lshr i32 %1518, 11
  %1524 = shl i32 %1518, 21
  %1525 = or i32 %1523, %1524
  %1526 = xor i32 %1522, %1525
  %1527 = lshr i32 %1518, 25
  %1528 = shl i32 %1518, 7
  %1529 = or i32 %1527, %1528
  %1530 = xor i32 %1526, %1529
  %1531 = xor i32 %1461, %1404
  %1532 = and i32 %1518, %1531
  %1533 = xor i32 %1532, %1404
  %1534 = lshr i32 %1440, 17
  %1535 = shl i32 %1440, 15
  %1536 = or i32 %1534, %1535
  %1537 = lshr i32 %1440, 19
  %1538 = shl i32 %1440, 13
  %1539 = or i32 %1537, %1538
  %1540 = lshr i32 %1440, 10
  %1541 = xor i32 %1539, %1540
  %1542 = xor i32 %1541, %1536
  %1543 = lshr i32 %235, 7
  %1544 = shl i32 %234, 25
  %1545 = or i32 %1543, %1544
  %1546 = lshr i32 %226, 18
  %1547 = shl i32 %235, 14
  %1548 = or i32 %1547, %1546
  %1549 = lshr i32 %235, 3
  %1550 = xor i32 %1548, %1549
  %1551 = xor i32 %1550, %1545
  %1552 = add i32 %1551, %217
  %1553 = add i32 %1552, %1155
  %1554 = add i32 %1553, %1542
  %1555 = add i32 %1554, -1084653625
  %1556 = add i32 %1555, %1347
  %1557 = add i32 %1556, %1533
  %1558 = add i32 %1557, %1530
  %1559 = lshr i32 %1519, 2
  %1560 = shl i32 %1519, 30
  %1561 = or i32 %1559, %1560
  %1562 = lshr i32 %1519, 13
  %1563 = shl i32 %1519, 19
  %1564 = or i32 %1562, %1563
  %1565 = xor i32 %1561, %1564
  %1566 = lshr i32 %1519, 22
  %1567 = shl i32 %1519, 10
  %1568 = or i32 %1566, %1567
  %1569 = xor i32 %1565, %1568
  %1570 = and i32 %1519, %1462
  %1571 = or i32 %1519, %1462
  %1572 = and i32 %1571, %1405
  %1573 = or i32 %1572, %1570
  %1574 = add i32 %1569, %1573
  %1575 = add i32 %1558, %1348
  %1576 = add i32 %1574, %1558
  %1577 = lshr i32 %1575, 6
  %1578 = shl i32 %1575, 26
  %1579 = or i32 %1577, %1578
  %1580 = lshr i32 %1575, 11
  %1581 = shl i32 %1575, 21
  %1582 = or i32 %1580, %1581
  %1583 = xor i32 %1579, %1582
  %1584 = lshr i32 %1575, 25
  %1585 = shl i32 %1575, 7
  %1586 = or i32 %1584, %1585
  %1587 = xor i32 %1583, %1586
  %1588 = xor i32 %1518, %1461
  %1589 = and i32 %1575, %1588
  %1590 = xor i32 %1589, %1461
  %1591 = lshr i32 %1497, 17
  %1592 = shl i32 %1497, 15
  %1593 = or i32 %1591, %1592
  %1594 = lshr i32 %1497, 19
  %1595 = shl i32 %1497, 13
  %1596 = or i32 %1594, %1595
  %1597 = lshr i32 %1497, 10
  %1598 = xor i32 %1596, %1597
  %1599 = xor i32 %1598, %1593
  %1600 = lshr i32 %253, 7
  %1601 = shl i32 %252, 25
  %1602 = or i32 %1600, %1601
  %1603 = lshr i32 %244, 18
  %1604 = shl i32 %253, 14
  %1605 = or i32 %1604, %1603
  %1606 = lshr i32 %253, 3
  %1607 = xor i32 %1605, %1606
  %1608 = xor i32 %1607, %1602
  %1609 = add i32 %1608, %235
  %1610 = add i32 %1609, %1212
  %1611 = add i32 %1610, %1599
  %1612 = add i32 %1611, -958395405
  %1613 = add i32 %1612, %1404
  %1614 = add i32 %1613, %1590
  %1615 = add i32 %1614, %1587
  %1616 = lshr i32 %1576, 2
  %1617 = shl i32 %1576, 30
  %1618 = or i32 %1616, %1617
  %1619 = lshr i32 %1576, 13
  %1620 = shl i32 %1576, 19
  %1621 = or i32 %1619, %1620
  %1622 = xor i32 %1618, %1621
  %1623 = lshr i32 %1576, 22
  %1624 = shl i32 %1576, 10
  %1625 = or i32 %1623, %1624
  %1626 = xor i32 %1622, %1625
  %1627 = and i32 %1576, %1519
  %1628 = or i32 %1576, %1519
  %1629 = and i32 %1628, %1462
  %1630 = or i32 %1629, %1627
  %1631 = add i32 %1626, %1630
  %1632 = add i32 %1615, %1405
  %1633 = add i32 %1631, %1615
  %1634 = lshr i32 %1632, 6
  %1635 = shl i32 %1632, 26
  %1636 = or i32 %1634, %1635
  %1637 = lshr i32 %1632, 11
  %1638 = shl i32 %1632, 21
  %1639 = or i32 %1637, %1638
  %1640 = xor i32 %1636, %1639
  %1641 = lshr i32 %1632, 25
  %1642 = shl i32 %1632, 7
  %1643 = or i32 %1641, %1642
  %1644 = xor i32 %1640, %1643
  %1645 = xor i32 %1575, %1518
  %1646 = and i32 %1632, %1645
  %1647 = xor i32 %1646, %1518
  %1648 = lshr i32 %1554, 17
  %1649 = shl i32 %1554, 15
  %1650 = or i32 %1648, %1649
  %1651 = lshr i32 %1554, 19
  %1652 = shl i32 %1554, 13
  %1653 = or i32 %1651, %1652
  %1654 = lshr i32 %1554, 10
  %1655 = xor i32 %1653, %1654
  %1656 = xor i32 %1655, %1650
  %1657 = lshr i32 %271, 7
  %1658 = shl i32 %270, 25
  %1659 = or i32 %1657, %1658
  %1660 = lshr i32 %262, 18
  %1661 = shl i32 %271, 14
  %1662 = or i32 %1661, %1660
  %1663 = lshr i32 %271, 3
  %1664 = xor i32 %1662, %1663
  %1665 = xor i32 %1664, %1659
  %1666 = add i32 %1665, %253
  %1667 = add i32 %1666, %1269
  %1668 = add i32 %1667, %1656
  %1669 = add i32 %1668, -710438585
  %1670 = add i32 %1669, %1461
  %1671 = add i32 %1670, %1647
  %1672 = add i32 %1671, %1644
  %1673 = lshr i32 %1633, 2
  %1674 = shl i32 %1633, 30
  %1675 = or i32 %1673, %1674
  %1676 = lshr i32 %1633, 13
  %1677 = shl i32 %1633, 19
  %1678 = or i32 %1676, %1677
  %1679 = xor i32 %1675, %1678
  %1680 = lshr i32 %1633, 22
  %1681 = shl i32 %1633, 10
  %1682 = or i32 %1680, %1681
  %1683 = xor i32 %1679, %1682
  %1684 = and i32 %1633, %1576
  %1685 = or i32 %1633, %1576
  %1686 = and i32 %1685, %1519
  %1687 = or i32 %1686, %1684
  %1688 = add i32 %1683, %1687
  %1689 = add i32 %1672, %1462
  %1690 = add i32 %1688, %1672
  %1691 = lshr i32 %1689, 6
  %1692 = shl i32 %1689, 26
  %1693 = or i32 %1691, %1692
  %1694 = lshr i32 %1689, 11
  %1695 = shl i32 %1689, 21
  %1696 = or i32 %1694, %1695
  %1697 = xor i32 %1693, %1696
  %1698 = lshr i32 %1689, 25
  %1699 = shl i32 %1689, 7
  %1700 = or i32 %1698, %1699
  %1701 = xor i32 %1697, %1700
  %1702 = xor i32 %1632, %1575
  %1703 = and i32 %1689, %1702
  %1704 = xor i32 %1703, %1575
  %1705 = lshr i32 %1611, 17
  %1706 = shl i32 %1611, 15
  %1707 = or i32 %1705, %1706
  %1708 = lshr i32 %1611, 19
  %1709 = shl i32 %1611, 13
  %1710 = or i32 %1708, %1709
  %1711 = lshr i32 %1611, 10
  %1712 = xor i32 %1710, %1711
  %1713 = xor i32 %1712, %1707
  %1714 = lshr i32 %289, 7
  %1715 = shl i32 %288, 25
  %1716 = or i32 %1714, %1715
  %1717 = lshr i32 %280, 18
  %1718 = shl i32 %289, 14
  %1719 = or i32 %1718, %1717
  %1720 = lshr i32 %289, 3
  %1721 = xor i32 %1719, %1720
  %1722 = xor i32 %1721, %1716
  %1723 = add i32 %1722, %271
  %1724 = add i32 %1723, %1326
  %1725 = add i32 %1724, %1713
  %1726 = add i32 %1725, 113926993
  %1727 = add i32 %1726, %1518
  %1728 = add i32 %1727, %1704
  %1729 = add i32 %1728, %1701
  %1730 = lshr i32 %1690, 2
  %1731 = shl i32 %1690, 30
  %1732 = or i32 %1730, %1731
  %1733 = lshr i32 %1690, 13
  %1734 = shl i32 %1690, 19
  %1735 = or i32 %1733, %1734
  %1736 = xor i32 %1732, %1735
  %1737 = lshr i32 %1690, 22
  %1738 = shl i32 %1690, 10
  %1739 = or i32 %1737, %1738
  %1740 = xor i32 %1736, %1739
  %1741 = and i32 %1690, %1633
  %1742 = or i32 %1690, %1633
  %1743 = and i32 %1742, %1576
  %1744 = or i32 %1743, %1741
  %1745 = add i32 %1740, %1744
  %1746 = add i32 %1729, %1519
  %1747 = add i32 %1745, %1729
  %1748 = lshr i32 %1746, 6
  %1749 = shl i32 %1746, 26
  %1750 = or i32 %1748, %1749
  %1751 = lshr i32 %1746, 11
  %1752 = shl i32 %1746, 21
  %1753 = or i32 %1751, %1752
  %1754 = xor i32 %1750, %1753
  %1755 = lshr i32 %1746, 25
  %1756 = shl i32 %1746, 7
  %1757 = or i32 %1755, %1756
  %1758 = xor i32 %1754, %1757
  %1759 = xor i32 %1689, %1632
  %1760 = and i32 %1746, %1759
  %1761 = xor i32 %1760, %1632
  %1762 = lshr i32 %1668, 17
  %1763 = shl i32 %1668, 15
  %1764 = or i32 %1762, %1763
  %1765 = lshr i32 %1668, 19
  %1766 = shl i32 %1668, 13
  %1767 = or i32 %1765, %1766
  %1768 = lshr i32 %1668, 10
  %1769 = xor i32 %1767, %1768
  %1770 = xor i32 %1769, %1764
  %1771 = lshr i32 %927, 7
  %1772 = shl i32 %927, 25
  %1773 = or i32 %1771, %1772
  %1774 = lshr i32 %927, 18
  %1775 = shl i32 %927, 14
  %1776 = or i32 %1774, %1775
  %1777 = lshr i32 %927, 3
  %1778 = xor i32 %1776, %1777
  %1779 = xor i32 %1778, %1773
  %1780 = add i32 %1779, %289
  %1781 = add i32 %1780, %1383
  %1782 = add i32 %1781, %1770
  %1783 = add i32 %1782, 338241895
  %1784 = add i32 %1783, %1575
  %1785 = add i32 %1784, %1761
  %1786 = add i32 %1785, %1758
  %1787 = lshr i32 %1747, 2
  %1788 = shl i32 %1747, 30
  %1789 = or i32 %1787, %1788
  %1790 = lshr i32 %1747, 13
  %1791 = shl i32 %1747, 19
  %1792 = or i32 %1790, %1791
  %1793 = xor i32 %1789, %1792
  %1794 = lshr i32 %1747, 22
  %1795 = shl i32 %1747, 10
  %1796 = or i32 %1794, %1795
  %1797 = xor i32 %1793, %1796
  %1798 = and i32 %1747, %1690
  %1799 = or i32 %1747, %1690
  %1800 = and i32 %1799, %1633
  %1801 = or i32 %1800, %1798
  %1802 = add i32 %1797, %1801
  %1803 = add i32 %1786, %1576
  %1804 = add i32 %1802, %1786
  %1805 = lshr i32 %1803, 6
  %1806 = shl i32 %1803, 26
  %1807 = or i32 %1805, %1806
  %1808 = lshr i32 %1803, 11
  %1809 = shl i32 %1803, 21
  %1810 = or i32 %1808, %1809
  %1811 = xor i32 %1807, %1810
  %1812 = lshr i32 %1803, 25
  %1813 = shl i32 %1803, 7
  %1814 = or i32 %1812, %1813
  %1815 = xor i32 %1811, %1814
  %1816 = xor i32 %1746, %1689
  %1817 = and i32 %1803, %1816
  %1818 = xor i32 %1817, %1689
  %1819 = lshr i32 %1725, 17
  %1820 = shl i32 %1725, 15
  %1821 = or i32 %1819, %1820
  %1822 = lshr i32 %1725, 19
  %1823 = shl i32 %1725, 13
  %1824 = or i32 %1822, %1823
  %1825 = lshr i32 %1725, 10
  %1826 = xor i32 %1824, %1825
  %1827 = xor i32 %1826, %1821
  %1828 = lshr i32 %984, 7
  %1829 = shl i32 %984, 25
  %1830 = or i32 %1828, %1829
  %1831 = lshr i32 %984, 18
  %1832 = shl i32 %984, 14
  %1833 = or i32 %1831, %1832
  %1834 = lshr i32 %984, 3
  %1835 = xor i32 %1833, %1834
  %1836 = xor i32 %1835, %1830
  %1837 = add i32 %1836, %927
  %1838 = add i32 %1837, %1440
  %1839 = add i32 %1838, %1827
  %1840 = add i32 %1839, 666307205
  %1841 = add i32 %1840, %1632
  %1842 = add i32 %1841, %1818
  %1843 = add i32 %1842, %1815
  %1844 = lshr i32 %1804, 2
  %1845 = shl i32 %1804, 30
  %1846 = or i32 %1844, %1845
  %1847 = lshr i32 %1804, 13
  %1848 = shl i32 %1804, 19
  %1849 = or i32 %1847, %1848
  %1850 = xor i32 %1846, %1849
  %1851 = lshr i32 %1804, 22
  %1852 = shl i32 %1804, 10
  %1853 = or i32 %1851, %1852
  %1854 = xor i32 %1850, %1853
  %1855 = and i32 %1804, %1747
  %1856 = or i32 %1804, %1747
  %1857 = and i32 %1856, %1690
  %1858 = or i32 %1857, %1855
  %1859 = add i32 %1854, %1858
  %1860 = add i32 %1843, %1633
  %1861 = add i32 %1859, %1843
  %1862 = lshr i32 %1860, 6
  %1863 = shl i32 %1860, 26
  %1864 = or i32 %1862, %1863
  %1865 = lshr i32 %1860, 11
  %1866 = shl i32 %1860, 21
  %1867 = or i32 %1865, %1866
  %1868 = xor i32 %1864, %1867
  %1869 = lshr i32 %1860, 25
  %1870 = shl i32 %1860, 7
  %1871 = or i32 %1869, %1870
  %1872 = xor i32 %1868, %1871
  %1873 = xor i32 %1803, %1746
  %1874 = and i32 %1860, %1873
  %1875 = xor i32 %1874, %1746
  %1876 = lshr i32 %1782, 17
  %1877 = shl i32 %1782, 15
  %1878 = or i32 %1876, %1877
  %1879 = lshr i32 %1782, 19
  %1880 = shl i32 %1782, 13
  %1881 = or i32 %1879, %1880
  %1882 = lshr i32 %1782, 10
  %1883 = xor i32 %1881, %1882
  %1884 = xor i32 %1883, %1878
  %1885 = lshr i32 %1041, 7
  %1886 = shl i32 %1041, 25
  %1887 = or i32 %1885, %1886
  %1888 = lshr i32 %1041, 18
  %1889 = shl i32 %1041, 14
  %1890 = or i32 %1888, %1889
  %1891 = lshr i32 %1041, 3
  %1892 = xor i32 %1890, %1891
  %1893 = xor i32 %1892, %1887
  %1894 = add i32 %1893, %984
  %1895 = add i32 %1894, %1497
  %1896 = add i32 %1895, %1884
  %1897 = add i32 %1896, 773529912
  %1898 = add i32 %1897, %1689
  %1899 = add i32 %1898, %1875
  %1900 = add i32 %1899, %1872
  %1901 = lshr i32 %1861, 2
  %1902 = shl i32 %1861, 30
  %1903 = or i32 %1901, %1902
  %1904 = lshr i32 %1861, 13
  %1905 = shl i32 %1861, 19
  %1906 = or i32 %1904, %1905
  %1907 = xor i32 %1903, %1906
  %1908 = lshr i32 %1861, 22
  %1909 = shl i32 %1861, 10
  %1910 = or i32 %1908, %1909
  %1911 = xor i32 %1907, %1910
  %1912 = and i32 %1861, %1804
  %1913 = or i32 %1861, %1804
  %1914 = and i32 %1913, %1747
  %1915 = or i32 %1914, %1912
  %1916 = add i32 %1911, %1915
  %1917 = add i32 %1900, %1690
  %1918 = add i32 %1916, %1900
  %1919 = lshr i32 %1917, 6
  %1920 = shl i32 %1917, 26
  %1921 = or i32 %1919, %1920
  %1922 = lshr i32 %1917, 11
  %1923 = shl i32 %1917, 21
  %1924 = or i32 %1922, %1923
  %1925 = xor i32 %1921, %1924
  %1926 = lshr i32 %1917, 25
  %1927 = shl i32 %1917, 7
  %1928 = or i32 %1926, %1927
  %1929 = xor i32 %1925, %1928
  %1930 = xor i32 %1860, %1803
  %1931 = and i32 %1917, %1930
  %1932 = xor i32 %1931, %1803
  %1933 = lshr i32 %1839, 17
  %1934 = shl i32 %1839, 15
  %1935 = or i32 %1933, %1934
  %1936 = lshr i32 %1839, 19
  %1937 = shl i32 %1839, 13
  %1938 = or i32 %1936, %1937
  %1939 = lshr i32 %1839, 10
  %1940 = xor i32 %1938, %1939
  %1941 = xor i32 %1940, %1935
  %1942 = lshr i32 %1098, 7
  %1943 = shl i32 %1098, 25
  %1944 = or i32 %1942, %1943
  %1945 = lshr i32 %1098, 18
  %1946 = shl i32 %1098, 14
  %1947 = or i32 %1945, %1946
  %1948 = lshr i32 %1098, 3
  %1949 = xor i32 %1947, %1948
  %1950 = xor i32 %1949, %1944
  %1951 = add i32 %1950, %1041
  %1952 = add i32 %1951, %1554
  %1953 = add i32 %1952, %1941
  %1954 = add i32 %1953, 1294757372
  %1955 = add i32 %1954, %1746
  %1956 = add i32 %1955, %1932
  %1957 = add i32 %1956, %1929
  %1958 = lshr i32 %1918, 2
  %1959 = shl i32 %1918, 30
  %1960 = or i32 %1958, %1959
  %1961 = lshr i32 %1918, 13
  %1962 = shl i32 %1918, 19
  %1963 = or i32 %1961, %1962
  %1964 = xor i32 %1960, %1963
  %1965 = lshr i32 %1918, 22
  %1966 = shl i32 %1918, 10
  %1967 = or i32 %1965, %1966
  %1968 = xor i32 %1964, %1967
  %1969 = and i32 %1918, %1861
  %1970 = or i32 %1918, %1861
  %1971 = and i32 %1970, %1804
  %1972 = or i32 %1971, %1969
  %1973 = add i32 %1968, %1972
  %1974 = add i32 %1957, %1747
  %1975 = add i32 %1973, %1957
  %1976 = lshr i32 %1974, 6
  %1977 = shl i32 %1974, 26
  %1978 = or i32 %1976, %1977
  %1979 = lshr i32 %1974, 11
  %1980 = shl i32 %1974, 21
  %1981 = or i32 %1979, %1980
  %1982 = xor i32 %1978, %1981
  %1983 = lshr i32 %1974, 25
  %1984 = shl i32 %1974, 7
  %1985 = or i32 %1983, %1984
  %1986 = xor i32 %1982, %1985
  %1987 = xor i32 %1917, %1860
  %1988 = and i32 %1974, %1987
  %1989 = xor i32 %1988, %1860
  %1990 = lshr i32 %1896, 17
  %1991 = shl i32 %1896, 15
  %1992 = or i32 %1990, %1991
  %1993 = lshr i32 %1896, 19
  %1994 = shl i32 %1896, 13
  %1995 = or i32 %1993, %1994
  %1996 = lshr i32 %1896, 10
  %1997 = xor i32 %1995, %1996
  %1998 = xor i32 %1997, %1992
  %1999 = lshr i32 %1155, 7
  %2000 = shl i32 %1155, 25
  %2001 = or i32 %1999, %2000
  %2002 = lshr i32 %1155, 18
  %2003 = shl i32 %1155, 14
  %2004 = or i32 %2002, %2003
  %2005 = lshr i32 %1155, 3
  %2006 = xor i32 %2004, %2005
  %2007 = xor i32 %2006, %2001
  %2008 = add i32 %2007, %1098
  %2009 = add i32 %2008, %1611
  %2010 = add i32 %2009, %1998
  %2011 = add i32 %2010, 1396182291
  %2012 = add i32 %2011, %1803
  %2013 = add i32 %2012, %1989
  %2014 = add i32 %2013, %1986
  %2015 = lshr i32 %1975, 2
  %2016 = shl i32 %1975, 30
  %2017 = or i32 %2015, %2016
  %2018 = lshr i32 %1975, 13
  %2019 = shl i32 %1975, 19
  %2020 = or i32 %2018, %2019
  %2021 = xor i32 %2017, %2020
  %2022 = lshr i32 %1975, 22
  %2023 = shl i32 %1975, 10
  %2024 = or i32 %2022, %2023
  %2025 = xor i32 %2021, %2024
  %2026 = and i32 %1975, %1918
  %2027 = or i32 %1975, %1918
  %2028 = and i32 %2027, %1861
  %2029 = or i32 %2028, %2026
  %2030 = add i32 %2025, %2029
  %2031 = add i32 %2014, %1804
  %2032 = add i32 %2030, %2014
  %2033 = lshr i32 %2031, 6
  %2034 = shl i32 %2031, 26
  %2035 = or i32 %2033, %2034
  %2036 = lshr i32 %2031, 11
  %2037 = shl i32 %2031, 21
  %2038 = or i32 %2036, %2037
  %2039 = xor i32 %2035, %2038
  %2040 = lshr i32 %2031, 25
  %2041 = shl i32 %2031, 7
  %2042 = or i32 %2040, %2041
  %2043 = xor i32 %2039, %2042
  %2044 = xor i32 %1974, %1917
  %2045 = and i32 %2031, %2044
  %2046 = xor i32 %2045, %1917
  %2047 = lshr i32 %1953, 17
  %2048 = shl i32 %1953, 15
  %2049 = or i32 %2047, %2048
  %2050 = lshr i32 %1953, 19
  %2051 = shl i32 %1953, 13
  %2052 = or i32 %2050, %2051
  %2053 = lshr i32 %1953, 10
  %2054 = xor i32 %2052, %2053
  %2055 = xor i32 %2054, %2049
  %2056 = lshr i32 %1212, 7
  %2057 = shl i32 %1212, 25
  %2058 = or i32 %2056, %2057
  %2059 = lshr i32 %1212, 18
  %2060 = shl i32 %1212, 14
  %2061 = or i32 %2059, %2060
  %2062 = lshr i32 %1212, 3
  %2063 = xor i32 %2061, %2062
  %2064 = xor i32 %2063, %2058
  %2065 = add i32 %2064, %1155
  %2066 = add i32 %2065, %1668
  %2067 = add i32 %2066, %2055
  %2068 = add i32 %2067, 1695183700
  %2069 = add i32 %2068, %1860
  %2070 = add i32 %2069, %2046
  %2071 = add i32 %2070, %2043
  %2072 = lshr i32 %2032, 2
  %2073 = shl i32 %2032, 30
  %2074 = or i32 %2072, %2073
  %2075 = lshr i32 %2032, 13
  %2076 = shl i32 %2032, 19
  %2077 = or i32 %2075, %2076
  %2078 = xor i32 %2074, %2077
  %2079 = lshr i32 %2032, 22
  %2080 = shl i32 %2032, 10
  %2081 = or i32 %2079, %2080
  %2082 = xor i32 %2078, %2081
  %2083 = and i32 %2032, %1975
  %2084 = or i32 %2032, %1975
  %2085 = and i32 %2084, %1918
  %2086 = or i32 %2085, %2083
  %2087 = add i32 %2082, %2086
  %2088 = add i32 %2071, %1861
  %2089 = add i32 %2087, %2071
  %2090 = lshr i32 %2088, 6
  %2091 = shl i32 %2088, 26
  %2092 = or i32 %2090, %2091
  %2093 = lshr i32 %2088, 11
  %2094 = shl i32 %2088, 21
  %2095 = or i32 %2093, %2094
  %2096 = xor i32 %2092, %2095
  %2097 = lshr i32 %2088, 25
  %2098 = shl i32 %2088, 7
  %2099 = or i32 %2097, %2098
  %2100 = xor i32 %2096, %2099
  %2101 = xor i32 %2031, %1974
  %2102 = and i32 %2088, %2101
  %2103 = xor i32 %2102, %1974
  %2104 = lshr i32 %2010, 17
  %2105 = shl i32 %2010, 15
  %2106 = or i32 %2104, %2105
  %2107 = lshr i32 %2010, 19
  %2108 = shl i32 %2010, 13
  %2109 = or i32 %2107, %2108
  %2110 = lshr i32 %2010, 10
  %2111 = xor i32 %2109, %2110
  %2112 = xor i32 %2111, %2106
  %2113 = lshr i32 %1269, 7
  %2114 = shl i32 %1269, 25
  %2115 = or i32 %2113, %2114
  %2116 = lshr i32 %1269, 18
  %2117 = shl i32 %1269, 14
  %2118 = or i32 %2116, %2117
  %2119 = lshr i32 %1269, 3
  %2120 = xor i32 %2118, %2119
  %2121 = xor i32 %2120, %2115
  %2122 = add i32 %2121, %1212
  %2123 = add i32 %2122, %1725
  %2124 = add i32 %2123, %2112
  %2125 = add i32 %2124, 1986661051
  %2126 = add i32 %2125, %1917
  %2127 = add i32 %2126, %2103
  %2128 = add i32 %2127, %2100
  %2129 = lshr i32 %2089, 2
  %2130 = shl i32 %2089, 30
  %2131 = or i32 %2129, %2130
  %2132 = lshr i32 %2089, 13
  %2133 = shl i32 %2089, 19
  %2134 = or i32 %2132, %2133
  %2135 = xor i32 %2131, %2134
  %2136 = lshr i32 %2089, 22
  %2137 = shl i32 %2089, 10
  %2138 = or i32 %2136, %2137
  %2139 = xor i32 %2135, %2138
  %2140 = and i32 %2089, %2032
  %2141 = or i32 %2089, %2032
  %2142 = and i32 %2141, %1975
  %2143 = or i32 %2142, %2140
  %2144 = add i32 %2139, %2143
  %2145 = add i32 %2128, %1918
  %2146 = add i32 %2144, %2128
  %2147 = lshr i32 %2145, 6
  %2148 = shl i32 %2145, 26
  %2149 = or i32 %2147, %2148
  %2150 = lshr i32 %2145, 11
  %2151 = shl i32 %2145, 21
  %2152 = or i32 %2150, %2151
  %2153 = xor i32 %2149, %2152
  %2154 = lshr i32 %2145, 25
  %2155 = shl i32 %2145, 7
  %2156 = or i32 %2154, %2155
  %2157 = xor i32 %2153, %2156
  %2158 = xor i32 %2088, %2031
  %2159 = and i32 %2145, %2158
  %2160 = xor i32 %2159, %2031
  %2161 = lshr i32 %2067, 17
  %2162 = shl i32 %2067, 15
  %2163 = or i32 %2161, %2162
  %2164 = lshr i32 %2067, 19
  %2165 = shl i32 %2067, 13
  %2166 = or i32 %2164, %2165
  %2167 = lshr i32 %2067, 10
  %2168 = xor i32 %2166, %2167
  %2169 = xor i32 %2168, %2163
  %2170 = lshr i32 %1326, 7
  %2171 = shl i32 %1326, 25
  %2172 = or i32 %2170, %2171
  %2173 = lshr i32 %1326, 18
  %2174 = shl i32 %1326, 14
  %2175 = or i32 %2173, %2174
  %2176 = lshr i32 %1326, 3
  %2177 = xor i32 %2175, %2176
  %2178 = xor i32 %2177, %2172
  %2179 = add i32 %2178, %1269
  %2180 = add i32 %2179, %1782
  %2181 = add i32 %2180, %2169
  %2182 = add i32 %2181, -2117940946
  %2183 = add i32 %2182, %1974
  %2184 = add i32 %2183, %2160
  %2185 = add i32 %2184, %2157
  %2186 = lshr i32 %2146, 2
  %2187 = shl i32 %2146, 30
  %2188 = or i32 %2186, %2187
  %2189 = lshr i32 %2146, 13
  %2190 = shl i32 %2146, 19
  %2191 = or i32 %2189, %2190
  %2192 = xor i32 %2188, %2191
  %2193 = lshr i32 %2146, 22
  %2194 = shl i32 %2146, 10
  %2195 = or i32 %2193, %2194
  %2196 = xor i32 %2192, %2195
  %2197 = and i32 %2146, %2089
  %2198 = or i32 %2146, %2089
  %2199 = and i32 %2198, %2032
  %2200 = or i32 %2199, %2197
  %2201 = add i32 %2196, %2200
  %2202 = add i32 %2185, %1975
  %2203 = add i32 %2201, %2185
  %2204 = lshr i32 %2202, 6
  %2205 = shl i32 %2202, 26
  %2206 = or i32 %2204, %2205
  %2207 = lshr i32 %2202, 11
  %2208 = shl i32 %2202, 21
  %2209 = or i32 %2207, %2208
  %2210 = xor i32 %2206, %2209
  %2211 = lshr i32 %2202, 25
  %2212 = shl i32 %2202, 7
  %2213 = or i32 %2211, %2212
  %2214 = xor i32 %2210, %2213
  %2215 = xor i32 %2145, %2088
  %2216 = and i32 %2202, %2215
  %2217 = xor i32 %2216, %2088
  %2218 = lshr i32 %2124, 17
  %2219 = shl i32 %2124, 15
  %2220 = or i32 %2218, %2219
  %2221 = lshr i32 %2124, 19
  %2222 = shl i32 %2124, 13
  %2223 = or i32 %2221, %2222
  %2224 = lshr i32 %2124, 10
  %2225 = xor i32 %2223, %2224
  %2226 = xor i32 %2225, %2220
  %2227 = lshr i32 %1383, 7
  %2228 = shl i32 %1383, 25
  %2229 = or i32 %2227, %2228
  %2230 = lshr i32 %1383, 18
  %2231 = shl i32 %1383, 14
  %2232 = or i32 %2230, %2231
  %2233 = lshr i32 %1383, 3
  %2234 = xor i32 %2232, %2233
  %2235 = xor i32 %2234, %2229
  %2236 = add i32 %2235, %1326
  %2237 = add i32 %2236, %1839
  %2238 = add i32 %2237, %2226
  %2239 = add i32 %2238, -1838011259
  %2240 = add i32 %2239, %2031
  %2241 = add i32 %2240, %2217
  %2242 = add i32 %2241, %2214
  %2243 = lshr i32 %2203, 2
  %2244 = shl i32 %2203, 30
  %2245 = or i32 %2243, %2244
  %2246 = lshr i32 %2203, 13
  %2247 = shl i32 %2203, 19
  %2248 = or i32 %2246, %2247
  %2249 = xor i32 %2245, %2248
  %2250 = lshr i32 %2203, 22
  %2251 = shl i32 %2203, 10
  %2252 = or i32 %2250, %2251
  %2253 = xor i32 %2249, %2252
  %2254 = and i32 %2203, %2146
  %2255 = or i32 %2203, %2146
  %2256 = and i32 %2255, %2089
  %2257 = or i32 %2256, %2254
  %2258 = add i32 %2253, %2257
  %2259 = add i32 %2242, %2032
  %2260 = add i32 %2258, %2242
  %2261 = lshr i32 %2259, 6
  %2262 = shl i32 %2259, 26
  %2263 = or i32 %2261, %2262
  %2264 = lshr i32 %2259, 11
  %2265 = shl i32 %2259, 21
  %2266 = or i32 %2264, %2265
  %2267 = xor i32 %2263, %2266
  %2268 = lshr i32 %2259, 25
  %2269 = shl i32 %2259, 7
  %2270 = or i32 %2268, %2269
  %2271 = xor i32 %2267, %2270
  %2272 = xor i32 %2202, %2145
  %2273 = and i32 %2259, %2272
  %2274 = xor i32 %2273, %2145
  %2275 = lshr i32 %2181, 17
  %2276 = shl i32 %2181, 15
  %2277 = or i32 %2275, %2276
  %2278 = lshr i32 %2181, 19
  %2279 = shl i32 %2181, 13
  %2280 = or i32 %2278, %2279
  %2281 = lshr i32 %2181, 10
  %2282 = xor i32 %2280, %2281
  %2283 = xor i32 %2282, %2277
  %2284 = lshr i32 %1440, 7
  %2285 = shl i32 %1440, 25
  %2286 = or i32 %2284, %2285
  %2287 = lshr i32 %1440, 18
  %2288 = shl i32 %1440, 14
  %2289 = or i32 %2287, %2288
  %2290 = lshr i32 %1440, 3
  %2291 = xor i32 %2289, %2290
  %2292 = xor i32 %2291, %2286
  %2293 = add i32 %2292, %1383
  %2294 = add i32 %2293, %1896
  %2295 = add i32 %2294, %2283
  %2296 = add i32 %2295, -1564481375
  %2297 = add i32 %2296, %2088
  %2298 = add i32 %2297, %2274
  %2299 = add i32 %2298, %2271
  %2300 = lshr i32 %2260, 2
  %2301 = shl i32 %2260, 30
  %2302 = or i32 %2300, %2301
  %2303 = lshr i32 %2260, 13
  %2304 = shl i32 %2260, 19
  %2305 = or i32 %2303, %2304
  %2306 = xor i32 %2302, %2305
  %2307 = lshr i32 %2260, 22
  %2308 = shl i32 %2260, 10
  %2309 = or i32 %2307, %2308
  %2310 = xor i32 %2306, %2309
  %2311 = and i32 %2260, %2203
  %2312 = or i32 %2260, %2203
  %2313 = and i32 %2312, %2146
  %2314 = or i32 %2313, %2311
  %2315 = add i32 %2310, %2314
  %2316 = add i32 %2299, %2089
  %2317 = add i32 %2315, %2299
  %2318 = lshr i32 %2316, 6
  %2319 = shl i32 %2316, 26
  %2320 = or i32 %2318, %2319
  %2321 = lshr i32 %2316, 11
  %2322 = shl i32 %2316, 21
  %2323 = or i32 %2321, %2322
  %2324 = xor i32 %2320, %2323
  %2325 = lshr i32 %2316, 25
  %2326 = shl i32 %2316, 7
  %2327 = or i32 %2325, %2326
  %2328 = xor i32 %2324, %2327
  %2329 = xor i32 %2259, %2202
  %2330 = and i32 %2316, %2329
  %2331 = xor i32 %2330, %2202
  %2332 = lshr i32 %2238, 17
  %2333 = shl i32 %2238, 15
  %2334 = or i32 %2332, %2333
  %2335 = lshr i32 %2238, 19
  %2336 = shl i32 %2238, 13
  %2337 = or i32 %2335, %2336
  %2338 = lshr i32 %2238, 10
  %2339 = xor i32 %2337, %2338
  %2340 = xor i32 %2339, %2334
  %2341 = lshr i32 %1497, 7
  %2342 = shl i32 %1497, 25
  %2343 = or i32 %2341, %2342
  %2344 = lshr i32 %1497, 18
  %2345 = shl i32 %1497, 14
  %2346 = or i32 %2344, %2345
  %2347 = lshr i32 %1497, 3
  %2348 = xor i32 %2346, %2347
  %2349 = xor i32 %2348, %2343
  %2350 = add i32 %2349, %1440
  %2351 = add i32 %2350, %1953
  %2352 = add i32 %2351, %2340
  %2353 = add i32 %2352, -1474664885
  %2354 = add i32 %2353, %2145
  %2355 = add i32 %2354, %2331
  %2356 = add i32 %2355, %2328
  %2357 = lshr i32 %2317, 2
  %2358 = shl i32 %2317, 30
  %2359 = or i32 %2357, %2358
  %2360 = lshr i32 %2317, 13
  %2361 = shl i32 %2317, 19
  %2362 = or i32 %2360, %2361
  %2363 = xor i32 %2359, %2362
  %2364 = lshr i32 %2317, 22
  %2365 = shl i32 %2317, 10
  %2366 = or i32 %2364, %2365
  %2367 = xor i32 %2363, %2366
  %2368 = and i32 %2317, %2260
  %2369 = or i32 %2317, %2260
  %2370 = and i32 %2369, %2203
  %2371 = or i32 %2370, %2368
  %2372 = add i32 %2367, %2371
  %2373 = add i32 %2356, %2146
  %2374 = add i32 %2372, %2356
  %2375 = lshr i32 %2373, 6
  %2376 = shl i32 %2373, 26
  %2377 = or i32 %2375, %2376
  %2378 = lshr i32 %2373, 11
  %2379 = shl i32 %2373, 21
  %2380 = or i32 %2378, %2379
  %2381 = xor i32 %2377, %2380
  %2382 = lshr i32 %2373, 25
  %2383 = shl i32 %2373, 7
  %2384 = or i32 %2382, %2383
  %2385 = xor i32 %2381, %2384
  %2386 = xor i32 %2316, %2259
  %2387 = and i32 %2373, %2386
  %2388 = xor i32 %2387, %2259
  %2389 = lshr i32 %2295, 17
  %2390 = shl i32 %2295, 15
  %2391 = or i32 %2389, %2390
  %2392 = lshr i32 %2295, 19
  %2393 = shl i32 %2295, 13
  %2394 = or i32 %2392, %2393
  %2395 = lshr i32 %2295, 10
  %2396 = xor i32 %2394, %2395
  %2397 = xor i32 %2396, %2391
  %2398 = lshr i32 %1554, 7
  %2399 = shl i32 %1554, 25
  %2400 = or i32 %2398, %2399
  %2401 = lshr i32 %1554, 18
  %2402 = shl i32 %1554, 14
  %2403 = or i32 %2401, %2402
  %2404 = lshr i32 %1554, 3
  %2405 = xor i32 %2403, %2404
  %2406 = xor i32 %2405, %2400
  %2407 = add i32 %2406, %1497
  %2408 = add i32 %2407, %2010
  %2409 = add i32 %2408, %2397
  %2410 = add i32 %2409, -1035236496
  %2411 = add i32 %2410, %2202
  %2412 = add i32 %2411, %2388
  %2413 = add i32 %2412, %2385
  %2414 = lshr i32 %2374, 2
  %2415 = shl i32 %2374, 30
  %2416 = or i32 %2414, %2415
  %2417 = lshr i32 %2374, 13
  %2418 = shl i32 %2374, 19
  %2419 = or i32 %2417, %2418
  %2420 = xor i32 %2416, %2419
  %2421 = lshr i32 %2374, 22
  %2422 = shl i32 %2374, 10
  %2423 = or i32 %2421, %2422
  %2424 = xor i32 %2420, %2423
  %2425 = and i32 %2374, %2317
  %2426 = or i32 %2374, %2317
  %2427 = and i32 %2426, %2260
  %2428 = or i32 %2427, %2425
  %2429 = add i32 %2424, %2428
  %2430 = add i32 %2413, %2203
  %2431 = add i32 %2429, %2413
  %2432 = lshr i32 %2430, 6
  %2433 = shl i32 %2430, 26
  %2434 = or i32 %2432, %2433
  %2435 = lshr i32 %2430, 11
  %2436 = shl i32 %2430, 21
  %2437 = or i32 %2435, %2436
  %2438 = xor i32 %2434, %2437
  %2439 = lshr i32 %2430, 25
  %2440 = shl i32 %2430, 7
  %2441 = or i32 %2439, %2440
  %2442 = xor i32 %2438, %2441
  %2443 = xor i32 %2373, %2316
  %2444 = and i32 %2430, %2443
  %2445 = xor i32 %2444, %2316
  %2446 = lshr i32 %2352, 17
  %2447 = shl i32 %2352, 15
  %2448 = or i32 %2446, %2447
  %2449 = lshr i32 %2352, 19
  %2450 = shl i32 %2352, 13
  %2451 = or i32 %2449, %2450
  %2452 = lshr i32 %2352, 10
  %2453 = xor i32 %2451, %2452
  %2454 = xor i32 %2453, %2448
  %2455 = lshr i32 %1611, 7
  %2456 = shl i32 %1611, 25
  %2457 = or i32 %2455, %2456
  %2458 = lshr i32 %1611, 18
  %2459 = shl i32 %1611, 14
  %2460 = or i32 %2458, %2459
  %2461 = lshr i32 %1611, 3
  %2462 = xor i32 %2460, %2461
  %2463 = xor i32 %2462, %2457
  %2464 = add i32 %2463, %1554
  %2465 = add i32 %2464, %2067
  %2466 = add i32 %2465, %2454
  %2467 = add i32 %2466, -949202525
  %2468 = add i32 %2467, %2259
  %2469 = add i32 %2468, %2445
  %2470 = add i32 %2469, %2442
  %2471 = lshr i32 %2431, 2
  %2472 = shl i32 %2431, 30
  %2473 = or i32 %2471, %2472
  %2474 = lshr i32 %2431, 13
  %2475 = shl i32 %2431, 19
  %2476 = or i32 %2474, %2475
  %2477 = xor i32 %2473, %2476
  %2478 = lshr i32 %2431, 22
  %2479 = shl i32 %2431, 10
  %2480 = or i32 %2478, %2479
  %2481 = xor i32 %2477, %2480
  %2482 = and i32 %2431, %2374
  %2483 = or i32 %2431, %2374
  %2484 = and i32 %2483, %2317
  %2485 = or i32 %2484, %2482
  %2486 = add i32 %2481, %2485
  %2487 = add i32 %2470, %2260
  %2488 = add i32 %2486, %2470
  %2489 = lshr i32 %2487, 6
  %2490 = shl i32 %2487, 26
  %2491 = or i32 %2489, %2490
  %2492 = lshr i32 %2487, 11
  %2493 = shl i32 %2487, 21
  %2494 = or i32 %2492, %2493
  %2495 = xor i32 %2491, %2494
  %2496 = lshr i32 %2487, 25
  %2497 = shl i32 %2487, 7
  %2498 = or i32 %2496, %2497
  %2499 = xor i32 %2495, %2498
  %2500 = xor i32 %2430, %2373
  %2501 = and i32 %2487, %2500
  %2502 = xor i32 %2501, %2373
  %2503 = lshr i32 %2409, 17
  %2504 = shl i32 %2409, 15
  %2505 = or i32 %2503, %2504
  %2506 = lshr i32 %2409, 19
  %2507 = shl i32 %2409, 13
  %2508 = or i32 %2506, %2507
  %2509 = lshr i32 %2409, 10
  %2510 = xor i32 %2508, %2509
  %2511 = xor i32 %2510, %2505
  %2512 = lshr i32 %1668, 7
  %2513 = shl i32 %1668, 25
  %2514 = or i32 %2512, %2513
  %2515 = lshr i32 %1668, 18
  %2516 = shl i32 %1668, 14
  %2517 = or i32 %2515, %2516
  %2518 = lshr i32 %1668, 3
  %2519 = xor i32 %2517, %2518
  %2520 = xor i32 %2519, %2514
  %2521 = add i32 %2520, %1611
  %2522 = add i32 %2521, %2124
  %2523 = add i32 %2522, %2511
  %2524 = add i32 %2523, -778901479
  %2525 = add i32 %2524, %2316
  %2526 = add i32 %2525, %2502
  %2527 = add i32 %2526, %2499
  %2528 = lshr i32 %2488, 2
  %2529 = shl i32 %2488, 30
  %2530 = or i32 %2528, %2529
  %2531 = lshr i32 %2488, 13
  %2532 = shl i32 %2488, 19
  %2533 = or i32 %2531, %2532
  %2534 = xor i32 %2530, %2533
  %2535 = lshr i32 %2488, 22
  %2536 = shl i32 %2488, 10
  %2537 = or i32 %2535, %2536
  %2538 = xor i32 %2534, %2537
  %2539 = and i32 %2488, %2431
  %2540 = or i32 %2488, %2431
  %2541 = and i32 %2540, %2374
  %2542 = or i32 %2541, %2539
  %2543 = add i32 %2538, %2542
  %2544 = add i32 %2527, %2317
  %2545 = add i32 %2543, %2527
  %2546 = lshr i32 %2544, 6
  %2547 = shl i32 %2544, 26
  %2548 = or i32 %2546, %2547
  %2549 = lshr i32 %2544, 11
  %2550 = shl i32 %2544, 21
  %2551 = or i32 %2549, %2550
  %2552 = xor i32 %2548, %2551
  %2553 = lshr i32 %2544, 25
  %2554 = shl i32 %2544, 7
  %2555 = or i32 %2553, %2554
  %2556 = xor i32 %2552, %2555
  %2557 = xor i32 %2487, %2430
  %2558 = and i32 %2544, %2557
  %2559 = xor i32 %2558, %2430
  %2560 = lshr i32 %2466, 17
  %2561 = shl i32 %2466, 15
  %2562 = or i32 %2560, %2561
  %2563 = lshr i32 %2466, 19
  %2564 = shl i32 %2466, 13
  %2565 = or i32 %2563, %2564
  %2566 = lshr i32 %2466, 10
  %2567 = xor i32 %2565, %2566
  %2568 = xor i32 %2567, %2562
  %2569 = lshr i32 %1725, 7
  %2570 = shl i32 %1725, 25
  %2571 = or i32 %2569, %2570
  %2572 = lshr i32 %1725, 18
  %2573 = shl i32 %1725, 14
  %2574 = or i32 %2572, %2573
  %2575 = lshr i32 %1725, 3
  %2576 = xor i32 %2574, %2575
  %2577 = xor i32 %2576, %2571
  %2578 = add i32 %2577, %1668
  %2579 = add i32 %2578, %2181
  %2580 = add i32 %2579, %2568
  %2581 = add i32 %2580, -694614492
  %2582 = add i32 %2581, %2373
  %2583 = add i32 %2582, %2559
  %2584 = add i32 %2583, %2556
  %2585 = lshr i32 %2545, 2
  %2586 = shl i32 %2545, 30
  %2587 = or i32 %2585, %2586
  %2588 = lshr i32 %2545, 13
  %2589 = shl i32 %2545, 19
  %2590 = or i32 %2588, %2589
  %2591 = xor i32 %2587, %2590
  %2592 = lshr i32 %2545, 22
  %2593 = shl i32 %2545, 10
  %2594 = or i32 %2592, %2593
  %2595 = xor i32 %2591, %2594
  %2596 = and i32 %2545, %2488
  %2597 = or i32 %2545, %2488
  %2598 = and i32 %2597, %2431
  %2599 = or i32 %2598, %2596
  %2600 = add i32 %2595, %2599
  %2601 = add i32 %2584, %2374
  %2602 = add i32 %2600, %2584
  %2603 = lshr i32 %2601, 6
  %2604 = shl i32 %2601, 26
  %2605 = or i32 %2603, %2604
  %2606 = lshr i32 %2601, 11
  %2607 = shl i32 %2601, 21
  %2608 = or i32 %2606, %2607
  %2609 = xor i32 %2605, %2608
  %2610 = lshr i32 %2601, 25
  %2611 = shl i32 %2601, 7
  %2612 = or i32 %2610, %2611
  %2613 = xor i32 %2609, %2612
  %2614 = xor i32 %2544, %2487
  %2615 = and i32 %2601, %2614
  %2616 = xor i32 %2615, %2487
  %2617 = lshr i32 %2523, 17
  %2618 = shl i32 %2523, 15
  %2619 = or i32 %2617, %2618
  %2620 = lshr i32 %2523, 19
  %2621 = shl i32 %2523, 13
  %2622 = or i32 %2620, %2621
  %2623 = lshr i32 %2523, 10
  %2624 = xor i32 %2622, %2623
  %2625 = xor i32 %2624, %2619
  %2626 = lshr i32 %1782, 7
  %2627 = shl i32 %1782, 25
  %2628 = or i32 %2626, %2627
  %2629 = lshr i32 %1782, 18
  %2630 = shl i32 %1782, 14
  %2631 = or i32 %2629, %2630
  %2632 = lshr i32 %1782, 3
  %2633 = xor i32 %2631, %2632
  %2634 = xor i32 %2633, %2628
  %2635 = add i32 %2634, %1725
  %2636 = add i32 %2635, %2238
  %2637 = add i32 %2636, %2625
  %2638 = add i32 %2637, -200395387
  %2639 = add i32 %2638, %2430
  %2640 = add i32 %2639, %2616
  %2641 = add i32 %2640, %2613
  %2642 = lshr i32 %2602, 2
  %2643 = shl i32 %2602, 30
  %2644 = or i32 %2642, %2643
  %2645 = lshr i32 %2602, 13
  %2646 = shl i32 %2602, 19
  %2647 = or i32 %2645, %2646
  %2648 = xor i32 %2644, %2647
  %2649 = lshr i32 %2602, 22
  %2650 = shl i32 %2602, 10
  %2651 = or i32 %2649, %2650
  %2652 = xor i32 %2648, %2651
  %2653 = and i32 %2602, %2545
  %2654 = or i32 %2602, %2545
  %2655 = and i32 %2654, %2488
  %2656 = or i32 %2655, %2653
  %2657 = add i32 %2652, %2656
  %2658 = add i32 %2641, %2431
  %2659 = add i32 %2657, %2641
  %2660 = lshr i32 %2658, 6
  %2661 = shl i32 %2658, 26
  %2662 = or i32 %2660, %2661
  %2663 = lshr i32 %2658, 11
  %2664 = shl i32 %2658, 21
  %2665 = or i32 %2663, %2664
  %2666 = xor i32 %2662, %2665
  %2667 = lshr i32 %2658, 25
  %2668 = shl i32 %2658, 7
  %2669 = or i32 %2667, %2668
  %2670 = xor i32 %2666, %2669
  %2671 = xor i32 %2601, %2544
  %2672 = and i32 %2658, %2671
  %2673 = xor i32 %2672, %2544
  %2674 = lshr i32 %2580, 17
  %2675 = shl i32 %2580, 15
  %2676 = or i32 %2674, %2675
  %2677 = lshr i32 %2580, 19
  %2678 = shl i32 %2580, 13
  %2679 = or i32 %2677, %2678
  %2680 = lshr i32 %2580, 10
  %2681 = xor i32 %2679, %2680
  %2682 = xor i32 %2681, %2676
  %2683 = lshr i32 %1839, 7
  %2684 = shl i32 %1839, 25
  %2685 = or i32 %2683, %2684
  %2686 = lshr i32 %1839, 18
  %2687 = shl i32 %1839, 14
  %2688 = or i32 %2686, %2687
  %2689 = lshr i32 %1839, 3
  %2690 = xor i32 %2688, %2689
  %2691 = xor i32 %2690, %2685
  %2692 = add i32 %2691, %1782
  %2693 = add i32 %2692, %2295
  %2694 = add i32 %2693, %2682
  %2695 = add i32 %2694, 275423344
  %2696 = add i32 %2695, %2487
  %2697 = add i32 %2696, %2673
  %2698 = add i32 %2697, %2670
  %2699 = lshr i32 %2659, 2
  %2700 = shl i32 %2659, 30
  %2701 = or i32 %2699, %2700
  %2702 = lshr i32 %2659, 13
  %2703 = shl i32 %2659, 19
  %2704 = or i32 %2702, %2703
  %2705 = xor i32 %2701, %2704
  %2706 = lshr i32 %2659, 22
  %2707 = shl i32 %2659, 10
  %2708 = or i32 %2706, %2707
  %2709 = xor i32 %2705, %2708
  %2710 = and i32 %2659, %2602
  %2711 = or i32 %2659, %2602
  %2712 = and i32 %2711, %2545
  %2713 = or i32 %2712, %2710
  %2714 = add i32 %2709, %2713
  %2715 = add i32 %2698, %2488
  %2716 = add i32 %2714, %2698
  %2717 = lshr i32 %2715, 6
  %2718 = shl i32 %2715, 26
  %2719 = or i32 %2717, %2718
  %2720 = lshr i32 %2715, 11
  %2721 = shl i32 %2715, 21
  %2722 = or i32 %2720, %2721
  %2723 = xor i32 %2719, %2722
  %2724 = lshr i32 %2715, 25
  %2725 = shl i32 %2715, 7
  %2726 = or i32 %2724, %2725
  %2727 = xor i32 %2723, %2726
  %2728 = xor i32 %2658, %2601
  %2729 = and i32 %2715, %2728
  %2730 = xor i32 %2729, %2601
  %2731 = lshr i32 %2637, 17
  %2732 = shl i32 %2637, 15
  %2733 = or i32 %2731, %2732
  %2734 = lshr i32 %2637, 19
  %2735 = shl i32 %2637, 13
  %2736 = or i32 %2734, %2735
  %2737 = lshr i32 %2637, 10
  %2738 = xor i32 %2736, %2737
  %2739 = xor i32 %2738, %2733
  %2740 = lshr i32 %1896, 7
  %2741 = shl i32 %1896, 25
  %2742 = or i32 %2740, %2741
  %2743 = lshr i32 %1896, 18
  %2744 = shl i32 %1896, 14
  %2745 = or i32 %2743, %2744
  %2746 = lshr i32 %1896, 3
  %2747 = xor i32 %2745, %2746
  %2748 = xor i32 %2747, %2742
  %2749 = add i32 %2748, %1839
  %2750 = add i32 %2749, %2352
  %2751 = add i32 %2750, %2739
  %2752 = add i32 %2751, 430227734
  %2753 = add i32 %2752, %2544
  %2754 = add i32 %2753, %2730
  %2755 = add i32 %2754, %2727
  %2756 = lshr i32 %2716, 2
  %2757 = shl i32 %2716, 30
  %2758 = or i32 %2756, %2757
  %2759 = lshr i32 %2716, 13
  %2760 = shl i32 %2716, 19
  %2761 = or i32 %2759, %2760
  %2762 = xor i32 %2758, %2761
  %2763 = lshr i32 %2716, 22
  %2764 = shl i32 %2716, 10
  %2765 = or i32 %2763, %2764
  %2766 = xor i32 %2762, %2765
  %2767 = and i32 %2716, %2659
  %2768 = or i32 %2716, %2659
  %2769 = and i32 %2768, %2602
  %2770 = or i32 %2769, %2767
  %2771 = add i32 %2766, %2770
  %2772 = add i32 %2755, %2545
  %2773 = add i32 %2771, %2755
  %2774 = lshr i32 %2772, 6
  %2775 = shl i32 %2772, 26
  %2776 = or i32 %2774, %2775
  %2777 = lshr i32 %2772, 11
  %2778 = shl i32 %2772, 21
  %2779 = or i32 %2777, %2778
  %2780 = xor i32 %2776, %2779
  %2781 = lshr i32 %2772, 25
  %2782 = shl i32 %2772, 7
  %2783 = or i32 %2781, %2782
  %2784 = xor i32 %2780, %2783
  %2785 = xor i32 %2715, %2658
  %2786 = and i32 %2772, %2785
  %2787 = xor i32 %2786, %2658
  %2788 = lshr i32 %2694, 17
  %2789 = shl i32 %2694, 15
  %2790 = or i32 %2788, %2789
  %2791 = lshr i32 %2694, 19
  %2792 = shl i32 %2694, 13
  %2793 = or i32 %2791, %2792
  %2794 = lshr i32 %2694, 10
  %2795 = xor i32 %2793, %2794
  %2796 = xor i32 %2795, %2790
  %2797 = lshr i32 %1953, 7
  %2798 = shl i32 %1953, 25
  %2799 = or i32 %2797, %2798
  %2800 = lshr i32 %1953, 18
  %2801 = shl i32 %1953, 14
  %2802 = or i32 %2800, %2801
  %2803 = lshr i32 %1953, 3
  %2804 = xor i32 %2802, %2803
  %2805 = xor i32 %2804, %2799
  %2806 = add i32 %2805, %1896
  %2807 = add i32 %2806, %2409
  %2808 = add i32 %2807, %2796
  %2809 = add i32 %2808, 506948616
  %2810 = add i32 %2809, %2601
  %2811 = add i32 %2810, %2787
  %2812 = add i32 %2811, %2784
  %2813 = lshr i32 %2773, 2
  %2814 = shl i32 %2773, 30
  %2815 = or i32 %2813, %2814
  %2816 = lshr i32 %2773, 13
  %2817 = shl i32 %2773, 19
  %2818 = or i32 %2816, %2817
  %2819 = xor i32 %2815, %2818
  %2820 = lshr i32 %2773, 22
  %2821 = shl i32 %2773, 10
  %2822 = or i32 %2820, %2821
  %2823 = xor i32 %2819, %2822
  %2824 = and i32 %2773, %2716
  %2825 = or i32 %2773, %2716
  %2826 = and i32 %2825, %2659
  %2827 = or i32 %2826, %2824
  %2828 = add i32 %2823, %2827
  %2829 = add i32 %2812, %2602
  %2830 = add i32 %2828, %2812
  %2831 = lshr i32 %2829, 6
  %2832 = shl i32 %2829, 26
  %2833 = or i32 %2831, %2832
  %2834 = lshr i32 %2829, 11
  %2835 = shl i32 %2829, 21
  %2836 = or i32 %2834, %2835
  %2837 = xor i32 %2833, %2836
  %2838 = lshr i32 %2829, 25
  %2839 = shl i32 %2829, 7
  %2840 = or i32 %2838, %2839
  %2841 = xor i32 %2837, %2840
  %2842 = xor i32 %2772, %2715
  %2843 = and i32 %2829, %2842
  %2844 = xor i32 %2843, %2715
  %2845 = lshr i32 %2751, 17
  %2846 = shl i32 %2751, 15
  %2847 = or i32 %2845, %2846
  %2848 = lshr i32 %2751, 19
  %2849 = shl i32 %2751, 13
  %2850 = or i32 %2848, %2849
  %2851 = lshr i32 %2751, 10
  %2852 = xor i32 %2850, %2851
  %2853 = xor i32 %2852, %2847
  %2854 = lshr i32 %2010, 7
  %2855 = shl i32 %2010, 25
  %2856 = or i32 %2854, %2855
  %2857 = lshr i32 %2010, 18
  %2858 = shl i32 %2010, 14
  %2859 = or i32 %2857, %2858
  %2860 = lshr i32 %2010, 3
  %2861 = xor i32 %2859, %2860
  %2862 = xor i32 %2861, %2856
  %2863 = add i32 %2862, %1953
  %2864 = add i32 %2863, %2466
  %2865 = add i32 %2864, %2853
  %2866 = add i32 %2865, 659060556
  %2867 = add i32 %2866, %2658
  %2868 = add i32 %2867, %2844
  %2869 = add i32 %2868, %2841
  %2870 = lshr i32 %2830, 2
  %2871 = shl i32 %2830, 30
  %2872 = or i32 %2870, %2871
  %2873 = lshr i32 %2830, 13
  %2874 = shl i32 %2830, 19
  %2875 = or i32 %2873, %2874
  %2876 = xor i32 %2872, %2875
  %2877 = lshr i32 %2830, 22
  %2878 = shl i32 %2830, 10
  %2879 = or i32 %2877, %2878
  %2880 = xor i32 %2876, %2879
  %2881 = and i32 %2830, %2773
  %2882 = or i32 %2830, %2773
  %2883 = and i32 %2882, %2716
  %2884 = or i32 %2883, %2881
  %2885 = add i32 %2880, %2884
  %2886 = add i32 %2869, %2659
  %2887 = add i32 %2885, %2869
  %2888 = lshr i32 %2886, 6
  %2889 = shl i32 %2886, 26
  %2890 = or i32 %2888, %2889
  %2891 = lshr i32 %2886, 11
  %2892 = shl i32 %2886, 21
  %2893 = or i32 %2891, %2892
  %2894 = xor i32 %2890, %2893
  %2895 = lshr i32 %2886, 25
  %2896 = shl i32 %2886, 7
  %2897 = or i32 %2895, %2896
  %2898 = xor i32 %2894, %2897
  %2899 = xor i32 %2829, %2772
  %2900 = and i32 %2886, %2899
  %2901 = xor i32 %2900, %2772
  %2902 = lshr i32 %2808, 17
  %2903 = shl i32 %2808, 15
  %2904 = or i32 %2902, %2903
  %2905 = lshr i32 %2808, 19
  %2906 = shl i32 %2808, 13
  %2907 = or i32 %2905, %2906
  %2908 = lshr i32 %2808, 10
  %2909 = xor i32 %2907, %2908
  %2910 = xor i32 %2909, %2904
  %2911 = lshr i32 %2067, 7
  %2912 = shl i32 %2067, 25
  %2913 = or i32 %2911, %2912
  %2914 = lshr i32 %2067, 18
  %2915 = shl i32 %2067, 14
  %2916 = or i32 %2914, %2915
  %2917 = lshr i32 %2067, 3
  %2918 = xor i32 %2916, %2917
  %2919 = xor i32 %2918, %2913
  %2920 = add i32 %2919, %2010
  %2921 = add i32 %2920, %2523
  %2922 = add i32 %2921, %2910
  %2923 = add i32 %2922, 883997877
  %2924 = add i32 %2923, %2715
  %2925 = add i32 %2924, %2901
  %2926 = add i32 %2925, %2898
  %2927 = lshr i32 %2887, 2
  %2928 = shl i32 %2887, 30
  %2929 = or i32 %2927, %2928
  %2930 = lshr i32 %2887, 13
  %2931 = shl i32 %2887, 19
  %2932 = or i32 %2930, %2931
  %2933 = xor i32 %2929, %2932
  %2934 = lshr i32 %2887, 22
  %2935 = shl i32 %2887, 10
  %2936 = or i32 %2934, %2935
  %2937 = xor i32 %2933, %2936
  %2938 = and i32 %2887, %2830
  %2939 = or i32 %2887, %2830
  %2940 = and i32 %2939, %2773
  %2941 = or i32 %2940, %2938
  %2942 = add i32 %2937, %2941
  %2943 = add i32 %2926, %2716
  %2944 = add i32 %2942, %2926
  %2945 = lshr i32 %2943, 6
  %2946 = shl i32 %2943, 26
  %2947 = or i32 %2945, %2946
  %2948 = lshr i32 %2943, 11
  %2949 = shl i32 %2943, 21
  %2950 = or i32 %2948, %2949
  %2951 = xor i32 %2947, %2950
  %2952 = lshr i32 %2943, 25
  %2953 = shl i32 %2943, 7
  %2954 = or i32 %2952, %2953
  %2955 = xor i32 %2951, %2954
  %2956 = xor i32 %2886, %2829
  %2957 = and i32 %2943, %2956
  %2958 = xor i32 %2957, %2829
  %2959 = lshr i32 %2865, 17
  %2960 = shl i32 %2865, 15
  %2961 = or i32 %2959, %2960
  %2962 = lshr i32 %2865, 19
  %2963 = shl i32 %2865, 13
  %2964 = or i32 %2962, %2963
  %2965 = lshr i32 %2865, 10
  %2966 = xor i32 %2964, %2965
  %2967 = xor i32 %2966, %2961
  %2968 = lshr i32 %2124, 7
  %2969 = shl i32 %2124, 25
  %2970 = or i32 %2968, %2969
  %2971 = lshr i32 %2124, 18
  %2972 = shl i32 %2124, 14
  %2973 = or i32 %2971, %2972
  %2974 = lshr i32 %2124, 3
  %2975 = xor i32 %2973, %2974
  %2976 = xor i32 %2975, %2970
  %2977 = add i32 %2976, %2067
  %2978 = add i32 %2977, %2580
  %2979 = add i32 %2978, %2967
  %2980 = add i32 %2979, 958139571
  %2981 = add i32 %2980, %2772
  %2982 = add i32 %2981, %2958
  %2983 = add i32 %2982, %2955
  %2984 = lshr i32 %2944, 2
  %2985 = shl i32 %2944, 30
  %2986 = or i32 %2984, %2985
  %2987 = lshr i32 %2944, 13
  %2988 = shl i32 %2944, 19
  %2989 = or i32 %2987, %2988
  %2990 = xor i32 %2986, %2989
  %2991 = lshr i32 %2944, 22
  %2992 = shl i32 %2944, 10
  %2993 = or i32 %2991, %2992
  %2994 = xor i32 %2990, %2993
  %2995 = and i32 %2944, %2887
  %2996 = or i32 %2944, %2887
  %2997 = and i32 %2996, %2830
  %2998 = or i32 %2997, %2995
  %2999 = add i32 %2994, %2998
  %3000 = add i32 %2983, %2773
  %3001 = add i32 %2999, %2983
  %3002 = lshr i32 %3000, 6
  %3003 = shl i32 %3000, 26
  %3004 = or i32 %3002, %3003
  %3005 = lshr i32 %3000, 11
  %3006 = shl i32 %3000, 21
  %3007 = or i32 %3005, %3006
  %3008 = xor i32 %3004, %3007
  %3009 = lshr i32 %3000, 25
  %3010 = shl i32 %3000, 7
  %3011 = or i32 %3009, %3010
  %3012 = xor i32 %3008, %3011
  %3013 = xor i32 %2943, %2886
  %3014 = and i32 %3000, %3013
  %3015 = xor i32 %3014, %2886
  %3016 = lshr i32 %2922, 17
  %3017 = shl i32 %2922, 15
  %3018 = or i32 %3016, %3017
  %3019 = lshr i32 %2922, 19
  %3020 = shl i32 %2922, 13
  %3021 = or i32 %3019, %3020
  %3022 = lshr i32 %2922, 10
  %3023 = xor i32 %3021, %3022
  %3024 = xor i32 %3023, %3018
  %3025 = lshr i32 %2181, 7
  %3026 = shl i32 %2181, 25
  %3027 = or i32 %3025, %3026
  %3028 = lshr i32 %2181, 18
  %3029 = shl i32 %2181, 14
  %3030 = or i32 %3028, %3029
  %3031 = lshr i32 %2181, 3
  %3032 = xor i32 %3030, %3031
  %3033 = xor i32 %3032, %3027
  %3034 = add i32 %3033, %2124
  %3035 = add i32 %3034, %2637
  %3036 = add i32 %3035, %3024
  %3037 = add i32 %3036, 1322822218
  %3038 = add i32 %3037, %2829
  %3039 = add i32 %3038, %3015
  %3040 = add i32 %3039, %3012
  %3041 = lshr i32 %3001, 2
  %3042 = shl i32 %3001, 30
  %3043 = or i32 %3041, %3042
  %3044 = lshr i32 %3001, 13
  %3045 = shl i32 %3001, 19
  %3046 = or i32 %3044, %3045
  %3047 = xor i32 %3043, %3046
  %3048 = lshr i32 %3001, 22
  %3049 = shl i32 %3001, 10
  %3050 = or i32 %3048, %3049
  %3051 = xor i32 %3047, %3050
  %3052 = and i32 %3001, %2944
  %3053 = or i32 %3001, %2944
  %3054 = and i32 %3053, %2887
  %3055 = or i32 %3054, %3052
  %3056 = add i32 %3051, %3055
  %3057 = add i32 %3040, %2830
  %3058 = add i32 %3056, %3040
  %3059 = lshr i32 %3057, 6
  %3060 = shl i32 %3057, 26
  %3061 = or i32 %3059, %3060
  %3062 = lshr i32 %3057, 11
  %3063 = shl i32 %3057, 21
  %3064 = or i32 %3062, %3063
  %3065 = xor i32 %3061, %3064
  %3066 = lshr i32 %3057, 25
  %3067 = shl i32 %3057, 7
  %3068 = or i32 %3066, %3067
  %3069 = xor i32 %3065, %3068
  %3070 = xor i32 %3000, %2943
  %3071 = and i32 %3057, %3070
  %3072 = xor i32 %3071, %2943
  %3073 = lshr i32 %2979, 17
  %3074 = shl i32 %2979, 15
  %3075 = or i32 %3073, %3074
  %3076 = lshr i32 %2979, 19
  %3077 = shl i32 %2979, 13
  %3078 = or i32 %3076, %3077
  %3079 = lshr i32 %2979, 10
  %3080 = xor i32 %3078, %3079
  %3081 = xor i32 %3080, %3075
  %3082 = lshr i32 %2238, 7
  %3083 = shl i32 %2238, 25
  %3084 = or i32 %3082, %3083
  %3085 = lshr i32 %2238, 18
  %3086 = shl i32 %2238, 14
  %3087 = or i32 %3085, %3086
  %3088 = lshr i32 %2238, 3
  %3089 = xor i32 %3087, %3088
  %3090 = xor i32 %3089, %3084
  %3091 = add i32 %3090, %2181
  %3092 = add i32 %3091, %2694
  %3093 = add i32 %3092, %3081
  %3094 = add i32 %3093, 1537002063
  %3095 = add i32 %3094, %2886
  %3096 = add i32 %3095, %3072
  %3097 = add i32 %3096, %3069
  %3098 = lshr i32 %3058, 2
  %3099 = shl i32 %3058, 30
  %3100 = or i32 %3098, %3099
  %3101 = lshr i32 %3058, 13
  %3102 = shl i32 %3058, 19
  %3103 = or i32 %3101, %3102
  %3104 = xor i32 %3100, %3103
  %3105 = lshr i32 %3058, 22
  %3106 = shl i32 %3058, 10
  %3107 = or i32 %3105, %3106
  %3108 = xor i32 %3104, %3107
  %3109 = and i32 %3058, %3001
  %3110 = or i32 %3058, %3001
  %3111 = and i32 %3110, %2944
  %3112 = or i32 %3111, %3109
  %3113 = add i32 %3108, %3112
  %3114 = add i32 %3097, %2887
  %3115 = add i32 %3113, %3097
  %3116 = lshr i32 %3114, 6
  %3117 = shl i32 %3114, 26
  %3118 = or i32 %3116, %3117
  %3119 = lshr i32 %3114, 11
  %3120 = shl i32 %3114, 21
  %3121 = or i32 %3119, %3120
  %3122 = xor i32 %3118, %3121
  %3123 = lshr i32 %3114, 25
  %3124 = shl i32 %3114, 7
  %3125 = or i32 %3123, %3124
  %3126 = xor i32 %3122, %3125
  %3127 = xor i32 %3057, %3000
  %3128 = and i32 %3114, %3127
  %3129 = xor i32 %3128, %3000
  %3130 = lshr i32 %3036, 17
  %3131 = shl i32 %3036, 15
  %3132 = or i32 %3130, %3131
  %3133 = lshr i32 %3036, 19
  %3134 = shl i32 %3036, 13
  %3135 = or i32 %3133, %3134
  %3136 = lshr i32 %3036, 10
  %3137 = xor i32 %3135, %3136
  %3138 = xor i32 %3137, %3132
  %3139 = lshr i32 %2295, 7
  %3140 = shl i32 %2295, 25
  %3141 = or i32 %3139, %3140
  %3142 = lshr i32 %2295, 18
  %3143 = shl i32 %2295, 14
  %3144 = or i32 %3142, %3143
  %3145 = lshr i32 %2295, 3
  %3146 = xor i32 %3144, %3145
  %3147 = xor i32 %3146, %3141
  %3148 = add i32 %3147, %2238
  %3149 = add i32 %3148, %2751
  %3150 = add i32 %3149, %3138
  %3151 = add i32 %3150, 1747873779
  %3152 = add i32 %3151, %2943
  %3153 = add i32 %3152, %3129
  %3154 = add i32 %3153, %3126
  %3155 = lshr i32 %3115, 2
  %3156 = shl i32 %3115, 30
  %3157 = or i32 %3155, %3156
  %3158 = lshr i32 %3115, 13
  %3159 = shl i32 %3115, 19
  %3160 = or i32 %3158, %3159
  %3161 = xor i32 %3157, %3160
  %3162 = lshr i32 %3115, 22
  %3163 = shl i32 %3115, 10
  %3164 = or i32 %3162, %3163
  %3165 = xor i32 %3161, %3164
  %3166 = and i32 %3115, %3058
  %3167 = or i32 %3115, %3058
  %3168 = and i32 %3167, %3001
  %3169 = or i32 %3168, %3166
  %3170 = add i32 %3165, %3169
  %3171 = add i32 %3154, %2944
  %3172 = add i32 %3170, %3154
  %3173 = lshr i32 %3171, 6
  %3174 = shl i32 %3171, 26
  %3175 = or i32 %3173, %3174
  %3176 = lshr i32 %3171, 11
  %3177 = shl i32 %3171, 21
  %3178 = or i32 %3176, %3177
  %3179 = xor i32 %3175, %3178
  %3180 = lshr i32 %3171, 25
  %3181 = shl i32 %3171, 7
  %3182 = or i32 %3180, %3181
  %3183 = xor i32 %3179, %3182
  %3184 = xor i32 %3114, %3057
  %3185 = and i32 %3171, %3184
  %3186 = xor i32 %3185, %3057
  %3187 = lshr i32 %3093, 17
  %3188 = shl i32 %3093, 15
  %3189 = or i32 %3187, %3188
  %3190 = lshr i32 %3093, 19
  %3191 = shl i32 %3093, 13
  %3192 = or i32 %3190, %3191
  %3193 = lshr i32 %3093, 10
  %3194 = xor i32 %3192, %3193
  %3195 = xor i32 %3194, %3189
  %3196 = lshr i32 %2352, 7
  %3197 = shl i32 %2352, 25
  %3198 = or i32 %3196, %3197
  %3199 = lshr i32 %2352, 18
  %3200 = shl i32 %2352, 14
  %3201 = or i32 %3199, %3200
  %3202 = lshr i32 %2352, 3
  %3203 = xor i32 %3201, %3202
  %3204 = xor i32 %3203, %3198
  %3205 = add i32 %3204, %2295
  %3206 = add i32 %3205, %2808
  %3207 = add i32 %3206, %3195
  %3208 = add i32 %3207, 1955562222
  %3209 = add i32 %3208, %3000
  %3210 = add i32 %3209, %3186
  %3211 = add i32 %3210, %3183
  %3212 = lshr i32 %3172, 2
  %3213 = shl i32 %3172, 30
  %3214 = or i32 %3212, %3213
  %3215 = lshr i32 %3172, 13
  %3216 = shl i32 %3172, 19
  %3217 = or i32 %3215, %3216
  %3218 = xor i32 %3214, %3217
  %3219 = lshr i32 %3172, 22
  %3220 = shl i32 %3172, 10
  %3221 = or i32 %3219, %3220
  %3222 = xor i32 %3218, %3221
  %3223 = and i32 %3172, %3115
  %3224 = or i32 %3172, %3115
  %3225 = and i32 %3224, %3058
  %3226 = or i32 %3225, %3223
  %3227 = add i32 %3222, %3226
  %3228 = add i32 %3211, %3001
  %3229 = add i32 %3227, %3211
  %3230 = lshr i32 %3228, 6
  %3231 = shl i32 %3228, 26
  %3232 = or i32 %3230, %3231
  %3233 = lshr i32 %3228, 11
  %3234 = shl i32 %3228, 21
  %3235 = or i32 %3233, %3234
  %3236 = xor i32 %3232, %3235
  %3237 = lshr i32 %3228, 25
  %3238 = shl i32 %3228, 7
  %3239 = or i32 %3237, %3238
  %3240 = xor i32 %3236, %3239
  %3241 = xor i32 %3171, %3114
  %3242 = and i32 %3228, %3241
  %3243 = xor i32 %3242, %3114
  %3244 = lshr i32 %3150, 17
  %3245 = shl i32 %3150, 15
  %3246 = or i32 %3244, %3245
  %3247 = lshr i32 %3150, 19
  %3248 = shl i32 %3150, 13
  %3249 = or i32 %3247, %3248
  %3250 = lshr i32 %3150, 10
  %3251 = xor i32 %3249, %3250
  %3252 = xor i32 %3251, %3246
  %3253 = lshr i32 %2409, 7
  %3254 = shl i32 %2409, 25
  %3255 = or i32 %3253, %3254
  %3256 = lshr i32 %2409, 18
  %3257 = shl i32 %2409, 14
  %3258 = or i32 %3256, %3257
  %3259 = lshr i32 %2409, 3
  %3260 = xor i32 %3258, %3259
  %3261 = xor i32 %3260, %3255
  %3262 = add i32 %3261, %2352
  %3263 = add i32 %3262, %2865
  %3264 = add i32 %3263, %3252
  %3265 = add i32 %3264, 2024104815
  %3266 = add i32 %3265, %3057
  %3267 = add i32 %3266, %3243
  %3268 = add i32 %3267, %3240
  %3269 = lshr i32 %3229, 2
  %3270 = shl i32 %3229, 30
  %3271 = or i32 %3269, %3270
  %3272 = lshr i32 %3229, 13
  %3273 = shl i32 %3229, 19
  %3274 = or i32 %3272, %3273
  %3275 = xor i32 %3271, %3274
  %3276 = lshr i32 %3229, 22
  %3277 = shl i32 %3229, 10
  %3278 = or i32 %3276, %3277
  %3279 = xor i32 %3275, %3278
  %3280 = and i32 %3229, %3172
  %3281 = or i32 %3229, %3172
  %3282 = and i32 %3281, %3115
  %3283 = or i32 %3282, %3280
  %3284 = add i32 %3279, %3283
  %3285 = add i32 %3268, %3058
  %3286 = add i32 %3284, %3268
  %3287 = lshr i32 %3285, 6
  %3288 = shl i32 %3285, 26
  %3289 = or i32 %3287, %3288
  %3290 = lshr i32 %3285, 11
  %3291 = shl i32 %3285, 21
  %3292 = or i32 %3290, %3291
  %3293 = xor i32 %3289, %3292
  %3294 = lshr i32 %3285, 25
  %3295 = shl i32 %3285, 7
  %3296 = or i32 %3294, %3295
  %3297 = xor i32 %3293, %3296
  %3298 = xor i32 %3228, %3171
  %3299 = and i32 %3285, %3298
  %3300 = xor i32 %3299, %3171
  %3301 = lshr i32 %3207, 17
  %3302 = shl i32 %3207, 15
  %3303 = or i32 %3301, %3302
  %3304 = lshr i32 %3207, 19
  %3305 = shl i32 %3207, 13
  %3306 = or i32 %3304, %3305
  %3307 = lshr i32 %3207, 10
  %3308 = xor i32 %3306, %3307
  %3309 = xor i32 %3308, %3303
  %3310 = lshr i32 %2466, 7
  %3311 = shl i32 %2466, 25
  %3312 = or i32 %3310, %3311
  %3313 = lshr i32 %2466, 18
  %3314 = shl i32 %2466, 14
  %3315 = or i32 %3313, %3314
  %3316 = lshr i32 %2466, 3
  %3317 = xor i32 %3315, %3316
  %3318 = xor i32 %3317, %3312
  %3319 = add i32 %3318, %2409
  %3320 = add i32 %3319, %2922
  %3321 = add i32 %3320, %3309
  %3322 = add i32 %3321, -2067236844
  %3323 = add i32 %3322, %3114
  %3324 = add i32 %3323, %3300
  %3325 = add i32 %3324, %3297
  %3326 = lshr i32 %3286, 2
  %3327 = shl i32 %3286, 30
  %3328 = or i32 %3326, %3327
  %3329 = lshr i32 %3286, 13
  %3330 = shl i32 %3286, 19
  %3331 = or i32 %3329, %3330
  %3332 = xor i32 %3328, %3331
  %3333 = lshr i32 %3286, 22
  %3334 = shl i32 %3286, 10
  %3335 = or i32 %3333, %3334
  %3336 = xor i32 %3332, %3335
  %3337 = and i32 %3286, %3229
  %3338 = or i32 %3286, %3229
  %3339 = and i32 %3338, %3172
  %3340 = or i32 %3339, %3337
  %3341 = add i32 %3336, %3340
  %3342 = add i32 %3325, %3115
  %3343 = add i32 %3341, %3325
  %3344 = lshr i32 %3342, 6
  %3345 = shl i32 %3342, 26
  %3346 = or i32 %3344, %3345
  %3347 = lshr i32 %3342, 11
  %3348 = shl i32 %3342, 21
  %3349 = or i32 %3347, %3348
  %3350 = xor i32 %3346, %3349
  %3351 = lshr i32 %3342, 25
  %3352 = shl i32 %3342, 7
  %3353 = or i32 %3351, %3352
  %3354 = xor i32 %3350, %3353
  %3355 = xor i32 %3285, %3228
  %3356 = and i32 %3342, %3355
  %3357 = xor i32 %3356, %3228
  %3358 = lshr i32 %3264, 17
  %3359 = shl i32 %3264, 15
  %3360 = or i32 %3358, %3359
  %3361 = lshr i32 %3264, 19
  %3362 = shl i32 %3264, 13
  %3363 = or i32 %3361, %3362
  %3364 = lshr i32 %3264, 10
  %3365 = xor i32 %3363, %3364
  %3366 = xor i32 %3365, %3360
  %3367 = lshr i32 %2523, 7
  %3368 = shl i32 %2523, 25
  %3369 = or i32 %3367, %3368
  %3370 = lshr i32 %2523, 18
  %3371 = shl i32 %2523, 14
  %3372 = or i32 %3370, %3371
  %3373 = lshr i32 %2523, 3
  %3374 = xor i32 %3372, %3373
  %3375 = xor i32 %3374, %3369
  %3376 = add i32 %3375, %2466
  %3377 = add i32 %3376, %2979
  %3378 = add i32 %3377, %3366
  %3379 = add i32 %3378, -1933114872
  %3380 = add i32 %3379, %3171
  %3381 = add i32 %3380, %3357
  %3382 = add i32 %3381, %3354
  %3383 = lshr i32 %3343, 2
  %3384 = shl i32 %3343, 30
  %3385 = or i32 %3383, %3384
  %3386 = lshr i32 %3343, 13
  %3387 = shl i32 %3343, 19
  %3388 = or i32 %3386, %3387
  %3389 = xor i32 %3385, %3388
  %3390 = lshr i32 %3343, 22
  %3391 = shl i32 %3343, 10
  %3392 = or i32 %3390, %3391
  %3393 = xor i32 %3389, %3392
  %3394 = and i32 %3343, %3286
  %3395 = or i32 %3343, %3286
  %3396 = and i32 %3395, %3229
  %3397 = or i32 %3396, %3394
  %3398 = add i32 %3393, %3397
  %3399 = add i32 %3382, %3172
  %3400 = add i32 %3398, %3382
  %3401 = lshr i32 %3399, 6
  %3402 = shl i32 %3399, 26
  %3403 = or i32 %3401, %3402
  %3404 = lshr i32 %3399, 11
  %3405 = shl i32 %3399, 21
  %3406 = or i32 %3404, %3405
  %3407 = xor i32 %3403, %3406
  %3408 = lshr i32 %3399, 25
  %3409 = shl i32 %3399, 7
  %3410 = or i32 %3408, %3409
  %3411 = xor i32 %3407, %3410
  %3412 = xor i32 %3342, %3285
  %3413 = and i32 %3399, %3412
  %3414 = xor i32 %3413, %3285
  %3415 = lshr i32 %3321, 17
  %3416 = shl i32 %3321, 15
  %3417 = or i32 %3415, %3416
  %3418 = lshr i32 %3321, 19
  %3419 = shl i32 %3321, 13
  %3420 = or i32 %3418, %3419
  %3421 = lshr i32 %3321, 10
  %3422 = xor i32 %3420, %3421
  %3423 = xor i32 %3422, %3417
  %3424 = lshr i32 %2580, 7
  %3425 = shl i32 %2580, 25
  %3426 = or i32 %3424, %3425
  %3427 = lshr i32 %2580, 18
  %3428 = shl i32 %2580, 14
  %3429 = or i32 %3427, %3428
  %3430 = lshr i32 %2580, 3
  %3431 = xor i32 %3429, %3430
  %3432 = xor i32 %3431, %3426
  %3433 = add i32 %3432, %2523
  %3434 = add i32 %3433, %3036
  %3435 = add i32 %3434, %3423
  %3436 = add i32 %3435, -1866530822
  %3437 = add i32 %3436, %3228
  %3438 = add i32 %3437, %3414
  %3439 = add i32 %3438, %3411
  %3440 = lshr i32 %3400, 2
  %3441 = shl i32 %3400, 30
  %3442 = or i32 %3440, %3441
  %3443 = lshr i32 %3400, 13
  %3444 = shl i32 %3400, 19
  %3445 = or i32 %3443, %3444
  %3446 = xor i32 %3442, %3445
  %3447 = lshr i32 %3400, 22
  %3448 = shl i32 %3400, 10
  %3449 = or i32 %3447, %3448
  %3450 = xor i32 %3446, %3449
  %3451 = and i32 %3400, %3343
  %3452 = or i32 %3400, %3343
  %3453 = and i32 %3452, %3286
  %3454 = or i32 %3453, %3451
  %3455 = add i32 %3450, %3454
  %3456 = add i32 %3439, %3229
  %3457 = add i32 %3455, %3439
  %3458 = lshr i32 %3456, 6
  %3459 = shl i32 %3456, 26
  %3460 = or i32 %3458, %3459
  %3461 = lshr i32 %3456, 11
  %3462 = shl i32 %3456, 21
  %3463 = or i32 %3461, %3462
  %3464 = xor i32 %3460, %3463
  %3465 = lshr i32 %3456, 25
  %3466 = shl i32 %3456, 7
  %3467 = or i32 %3465, %3466
  %3468 = xor i32 %3464, %3467
  %3469 = xor i32 %3399, %3342
  %3470 = and i32 %3456, %3469
  %3471 = xor i32 %3470, %3342
  %3472 = lshr i32 %3378, 17
  %3473 = shl i32 %3378, 15
  %3474 = or i32 %3472, %3473
  %3475 = lshr i32 %3378, 19
  %3476 = shl i32 %3378, 13
  %3477 = or i32 %3475, %3476
  %3478 = lshr i32 %3378, 10
  %3479 = xor i32 %3477, %3478
  %3480 = xor i32 %3479, %3474
  %3481 = lshr i32 %2637, 7
  %3482 = shl i32 %2637, 25
  %3483 = or i32 %3481, %3482
  %3484 = lshr i32 %2637, 18
  %3485 = shl i32 %2637, 14
  %3486 = or i32 %3484, %3485
  %3487 = lshr i32 %2637, 3
  %3488 = xor i32 %3486, %3487
  %3489 = xor i32 %3488, %3483
  %3490 = add i32 %3489, %2580
  %3491 = add i32 %3490, %3093
  %3492 = add i32 %3491, %3480
  %3493 = add i32 %3492, -1538233109
  %3494 = add i32 %3493, %3285
  %3495 = add i32 %3494, %3471
  %3496 = add i32 %3495, %3468
  %3497 = lshr i32 %3457, 2
  %3498 = shl i32 %3457, 30
  %3499 = or i32 %3497, %3498
  %3500 = lshr i32 %3457, 13
  %3501 = shl i32 %3457, 19
  %3502 = or i32 %3500, %3501
  %3503 = xor i32 %3499, %3502
  %3504 = lshr i32 %3457, 22
  %3505 = shl i32 %3457, 10
  %3506 = or i32 %3504, %3505
  %3507 = xor i32 %3503, %3506
  %3508 = and i32 %3457, %3400
  %3509 = or i32 %3457, %3400
  %3510 = and i32 %3509, %3343
  %3511 = or i32 %3510, %3508
  %3512 = add i32 %3507, %3511
  %3513 = add i32 %3496, %3286
  %3514 = add i32 %3512, %3496
  %3515 = lshr i32 %3513, 6
  %3516 = shl i32 %3513, 26
  %3517 = or i32 %3515, %3516
  %3518 = lshr i32 %3513, 11
  %3519 = shl i32 %3513, 21
  %3520 = or i32 %3518, %3519
  %3521 = xor i32 %3517, %3520
  %3522 = lshr i32 %3513, 25
  %3523 = shl i32 %3513, 7
  %3524 = or i32 %3522, %3523
  %3525 = xor i32 %3521, %3524
  %3526 = xor i32 %3456, %3399
  %3527 = and i32 %3513, %3526
  %3528 = xor i32 %3527, %3399
  %3529 = lshr i32 %3435, 17
  %3530 = shl i32 %3435, 15
  %3531 = or i32 %3529, %3530
  %3532 = lshr i32 %3435, 19
  %3533 = shl i32 %3435, 13
  %3534 = or i32 %3532, %3533
  %3535 = lshr i32 %3435, 10
  %3536 = xor i32 %3534, %3535
  %3537 = xor i32 %3536, %3531
  %3538 = lshr i32 %2694, 7
  %3539 = shl i32 %2694, 25
  %3540 = or i32 %3538, %3539
  %3541 = lshr i32 %2694, 18
  %3542 = shl i32 %2694, 14
  %3543 = or i32 %3541, %3542
  %3544 = lshr i32 %2694, 3
  %3545 = xor i32 %3543, %3544
  %3546 = xor i32 %3545, %3540
  %3547 = add i32 %2637, -1090935817
  %3548 = add i32 %3547, %3546
  %3549 = add i32 %3548, %3150
  %3550 = add i32 %3549, %3537
  %3551 = add i32 %3550, %3342
  %3552 = add i32 %3551, %3528
  %3553 = add i32 %3552, %3525
  %3554 = lshr i32 %3514, 2
  %3555 = shl i32 %3514, 30
  %3556 = or i32 %3554, %3555
  %3557 = lshr i32 %3514, 13
  %3558 = shl i32 %3514, 19
  %3559 = or i32 %3557, %3558
  %3560 = xor i32 %3556, %3559
  %3561 = lshr i32 %3514, 22
  %3562 = shl i32 %3514, 10
  %3563 = or i32 %3561, %3562
  %3564 = xor i32 %3560, %3563
  %3565 = and i32 %3514, %3457
  %3566 = or i32 %3514, %3457
  %3567 = and i32 %3566, %3400
  %3568 = or i32 %3567, %3565
  %3569 = add i32 %3564, %3568
  %3570 = add i32 %3553, %3343
  %3571 = add i32 %3569, %3553
  %3572 = lshr i32 %3570, 6
  %3573 = shl i32 %3570, 26
  %3574 = or i32 %3572, %3573
  %3575 = lshr i32 %3570, 11
  %3576 = shl i32 %3570, 21
  %3577 = or i32 %3575, %3576
  %3578 = xor i32 %3574, %3577
  %3579 = lshr i32 %3570, 25
  %3580 = shl i32 %3570, 7
  %3581 = or i32 %3579, %3580
  %3582 = xor i32 %3578, %3581
  %3583 = xor i32 %3513, %3456
  %3584 = and i32 %3570, %3583
  %3585 = xor i32 %3584, %3456
  %3586 = lshr i32 %3492, 17
  %3587 = shl i32 %3492, 15
  %3588 = or i32 %3586, %3587
  %3589 = lshr i32 %3492, 19
  %3590 = shl i32 %3492, 13
  %3591 = or i32 %3589, %3590
  %3592 = lshr i32 %3492, 10
  %3593 = xor i32 %3591, %3592
  %3594 = xor i32 %3593, %3588
  %3595 = lshr i32 %2751, 7
  %3596 = shl i32 %2751, 25
  %3597 = or i32 %3595, %3596
  %3598 = lshr i32 %2751, 18
  %3599 = shl i32 %2751, 14
  %3600 = or i32 %3598, %3599
  %3601 = lshr i32 %2751, 3
  %3602 = xor i32 %3600, %3601
  %3603 = xor i32 %3602, %3597
  %3604 = add i32 %2694, -965641998
  %3605 = add i32 %3604, %3603
  %3606 = add i32 %3605, %3207
  %3607 = add i32 %3606, %3594
  %3608 = add i32 %3607, %3399
  %3609 = add i32 %3608, %3585
  %3610 = add i32 %3609, %3582
  %3611 = lshr i32 %3571, 2
  %3612 = shl i32 %3571, 30
  %3613 = or i32 %3611, %3612
  %3614 = lshr i32 %3571, 13
  %3615 = shl i32 %3571, 19
  %3616 = or i32 %3614, %3615
  %3617 = xor i32 %3613, %3616
  %3618 = lshr i32 %3571, 22
  %3619 = shl i32 %3571, 10
  %3620 = or i32 %3618, %3619
  %3621 = xor i32 %3617, %3620
  %3622 = and i32 %3571, %3514
  %3623 = or i32 %3571, %3514
  %3624 = and i32 %3623, %3457
  %3625 = or i32 %3624, %3622
  %3626 = add i32 %3621, %3625
  %3627 = add i32 %3610, %3400
  %3628 = add i32 %3626, %3610
  %3629 = zext i32 %3628 to i64
  %3630 = add i64 %291, %3629
  store i64 %3630, i64* %290, align 8
  %3631 = zext i32 %3571 to i64
  %3632 = add i64 %294, %3631
  store i64 %3632, i64* %293, align 8
  %3633 = zext i32 %3514 to i64
  %3634 = add i64 %297, %3633
  store i64 %3634, i64* %296, align 8
  %3635 = zext i32 %3457 to i64
  %3636 = add i64 %300, %3635
  store i64 %3636, i64* %299, align 8
  %3637 = zext i32 %3627 to i64
  %3638 = add i64 %303, %3637
  store i64 %3638, i64* %302, align 8
  %3639 = zext i32 %3570 to i64
  %3640 = add i64 %306, %3639
  store i64 %3640, i64* %305, align 8
  %3641 = zext i32 %3513 to i64
  %3642 = add i64 %309, %3641
  store i64 %3642, i64* %308, align 8
  %3643 = zext i32 %3456 to i64
  %3644 = add i64 %312, %3643
  store i64 %3644, i64* %311, align 8
  ret void
}

; Function Attrs: nounwind ssp uwtable
define hidden void @_Z18CRYPT_SHA256FinishP18CRYPT_sha2_contextPh(%struct.CRYPT_sha2_context*, i8*) local_unnamed_addr #1 {
  %3 = alloca i64, align 8
  %4 = bitcast i64* %3 to [8 x i8]*
  %5 = bitcast i64* %3 to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %5) #4
  %6 = getelementptr inbounds [8 x i8], [8 x i8]* %4, i64 0, i64 1
  %7 = getelementptr inbounds [8 x i8], [8 x i8]* %4, i64 0, i64 2
  %8 = getelementptr inbounds [8 x i8], [8 x i8]* %4, i64 0, i64 3
  %9 = getelementptr inbounds [8 x i8], [8 x i8]* %4, i64 0, i64 4
  %10 = getelementptr inbounds [8 x i8], [8 x i8]* %4, i64 0, i64 5
  %11 = getelementptr inbounds [8 x i8], [8 x i8]* %4, i64 0, i64 6
  %12 = getelementptr inbounds [8 x i8], [8 x i8]* %4, i64 0, i64 7
  %13 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %0, i64 0, i32 0
  %14 = load i64, i64* %13, align 8
  %15 = lshr i64 %14, 53
  %16 = trunc i64 %15 to i8
  store i8 %16, i8* %5, align 8
  %17 = lshr i64 %14, 45
  %18 = trunc i64 %17 to i8
  store i8 %18, i8* %6, align 1
  %19 = lshr i64 %14, 37
  %20 = trunc i64 %19 to i8
  store i8 %20, i8* %7, align 2
  %21 = lshr i64 %14, 29
  %22 = trunc i64 %21 to i8
  store i8 %22, i8* %8, align 1
  %23 = lshr i64 %14, 21
  %24 = trunc i64 %23 to i8
  store i8 %24, i8* %9, align 4
  %25 = lshr i64 %14, 13
  %26 = trunc i64 %25 to i8
  store i8 %26, i8* %10, align 1
  %27 = lshr i64 %14, 5
  %28 = trunc i64 %27 to i8
  store i8 %28, i8* %11, align 2
  %29 = trunc i64 %14 to i8
  %30 = shl i8 %29, 3
  store i8 %30, i8* %12, align 1
  %31 = trunc i64 %14 to i32
  %32 = and i32 %31, 63
  %33 = icmp ult i32 %32, 56
  %34 = select i1 %33, i32 56, i32 120
  %35 = sub nsw i32 %34, %32
  %36 = icmp eq i32 %35, 0
  br i1 %36, label %72, label %37

37:                                               ; preds = %2
  %38 = sub nuw nsw i32 64, %32
  %39 = zext i32 %35 to i64
  %40 = add i64 %14, %39
  store i64 %40, i64* %13, align 8
  %41 = icmp eq i32 %32, 0
  br i1 %41, label %51, label %42

42:                                               ; preds = %37
  %43 = icmp ult i32 %35, %38
  br i1 %43, label %51, label %44

44:                                               ; preds = %42
  %45 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %0, i64 0, i32 2, i64 0
  %46 = zext i32 %32 to i64
  %47 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %0, i64 0, i32 2, i64 %46
  %48 = zext i32 %38 to i64
  tail call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 1 %47, i8* align 16 getelementptr inbounds (<{ i8, [63 x i8] }>, <{ i8, [63 x i8] }>* @_ZN12_GLOBAL__N_114sha256_paddingE, i64 0, i32 0), i64 %48, i1 false) #4
  tail call fastcc void @_ZN12_GLOBAL__N_114sha256_processEP18CRYPT_sha2_contextPKh(%struct.CRYPT_sha2_context* %0, i8* %45) #4
  %49 = sub nsw i32 %35, %38
  %50 = getelementptr inbounds i8, i8* getelementptr inbounds (<{ i8, [63 x i8] }>, <{ i8, [63 x i8] }>* @_ZN12_GLOBAL__N_114sha256_paddingE, i64 0, i32 0), i64 %48
  br label %51

51:                                               ; preds = %44, %42, %37
  %52 = phi i8* [ %50, %44 ], [ getelementptr inbounds (<{ i8, [63 x i8] }>, <{ i8, [63 x i8] }>* @_ZN12_GLOBAL__N_114sha256_paddingE, i64 0, i32 0), %42 ], [ getelementptr inbounds (<{ i8, [63 x i8] }>, <{ i8, [63 x i8] }>* @_ZN12_GLOBAL__N_114sha256_paddingE, i64 0, i32 0), %37 ]
  %53 = phi i32 [ %49, %44 ], [ %35, %42 ], [ %35, %37 ]
  %54 = phi i32 [ 0, %44 ], [ %32, %42 ], [ 0, %37 ]
  %55 = icmp ugt i32 %53, 63
  br i1 %55, label %56, label %64

56:                                               ; preds = %51, %56
  %57 = phi i32 [ %59, %56 ], [ %53, %51 ]
  %58 = phi i8* [ %60, %56 ], [ %52, %51 ]
  tail call fastcc void @_ZN12_GLOBAL__N_114sha256_processEP18CRYPT_sha2_contextPKh(%struct.CRYPT_sha2_context* %0, i8* %58) #4
  %59 = add i32 %57, -64
  %60 = getelementptr inbounds i8, i8* %58, i64 64
  %61 = icmp ugt i32 %59, 63
  br i1 %61, label %56, label %62

62:                                               ; preds = %56
  %63 = and i32 %53, 63
  br label %64

64:                                               ; preds = %62, %51
  %65 = phi i8* [ %52, %51 ], [ %60, %62 ]
  %66 = phi i32 [ %53, %51 ], [ %63, %62 ]
  %67 = icmp eq i32 %66, 0
  br i1 %67, label %72, label %68

68:                                               ; preds = %64
  %69 = zext i32 %54 to i64
  %70 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %0, i64 0, i32 2, i64 %69
  %71 = zext i32 %66 to i64
  tail call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 1 %70, i8* align 1 %65, i64 %71, i1 false) #4
  br label %72

72:                                               ; preds = %2, %64, %68
  %73 = load i64, i64* %13, align 8
  %74 = trunc i64 %73 to i32
  %75 = and i32 %74, 63
  %76 = sub nuw nsw i32 64, %75
  %77 = add i64 %73, 8
  store i64 %77, i64* %13, align 8
  %78 = icmp eq i32 %75, 0
  br i1 %78, label %89, label %79

79:                                               ; preds = %72
  %80 = icmp ult i32 %75, 56
  br i1 %80, label %89, label %81

81:                                               ; preds = %79
  %82 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %0, i64 0, i32 2, i64 0
  %83 = zext i32 %75 to i64
  %84 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %0, i64 0, i32 2, i64 %83
  %85 = zext i32 %76 to i64
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 1 %84, i8* nonnull align 8 %5, i64 %85, i1 false) #4
  tail call fastcc void @_ZN12_GLOBAL__N_114sha256_processEP18CRYPT_sha2_contextPKh(%struct.CRYPT_sha2_context* %0, i8* %82) #4
  %86 = add nsw i32 %75, -56
  %87 = getelementptr inbounds [8 x i8], [8 x i8]* %4, i64 0, i64 %85
  %88 = icmp eq i32 %86, 0
  br i1 %88, label %96, label %89

89:                                               ; preds = %72, %79, %81
  %90 = phi i32 [ 0, %81 ], [ 0, %72 ], [ %75, %79 ]
  %91 = phi i32 [ %86, %81 ], [ 8, %72 ], [ 8, %79 ]
  %92 = phi i8* [ %87, %81 ], [ %5, %72 ], [ %5, %79 ]
  %93 = zext i32 %90 to i64
  %94 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %0, i64 0, i32 2, i64 %93
  %95 = zext i32 %91 to i64
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 1 %94, i8* align 1 %92, i64 %95, i1 false) #4
  br label %96

96:                                               ; preds = %81, %89
  %97 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %0, i64 0, i32 1, i64 0
  %98 = load i64, i64* %97, align 8
  %99 = lshr i64 %98, 24
  %100 = trunc i64 %99 to i8
  store i8 %100, i8* %1, align 1
  %101 = load i64, i64* %97, align 8
  %102 = lshr i64 %101, 16
  %103 = trunc i64 %102 to i8
  %104 = getelementptr inbounds i8, i8* %1, i64 1
  store i8 %103, i8* %104, align 1
  %105 = load i64, i64* %97, align 8
  %106 = lshr i64 %105, 8
  %107 = trunc i64 %106 to i8
  %108 = getelementptr inbounds i8, i8* %1, i64 2
  store i8 %107, i8* %108, align 1
  %109 = load i64, i64* %97, align 8
  %110 = trunc i64 %109 to i8
  %111 = getelementptr inbounds i8, i8* %1, i64 3
  store i8 %110, i8* %111, align 1
  %112 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %0, i64 0, i32 1, i64 1
  %113 = load i64, i64* %112, align 8
  %114 = lshr i64 %113, 24
  %115 = trunc i64 %114 to i8
  %116 = getelementptr inbounds i8, i8* %1, i64 4
  store i8 %115, i8* %116, align 1
  %117 = load i64, i64* %112, align 8
  %118 = lshr i64 %117, 16
  %119 = trunc i64 %118 to i8
  %120 = getelementptr inbounds i8, i8* %1, i64 5
  store i8 %119, i8* %120, align 1
  %121 = load i64, i64* %112, align 8
  %122 = lshr i64 %121, 8
  %123 = trunc i64 %122 to i8
  %124 = getelementptr inbounds i8, i8* %1, i64 6
  store i8 %123, i8* %124, align 1
  %125 = load i64, i64* %112, align 8
  %126 = trunc i64 %125 to i8
  %127 = getelementptr inbounds i8, i8* %1, i64 7
  store i8 %126, i8* %127, align 1
  %128 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %0, i64 0, i32 1, i64 2
  %129 = load i64, i64* %128, align 8
  %130 = lshr i64 %129, 24
  %131 = trunc i64 %130 to i8
  %132 = getelementptr inbounds i8, i8* %1, i64 8
  store i8 %131, i8* %132, align 1
  %133 = load i64, i64* %128, align 8
  %134 = lshr i64 %133, 16
  %135 = trunc i64 %134 to i8
  %136 = getelementptr inbounds i8, i8* %1, i64 9
  store i8 %135, i8* %136, align 1
  %137 = load i64, i64* %128, align 8
  %138 = lshr i64 %137, 8
  %139 = trunc i64 %138 to i8
  %140 = getelementptr inbounds i8, i8* %1, i64 10
  store i8 %139, i8* %140, align 1
  %141 = load i64, i64* %128, align 8
  %142 = trunc i64 %141 to i8
  %143 = getelementptr inbounds i8, i8* %1, i64 11
  store i8 %142, i8* %143, align 1
  %144 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %0, i64 0, i32 1, i64 3
  %145 = load i64, i64* %144, align 8
  %146 = lshr i64 %145, 24
  %147 = trunc i64 %146 to i8
  %148 = getelementptr inbounds i8, i8* %1, i64 12
  store i8 %147, i8* %148, align 1
  %149 = load i64, i64* %144, align 8
  %150 = lshr i64 %149, 16
  %151 = trunc i64 %150 to i8
  %152 = getelementptr inbounds i8, i8* %1, i64 13
  store i8 %151, i8* %152, align 1
  %153 = load i64, i64* %144, align 8
  %154 = lshr i64 %153, 8
  %155 = trunc i64 %154 to i8
  %156 = getelementptr inbounds i8, i8* %1, i64 14
  store i8 %155, i8* %156, align 1
  %157 = load i64, i64* %144, align 8
  %158 = trunc i64 %157 to i8
  %159 = getelementptr inbounds i8, i8* %1, i64 15
  store i8 %158, i8* %159, align 1
  %160 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %0, i64 0, i32 1, i64 4
  %161 = load i64, i64* %160, align 8
  %162 = lshr i64 %161, 24
  %163 = trunc i64 %162 to i8
  %164 = getelementptr inbounds i8, i8* %1, i64 16
  store i8 %163, i8* %164, align 1
  %165 = load i64, i64* %160, align 8
  %166 = lshr i64 %165, 16
  %167 = trunc i64 %166 to i8
  %168 = getelementptr inbounds i8, i8* %1, i64 17
  store i8 %167, i8* %168, align 1
  %169 = load i64, i64* %160, align 8
  %170 = lshr i64 %169, 8
  %171 = trunc i64 %170 to i8
  %172 = getelementptr inbounds i8, i8* %1, i64 18
  store i8 %171, i8* %172, align 1
  %173 = load i64, i64* %160, align 8
  %174 = trunc i64 %173 to i8
  %175 = getelementptr inbounds i8, i8* %1, i64 19
  store i8 %174, i8* %175, align 1
  %176 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %0, i64 0, i32 1, i64 5
  %177 = load i64, i64* %176, align 8
  %178 = lshr i64 %177, 24
  %179 = trunc i64 %178 to i8
  %180 = getelementptr inbounds i8, i8* %1, i64 20
  store i8 %179, i8* %180, align 1
  %181 = load i64, i64* %176, align 8
  %182 = lshr i64 %181, 16
  %183 = trunc i64 %182 to i8
  %184 = getelementptr inbounds i8, i8* %1, i64 21
  store i8 %183, i8* %184, align 1
  %185 = load i64, i64* %176, align 8
  %186 = lshr i64 %185, 8
  %187 = trunc i64 %186 to i8
  %188 = getelementptr inbounds i8, i8* %1, i64 22
  store i8 %187, i8* %188, align 1
  %189 = load i64, i64* %176, align 8
  %190 = trunc i64 %189 to i8
  %191 = getelementptr inbounds i8, i8* %1, i64 23
  store i8 %190, i8* %191, align 1
  %192 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %0, i64 0, i32 1, i64 6
  %193 = load i64, i64* %192, align 8
  %194 = lshr i64 %193, 24
  %195 = trunc i64 %194 to i8
  %196 = getelementptr inbounds i8, i8* %1, i64 24
  store i8 %195, i8* %196, align 1
  %197 = load i64, i64* %192, align 8
  %198 = lshr i64 %197, 16
  %199 = trunc i64 %198 to i8
  %200 = getelementptr inbounds i8, i8* %1, i64 25
  store i8 %199, i8* %200, align 1
  %201 = load i64, i64* %192, align 8
  %202 = lshr i64 %201, 8
  %203 = trunc i64 %202 to i8
  %204 = getelementptr inbounds i8, i8* %1, i64 26
  store i8 %203, i8* %204, align 1
  %205 = load i64, i64* %192, align 8
  %206 = trunc i64 %205 to i8
  %207 = getelementptr inbounds i8, i8* %1, i64 27
  store i8 %206, i8* %207, align 1
  %208 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %0, i64 0, i32 1, i64 7
  %209 = load i64, i64* %208, align 8
  %210 = lshr i64 %209, 24
  %211 = trunc i64 %210 to i8
  %212 = getelementptr inbounds i8, i8* %1, i64 28
  store i8 %211, i8* %212, align 1
  %213 = load i64, i64* %208, align 8
  %214 = lshr i64 %213, 16
  %215 = trunc i64 %214 to i8
  %216 = getelementptr inbounds i8, i8* %1, i64 29
  store i8 %215, i8* %216, align 1
  %217 = load i64, i64* %208, align 8
  %218 = lshr i64 %217, 8
  %219 = trunc i64 %218 to i8
  %220 = getelementptr inbounds i8, i8* %1, i64 30
  store i8 %219, i8* %220, align 1
  %221 = load i64, i64* %208, align 8
  %222 = trunc i64 %221 to i8
  %223 = getelementptr inbounds i8, i8* %1, i64 31
  store i8 %222, i8* %223, align 1
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %5) #4
  ret void
}

; Function Attrs: nounwind ssp uwtable
define hidden void @_Z20CRYPT_SHA256GeneratePKhjPh(i8* readonly, i32, i8*) local_unnamed_addr #1 {
  %4 = alloca %struct.CRYPT_sha2_context, align 16
  %5 = bitcast %struct.CRYPT_sha2_context* %4 to i8*
  call void @llvm.lifetime.start.p0i8(i64 200, i8* nonnull %5) #4
  %6 = bitcast %struct.CRYPT_sha2_context* %4 to <2 x i64>*
  store <2 x i64> <i64 0, i64 1779033703>, <2 x i64>* %6, align 16
  %7 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %4, i64 0, i32 1, i64 1
  %8 = bitcast i64* %7 to <2 x i64>*
  store <2 x i64> <i64 3144134277, i64 1013904242>, <2 x i64>* %8, align 8
  %9 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %4, i64 0, i32 1, i64 3
  %10 = bitcast i64* %9 to <2 x i64>*
  store <2 x i64> <i64 2773480762, i64 1359893119>, <2 x i64>* %10, align 8
  %11 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %4, i64 0, i32 1, i64 5
  %12 = bitcast i64* %11 to <2 x i64>*
  store <2 x i64> <i64 2600822924, i64 528734635>, <2 x i64>* %12, align 8
  %13 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %4, i64 0, i32 1, i64 7
  store i64 1541459225, i64* %13, align 8
  %14 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %4, i64 0, i32 2, i64 0
  call void @llvm.memset.p0i8.i64(i8* align 8 %14, i8 0, i64 128, i1 false) #4
  %15 = icmp eq i32 %1, 0
  br i1 %15, label %34, label %16

16:                                               ; preds = %3
  %17 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %4, i64 0, i32 0
  %18 = zext i32 %1 to i64
  store i64 %18, i64* %17, align 16
  %19 = icmp ugt i32 %1, 63
  br i1 %19, label %20, label %31

20:                                               ; preds = %16, %20
  %21 = phi i32 [ %23, %20 ], [ %1, %16 ]
  %22 = phi i8* [ %24, %20 ], [ %0, %16 ]
  call fastcc void @_ZN12_GLOBAL__N_114sha256_processEP18CRYPT_sha2_contextPKh(%struct.CRYPT_sha2_context* nonnull %4, i8* %22) #4
  %23 = add i32 %21, -64
  %24 = getelementptr inbounds i8, i8* %22, i64 64
  %25 = icmp ugt i32 %23, 63
  br i1 %25, label %20, label %26

26:                                               ; preds = %20
  %27 = and i32 %1, 63
  %28 = icmp eq i32 %27, 0
  br i1 %28, label %34, label %29

29:                                               ; preds = %26
  %30 = zext i32 %27 to i64
  br label %31

31:                                               ; preds = %29, %16
  %32 = phi i64 [ %30, %29 ], [ %18, %16 ]
  %33 = phi i8* [ %24, %29 ], [ %0, %16 ]
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 8 %14, i8* align 1 %33, i64 %32, i1 false) #4
  br label %34

34:                                               ; preds = %3, %26, %31
  call void @_Z18CRYPT_SHA256FinishP18CRYPT_sha2_contextPh(%struct.CRYPT_sha2_context* nonnull %4, i8* %2)
  call void @llvm.lifetime.end.p0i8(i64 200, i8* nonnull %5) #4
  ret void
}

; Function Attrs: nounwind ssp uwtable
define hidden void @_Z17CRYPT_SHA384StartP18CRYPT_sha2_context(%struct.CRYPT_sha2_context* nocapture) local_unnamed_addr #1 {
  %2 = bitcast %struct.CRYPT_sha2_context* %0 to <2 x i64>*
  store <2 x i64> <i64 0, i64 -3766243637369397544>, <2 x i64>* %2, align 8
  %3 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %0, i64 0, i32 1, i64 1
  %4 = bitcast i64* %3 to <2 x i64>*
  store <2 x i64> <i64 7105036623409894663, i64 -7973340178411365097>, <2 x i64>* %4, align 8
  %5 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %0, i64 0, i32 1, i64 3
  %6 = bitcast i64* %5 to <2 x i64>*
  store <2 x i64> <i64 1526699215303891257, i64 7436329637833083697>, <2 x i64>* %6, align 8
  %7 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %0, i64 0, i32 1, i64 5
  %8 = bitcast i64* %7 to <2 x i64>*
  store <2 x i64> <i64 -8163818279084223215, i64 -2662702644619276377>, <2 x i64>* %8, align 8
  %9 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %0, i64 0, i32 1, i64 7
  store i64 5167115440072839076, i64* %9, align 8
  %10 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %0, i64 0, i32 2, i64 0
  tail call void @llvm.memset.p0i8.i64(i8* align 8 %10, i8 0, i64 128, i1 false)
  ret void
}

; Function Attrs: nounwind ssp uwtable
define hidden void @_Z18CRYPT_SHA384UpdateP18CRYPT_sha2_contextPKhj(%struct.CRYPT_sha2_context*, i8* readonly, i32) local_unnamed_addr #1 {
  %4 = icmp eq i32 %2, 0
  br i1 %4, label %44, label %5

5:                                                ; preds = %3
  %6 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %0, i64 0, i32 0
  %7 = load i64, i64* %6, align 8
  %8 = trunc i64 %7 to i32
  %9 = and i32 %8, 127
  %10 = sub nuw nsw i32 128, %9
  %11 = zext i32 %2 to i64
  %12 = add i64 %7, %11
  store i64 %12, i64* %6, align 8
  %13 = icmp eq i32 %9, 0
  br i1 %13, label %23, label %14

14:                                               ; preds = %5
  %15 = icmp ugt i32 %10, %2
  br i1 %15, label %23, label %16

16:                                               ; preds = %14
  %17 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %0, i64 0, i32 2, i64 0
  %18 = zext i32 %9 to i64
  %19 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %0, i64 0, i32 2, i64 %18
  %20 = zext i32 %10 to i64
  tail call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 1 %19, i8* align 1 %1, i64 %20, i1 false)
  tail call fastcc void @_ZN12_GLOBAL__N_114sha384_processEP18CRYPT_sha2_contextPKh(%struct.CRYPT_sha2_context* %0, i8* %17)
  %21 = sub i32 %2, %10
  %22 = getelementptr inbounds i8, i8* %1, i64 %20
  br label %23

23:                                               ; preds = %14, %5, %16
  %24 = phi i8* [ %22, %16 ], [ %1, %14 ], [ %1, %5 ]
  %25 = phi i32 [ %21, %16 ], [ %2, %14 ], [ %2, %5 ]
  %26 = phi i32 [ 0, %16 ], [ %9, %14 ], [ 0, %5 ]
  %27 = icmp ugt i32 %25, 127
  br i1 %27, label %28, label %36

28:                                               ; preds = %23, %28
  %29 = phi i32 [ %31, %28 ], [ %25, %23 ]
  %30 = phi i8* [ %32, %28 ], [ %24, %23 ]
  tail call fastcc void @_ZN12_GLOBAL__N_114sha384_processEP18CRYPT_sha2_contextPKh(%struct.CRYPT_sha2_context* %0, i8* %30)
  %31 = add i32 %29, -128
  %32 = getelementptr inbounds i8, i8* %30, i64 128
  %33 = icmp ugt i32 %31, 127
  br i1 %33, label %28, label %34

34:                                               ; preds = %28
  %35 = and i32 %25, 127
  br label %36

36:                                               ; preds = %34, %23
  %37 = phi i8* [ %24, %23 ], [ %32, %34 ]
  %38 = phi i32 [ %25, %23 ], [ %35, %34 ]
  %39 = icmp eq i32 %38, 0
  br i1 %39, label %44, label %40

40:                                               ; preds = %36
  %41 = zext i32 %26 to i64
  %42 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %0, i64 0, i32 2, i64 %41
  %43 = zext i32 %38 to i64
  tail call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 1 %42, i8* align 1 %37, i64 %43, i1 false)
  br label %44

44:                                               ; preds = %40, %36, %3
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal fastcc void @_ZN12_GLOBAL__N_114sha384_processEP18CRYPT_sha2_contextPKh(%struct.CRYPT_sha2_context* nocapture, i8* readonly) unnamed_addr #1 {
  %3 = alloca [80 x i64], align 16
  %4 = bitcast [80 x i64]* %3 to i8*
  call void @llvm.lifetime.start.p0i8(i64 640, i8* nonnull %4) #4
  %5 = getelementptr inbounds [80 x i64], [80 x i64]* %3, i64 0, i64 2
  %6 = bitcast i64* %5 to i8*
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %6, i8 -86, i64 624, i1 false)
  %7 = load i8, i8* %1, align 1
  %8 = zext i8 %7 to i64
  %9 = shl nuw i64 %8, 56
  %10 = getelementptr inbounds i8, i8* %1, i64 1
  %11 = load i8, i8* %10, align 1
  %12 = zext i8 %11 to i64
  %13 = shl nuw nsw i64 %12, 48
  %14 = or i64 %13, %9
  %15 = getelementptr inbounds i8, i8* %1, i64 2
  %16 = load i8, i8* %15, align 1
  %17 = zext i8 %16 to i64
  %18 = shl nuw nsw i64 %17, 40
  %19 = or i64 %14, %18
  %20 = getelementptr inbounds i8, i8* %1, i64 3
  %21 = load i8, i8* %20, align 1
  %22 = zext i8 %21 to i64
  %23 = shl nuw nsw i64 %22, 32
  %24 = or i64 %19, %23
  %25 = getelementptr inbounds i8, i8* %1, i64 4
  %26 = load i8, i8* %25, align 1
  %27 = zext i8 %26 to i64
  %28 = shl nuw nsw i64 %27, 24
  %29 = or i64 %24, %28
  %30 = getelementptr inbounds i8, i8* %1, i64 5
  %31 = load i8, i8* %30, align 1
  %32 = zext i8 %31 to i64
  %33 = shl nuw nsw i64 %32, 16
  %34 = or i64 %29, %33
  %35 = getelementptr inbounds i8, i8* %1, i64 6
  %36 = load i8, i8* %35, align 1
  %37 = zext i8 %36 to i64
  %38 = shl nuw nsw i64 %37, 8
  %39 = or i64 %34, %38
  %40 = getelementptr inbounds i8, i8* %1, i64 7
  %41 = load i8, i8* %40, align 1
  %42 = zext i8 %41 to i64
  %43 = or i64 %39, %42
  %44 = getelementptr inbounds [80 x i64], [80 x i64]* %3, i64 0, i64 0
  store i64 %43, i64* %44, align 16
  %45 = getelementptr inbounds i8, i8* %1, i64 8
  %46 = load i8, i8* %45, align 1
  %47 = zext i8 %46 to i64
  %48 = shl nuw i64 %47, 56
  %49 = getelementptr inbounds i8, i8* %1, i64 9
  %50 = load i8, i8* %49, align 1
  %51 = zext i8 %50 to i64
  %52 = shl nuw nsw i64 %51, 48
  %53 = or i64 %52, %48
  %54 = getelementptr inbounds i8, i8* %1, i64 10
  %55 = load i8, i8* %54, align 1
  %56 = zext i8 %55 to i64
  %57 = shl nuw nsw i64 %56, 40
  %58 = or i64 %53, %57
  %59 = getelementptr inbounds i8, i8* %1, i64 11
  %60 = load i8, i8* %59, align 1
  %61 = zext i8 %60 to i64
  %62 = shl nuw nsw i64 %61, 32
  %63 = or i64 %58, %62
  %64 = getelementptr inbounds i8, i8* %1, i64 12
  %65 = load i8, i8* %64, align 1
  %66 = zext i8 %65 to i64
  %67 = shl nuw nsw i64 %66, 24
  %68 = or i64 %63, %67
  %69 = getelementptr inbounds i8, i8* %1, i64 13
  %70 = load i8, i8* %69, align 1
  %71 = zext i8 %70 to i64
  %72 = shl nuw nsw i64 %71, 16
  %73 = or i64 %68, %72
  %74 = getelementptr inbounds i8, i8* %1, i64 14
  %75 = load i8, i8* %74, align 1
  %76 = zext i8 %75 to i64
  %77 = shl nuw nsw i64 %76, 8
  %78 = or i64 %73, %77
  %79 = getelementptr inbounds i8, i8* %1, i64 15
  %80 = load i8, i8* %79, align 1
  %81 = zext i8 %80 to i64
  %82 = or i64 %78, %81
  %83 = getelementptr inbounds [80 x i64], [80 x i64]* %3, i64 0, i64 1
  store i64 %82, i64* %83, align 8
  %84 = getelementptr inbounds i8, i8* %1, i64 16
  %85 = load i8, i8* %84, align 1
  %86 = zext i8 %85 to i64
  %87 = shl nuw i64 %86, 56
  %88 = getelementptr inbounds i8, i8* %1, i64 17
  %89 = load i8, i8* %88, align 1
  %90 = zext i8 %89 to i64
  %91 = shl nuw nsw i64 %90, 48
  %92 = or i64 %91, %87
  %93 = getelementptr inbounds i8, i8* %1, i64 18
  %94 = load i8, i8* %93, align 1
  %95 = zext i8 %94 to i64
  %96 = shl nuw nsw i64 %95, 40
  %97 = or i64 %92, %96
  %98 = getelementptr inbounds i8, i8* %1, i64 19
  %99 = load i8, i8* %98, align 1
  %100 = zext i8 %99 to i64
  %101 = shl nuw nsw i64 %100, 32
  %102 = or i64 %97, %101
  %103 = getelementptr inbounds i8, i8* %1, i64 20
  %104 = load i8, i8* %103, align 1
  %105 = zext i8 %104 to i64
  %106 = shl nuw nsw i64 %105, 24
  %107 = or i64 %102, %106
  %108 = getelementptr inbounds i8, i8* %1, i64 21
  %109 = load i8, i8* %108, align 1
  %110 = zext i8 %109 to i64
  %111 = shl nuw nsw i64 %110, 16
  %112 = or i64 %107, %111
  %113 = getelementptr inbounds i8, i8* %1, i64 22
  %114 = load i8, i8* %113, align 1
  %115 = zext i8 %114 to i64
  %116 = shl nuw nsw i64 %115, 8
  %117 = or i64 %112, %116
  %118 = getelementptr inbounds i8, i8* %1, i64 23
  %119 = load i8, i8* %118, align 1
  %120 = zext i8 %119 to i64
  %121 = or i64 %117, %120
  %122 = getelementptr inbounds [80 x i64], [80 x i64]* %3, i64 0, i64 2
  store i64 %121, i64* %122, align 16
  %123 = getelementptr inbounds i8, i8* %1, i64 24
  %124 = load i8, i8* %123, align 1
  %125 = zext i8 %124 to i64
  %126 = shl nuw i64 %125, 56
  %127 = getelementptr inbounds i8, i8* %1, i64 25
  %128 = load i8, i8* %127, align 1
  %129 = zext i8 %128 to i64
  %130 = shl nuw nsw i64 %129, 48
  %131 = or i64 %130, %126
  %132 = getelementptr inbounds i8, i8* %1, i64 26
  %133 = load i8, i8* %132, align 1
  %134 = zext i8 %133 to i64
  %135 = shl nuw nsw i64 %134, 40
  %136 = or i64 %131, %135
  %137 = getelementptr inbounds i8, i8* %1, i64 27
  %138 = load i8, i8* %137, align 1
  %139 = zext i8 %138 to i64
  %140 = shl nuw nsw i64 %139, 32
  %141 = or i64 %136, %140
  %142 = getelementptr inbounds i8, i8* %1, i64 28
  %143 = load i8, i8* %142, align 1
  %144 = zext i8 %143 to i64
  %145 = shl nuw nsw i64 %144, 24
  %146 = or i64 %141, %145
  %147 = getelementptr inbounds i8, i8* %1, i64 29
  %148 = load i8, i8* %147, align 1
  %149 = zext i8 %148 to i64
  %150 = shl nuw nsw i64 %149, 16
  %151 = or i64 %146, %150
  %152 = getelementptr inbounds i8, i8* %1, i64 30
  %153 = load i8, i8* %152, align 1
  %154 = zext i8 %153 to i64
  %155 = shl nuw nsw i64 %154, 8
  %156 = or i64 %151, %155
  %157 = getelementptr inbounds i8, i8* %1, i64 31
  %158 = load i8, i8* %157, align 1
  %159 = zext i8 %158 to i64
  %160 = or i64 %156, %159
  %161 = getelementptr inbounds [80 x i64], [80 x i64]* %3, i64 0, i64 3
  store i64 %160, i64* %161, align 8
  %162 = getelementptr inbounds i8, i8* %1, i64 32
  %163 = load i8, i8* %162, align 1
  %164 = zext i8 %163 to i64
  %165 = shl nuw i64 %164, 56
  %166 = getelementptr inbounds i8, i8* %1, i64 33
  %167 = load i8, i8* %166, align 1
  %168 = zext i8 %167 to i64
  %169 = shl nuw nsw i64 %168, 48
  %170 = or i64 %169, %165
  %171 = getelementptr inbounds i8, i8* %1, i64 34
  %172 = load i8, i8* %171, align 1
  %173 = zext i8 %172 to i64
  %174 = shl nuw nsw i64 %173, 40
  %175 = or i64 %170, %174
  %176 = getelementptr inbounds i8, i8* %1, i64 35
  %177 = load i8, i8* %176, align 1
  %178 = zext i8 %177 to i64
  %179 = shl nuw nsw i64 %178, 32
  %180 = or i64 %175, %179
  %181 = getelementptr inbounds i8, i8* %1, i64 36
  %182 = load i8, i8* %181, align 1
  %183 = zext i8 %182 to i64
  %184 = shl nuw nsw i64 %183, 24
  %185 = or i64 %180, %184
  %186 = getelementptr inbounds i8, i8* %1, i64 37
  %187 = load i8, i8* %186, align 1
  %188 = zext i8 %187 to i64
  %189 = shl nuw nsw i64 %188, 16
  %190 = or i64 %185, %189
  %191 = getelementptr inbounds i8, i8* %1, i64 38
  %192 = load i8, i8* %191, align 1
  %193 = zext i8 %192 to i64
  %194 = shl nuw nsw i64 %193, 8
  %195 = or i64 %190, %194
  %196 = getelementptr inbounds i8, i8* %1, i64 39
  %197 = load i8, i8* %196, align 1
  %198 = zext i8 %197 to i64
  %199 = or i64 %195, %198
  %200 = getelementptr inbounds [80 x i64], [80 x i64]* %3, i64 0, i64 4
  store i64 %199, i64* %200, align 16
  %201 = getelementptr inbounds i8, i8* %1, i64 40
  %202 = load i8, i8* %201, align 1
  %203 = zext i8 %202 to i64
  %204 = shl nuw i64 %203, 56
  %205 = getelementptr inbounds i8, i8* %1, i64 41
  %206 = load i8, i8* %205, align 1
  %207 = zext i8 %206 to i64
  %208 = shl nuw nsw i64 %207, 48
  %209 = or i64 %208, %204
  %210 = getelementptr inbounds i8, i8* %1, i64 42
  %211 = load i8, i8* %210, align 1
  %212 = zext i8 %211 to i64
  %213 = shl nuw nsw i64 %212, 40
  %214 = or i64 %209, %213
  %215 = getelementptr inbounds i8, i8* %1, i64 43
  %216 = load i8, i8* %215, align 1
  %217 = zext i8 %216 to i64
  %218 = shl nuw nsw i64 %217, 32
  %219 = or i64 %214, %218
  %220 = getelementptr inbounds i8, i8* %1, i64 44
  %221 = load i8, i8* %220, align 1
  %222 = zext i8 %221 to i64
  %223 = shl nuw nsw i64 %222, 24
  %224 = or i64 %219, %223
  %225 = getelementptr inbounds i8, i8* %1, i64 45
  %226 = load i8, i8* %225, align 1
  %227 = zext i8 %226 to i64
  %228 = shl nuw nsw i64 %227, 16
  %229 = or i64 %224, %228
  %230 = getelementptr inbounds i8, i8* %1, i64 46
  %231 = load i8, i8* %230, align 1
  %232 = zext i8 %231 to i64
  %233 = shl nuw nsw i64 %232, 8
  %234 = or i64 %229, %233
  %235 = getelementptr inbounds i8, i8* %1, i64 47
  %236 = load i8, i8* %235, align 1
  %237 = zext i8 %236 to i64
  %238 = or i64 %234, %237
  %239 = getelementptr inbounds [80 x i64], [80 x i64]* %3, i64 0, i64 5
  store i64 %238, i64* %239, align 8
  %240 = getelementptr inbounds i8, i8* %1, i64 48
  %241 = load i8, i8* %240, align 1
  %242 = zext i8 %241 to i64
  %243 = shl nuw i64 %242, 56
  %244 = getelementptr inbounds i8, i8* %1, i64 49
  %245 = load i8, i8* %244, align 1
  %246 = zext i8 %245 to i64
  %247 = shl nuw nsw i64 %246, 48
  %248 = or i64 %247, %243
  %249 = getelementptr inbounds i8, i8* %1, i64 50
  %250 = load i8, i8* %249, align 1
  %251 = zext i8 %250 to i64
  %252 = shl nuw nsw i64 %251, 40
  %253 = or i64 %248, %252
  %254 = getelementptr inbounds i8, i8* %1, i64 51
  %255 = load i8, i8* %254, align 1
  %256 = zext i8 %255 to i64
  %257 = shl nuw nsw i64 %256, 32
  %258 = or i64 %253, %257
  %259 = getelementptr inbounds i8, i8* %1, i64 52
  %260 = load i8, i8* %259, align 1
  %261 = zext i8 %260 to i64
  %262 = shl nuw nsw i64 %261, 24
  %263 = or i64 %258, %262
  %264 = getelementptr inbounds i8, i8* %1, i64 53
  %265 = load i8, i8* %264, align 1
  %266 = zext i8 %265 to i64
  %267 = shl nuw nsw i64 %266, 16
  %268 = or i64 %263, %267
  %269 = getelementptr inbounds i8, i8* %1, i64 54
  %270 = load i8, i8* %269, align 1
  %271 = zext i8 %270 to i64
  %272 = shl nuw nsw i64 %271, 8
  %273 = or i64 %268, %272
  %274 = getelementptr inbounds i8, i8* %1, i64 55
  %275 = load i8, i8* %274, align 1
  %276 = zext i8 %275 to i64
  %277 = or i64 %273, %276
  %278 = getelementptr inbounds [80 x i64], [80 x i64]* %3, i64 0, i64 6
  store i64 %277, i64* %278, align 16
  %279 = getelementptr inbounds i8, i8* %1, i64 56
  %280 = load i8, i8* %279, align 1
  %281 = zext i8 %280 to i64
  %282 = shl nuw i64 %281, 56
  %283 = getelementptr inbounds i8, i8* %1, i64 57
  %284 = load i8, i8* %283, align 1
  %285 = zext i8 %284 to i64
  %286 = shl nuw nsw i64 %285, 48
  %287 = or i64 %286, %282
  %288 = getelementptr inbounds i8, i8* %1, i64 58
  %289 = load i8, i8* %288, align 1
  %290 = zext i8 %289 to i64
  %291 = shl nuw nsw i64 %290, 40
  %292 = or i64 %287, %291
  %293 = getelementptr inbounds i8, i8* %1, i64 59
  %294 = load i8, i8* %293, align 1
  %295 = zext i8 %294 to i64
  %296 = shl nuw nsw i64 %295, 32
  %297 = or i64 %292, %296
  %298 = getelementptr inbounds i8, i8* %1, i64 60
  %299 = load i8, i8* %298, align 1
  %300 = zext i8 %299 to i64
  %301 = shl nuw nsw i64 %300, 24
  %302 = or i64 %297, %301
  %303 = getelementptr inbounds i8, i8* %1, i64 61
  %304 = load i8, i8* %303, align 1
  %305 = zext i8 %304 to i64
  %306 = shl nuw nsw i64 %305, 16
  %307 = or i64 %302, %306
  %308 = getelementptr inbounds i8, i8* %1, i64 62
  %309 = load i8, i8* %308, align 1
  %310 = zext i8 %309 to i64
  %311 = shl nuw nsw i64 %310, 8
  %312 = or i64 %307, %311
  %313 = getelementptr inbounds i8, i8* %1, i64 63
  %314 = load i8, i8* %313, align 1
  %315 = zext i8 %314 to i64
  %316 = or i64 %312, %315
  %317 = getelementptr inbounds [80 x i64], [80 x i64]* %3, i64 0, i64 7
  store i64 %316, i64* %317, align 8
  %318 = getelementptr inbounds i8, i8* %1, i64 64
  %319 = load i8, i8* %318, align 1
  %320 = zext i8 %319 to i64
  %321 = shl nuw i64 %320, 56
  %322 = getelementptr inbounds i8, i8* %1, i64 65
  %323 = load i8, i8* %322, align 1
  %324 = zext i8 %323 to i64
  %325 = shl nuw nsw i64 %324, 48
  %326 = or i64 %325, %321
  %327 = getelementptr inbounds i8, i8* %1, i64 66
  %328 = load i8, i8* %327, align 1
  %329 = zext i8 %328 to i64
  %330 = shl nuw nsw i64 %329, 40
  %331 = or i64 %326, %330
  %332 = getelementptr inbounds i8, i8* %1, i64 67
  %333 = load i8, i8* %332, align 1
  %334 = zext i8 %333 to i64
  %335 = shl nuw nsw i64 %334, 32
  %336 = or i64 %331, %335
  %337 = getelementptr inbounds i8, i8* %1, i64 68
  %338 = load i8, i8* %337, align 1
  %339 = zext i8 %338 to i64
  %340 = shl nuw nsw i64 %339, 24
  %341 = or i64 %336, %340
  %342 = getelementptr inbounds i8, i8* %1, i64 69
  %343 = load i8, i8* %342, align 1
  %344 = zext i8 %343 to i64
  %345 = shl nuw nsw i64 %344, 16
  %346 = or i64 %341, %345
  %347 = getelementptr inbounds i8, i8* %1, i64 70
  %348 = load i8, i8* %347, align 1
  %349 = zext i8 %348 to i64
  %350 = shl nuw nsw i64 %349, 8
  %351 = or i64 %346, %350
  %352 = getelementptr inbounds i8, i8* %1, i64 71
  %353 = load i8, i8* %352, align 1
  %354 = zext i8 %353 to i64
  %355 = or i64 %351, %354
  %356 = getelementptr inbounds [80 x i64], [80 x i64]* %3, i64 0, i64 8
  store i64 %355, i64* %356, align 16
  %357 = getelementptr inbounds i8, i8* %1, i64 72
  %358 = load i8, i8* %357, align 1
  %359 = zext i8 %358 to i64
  %360 = shl nuw i64 %359, 56
  %361 = getelementptr inbounds i8, i8* %1, i64 73
  %362 = load i8, i8* %361, align 1
  %363 = zext i8 %362 to i64
  %364 = shl nuw nsw i64 %363, 48
  %365 = or i64 %364, %360
  %366 = getelementptr inbounds i8, i8* %1, i64 74
  %367 = load i8, i8* %366, align 1
  %368 = zext i8 %367 to i64
  %369 = shl nuw nsw i64 %368, 40
  %370 = or i64 %365, %369
  %371 = getelementptr inbounds i8, i8* %1, i64 75
  %372 = load i8, i8* %371, align 1
  %373 = zext i8 %372 to i64
  %374 = shl nuw nsw i64 %373, 32
  %375 = or i64 %370, %374
  %376 = getelementptr inbounds i8, i8* %1, i64 76
  %377 = load i8, i8* %376, align 1
  %378 = zext i8 %377 to i64
  %379 = shl nuw nsw i64 %378, 24
  %380 = or i64 %375, %379
  %381 = getelementptr inbounds i8, i8* %1, i64 77
  %382 = load i8, i8* %381, align 1
  %383 = zext i8 %382 to i64
  %384 = shl nuw nsw i64 %383, 16
  %385 = or i64 %380, %384
  %386 = getelementptr inbounds i8, i8* %1, i64 78
  %387 = load i8, i8* %386, align 1
  %388 = zext i8 %387 to i64
  %389 = shl nuw nsw i64 %388, 8
  %390 = or i64 %385, %389
  %391 = getelementptr inbounds i8, i8* %1, i64 79
  %392 = load i8, i8* %391, align 1
  %393 = zext i8 %392 to i64
  %394 = or i64 %390, %393
  %395 = getelementptr inbounds [80 x i64], [80 x i64]* %3, i64 0, i64 9
  store i64 %394, i64* %395, align 8
  %396 = getelementptr inbounds i8, i8* %1, i64 80
  %397 = load i8, i8* %396, align 1
  %398 = zext i8 %397 to i64
  %399 = shl nuw i64 %398, 56
  %400 = getelementptr inbounds i8, i8* %1, i64 81
  %401 = load i8, i8* %400, align 1
  %402 = zext i8 %401 to i64
  %403 = shl nuw nsw i64 %402, 48
  %404 = or i64 %403, %399
  %405 = getelementptr inbounds i8, i8* %1, i64 82
  %406 = load i8, i8* %405, align 1
  %407 = zext i8 %406 to i64
  %408 = shl nuw nsw i64 %407, 40
  %409 = or i64 %404, %408
  %410 = getelementptr inbounds i8, i8* %1, i64 83
  %411 = load i8, i8* %410, align 1
  %412 = zext i8 %411 to i64
  %413 = shl nuw nsw i64 %412, 32
  %414 = or i64 %409, %413
  %415 = getelementptr inbounds i8, i8* %1, i64 84
  %416 = load i8, i8* %415, align 1
  %417 = zext i8 %416 to i64
  %418 = shl nuw nsw i64 %417, 24
  %419 = or i64 %414, %418
  %420 = getelementptr inbounds i8, i8* %1, i64 85
  %421 = load i8, i8* %420, align 1
  %422 = zext i8 %421 to i64
  %423 = shl nuw nsw i64 %422, 16
  %424 = or i64 %419, %423
  %425 = getelementptr inbounds i8, i8* %1, i64 86
  %426 = load i8, i8* %425, align 1
  %427 = zext i8 %426 to i64
  %428 = shl nuw nsw i64 %427, 8
  %429 = or i64 %424, %428
  %430 = getelementptr inbounds i8, i8* %1, i64 87
  %431 = load i8, i8* %430, align 1
  %432 = zext i8 %431 to i64
  %433 = or i64 %429, %432
  %434 = getelementptr inbounds [80 x i64], [80 x i64]* %3, i64 0, i64 10
  store i64 %433, i64* %434, align 16
  %435 = getelementptr inbounds i8, i8* %1, i64 88
  %436 = load i8, i8* %435, align 1
  %437 = zext i8 %436 to i64
  %438 = shl nuw i64 %437, 56
  %439 = getelementptr inbounds i8, i8* %1, i64 89
  %440 = load i8, i8* %439, align 1
  %441 = zext i8 %440 to i64
  %442 = shl nuw nsw i64 %441, 48
  %443 = or i64 %442, %438
  %444 = getelementptr inbounds i8, i8* %1, i64 90
  %445 = load i8, i8* %444, align 1
  %446 = zext i8 %445 to i64
  %447 = shl nuw nsw i64 %446, 40
  %448 = or i64 %443, %447
  %449 = getelementptr inbounds i8, i8* %1, i64 91
  %450 = load i8, i8* %449, align 1
  %451 = zext i8 %450 to i64
  %452 = shl nuw nsw i64 %451, 32
  %453 = or i64 %448, %452
  %454 = getelementptr inbounds i8, i8* %1, i64 92
  %455 = load i8, i8* %454, align 1
  %456 = zext i8 %455 to i64
  %457 = shl nuw nsw i64 %456, 24
  %458 = or i64 %453, %457
  %459 = getelementptr inbounds i8, i8* %1, i64 93
  %460 = load i8, i8* %459, align 1
  %461 = zext i8 %460 to i64
  %462 = shl nuw nsw i64 %461, 16
  %463 = or i64 %458, %462
  %464 = getelementptr inbounds i8, i8* %1, i64 94
  %465 = load i8, i8* %464, align 1
  %466 = zext i8 %465 to i64
  %467 = shl nuw nsw i64 %466, 8
  %468 = or i64 %463, %467
  %469 = getelementptr inbounds i8, i8* %1, i64 95
  %470 = load i8, i8* %469, align 1
  %471 = zext i8 %470 to i64
  %472 = or i64 %468, %471
  %473 = getelementptr inbounds [80 x i64], [80 x i64]* %3, i64 0, i64 11
  store i64 %472, i64* %473, align 8
  %474 = getelementptr inbounds i8, i8* %1, i64 96
  %475 = load i8, i8* %474, align 1
  %476 = zext i8 %475 to i64
  %477 = shl nuw i64 %476, 56
  %478 = getelementptr inbounds i8, i8* %1, i64 97
  %479 = load i8, i8* %478, align 1
  %480 = zext i8 %479 to i64
  %481 = shl nuw nsw i64 %480, 48
  %482 = or i64 %481, %477
  %483 = getelementptr inbounds i8, i8* %1, i64 98
  %484 = load i8, i8* %483, align 1
  %485 = zext i8 %484 to i64
  %486 = shl nuw nsw i64 %485, 40
  %487 = or i64 %482, %486
  %488 = getelementptr inbounds i8, i8* %1, i64 99
  %489 = load i8, i8* %488, align 1
  %490 = zext i8 %489 to i64
  %491 = shl nuw nsw i64 %490, 32
  %492 = or i64 %487, %491
  %493 = getelementptr inbounds i8, i8* %1, i64 100
  %494 = load i8, i8* %493, align 1
  %495 = zext i8 %494 to i64
  %496 = shl nuw nsw i64 %495, 24
  %497 = or i64 %492, %496
  %498 = getelementptr inbounds i8, i8* %1, i64 101
  %499 = load i8, i8* %498, align 1
  %500 = zext i8 %499 to i64
  %501 = shl nuw nsw i64 %500, 16
  %502 = or i64 %497, %501
  %503 = getelementptr inbounds i8, i8* %1, i64 102
  %504 = load i8, i8* %503, align 1
  %505 = zext i8 %504 to i64
  %506 = shl nuw nsw i64 %505, 8
  %507 = or i64 %502, %506
  %508 = getelementptr inbounds i8, i8* %1, i64 103
  %509 = load i8, i8* %508, align 1
  %510 = zext i8 %509 to i64
  %511 = or i64 %507, %510
  %512 = getelementptr inbounds [80 x i64], [80 x i64]* %3, i64 0, i64 12
  store i64 %511, i64* %512, align 16
  %513 = getelementptr inbounds i8, i8* %1, i64 104
  %514 = load i8, i8* %513, align 1
  %515 = zext i8 %514 to i64
  %516 = shl nuw i64 %515, 56
  %517 = getelementptr inbounds i8, i8* %1, i64 105
  %518 = load i8, i8* %517, align 1
  %519 = zext i8 %518 to i64
  %520 = shl nuw nsw i64 %519, 48
  %521 = or i64 %520, %516
  %522 = getelementptr inbounds i8, i8* %1, i64 106
  %523 = load i8, i8* %522, align 1
  %524 = zext i8 %523 to i64
  %525 = shl nuw nsw i64 %524, 40
  %526 = or i64 %521, %525
  %527 = getelementptr inbounds i8, i8* %1, i64 107
  %528 = load i8, i8* %527, align 1
  %529 = zext i8 %528 to i64
  %530 = shl nuw nsw i64 %529, 32
  %531 = or i64 %526, %530
  %532 = getelementptr inbounds i8, i8* %1, i64 108
  %533 = load i8, i8* %532, align 1
  %534 = zext i8 %533 to i64
  %535 = shl nuw nsw i64 %534, 24
  %536 = or i64 %531, %535
  %537 = getelementptr inbounds i8, i8* %1, i64 109
  %538 = load i8, i8* %537, align 1
  %539 = zext i8 %538 to i64
  %540 = shl nuw nsw i64 %539, 16
  %541 = or i64 %536, %540
  %542 = getelementptr inbounds i8, i8* %1, i64 110
  %543 = load i8, i8* %542, align 1
  %544 = zext i8 %543 to i64
  %545 = shl nuw nsw i64 %544, 8
  %546 = or i64 %541, %545
  %547 = getelementptr inbounds i8, i8* %1, i64 111
  %548 = load i8, i8* %547, align 1
  %549 = zext i8 %548 to i64
  %550 = or i64 %546, %549
  %551 = getelementptr inbounds [80 x i64], [80 x i64]* %3, i64 0, i64 13
  store i64 %550, i64* %551, align 8
  %552 = getelementptr inbounds i8, i8* %1, i64 112
  %553 = load i8, i8* %552, align 1
  %554 = zext i8 %553 to i64
  %555 = shl nuw i64 %554, 56
  %556 = getelementptr inbounds i8, i8* %1, i64 113
  %557 = load i8, i8* %556, align 1
  %558 = zext i8 %557 to i64
  %559 = shl nuw nsw i64 %558, 48
  %560 = or i64 %559, %555
  %561 = getelementptr inbounds i8, i8* %1, i64 114
  %562 = load i8, i8* %561, align 1
  %563 = zext i8 %562 to i64
  %564 = shl nuw nsw i64 %563, 40
  %565 = or i64 %560, %564
  %566 = getelementptr inbounds i8, i8* %1, i64 115
  %567 = load i8, i8* %566, align 1
  %568 = zext i8 %567 to i64
  %569 = shl nuw nsw i64 %568, 32
  %570 = or i64 %565, %569
  %571 = getelementptr inbounds i8, i8* %1, i64 116
  %572 = load i8, i8* %571, align 1
  %573 = zext i8 %572 to i64
  %574 = shl nuw nsw i64 %573, 24
  %575 = or i64 %570, %574
  %576 = getelementptr inbounds i8, i8* %1, i64 117
  %577 = load i8, i8* %576, align 1
  %578 = zext i8 %577 to i64
  %579 = shl nuw nsw i64 %578, 16
  %580 = or i64 %575, %579
  %581 = getelementptr inbounds i8, i8* %1, i64 118
  %582 = load i8, i8* %581, align 1
  %583 = zext i8 %582 to i64
  %584 = shl nuw nsw i64 %583, 8
  %585 = or i64 %580, %584
  %586 = getelementptr inbounds i8, i8* %1, i64 119
  %587 = load i8, i8* %586, align 1
  %588 = zext i8 %587 to i64
  %589 = or i64 %585, %588
  %590 = getelementptr inbounds [80 x i64], [80 x i64]* %3, i64 0, i64 14
  store i64 %589, i64* %590, align 16
  %591 = getelementptr inbounds i8, i8* %1, i64 120
  %592 = load i8, i8* %591, align 1
  %593 = zext i8 %592 to i64
  %594 = shl nuw i64 %593, 56
  %595 = getelementptr inbounds i8, i8* %1, i64 121
  %596 = load i8, i8* %595, align 1
  %597 = zext i8 %596 to i64
  %598 = shl nuw nsw i64 %597, 48
  %599 = or i64 %598, %594
  %600 = getelementptr inbounds i8, i8* %1, i64 122
  %601 = load i8, i8* %600, align 1
  %602 = zext i8 %601 to i64
  %603 = shl nuw nsw i64 %602, 40
  %604 = or i64 %599, %603
  %605 = getelementptr inbounds i8, i8* %1, i64 123
  %606 = load i8, i8* %605, align 1
  %607 = zext i8 %606 to i64
  %608 = shl nuw nsw i64 %607, 32
  %609 = or i64 %604, %608
  %610 = getelementptr inbounds i8, i8* %1, i64 124
  %611 = load i8, i8* %610, align 1
  %612 = zext i8 %611 to i64
  %613 = shl nuw nsw i64 %612, 24
  %614 = or i64 %609, %613
  %615 = getelementptr inbounds i8, i8* %1, i64 125
  %616 = load i8, i8* %615, align 1
  %617 = zext i8 %616 to i64
  %618 = shl nuw nsw i64 %617, 16
  %619 = or i64 %614, %618
  %620 = getelementptr inbounds i8, i8* %1, i64 126
  %621 = load i8, i8* %620, align 1
  %622 = zext i8 %621 to i64
  %623 = shl nuw nsw i64 %622, 8
  %624 = or i64 %619, %623
  %625 = getelementptr inbounds i8, i8* %1, i64 127
  %626 = load i8, i8* %625, align 1
  %627 = zext i8 %626 to i64
  %628 = or i64 %624, %627
  %629 = getelementptr inbounds [80 x i64], [80 x i64]* %3, i64 0, i64 15
  store i64 %628, i64* %629, align 8
  %630 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %0, i64 0, i32 1, i64 0
  %631 = load i64, i64* %630, align 8
  %632 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %0, i64 0, i32 1, i64 1
  %633 = load i64, i64* %632, align 8
  %634 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %0, i64 0, i32 1, i64 2
  %635 = load i64, i64* %634, align 8
  %636 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %0, i64 0, i32 1, i64 3
  %637 = load i64, i64* %636, align 8
  %638 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %0, i64 0, i32 1, i64 4
  %639 = load i64, i64* %638, align 8
  %640 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %0, i64 0, i32 1, i64 5
  %641 = load i64, i64* %640, align 8
  %642 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %0, i64 0, i32 1, i64 6
  %643 = load i64, i64* %642, align 8
  %644 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %0, i64 0, i32 1, i64 7
  %645 = load i64, i64* %644, align 8
  br label %655

646:                                              ; preds = %896
  %647 = add i64 %1217, %631
  store i64 %647, i64* %630, align 8
  %648 = add i64 %1178, %633
  store i64 %648, i64* %632, align 8
  %649 = add i64 %1139, %635
  store i64 %649, i64* %634, align 8
  %650 = add i64 %1101, %637
  store i64 %650, i64* %636, align 8
  %651 = add i64 %1216, %639
  store i64 %651, i64* %638, align 8
  %652 = add i64 %1177, %641
  store i64 %652, i64* %640, align 8
  %653 = add i64 %1138, %643
  store i64 %653, i64* %642, align 8
  %654 = add i64 %1100, %645
  store i64 %654, i64* %644, align 8
  call void @llvm.lifetime.end.p0i8(i64 640, i8* nonnull %4) #4
  ret void

655:                                              ; preds = %896, %2
  %656 = phi i64 [ 0, %2 ], [ %1218, %896 ]
  %657 = phi i64 [ %631, %2 ], [ %1217, %896 ]
  %658 = phi i64 [ %633, %2 ], [ %1178, %896 ]
  %659 = phi i64 [ %635, %2 ], [ %1139, %896 ]
  %660 = phi i64 [ %637, %2 ], [ %1101, %896 ]
  %661 = phi i64 [ %639, %2 ], [ %1216, %896 ]
  %662 = phi i64 [ %641, %2 ], [ %1177, %896 ]
  %663 = phi i64 [ %643, %2 ], [ %1138, %896 ]
  %664 = phi i64 [ %645, %2 ], [ %1100, %896 ]
  %665 = icmp ult i64 %656, 2
  %666 = shl nsw i64 %656, 3
  br i1 %665, label %667, label %691

667:                                              ; preds = %655
  %668 = getelementptr inbounds [80 x i64], [80 x i64]* %3, i64 0, i64 %666
  %669 = load i64, i64* %668, align 16
  %670 = or i64 %666, 1
  %671 = getelementptr inbounds [80 x i64], [80 x i64]* %3, i64 0, i64 %670
  %672 = or i64 %666, 2
  %673 = bitcast i64* %671 to <2 x i64>*
  %674 = load <2 x i64>, <2 x i64>* %673, align 8
  %675 = shufflevector <2 x i64> %674, <2 x i64> undef, <2 x i32> <i32 1, i32 0>
  %676 = or i64 %666, 3
  %677 = getelementptr inbounds [80 x i64], [80 x i64]* %3, i64 0, i64 %676
  %678 = load i64, i64* %677, align 8
  %679 = or i64 %666, 4
  %680 = getelementptr inbounds [80 x i64], [80 x i64]* %3, i64 0, i64 %679
  %681 = load i64, i64* %680, align 16
  %682 = or i64 %666, 5
  %683 = getelementptr inbounds [80 x i64], [80 x i64]* %3, i64 0, i64 %682
  %684 = load i64, i64* %683, align 8
  %685 = or i64 %666, 6
  %686 = getelementptr inbounds [80 x i64], [80 x i64]* %3, i64 0, i64 %685
  %687 = or i64 %666, 7
  %688 = bitcast i64* %686 to <2 x i64>*
  %689 = load <2 x i64>, <2 x i64>* %688, align 16
  %690 = shufflevector <2 x i64> %689, <2 x i64> undef, <2 x i32> <i32 1, i32 0>
  br label %896

691:                                              ; preds = %655
  %692 = add nsw i64 %666, -2
  %693 = getelementptr inbounds [80 x i64], [80 x i64]* %3, i64 0, i64 %692
  %694 = load i64, i64* %693, align 16
  %695 = lshr i64 %694, 19
  %696 = shl i64 %694, 45
  %697 = or i64 %695, %696
  %698 = lshr i64 %694, 61
  %699 = shl i64 %694, 3
  %700 = or i64 %698, %699
  %701 = lshr i64 %694, 6
  %702 = xor i64 %700, %701
  %703 = xor i64 %702, %697
  %704 = add nsw i64 %666, -7
  %705 = getelementptr inbounds [80 x i64], [80 x i64]* %3, i64 0, i64 %704
  %706 = load i64, i64* %705, align 8
  %707 = add nsw i64 %666, -15
  %708 = getelementptr inbounds [80 x i64], [80 x i64]* %3, i64 0, i64 %707
  %709 = load i64, i64* %708, align 8
  %710 = lshr i64 %709, 1
  %711 = shl i64 %709, 63
  %712 = or i64 %710, %711
  %713 = lshr i64 %709, 8
  %714 = shl i64 %709, 56
  %715 = or i64 %713, %714
  %716 = lshr i64 %709, 7
  %717 = xor i64 %715, %716
  %718 = xor i64 %717, %712
  %719 = add nsw i64 %666, -16
  %720 = getelementptr inbounds [80 x i64], [80 x i64]* %3, i64 0, i64 %719
  %721 = load i64, i64* %720, align 16
  %722 = add i64 %721, %706
  %723 = add i64 %722, %703
  %724 = add i64 %723, %718
  %725 = getelementptr inbounds [80 x i64], [80 x i64]* %3, i64 0, i64 %666
  store i64 %724, i64* %725, align 16
  %726 = or i64 %666, 1
  %727 = add nsw i64 %726, -2
  %728 = getelementptr inbounds [80 x i64], [80 x i64]* %3, i64 0, i64 %727
  %729 = load i64, i64* %728, align 8
  %730 = lshr i64 %729, 19
  %731 = shl i64 %729, 45
  %732 = or i64 %730, %731
  %733 = lshr i64 %729, 61
  %734 = shl i64 %729, 3
  %735 = or i64 %733, %734
  %736 = lshr i64 %729, 6
  %737 = xor i64 %735, %736
  %738 = xor i64 %737, %732
  %739 = add nsw i64 %726, -7
  %740 = getelementptr inbounds [80 x i64], [80 x i64]* %3, i64 0, i64 %739
  %741 = add nsw i64 %726, -15
  %742 = getelementptr inbounds [80 x i64], [80 x i64]* %3, i64 0, i64 %741
  %743 = load i64, i64* %742, align 16
  %744 = getelementptr inbounds [80 x i64], [80 x i64]* %3, i64 0, i64 %726
  %745 = or i64 %666, 2
  %746 = lshr i64 %724, 19
  %747 = shl i64 %724, 45
  %748 = or i64 %746, %747
  %749 = lshr i64 %724, 61
  %750 = shl i64 %724, 3
  %751 = or i64 %749, %750
  %752 = lshr i64 %724, 6
  %753 = xor i64 %751, %752
  %754 = xor i64 %753, %748
  %755 = bitcast i64* %740 to <2 x i64>*
  %756 = load <2 x i64>, <2 x i64>* %755, align 16
  %757 = shufflevector <2 x i64> %756, <2 x i64> undef, <2 x i32> <i32 1, i32 0>
  %758 = insertelement <2 x i64> undef, i64 %754, i32 0
  %759 = insertelement <2 x i64> %758, i64 %709, i32 1
  %760 = add <2 x i64> %759, %757
  %761 = add nsw i64 %745, -15
  %762 = getelementptr inbounds [80 x i64], [80 x i64]* %3, i64 0, i64 %761
  %763 = load i64, i64* %762, align 8
  %764 = insertelement <2 x i64> undef, i64 %763, i32 0
  %765 = insertelement <2 x i64> %764, i64 %743, i32 1
  %766 = lshr <2 x i64> %765, <i64 1, i64 1>
  %767 = shl <2 x i64> %765, <i64 63, i64 63>
  %768 = or <2 x i64> %766, %767
  %769 = lshr <2 x i64> %765, <i64 8, i64 8>
  %770 = shl <2 x i64> %765, <i64 56, i64 56>
  %771 = or <2 x i64> %769, %770
  %772 = lshr <2 x i64> %765, <i64 7, i64 7>
  %773 = xor <2 x i64> %771, %772
  %774 = xor <2 x i64> %773, %768
  %775 = insertelement <2 x i64> undef, i64 %743, i32 0
  %776 = insertelement <2 x i64> %775, i64 %738, i32 1
  %777 = add <2 x i64> %760, %776
  %778 = add <2 x i64> %777, %774
  %779 = shufflevector <2 x i64> %778, <2 x i64> undef, <2 x i32> <i32 1, i32 0>
  %780 = bitcast i64* %744 to <2 x i64>*
  store <2 x i64> %779, <2 x i64>* %780, align 8
  %781 = or i64 %666, 3
  %782 = extractelement <2 x i64> %778, i32 1
  %783 = lshr i64 %782, 19
  %784 = shl i64 %782, 45
  %785 = or i64 %783, %784
  %786 = lshr i64 %782, 61
  %787 = shl i64 %782, 3
  %788 = or i64 %786, %787
  %789 = lshr i64 %782, 6
  %790 = xor i64 %788, %789
  %791 = xor i64 %790, %785
  %792 = add nsw i64 %781, -7
  %793 = getelementptr inbounds [80 x i64], [80 x i64]* %3, i64 0, i64 %792
  %794 = load i64, i64* %793, align 16
  %795 = add nsw i64 %781, -15
  %796 = getelementptr inbounds [80 x i64], [80 x i64]* %3, i64 0, i64 %795
  %797 = or i64 %666, 4
  %798 = bitcast i64* %796 to <2 x i64>*
  %799 = load <2 x i64>, <2 x i64>* %798, align 16
  %800 = extractelement <2 x i64> %799, i32 0
  %801 = lshr i64 %800, 1
  %802 = shl i64 %800, 63
  %803 = or i64 %801, %802
  %804 = lshr i64 %800, 8
  %805 = shl i64 %800, 56
  %806 = or i64 %804, %805
  %807 = lshr i64 %800, 7
  %808 = xor i64 %806, %807
  %809 = xor i64 %808, %803
  %810 = add i64 %763, %794
  %811 = add i64 %810, %791
  %812 = add i64 %811, %809
  %813 = getelementptr inbounds [80 x i64], [80 x i64]* %3, i64 0, i64 %781
  store i64 %812, i64* %813, align 8
  %814 = extractelement <2 x i64> %778, i32 0
  %815 = lshr i64 %814, 61
  %816 = shl i64 %814, 3
  %817 = add nsw i64 %797, -7
  %818 = getelementptr inbounds [80 x i64], [80 x i64]* %3, i64 0, i64 %817
  %819 = getelementptr inbounds [80 x i64], [80 x i64]* %3, i64 0, i64 %797
  %820 = or i64 %666, 5
  %821 = insertelement <2 x i64> undef, i64 %812, i32 0
  %822 = shufflevector <2 x i64> %821, <2 x i64> %778, <2 x i32> <i32 0, i32 2>
  %823 = lshr <2 x i64> %822, <i64 19, i64 19>
  %824 = shl <2 x i64> %822, <i64 45, i64 45>
  %825 = or <2 x i64> %823, %824
  %826 = lshr i64 %812, 61
  %827 = shl i64 %812, 3
  %828 = insertelement <2 x i64> undef, i64 %826, i32 0
  %829 = insertelement <2 x i64> %828, i64 %815, i32 1
  %830 = insertelement <2 x i64> undef, i64 %827, i32 0
  %831 = insertelement <2 x i64> %830, i64 %816, i32 1
  %832 = or <2 x i64> %829, %831
  %833 = lshr <2 x i64> %822, <i64 6, i64 6>
  %834 = xor <2 x i64> %832, %833
  %835 = xor <2 x i64> %834, %825
  %836 = bitcast i64* %818 to <2 x i64>*
  %837 = load <2 x i64>, <2 x i64>* %836, align 8
  %838 = add nsw i64 %820, -15
  %839 = getelementptr inbounds [80 x i64], [80 x i64]* %3, i64 0, i64 %838
  %840 = or i64 %666, 6
  %841 = bitcast i64* %839 to <2 x i64>*
  %842 = load <2 x i64>, <2 x i64>* %841, align 16
  %843 = shufflevector <2 x i64> %842, <2 x i64> undef, <2 x i32> <i32 1, i32 0>
  %844 = shufflevector <2 x i64> %842, <2 x i64> %799, <2 x i32> <i32 0, i32 3>
  %845 = lshr <2 x i64> %844, <i64 1, i64 1>
  %846 = shl <2 x i64> %844, <i64 63, i64 63>
  %847 = or <2 x i64> %845, %846
  %848 = lshr <2 x i64> %844, <i64 8, i64 8>
  %849 = shl <2 x i64> %844, <i64 56, i64 56>
  %850 = or <2 x i64> %848, %849
  %851 = lshr <2 x i64> %844, <i64 7, i64 7>
  %852 = xor <2 x i64> %850, %851
  %853 = xor <2 x i64> %852, %847
  %854 = add <2 x i64> %799, %837
  %855 = shufflevector <2 x i64> %854, <2 x i64> undef, <2 x i32> <i32 1, i32 0>
  %856 = add <2 x i64> %855, %835
  %857 = add <2 x i64> %856, %853
  %858 = shufflevector <2 x i64> %857, <2 x i64> undef, <2 x i32> <i32 1, i32 0>
  %859 = bitcast i64* %819 to <2 x i64>*
  store <2 x i64> %858, <2 x i64>* %859, align 16
  %860 = add nsw i64 %840, -7
  %861 = getelementptr inbounds [80 x i64], [80 x i64]* %3, i64 0, i64 %860
  %862 = load i64, i64* %861, align 8
  %863 = getelementptr inbounds [80 x i64], [80 x i64]* %3, i64 0, i64 %840
  %864 = or i64 %666, 7
  %865 = lshr <2 x i64> %857, <i64 19, i64 19>
  %866 = shl <2 x i64> %857, <i64 45, i64 45>
  %867 = or <2 x i64> %865, %866
  %868 = lshr <2 x i64> %857, <i64 61, i64 61>
  %869 = shl <2 x i64> %857, <i64 3, i64 3>
  %870 = or <2 x i64> %868, %869
  %871 = lshr <2 x i64> %857, <i64 6, i64 6>
  %872 = xor <2 x i64> %870, %871
  %873 = xor <2 x i64> %872, %867
  %874 = add nsw i64 %864, -15
  %875 = getelementptr inbounds [80 x i64], [80 x i64]* %3, i64 0, i64 %874
  %876 = load i64, i64* %875, align 16
  %877 = insertelement <2 x i64> %842, i64 %876, i32 0
  %878 = lshr <2 x i64> %877, <i64 1, i64 1>
  %879 = shl <2 x i64> %877, <i64 63, i64 63>
  %880 = or <2 x i64> %878, %879
  %881 = lshr <2 x i64> %877, <i64 8, i64 8>
  %882 = shl <2 x i64> %877, <i64 56, i64 56>
  %883 = or <2 x i64> %881, %882
  %884 = lshr <2 x i64> %877, <i64 7, i64 7>
  %885 = xor <2 x i64> %883, %884
  %886 = xor <2 x i64> %885, %880
  %887 = insertelement <2 x i64> undef, i64 %724, i32 0
  %888 = insertelement <2 x i64> %887, i64 %862, i32 1
  %889 = add <2 x i64> %843, %888
  %890 = add <2 x i64> %889, %873
  %891 = add <2 x i64> %890, %886
  %892 = shufflevector <2 x i64> %891, <2 x i64> undef, <2 x i32> <i32 1, i32 0>
  %893 = bitcast i64* %863 to <2 x i64>*
  store <2 x i64> %892, <2 x i64>* %893, align 16
  %894 = extractelement <2 x i64> %857, i32 0
  %895 = extractelement <2 x i64> %857, i32 1
  br label %896

896:                                              ; preds = %691, %667
  %897 = phi i64 [ %864, %691 ], [ %687, %667 ]
  %898 = phi i64 [ %840, %691 ], [ %685, %667 ]
  %899 = phi i64 [ %820, %691 ], [ %682, %667 ]
  %900 = phi i64 [ %797, %691 ], [ %679, %667 ]
  %901 = phi i64 [ %781, %691 ], [ %676, %667 ]
  %902 = phi i64 [ %745, %691 ], [ %672, %667 ]
  %903 = phi i64 [ %726, %691 ], [ %670, %667 ]
  %904 = phi i64 [ %894, %691 ], [ %684, %667 ]
  %905 = phi i64 [ %895, %691 ], [ %681, %667 ]
  %906 = phi i64 [ %812, %691 ], [ %678, %667 ]
  %907 = phi i64 [ %724, %691 ], [ %669, %667 ]
  %908 = phi <2 x i64> [ %891, %691 ], [ %690, %667 ]
  %909 = phi <2 x i64> [ %778, %691 ], [ %675, %667 ]
  %910 = lshr i64 %661, 14
  %911 = shl i64 %661, 50
  %912 = or i64 %910, %911
  %913 = lshr i64 %661, 18
  %914 = shl i64 %661, 46
  %915 = or i64 %913, %914
  %916 = xor i64 %912, %915
  %917 = lshr i64 %661, 41
  %918 = shl i64 %661, 23
  %919 = or i64 %917, %918
  %920 = xor i64 %916, %919
  %921 = xor i64 %662, %663
  %922 = and i64 %661, %921
  %923 = xor i64 %922, %663
  %924 = getelementptr inbounds [80 x i64], [80 x i64]* @_ZN12_GLOBAL__N_19constantsE, i64 0, i64 %666
  %925 = load i64, i64* %924, align 16
  %926 = add i64 %923, %664
  %927 = add i64 %926, %920
  %928 = add i64 %927, %907
  %929 = add i64 %928, %925
  %930 = lshr i64 %657, 28
  %931 = shl i64 %657, 36
  %932 = or i64 %930, %931
  %933 = lshr i64 %657, 34
  %934 = shl i64 %657, 30
  %935 = or i64 %933, %934
  %936 = xor i64 %932, %935
  %937 = lshr i64 %657, 39
  %938 = shl i64 %657, 25
  %939 = or i64 %937, %938
  %940 = xor i64 %936, %939
  %941 = and i64 %657, %658
  %942 = or i64 %657, %658
  %943 = and i64 %942, %659
  %944 = or i64 %943, %941
  %945 = add i64 %940, %944
  %946 = add i64 %929, %660
  %947 = add i64 %945, %929
  %948 = lshr i64 %946, 14
  %949 = shl i64 %946, 50
  %950 = or i64 %948, %949
  %951 = lshr i64 %946, 18
  %952 = shl i64 %946, 46
  %953 = or i64 %951, %952
  %954 = xor i64 %950, %953
  %955 = lshr i64 %946, 41
  %956 = shl i64 %946, 23
  %957 = or i64 %955, %956
  %958 = xor i64 %954, %957
  %959 = xor i64 %661, %662
  %960 = and i64 %946, %959
  %961 = xor i64 %960, %662
  %962 = getelementptr inbounds [80 x i64], [80 x i64]* @_ZN12_GLOBAL__N_19constantsE, i64 0, i64 %903
  %963 = load i64, i64* %962, align 8
  %964 = extractelement <2 x i64> %909, i32 1
  %965 = add i64 %964, %663
  %966 = add i64 %965, %963
  %967 = add i64 %966, %961
  %968 = add i64 %967, %958
  %969 = lshr i64 %947, 28
  %970 = shl i64 %947, 36
  %971 = or i64 %969, %970
  %972 = lshr i64 %947, 34
  %973 = shl i64 %947, 30
  %974 = or i64 %972, %973
  %975 = xor i64 %971, %974
  %976 = lshr i64 %947, 39
  %977 = shl i64 %947, 25
  %978 = or i64 %976, %977
  %979 = xor i64 %975, %978
  %980 = and i64 %947, %657
  %981 = or i64 %947, %657
  %982 = and i64 %981, %658
  %983 = or i64 %982, %980
  %984 = add i64 %979, %983
  %985 = add i64 %968, %659
  %986 = add i64 %984, %968
  %987 = lshr i64 %985, 14
  %988 = shl i64 %985, 50
  %989 = or i64 %987, %988
  %990 = lshr i64 %985, 18
  %991 = shl i64 %985, 46
  %992 = or i64 %990, %991
  %993 = xor i64 %989, %992
  %994 = lshr i64 %985, 41
  %995 = shl i64 %985, 23
  %996 = or i64 %994, %995
  %997 = xor i64 %993, %996
  %998 = xor i64 %946, %661
  %999 = and i64 %985, %998
  %1000 = xor i64 %999, %661
  %1001 = getelementptr inbounds [80 x i64], [80 x i64]* @_ZN12_GLOBAL__N_19constantsE, i64 0, i64 %902
  %1002 = load i64, i64* %1001, align 16
  %1003 = extractelement <2 x i64> %909, i32 0
  %1004 = add i64 %1003, %662
  %1005 = add i64 %1004, %1002
  %1006 = add i64 %1005, %1000
  %1007 = add i64 %1006, %997
  %1008 = lshr i64 %986, 28
  %1009 = shl i64 %986, 36
  %1010 = or i64 %1008, %1009
  %1011 = lshr i64 %986, 34
  %1012 = shl i64 %986, 30
  %1013 = or i64 %1011, %1012
  %1014 = xor i64 %1010, %1013
  %1015 = lshr i64 %986, 39
  %1016 = shl i64 %986, 25
  %1017 = or i64 %1015, %1016
  %1018 = xor i64 %1014, %1017
  %1019 = and i64 %986, %947
  %1020 = or i64 %986, %947
  %1021 = and i64 %1020, %657
  %1022 = or i64 %1021, %1019
  %1023 = add i64 %1018, %1022
  %1024 = add i64 %1007, %658
  %1025 = add i64 %1023, %1007
  %1026 = lshr i64 %1024, 14
  %1027 = shl i64 %1024, 50
  %1028 = or i64 %1026, %1027
  %1029 = lshr i64 %1024, 18
  %1030 = shl i64 %1024, 46
  %1031 = or i64 %1029, %1030
  %1032 = xor i64 %1028, %1031
  %1033 = lshr i64 %1024, 41
  %1034 = shl i64 %1024, 23
  %1035 = or i64 %1033, %1034
  %1036 = xor i64 %1032, %1035
  %1037 = xor i64 %985, %946
  %1038 = and i64 %1024, %1037
  %1039 = xor i64 %1038, %946
  %1040 = getelementptr inbounds [80 x i64], [80 x i64]* @_ZN12_GLOBAL__N_19constantsE, i64 0, i64 %901
  %1041 = load i64, i64* %1040, align 8
  %1042 = add i64 %906, %661
  %1043 = add i64 %1042, %1041
  %1044 = add i64 %1043, %1039
  %1045 = add i64 %1044, %1036
  %1046 = lshr i64 %1025, 28
  %1047 = shl i64 %1025, 36
  %1048 = or i64 %1046, %1047
  %1049 = lshr i64 %1025, 34
  %1050 = shl i64 %1025, 30
  %1051 = or i64 %1049, %1050
  %1052 = xor i64 %1048, %1051
  %1053 = lshr i64 %1025, 39
  %1054 = shl i64 %1025, 25
  %1055 = or i64 %1053, %1054
  %1056 = xor i64 %1052, %1055
  %1057 = and i64 %1025, %986
  %1058 = or i64 %1025, %986
  %1059 = and i64 %1058, %947
  %1060 = or i64 %1059, %1057
  %1061 = add i64 %1056, %1060
  %1062 = add i64 %1045, %657
  %1063 = add i64 %1061, %1045
  %1064 = lshr i64 %1062, 14
  %1065 = shl i64 %1062, 50
  %1066 = or i64 %1064, %1065
  %1067 = lshr i64 %1062, 18
  %1068 = shl i64 %1062, 46
  %1069 = or i64 %1067, %1068
  %1070 = xor i64 %1066, %1069
  %1071 = lshr i64 %1062, 41
  %1072 = shl i64 %1062, 23
  %1073 = or i64 %1071, %1072
  %1074 = xor i64 %1070, %1073
  %1075 = xor i64 %1024, %985
  %1076 = and i64 %1062, %1075
  %1077 = xor i64 %1076, %985
  %1078 = getelementptr inbounds [80 x i64], [80 x i64]* @_ZN12_GLOBAL__N_19constantsE, i64 0, i64 %900
  %1079 = load i64, i64* %1078, align 16
  %1080 = add i64 %946, %905
  %1081 = add i64 %1080, %1079
  %1082 = add i64 %1081, %1077
  %1083 = add i64 %1082, %1074
  %1084 = lshr i64 %1063, 28
  %1085 = shl i64 %1063, 36
  %1086 = or i64 %1084, %1085
  %1087 = lshr i64 %1063, 34
  %1088 = shl i64 %1063, 30
  %1089 = or i64 %1087, %1088
  %1090 = xor i64 %1086, %1089
  %1091 = lshr i64 %1063, 39
  %1092 = shl i64 %1063, 25
  %1093 = or i64 %1091, %1092
  %1094 = xor i64 %1090, %1093
  %1095 = and i64 %1063, %1025
  %1096 = or i64 %1063, %1025
  %1097 = and i64 %1096, %986
  %1098 = or i64 %1097, %1095
  %1099 = add i64 %1094, %1098
  %1100 = add i64 %1083, %947
  %1101 = add i64 %1099, %1083
  %1102 = lshr i64 %1100, 14
  %1103 = shl i64 %1100, 50
  %1104 = or i64 %1102, %1103
  %1105 = lshr i64 %1100, 18
  %1106 = shl i64 %1100, 46
  %1107 = or i64 %1105, %1106
  %1108 = xor i64 %1104, %1107
  %1109 = lshr i64 %1100, 41
  %1110 = shl i64 %1100, 23
  %1111 = or i64 %1109, %1110
  %1112 = xor i64 %1108, %1111
  %1113 = xor i64 %1062, %1024
  %1114 = and i64 %1100, %1113
  %1115 = xor i64 %1114, %1024
  %1116 = getelementptr inbounds [80 x i64], [80 x i64]* @_ZN12_GLOBAL__N_19constantsE, i64 0, i64 %899
  %1117 = load i64, i64* %1116, align 8
  %1118 = add i64 %1117, %904
  %1119 = add i64 %1118, %985
  %1120 = add i64 %1119, %1115
  %1121 = add i64 %1120, %1112
  %1122 = lshr i64 %1101, 28
  %1123 = shl i64 %1101, 36
  %1124 = or i64 %1122, %1123
  %1125 = lshr i64 %1101, 34
  %1126 = shl i64 %1101, 30
  %1127 = or i64 %1125, %1126
  %1128 = xor i64 %1124, %1127
  %1129 = lshr i64 %1101, 39
  %1130 = shl i64 %1101, 25
  %1131 = or i64 %1129, %1130
  %1132 = xor i64 %1128, %1131
  %1133 = and i64 %1101, %1063
  %1134 = or i64 %1101, %1063
  %1135 = and i64 %1134, %1025
  %1136 = or i64 %1135, %1133
  %1137 = add i64 %1132, %1136
  %1138 = add i64 %1121, %986
  %1139 = add i64 %1137, %1121
  %1140 = lshr i64 %1138, 14
  %1141 = shl i64 %1138, 50
  %1142 = or i64 %1140, %1141
  %1143 = lshr i64 %1138, 18
  %1144 = shl i64 %1138, 46
  %1145 = or i64 %1143, %1144
  %1146 = xor i64 %1142, %1145
  %1147 = lshr i64 %1138, 41
  %1148 = shl i64 %1138, 23
  %1149 = or i64 %1147, %1148
  %1150 = xor i64 %1146, %1149
  %1151 = xor i64 %1100, %1062
  %1152 = and i64 %1138, %1151
  %1153 = xor i64 %1152, %1062
  %1154 = getelementptr inbounds [80 x i64], [80 x i64]* @_ZN12_GLOBAL__N_19constantsE, i64 0, i64 %898
  %1155 = load i64, i64* %1154, align 16
  %1156 = extractelement <2 x i64> %908, i32 1
  %1157 = add i64 %1155, %1156
  %1158 = add i64 %1157, %1024
  %1159 = add i64 %1158, %1153
  %1160 = add i64 %1159, %1150
  %1161 = lshr i64 %1139, 28
  %1162 = shl i64 %1139, 36
  %1163 = or i64 %1161, %1162
  %1164 = lshr i64 %1139, 34
  %1165 = shl i64 %1139, 30
  %1166 = or i64 %1164, %1165
  %1167 = xor i64 %1163, %1166
  %1168 = lshr i64 %1139, 39
  %1169 = shl i64 %1139, 25
  %1170 = or i64 %1168, %1169
  %1171 = xor i64 %1167, %1170
  %1172 = and i64 %1139, %1101
  %1173 = or i64 %1139, %1101
  %1174 = and i64 %1173, %1063
  %1175 = or i64 %1174, %1172
  %1176 = add i64 %1171, %1175
  %1177 = add i64 %1160, %1025
  %1178 = add i64 %1176, %1160
  %1179 = lshr i64 %1177, 14
  %1180 = shl i64 %1177, 50
  %1181 = or i64 %1179, %1180
  %1182 = lshr i64 %1177, 18
  %1183 = shl i64 %1177, 46
  %1184 = or i64 %1182, %1183
  %1185 = xor i64 %1181, %1184
  %1186 = lshr i64 %1177, 41
  %1187 = shl i64 %1177, 23
  %1188 = or i64 %1186, %1187
  %1189 = xor i64 %1185, %1188
  %1190 = xor i64 %1138, %1100
  %1191 = and i64 %1177, %1190
  %1192 = xor i64 %1191, %1100
  %1193 = getelementptr inbounds [80 x i64], [80 x i64]* @_ZN12_GLOBAL__N_19constantsE, i64 0, i64 %897
  %1194 = load i64, i64* %1193, align 8
  %1195 = extractelement <2 x i64> %908, i32 0
  %1196 = add i64 %1194, %1195
  %1197 = add i64 %1196, %1062
  %1198 = add i64 %1197, %1192
  %1199 = add i64 %1198, %1189
  %1200 = lshr i64 %1178, 28
  %1201 = shl i64 %1178, 36
  %1202 = or i64 %1200, %1201
  %1203 = lshr i64 %1178, 34
  %1204 = shl i64 %1178, 30
  %1205 = or i64 %1203, %1204
  %1206 = xor i64 %1202, %1205
  %1207 = lshr i64 %1178, 39
  %1208 = shl i64 %1178, 25
  %1209 = or i64 %1207, %1208
  %1210 = xor i64 %1206, %1209
  %1211 = and i64 %1178, %1139
  %1212 = or i64 %1178, %1139
  %1213 = and i64 %1212, %1101
  %1214 = or i64 %1213, %1211
  %1215 = add i64 %1210, %1214
  %1216 = add i64 %1199, %1063
  %1217 = add i64 %1215, %1199
  %1218 = add nuw nsw i64 %656, 1
  %1219 = icmp eq i64 %1218, 10
  br i1 %1219, label %646, label %655
}

; Function Attrs: nounwind ssp uwtable
define hidden void @_Z18CRYPT_SHA384FinishP18CRYPT_sha2_contextPh(%struct.CRYPT_sha2_context*, i8*) local_unnamed_addr #1 {
  %3 = alloca [16 x i8], align 16
  %4 = getelementptr inbounds [16 x i8], [16 x i8]* %3, i64 0, i64 0
  call void @llvm.lifetime.start.p0i8(i64 16, i8* nonnull %4) #4
  %5 = getelementptr inbounds [16 x i8], [16 x i8]* %3, i64 0, i64 8
  %6 = getelementptr inbounds [16 x i8], [16 x i8]* %3, i64 0, i64 9
  %7 = getelementptr inbounds [16 x i8], [16 x i8]* %3, i64 0, i64 10
  %8 = getelementptr inbounds [16 x i8], [16 x i8]* %3, i64 0, i64 11
  %9 = getelementptr inbounds [16 x i8], [16 x i8]* %3, i64 0, i64 12
  %10 = getelementptr inbounds [16 x i8], [16 x i8]* %3, i64 0, i64 13
  %11 = getelementptr inbounds [16 x i8], [16 x i8]* %3, i64 0, i64 14
  %12 = getelementptr inbounds [16 x i8], [16 x i8]* %3, i64 0, i64 15
  %13 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %0, i64 0, i32 0
  %14 = load i64, i64* %13, align 8
  %15 = lshr i64 %14, 53
  %16 = trunc i64 %15 to i8
  %17 = bitcast [16 x i8]* %3 to i64*
  store i64 0, i64* %17, align 16
  store i8 %16, i8* %5, align 8
  %18 = lshr i64 %14, 45
  %19 = trunc i64 %18 to i8
  store i8 %19, i8* %6, align 1
  %20 = lshr i64 %14, 37
  %21 = trunc i64 %20 to i8
  store i8 %21, i8* %7, align 2
  %22 = lshr i64 %14, 29
  %23 = trunc i64 %22 to i8
  store i8 %23, i8* %8, align 1
  %24 = lshr i64 %14, 21
  %25 = trunc i64 %24 to i8
  store i8 %25, i8* %9, align 4
  %26 = lshr i64 %14, 13
  %27 = trunc i64 %26 to i8
  store i8 %27, i8* %10, align 1
  %28 = lshr i64 %14, 5
  %29 = trunc i64 %28 to i8
  store i8 %29, i8* %11, align 2
  %30 = trunc i64 %14 to i8
  %31 = shl i8 %30, 3
  store i8 %31, i8* %12, align 1
  %32 = trunc i64 %14 to i32
  %33 = and i32 %32, 127
  %34 = icmp ult i32 %33, 112
  %35 = select i1 %34, i32 112, i32 240
  %36 = sub nsw i32 %35, %33
  %37 = icmp eq i32 %36, 0
  br i1 %37, label %73, label %38

38:                                               ; preds = %2
  %39 = sub nuw nsw i32 128, %33
  %40 = zext i32 %36 to i64
  %41 = add i64 %14, %40
  store i64 %41, i64* %13, align 8
  %42 = icmp eq i32 %33, 0
  br i1 %42, label %52, label %43

43:                                               ; preds = %38
  %44 = icmp ult i32 %36, %39
  br i1 %44, label %52, label %45

45:                                               ; preds = %43
  %46 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %0, i64 0, i32 2, i64 0
  %47 = zext i32 %33 to i64
  %48 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %0, i64 0, i32 2, i64 %47
  %49 = zext i32 %39 to i64
  tail call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 1 %48, i8* align 16 getelementptr inbounds (<{ i8, [127 x i8] }>, <{ i8, [127 x i8] }>* @_ZN12_GLOBAL__N_114sha384_paddingE, i64 0, i32 0), i64 %49, i1 false) #4
  tail call fastcc void @_ZN12_GLOBAL__N_114sha384_processEP18CRYPT_sha2_contextPKh(%struct.CRYPT_sha2_context* %0, i8* %46) #4
  %50 = sub nsw i32 %36, %39
  %51 = getelementptr inbounds i8, i8* getelementptr inbounds (<{ i8, [127 x i8] }>, <{ i8, [127 x i8] }>* @_ZN12_GLOBAL__N_114sha384_paddingE, i64 0, i32 0), i64 %49
  br label %52

52:                                               ; preds = %45, %43, %38
  %53 = phi i8* [ %51, %45 ], [ getelementptr inbounds (<{ i8, [127 x i8] }>, <{ i8, [127 x i8] }>* @_ZN12_GLOBAL__N_114sha384_paddingE, i64 0, i32 0), %43 ], [ getelementptr inbounds (<{ i8, [127 x i8] }>, <{ i8, [127 x i8] }>* @_ZN12_GLOBAL__N_114sha384_paddingE, i64 0, i32 0), %38 ]
  %54 = phi i32 [ %50, %45 ], [ %36, %43 ], [ %36, %38 ]
  %55 = phi i32 [ 0, %45 ], [ %33, %43 ], [ 0, %38 ]
  %56 = icmp ugt i32 %54, 127
  br i1 %56, label %57, label %65

57:                                               ; preds = %52, %57
  %58 = phi i32 [ %60, %57 ], [ %54, %52 ]
  %59 = phi i8* [ %61, %57 ], [ %53, %52 ]
  tail call fastcc void @_ZN12_GLOBAL__N_114sha384_processEP18CRYPT_sha2_contextPKh(%struct.CRYPT_sha2_context* %0, i8* %59) #4
  %60 = add i32 %58, -128
  %61 = getelementptr inbounds i8, i8* %59, i64 128
  %62 = icmp ugt i32 %60, 127
  br i1 %62, label %57, label %63

63:                                               ; preds = %57
  %64 = and i32 %54, 127
  br label %65

65:                                               ; preds = %63, %52
  %66 = phi i8* [ %53, %52 ], [ %61, %63 ]
  %67 = phi i32 [ %54, %52 ], [ %64, %63 ]
  %68 = icmp eq i32 %67, 0
  br i1 %68, label %73, label %69

69:                                               ; preds = %65
  %70 = zext i32 %55 to i64
  %71 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %0, i64 0, i32 2, i64 %70
  %72 = zext i32 %67 to i64
  tail call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 1 %71, i8* align 1 %66, i64 %72, i1 false) #4
  br label %73

73:                                               ; preds = %2, %65, %69
  %74 = load i64, i64* %13, align 8
  %75 = trunc i64 %74 to i32
  %76 = and i32 %75, 127
  %77 = sub nuw nsw i32 128, %76
  %78 = add i64 %74, 16
  store i64 %78, i64* %13, align 8
  %79 = icmp eq i32 %76, 0
  br i1 %79, label %90, label %80

80:                                               ; preds = %73
  %81 = icmp ult i32 %76, 112
  br i1 %81, label %90, label %82

82:                                               ; preds = %80
  %83 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %0, i64 0, i32 2, i64 0
  %84 = zext i32 %76 to i64
  %85 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %0, i64 0, i32 2, i64 %84
  %86 = zext i32 %77 to i64
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 1 %85, i8* nonnull align 16 %4, i64 %86, i1 false) #4
  tail call fastcc void @_ZN12_GLOBAL__N_114sha384_processEP18CRYPT_sha2_contextPKh(%struct.CRYPT_sha2_context* %0, i8* %83) #4
  %87 = add nsw i32 %76, -112
  %88 = getelementptr inbounds [16 x i8], [16 x i8]* %3, i64 0, i64 %86
  %89 = icmp eq i32 %87, 0
  br i1 %89, label %97, label %90

90:                                               ; preds = %73, %80, %82
  %91 = phi i32 [ 0, %82 ], [ 0, %73 ], [ %76, %80 ]
  %92 = phi i32 [ %87, %82 ], [ 16, %73 ], [ 16, %80 ]
  %93 = phi i8* [ %88, %82 ], [ %4, %73 ], [ %4, %80 ]
  %94 = zext i32 %91 to i64
  %95 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %0, i64 0, i32 2, i64 %94
  %96 = zext i32 %92 to i64
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 1 %95, i8* align 1 %93, i64 %96, i1 false) #4
  br label %97

97:                                               ; preds = %82, %90
  %98 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %0, i64 0, i32 1, i64 0
  %99 = load i64, i64* %98, align 8
  %100 = lshr i64 %99, 56
  %101 = trunc i64 %100 to i8
  store i8 %101, i8* %1, align 1
  %102 = load i64, i64* %98, align 8
  %103 = lshr i64 %102, 48
  %104 = trunc i64 %103 to i8
  %105 = getelementptr inbounds i8, i8* %1, i64 1
  store i8 %104, i8* %105, align 1
  %106 = load i64, i64* %98, align 8
  %107 = lshr i64 %106, 40
  %108 = trunc i64 %107 to i8
  %109 = getelementptr inbounds i8, i8* %1, i64 2
  store i8 %108, i8* %109, align 1
  %110 = load i64, i64* %98, align 8
  %111 = lshr i64 %110, 32
  %112 = trunc i64 %111 to i8
  %113 = getelementptr inbounds i8, i8* %1, i64 3
  store i8 %112, i8* %113, align 1
  %114 = load i64, i64* %98, align 8
  %115 = lshr i64 %114, 24
  %116 = trunc i64 %115 to i8
  %117 = getelementptr inbounds i8, i8* %1, i64 4
  store i8 %116, i8* %117, align 1
  %118 = load i64, i64* %98, align 8
  %119 = lshr i64 %118, 16
  %120 = trunc i64 %119 to i8
  %121 = getelementptr inbounds i8, i8* %1, i64 5
  store i8 %120, i8* %121, align 1
  %122 = load i64, i64* %98, align 8
  %123 = lshr i64 %122, 8
  %124 = trunc i64 %123 to i8
  %125 = getelementptr inbounds i8, i8* %1, i64 6
  store i8 %124, i8* %125, align 1
  %126 = load i64, i64* %98, align 8
  %127 = trunc i64 %126 to i8
  %128 = getelementptr inbounds i8, i8* %1, i64 7
  store i8 %127, i8* %128, align 1
  %129 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %0, i64 0, i32 1, i64 1
  %130 = load i64, i64* %129, align 8
  %131 = lshr i64 %130, 56
  %132 = trunc i64 %131 to i8
  %133 = getelementptr inbounds i8, i8* %1, i64 8
  store i8 %132, i8* %133, align 1
  %134 = load i64, i64* %129, align 8
  %135 = lshr i64 %134, 48
  %136 = trunc i64 %135 to i8
  %137 = getelementptr inbounds i8, i8* %1, i64 9
  store i8 %136, i8* %137, align 1
  %138 = load i64, i64* %129, align 8
  %139 = lshr i64 %138, 40
  %140 = trunc i64 %139 to i8
  %141 = getelementptr inbounds i8, i8* %1, i64 10
  store i8 %140, i8* %141, align 1
  %142 = load i64, i64* %129, align 8
  %143 = lshr i64 %142, 32
  %144 = trunc i64 %143 to i8
  %145 = getelementptr inbounds i8, i8* %1, i64 11
  store i8 %144, i8* %145, align 1
  %146 = load i64, i64* %129, align 8
  %147 = lshr i64 %146, 24
  %148 = trunc i64 %147 to i8
  %149 = getelementptr inbounds i8, i8* %1, i64 12
  store i8 %148, i8* %149, align 1
  %150 = load i64, i64* %129, align 8
  %151 = lshr i64 %150, 16
  %152 = trunc i64 %151 to i8
  %153 = getelementptr inbounds i8, i8* %1, i64 13
  store i8 %152, i8* %153, align 1
  %154 = load i64, i64* %129, align 8
  %155 = lshr i64 %154, 8
  %156 = trunc i64 %155 to i8
  %157 = getelementptr inbounds i8, i8* %1, i64 14
  store i8 %156, i8* %157, align 1
  %158 = load i64, i64* %129, align 8
  %159 = trunc i64 %158 to i8
  %160 = getelementptr inbounds i8, i8* %1, i64 15
  store i8 %159, i8* %160, align 1
  %161 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %0, i64 0, i32 1, i64 2
  %162 = load i64, i64* %161, align 8
  %163 = lshr i64 %162, 56
  %164 = trunc i64 %163 to i8
  %165 = getelementptr inbounds i8, i8* %1, i64 16
  store i8 %164, i8* %165, align 1
  %166 = load i64, i64* %161, align 8
  %167 = lshr i64 %166, 48
  %168 = trunc i64 %167 to i8
  %169 = getelementptr inbounds i8, i8* %1, i64 17
  store i8 %168, i8* %169, align 1
  %170 = load i64, i64* %161, align 8
  %171 = lshr i64 %170, 40
  %172 = trunc i64 %171 to i8
  %173 = getelementptr inbounds i8, i8* %1, i64 18
  store i8 %172, i8* %173, align 1
  %174 = load i64, i64* %161, align 8
  %175 = lshr i64 %174, 32
  %176 = trunc i64 %175 to i8
  %177 = getelementptr inbounds i8, i8* %1, i64 19
  store i8 %176, i8* %177, align 1
  %178 = load i64, i64* %161, align 8
  %179 = lshr i64 %178, 24
  %180 = trunc i64 %179 to i8
  %181 = getelementptr inbounds i8, i8* %1, i64 20
  store i8 %180, i8* %181, align 1
  %182 = load i64, i64* %161, align 8
  %183 = lshr i64 %182, 16
  %184 = trunc i64 %183 to i8
  %185 = getelementptr inbounds i8, i8* %1, i64 21
  store i8 %184, i8* %185, align 1
  %186 = load i64, i64* %161, align 8
  %187 = lshr i64 %186, 8
  %188 = trunc i64 %187 to i8
  %189 = getelementptr inbounds i8, i8* %1, i64 22
  store i8 %188, i8* %189, align 1
  %190 = load i64, i64* %161, align 8
  %191 = trunc i64 %190 to i8
  %192 = getelementptr inbounds i8, i8* %1, i64 23
  store i8 %191, i8* %192, align 1
  %193 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %0, i64 0, i32 1, i64 3
  %194 = load i64, i64* %193, align 8
  %195 = lshr i64 %194, 56
  %196 = trunc i64 %195 to i8
  %197 = getelementptr inbounds i8, i8* %1, i64 24
  store i8 %196, i8* %197, align 1
  %198 = load i64, i64* %193, align 8
  %199 = lshr i64 %198, 48
  %200 = trunc i64 %199 to i8
  %201 = getelementptr inbounds i8, i8* %1, i64 25
  store i8 %200, i8* %201, align 1
  %202 = load i64, i64* %193, align 8
  %203 = lshr i64 %202, 40
  %204 = trunc i64 %203 to i8
  %205 = getelementptr inbounds i8, i8* %1, i64 26
  store i8 %204, i8* %205, align 1
  %206 = load i64, i64* %193, align 8
  %207 = lshr i64 %206, 32
  %208 = trunc i64 %207 to i8
  %209 = getelementptr inbounds i8, i8* %1, i64 27
  store i8 %208, i8* %209, align 1
  %210 = load i64, i64* %193, align 8
  %211 = lshr i64 %210, 24
  %212 = trunc i64 %211 to i8
  %213 = getelementptr inbounds i8, i8* %1, i64 28
  store i8 %212, i8* %213, align 1
  %214 = load i64, i64* %193, align 8
  %215 = lshr i64 %214, 16
  %216 = trunc i64 %215 to i8
  %217 = getelementptr inbounds i8, i8* %1, i64 29
  store i8 %216, i8* %217, align 1
  %218 = load i64, i64* %193, align 8
  %219 = lshr i64 %218, 8
  %220 = trunc i64 %219 to i8
  %221 = getelementptr inbounds i8, i8* %1, i64 30
  store i8 %220, i8* %221, align 1
  %222 = load i64, i64* %193, align 8
  %223 = trunc i64 %222 to i8
  %224 = getelementptr inbounds i8, i8* %1, i64 31
  store i8 %223, i8* %224, align 1
  %225 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %0, i64 0, i32 1, i64 4
  %226 = load i64, i64* %225, align 8
  %227 = lshr i64 %226, 56
  %228 = trunc i64 %227 to i8
  %229 = getelementptr inbounds i8, i8* %1, i64 32
  store i8 %228, i8* %229, align 1
  %230 = load i64, i64* %225, align 8
  %231 = lshr i64 %230, 48
  %232 = trunc i64 %231 to i8
  %233 = getelementptr inbounds i8, i8* %1, i64 33
  store i8 %232, i8* %233, align 1
  %234 = load i64, i64* %225, align 8
  %235 = lshr i64 %234, 40
  %236 = trunc i64 %235 to i8
  %237 = getelementptr inbounds i8, i8* %1, i64 34
  store i8 %236, i8* %237, align 1
  %238 = load i64, i64* %225, align 8
  %239 = lshr i64 %238, 32
  %240 = trunc i64 %239 to i8
  %241 = getelementptr inbounds i8, i8* %1, i64 35
  store i8 %240, i8* %241, align 1
  %242 = load i64, i64* %225, align 8
  %243 = lshr i64 %242, 24
  %244 = trunc i64 %243 to i8
  %245 = getelementptr inbounds i8, i8* %1, i64 36
  store i8 %244, i8* %245, align 1
  %246 = load i64, i64* %225, align 8
  %247 = lshr i64 %246, 16
  %248 = trunc i64 %247 to i8
  %249 = getelementptr inbounds i8, i8* %1, i64 37
  store i8 %248, i8* %249, align 1
  %250 = load i64, i64* %225, align 8
  %251 = lshr i64 %250, 8
  %252 = trunc i64 %251 to i8
  %253 = getelementptr inbounds i8, i8* %1, i64 38
  store i8 %252, i8* %253, align 1
  %254 = load i64, i64* %225, align 8
  %255 = trunc i64 %254 to i8
  %256 = getelementptr inbounds i8, i8* %1, i64 39
  store i8 %255, i8* %256, align 1
  %257 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %0, i64 0, i32 1, i64 5
  %258 = load i64, i64* %257, align 8
  %259 = lshr i64 %258, 56
  %260 = trunc i64 %259 to i8
  %261 = getelementptr inbounds i8, i8* %1, i64 40
  store i8 %260, i8* %261, align 1
  %262 = load i64, i64* %257, align 8
  %263 = lshr i64 %262, 48
  %264 = trunc i64 %263 to i8
  %265 = getelementptr inbounds i8, i8* %1, i64 41
  store i8 %264, i8* %265, align 1
  %266 = load i64, i64* %257, align 8
  %267 = lshr i64 %266, 40
  %268 = trunc i64 %267 to i8
  %269 = getelementptr inbounds i8, i8* %1, i64 42
  store i8 %268, i8* %269, align 1
  %270 = load i64, i64* %257, align 8
  %271 = lshr i64 %270, 32
  %272 = trunc i64 %271 to i8
  %273 = getelementptr inbounds i8, i8* %1, i64 43
  store i8 %272, i8* %273, align 1
  %274 = load i64, i64* %257, align 8
  %275 = lshr i64 %274, 24
  %276 = trunc i64 %275 to i8
  %277 = getelementptr inbounds i8, i8* %1, i64 44
  store i8 %276, i8* %277, align 1
  %278 = load i64, i64* %257, align 8
  %279 = lshr i64 %278, 16
  %280 = trunc i64 %279 to i8
  %281 = getelementptr inbounds i8, i8* %1, i64 45
  store i8 %280, i8* %281, align 1
  %282 = load i64, i64* %257, align 8
  %283 = lshr i64 %282, 8
  %284 = trunc i64 %283 to i8
  %285 = getelementptr inbounds i8, i8* %1, i64 46
  store i8 %284, i8* %285, align 1
  %286 = load i64, i64* %257, align 8
  %287 = trunc i64 %286 to i8
  %288 = getelementptr inbounds i8, i8* %1, i64 47
  store i8 %287, i8* %288, align 1
  call void @llvm.lifetime.end.p0i8(i64 16, i8* nonnull %4) #4
  ret void
}

; Function Attrs: nounwind ssp uwtable
define hidden void @_Z20CRYPT_SHA384GeneratePKhjPh(i8* readonly, i32, i8*) local_unnamed_addr #1 {
  %4 = alloca %struct.CRYPT_sha2_context, align 16
  %5 = bitcast %struct.CRYPT_sha2_context* %4 to i8*
  call void @llvm.lifetime.start.p0i8(i64 200, i8* nonnull %5) #4
  %6 = bitcast %struct.CRYPT_sha2_context* %4 to <2 x i64>*
  store <2 x i64> <i64 0, i64 -3766243637369397544>, <2 x i64>* %6, align 16
  %7 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %4, i64 0, i32 1, i64 1
  %8 = bitcast i64* %7 to <2 x i64>*
  store <2 x i64> <i64 7105036623409894663, i64 -7973340178411365097>, <2 x i64>* %8, align 8
  %9 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %4, i64 0, i32 1, i64 3
  %10 = bitcast i64* %9 to <2 x i64>*
  store <2 x i64> <i64 1526699215303891257, i64 7436329637833083697>, <2 x i64>* %10, align 8
  %11 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %4, i64 0, i32 1, i64 5
  %12 = bitcast i64* %11 to <2 x i64>*
  store <2 x i64> <i64 -8163818279084223215, i64 -2662702644619276377>, <2 x i64>* %12, align 8
  %13 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %4, i64 0, i32 1, i64 7
  store i64 5167115440072839076, i64* %13, align 8
  %14 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %4, i64 0, i32 2, i64 0
  call void @llvm.memset.p0i8.i64(i8* align 8 %14, i8 0, i64 128, i1 false) #4
  %15 = icmp eq i32 %1, 0
  br i1 %15, label %34, label %16

16:                                               ; preds = %3
  %17 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %4, i64 0, i32 0
  %18 = zext i32 %1 to i64
  store i64 %18, i64* %17, align 16
  %19 = icmp ugt i32 %1, 127
  br i1 %19, label %20, label %31

20:                                               ; preds = %16, %20
  %21 = phi i32 [ %23, %20 ], [ %1, %16 ]
  %22 = phi i8* [ %24, %20 ], [ %0, %16 ]
  call fastcc void @_ZN12_GLOBAL__N_114sha384_processEP18CRYPT_sha2_contextPKh(%struct.CRYPT_sha2_context* nonnull %4, i8* %22) #4
  %23 = add i32 %21, -128
  %24 = getelementptr inbounds i8, i8* %22, i64 128
  %25 = icmp ugt i32 %23, 127
  br i1 %25, label %20, label %26

26:                                               ; preds = %20
  %27 = and i32 %1, 127
  %28 = icmp eq i32 %27, 0
  br i1 %28, label %34, label %29

29:                                               ; preds = %26
  %30 = zext i32 %27 to i64
  br label %31

31:                                               ; preds = %29, %16
  %32 = phi i64 [ %30, %29 ], [ %18, %16 ]
  %33 = phi i8* [ %24, %29 ], [ %0, %16 ]
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 8 %14, i8* align 1 %33, i64 %32, i1 false) #4
  br label %34

34:                                               ; preds = %3, %26, %31
  call void @_Z18CRYPT_SHA384FinishP18CRYPT_sha2_contextPh(%struct.CRYPT_sha2_context* nonnull %4, i8* %2)
  call void @llvm.lifetime.end.p0i8(i64 200, i8* nonnull %5) #4
  ret void
}

; Function Attrs: nounwind ssp uwtable
define hidden void @_Z17CRYPT_SHA512StartP18CRYPT_sha2_context(%struct.CRYPT_sha2_context* nocapture) local_unnamed_addr #1 {
  %2 = bitcast %struct.CRYPT_sha2_context* %0 to <2 x i64>*
  store <2 x i64> <i64 0, i64 7640891576956012808>, <2 x i64>* %2, align 8
  %3 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %0, i64 0, i32 1, i64 1
  %4 = bitcast i64* %3 to <2 x i64>*
  store <2 x i64> <i64 -4942790177534073029, i64 4354685564936845355>, <2 x i64>* %4, align 8
  %5 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %0, i64 0, i32 1, i64 3
  %6 = bitcast i64* %5 to <2 x i64>*
  store <2 x i64> <i64 -6534734903238641935, i64 5840696475078001361>, <2 x i64>* %6, align 8
  %7 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %0, i64 0, i32 1, i64 5
  %8 = bitcast i64* %7 to <2 x i64>*
  store <2 x i64> <i64 -7276294671716946913, i64 2270897969802886507>, <2 x i64>* %8, align 8
  %9 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %0, i64 0, i32 1, i64 7
  store i64 6620516959819538809, i64* %9, align 8
  %10 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %0, i64 0, i32 2, i64 0
  tail call void @llvm.memset.p0i8.i64(i8* align 8 %10, i8 0, i64 128, i1 false)
  ret void
}

; Function Attrs: nounwind ssp uwtable
define hidden void @_Z18CRYPT_SHA512UpdateP18CRYPT_sha2_contextPKhj(%struct.CRYPT_sha2_context*, i8* readonly, i32) local_unnamed_addr #1 {
  %4 = icmp eq i32 %2, 0
  br i1 %4, label %44, label %5

5:                                                ; preds = %3
  %6 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %0, i64 0, i32 0
  %7 = load i64, i64* %6, align 8
  %8 = trunc i64 %7 to i32
  %9 = and i32 %8, 127
  %10 = sub nuw nsw i32 128, %9
  %11 = zext i32 %2 to i64
  %12 = add i64 %7, %11
  store i64 %12, i64* %6, align 8
  %13 = icmp eq i32 %9, 0
  br i1 %13, label %23, label %14

14:                                               ; preds = %5
  %15 = icmp ugt i32 %10, %2
  br i1 %15, label %23, label %16

16:                                               ; preds = %14
  %17 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %0, i64 0, i32 2, i64 0
  %18 = zext i32 %9 to i64
  %19 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %0, i64 0, i32 2, i64 %18
  %20 = zext i32 %10 to i64
  tail call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 1 %19, i8* align 1 %1, i64 %20, i1 false) #4
  tail call fastcc void @_ZN12_GLOBAL__N_114sha384_processEP18CRYPT_sha2_contextPKh(%struct.CRYPT_sha2_context* %0, i8* %17) #4
  %21 = sub i32 %2, %10
  %22 = getelementptr inbounds i8, i8* %1, i64 %20
  br label %23

23:                                               ; preds = %16, %14, %5
  %24 = phi i8* [ %22, %16 ], [ %1, %14 ], [ %1, %5 ]
  %25 = phi i32 [ %21, %16 ], [ %2, %14 ], [ %2, %5 ]
  %26 = phi i32 [ 0, %16 ], [ %9, %14 ], [ 0, %5 ]
  %27 = icmp ugt i32 %25, 127
  br i1 %27, label %28, label %36

28:                                               ; preds = %23, %28
  %29 = phi i32 [ %31, %28 ], [ %25, %23 ]
  %30 = phi i8* [ %32, %28 ], [ %24, %23 ]
  tail call fastcc void @_ZN12_GLOBAL__N_114sha384_processEP18CRYPT_sha2_contextPKh(%struct.CRYPT_sha2_context* %0, i8* %30) #4
  %31 = add i32 %29, -128
  %32 = getelementptr inbounds i8, i8* %30, i64 128
  %33 = icmp ugt i32 %31, 127
  br i1 %33, label %28, label %34

34:                                               ; preds = %28
  %35 = and i32 %25, 127
  br label %36

36:                                               ; preds = %34, %23
  %37 = phi i8* [ %24, %23 ], [ %32, %34 ]
  %38 = phi i32 [ %25, %23 ], [ %35, %34 ]
  %39 = icmp eq i32 %38, 0
  br i1 %39, label %44, label %40

40:                                               ; preds = %36
  %41 = zext i32 %26 to i64
  %42 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %0, i64 0, i32 2, i64 %41
  %43 = zext i32 %38 to i64
  tail call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 1 %42, i8* align 1 %37, i64 %43, i1 false) #4
  br label %44

44:                                               ; preds = %3, %36, %40
  ret void
}

; Function Attrs: nounwind ssp uwtable
define hidden void @_Z18CRYPT_SHA512FinishP18CRYPT_sha2_contextPh(%struct.CRYPT_sha2_context*, i8*) local_unnamed_addr #1 {
  %3 = alloca [16 x i8], align 16
  %4 = getelementptr inbounds [16 x i8], [16 x i8]* %3, i64 0, i64 0
  call void @llvm.lifetime.start.p0i8(i64 16, i8* nonnull %4) #4
  %5 = getelementptr inbounds [16 x i8], [16 x i8]* %3, i64 0, i64 8
  %6 = getelementptr inbounds [16 x i8], [16 x i8]* %3, i64 0, i64 9
  %7 = getelementptr inbounds [16 x i8], [16 x i8]* %3, i64 0, i64 10
  %8 = getelementptr inbounds [16 x i8], [16 x i8]* %3, i64 0, i64 11
  %9 = getelementptr inbounds [16 x i8], [16 x i8]* %3, i64 0, i64 12
  %10 = getelementptr inbounds [16 x i8], [16 x i8]* %3, i64 0, i64 13
  %11 = getelementptr inbounds [16 x i8], [16 x i8]* %3, i64 0, i64 14
  %12 = getelementptr inbounds [16 x i8], [16 x i8]* %3, i64 0, i64 15
  %13 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %0, i64 0, i32 0
  %14 = load i64, i64* %13, align 8
  %15 = lshr i64 %14, 53
  %16 = trunc i64 %15 to i8
  %17 = bitcast [16 x i8]* %3 to i64*
  store i64 0, i64* %17, align 16
  store i8 %16, i8* %5, align 8
  %18 = lshr i64 %14, 45
  %19 = trunc i64 %18 to i8
  store i8 %19, i8* %6, align 1
  %20 = lshr i64 %14, 37
  %21 = trunc i64 %20 to i8
  store i8 %21, i8* %7, align 2
  %22 = lshr i64 %14, 29
  %23 = trunc i64 %22 to i8
  store i8 %23, i8* %8, align 1
  %24 = lshr i64 %14, 21
  %25 = trunc i64 %24 to i8
  store i8 %25, i8* %9, align 4
  %26 = lshr i64 %14, 13
  %27 = trunc i64 %26 to i8
  store i8 %27, i8* %10, align 1
  %28 = lshr i64 %14, 5
  %29 = trunc i64 %28 to i8
  store i8 %29, i8* %11, align 2
  %30 = trunc i64 %14 to i8
  %31 = shl i8 %30, 3
  store i8 %31, i8* %12, align 1
  %32 = trunc i64 %14 to i32
  %33 = and i32 %32, 127
  %34 = icmp ult i32 %33, 112
  %35 = select i1 %34, i32 112, i32 240
  %36 = sub nsw i32 %35, %33
  %37 = icmp eq i32 %36, 0
  br i1 %37, label %73, label %38

38:                                               ; preds = %2
  %39 = sub nuw nsw i32 128, %33
  %40 = zext i32 %36 to i64
  %41 = add i64 %14, %40
  store i64 %41, i64* %13, align 8
  %42 = icmp eq i32 %33, 0
  br i1 %42, label %52, label %43

43:                                               ; preds = %38
  %44 = icmp ult i32 %36, %39
  br i1 %44, label %52, label %45

45:                                               ; preds = %43
  %46 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %0, i64 0, i32 2, i64 0
  %47 = zext i32 %33 to i64
  %48 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %0, i64 0, i32 2, i64 %47
  %49 = zext i32 %39 to i64
  tail call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 1 %48, i8* align 16 getelementptr inbounds (<{ i8, [127 x i8] }>, <{ i8, [127 x i8] }>* @_ZN12_GLOBAL__N_114sha384_paddingE, i64 0, i32 0), i64 %49, i1 false) #4
  tail call fastcc void @_ZN12_GLOBAL__N_114sha384_processEP18CRYPT_sha2_contextPKh(%struct.CRYPT_sha2_context* %0, i8* %46) #4
  %50 = sub nsw i32 %36, %39
  %51 = getelementptr inbounds i8, i8* getelementptr inbounds (<{ i8, [127 x i8] }>, <{ i8, [127 x i8] }>* @_ZN12_GLOBAL__N_114sha384_paddingE, i64 0, i32 0), i64 %49
  br label %52

52:                                               ; preds = %45, %43, %38
  %53 = phi i8* [ %51, %45 ], [ getelementptr inbounds (<{ i8, [127 x i8] }>, <{ i8, [127 x i8] }>* @_ZN12_GLOBAL__N_114sha384_paddingE, i64 0, i32 0), %43 ], [ getelementptr inbounds (<{ i8, [127 x i8] }>, <{ i8, [127 x i8] }>* @_ZN12_GLOBAL__N_114sha384_paddingE, i64 0, i32 0), %38 ]
  %54 = phi i32 [ %50, %45 ], [ %36, %43 ], [ %36, %38 ]
  %55 = phi i32 [ 0, %45 ], [ %33, %43 ], [ 0, %38 ]
  %56 = icmp ugt i32 %54, 127
  br i1 %56, label %57, label %65

57:                                               ; preds = %52, %57
  %58 = phi i32 [ %60, %57 ], [ %54, %52 ]
  %59 = phi i8* [ %61, %57 ], [ %53, %52 ]
  tail call fastcc void @_ZN12_GLOBAL__N_114sha384_processEP18CRYPT_sha2_contextPKh(%struct.CRYPT_sha2_context* %0, i8* %59) #4
  %60 = add i32 %58, -128
  %61 = getelementptr inbounds i8, i8* %59, i64 128
  %62 = icmp ugt i32 %60, 127
  br i1 %62, label %57, label %63

63:                                               ; preds = %57
  %64 = and i32 %54, 127
  br label %65

65:                                               ; preds = %63, %52
  %66 = phi i8* [ %53, %52 ], [ %61, %63 ]
  %67 = phi i32 [ %54, %52 ], [ %64, %63 ]
  %68 = icmp eq i32 %67, 0
  br i1 %68, label %73, label %69

69:                                               ; preds = %65
  %70 = zext i32 %55 to i64
  %71 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %0, i64 0, i32 2, i64 %70
  %72 = zext i32 %67 to i64
  tail call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 1 %71, i8* align 1 %66, i64 %72, i1 false) #4
  br label %73

73:                                               ; preds = %2, %65, %69
  %74 = load i64, i64* %13, align 8
  %75 = trunc i64 %74 to i32
  %76 = and i32 %75, 127
  %77 = sub nuw nsw i32 128, %76
  %78 = add i64 %74, 16
  store i64 %78, i64* %13, align 8
  %79 = icmp eq i32 %76, 0
  br i1 %79, label %90, label %80

80:                                               ; preds = %73
  %81 = icmp ult i32 %76, 112
  br i1 %81, label %90, label %82

82:                                               ; preds = %80
  %83 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %0, i64 0, i32 2, i64 0
  %84 = zext i32 %76 to i64
  %85 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %0, i64 0, i32 2, i64 %84
  %86 = zext i32 %77 to i64
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 1 %85, i8* nonnull align 16 %4, i64 %86, i1 false) #4
  tail call fastcc void @_ZN12_GLOBAL__N_114sha384_processEP18CRYPT_sha2_contextPKh(%struct.CRYPT_sha2_context* %0, i8* %83) #4
  %87 = add nsw i32 %76, -112
  %88 = getelementptr inbounds [16 x i8], [16 x i8]* %3, i64 0, i64 %86
  %89 = icmp eq i32 %87, 0
  br i1 %89, label %97, label %90

90:                                               ; preds = %73, %80, %82
  %91 = phi i32 [ 0, %82 ], [ 0, %73 ], [ %76, %80 ]
  %92 = phi i32 [ %87, %82 ], [ 16, %73 ], [ 16, %80 ]
  %93 = phi i8* [ %88, %82 ], [ %4, %73 ], [ %4, %80 ]
  %94 = zext i32 %91 to i64
  %95 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %0, i64 0, i32 2, i64 %94
  %96 = zext i32 %92 to i64
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 1 %95, i8* align 1 %93, i64 %96, i1 false) #4
  br label %97

97:                                               ; preds = %82, %90
  %98 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %0, i64 0, i32 1, i64 0
  %99 = load i64, i64* %98, align 8
  %100 = lshr i64 %99, 56
  %101 = trunc i64 %100 to i8
  store i8 %101, i8* %1, align 1
  %102 = load i64, i64* %98, align 8
  %103 = lshr i64 %102, 48
  %104 = trunc i64 %103 to i8
  %105 = getelementptr inbounds i8, i8* %1, i64 1
  store i8 %104, i8* %105, align 1
  %106 = load i64, i64* %98, align 8
  %107 = lshr i64 %106, 40
  %108 = trunc i64 %107 to i8
  %109 = getelementptr inbounds i8, i8* %1, i64 2
  store i8 %108, i8* %109, align 1
  %110 = load i64, i64* %98, align 8
  %111 = lshr i64 %110, 32
  %112 = trunc i64 %111 to i8
  %113 = getelementptr inbounds i8, i8* %1, i64 3
  store i8 %112, i8* %113, align 1
  %114 = load i64, i64* %98, align 8
  %115 = lshr i64 %114, 24
  %116 = trunc i64 %115 to i8
  %117 = getelementptr inbounds i8, i8* %1, i64 4
  store i8 %116, i8* %117, align 1
  %118 = load i64, i64* %98, align 8
  %119 = lshr i64 %118, 16
  %120 = trunc i64 %119 to i8
  %121 = getelementptr inbounds i8, i8* %1, i64 5
  store i8 %120, i8* %121, align 1
  %122 = load i64, i64* %98, align 8
  %123 = lshr i64 %122, 8
  %124 = trunc i64 %123 to i8
  %125 = getelementptr inbounds i8, i8* %1, i64 6
  store i8 %124, i8* %125, align 1
  %126 = load i64, i64* %98, align 8
  %127 = trunc i64 %126 to i8
  %128 = getelementptr inbounds i8, i8* %1, i64 7
  store i8 %127, i8* %128, align 1
  %129 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %0, i64 0, i32 1, i64 1
  %130 = load i64, i64* %129, align 8
  %131 = lshr i64 %130, 56
  %132 = trunc i64 %131 to i8
  %133 = getelementptr inbounds i8, i8* %1, i64 8
  store i8 %132, i8* %133, align 1
  %134 = load i64, i64* %129, align 8
  %135 = lshr i64 %134, 48
  %136 = trunc i64 %135 to i8
  %137 = getelementptr inbounds i8, i8* %1, i64 9
  store i8 %136, i8* %137, align 1
  %138 = load i64, i64* %129, align 8
  %139 = lshr i64 %138, 40
  %140 = trunc i64 %139 to i8
  %141 = getelementptr inbounds i8, i8* %1, i64 10
  store i8 %140, i8* %141, align 1
  %142 = load i64, i64* %129, align 8
  %143 = lshr i64 %142, 32
  %144 = trunc i64 %143 to i8
  %145 = getelementptr inbounds i8, i8* %1, i64 11
  store i8 %144, i8* %145, align 1
  %146 = load i64, i64* %129, align 8
  %147 = lshr i64 %146, 24
  %148 = trunc i64 %147 to i8
  %149 = getelementptr inbounds i8, i8* %1, i64 12
  store i8 %148, i8* %149, align 1
  %150 = load i64, i64* %129, align 8
  %151 = lshr i64 %150, 16
  %152 = trunc i64 %151 to i8
  %153 = getelementptr inbounds i8, i8* %1, i64 13
  store i8 %152, i8* %153, align 1
  %154 = load i64, i64* %129, align 8
  %155 = lshr i64 %154, 8
  %156 = trunc i64 %155 to i8
  %157 = getelementptr inbounds i8, i8* %1, i64 14
  store i8 %156, i8* %157, align 1
  %158 = load i64, i64* %129, align 8
  %159 = trunc i64 %158 to i8
  %160 = getelementptr inbounds i8, i8* %1, i64 15
  store i8 %159, i8* %160, align 1
  %161 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %0, i64 0, i32 1, i64 2
  %162 = load i64, i64* %161, align 8
  %163 = lshr i64 %162, 56
  %164 = trunc i64 %163 to i8
  %165 = getelementptr inbounds i8, i8* %1, i64 16
  store i8 %164, i8* %165, align 1
  %166 = load i64, i64* %161, align 8
  %167 = lshr i64 %166, 48
  %168 = trunc i64 %167 to i8
  %169 = getelementptr inbounds i8, i8* %1, i64 17
  store i8 %168, i8* %169, align 1
  %170 = load i64, i64* %161, align 8
  %171 = lshr i64 %170, 40
  %172 = trunc i64 %171 to i8
  %173 = getelementptr inbounds i8, i8* %1, i64 18
  store i8 %172, i8* %173, align 1
  %174 = load i64, i64* %161, align 8
  %175 = lshr i64 %174, 32
  %176 = trunc i64 %175 to i8
  %177 = getelementptr inbounds i8, i8* %1, i64 19
  store i8 %176, i8* %177, align 1
  %178 = load i64, i64* %161, align 8
  %179 = lshr i64 %178, 24
  %180 = trunc i64 %179 to i8
  %181 = getelementptr inbounds i8, i8* %1, i64 20
  store i8 %180, i8* %181, align 1
  %182 = load i64, i64* %161, align 8
  %183 = lshr i64 %182, 16
  %184 = trunc i64 %183 to i8
  %185 = getelementptr inbounds i8, i8* %1, i64 21
  store i8 %184, i8* %185, align 1
  %186 = load i64, i64* %161, align 8
  %187 = lshr i64 %186, 8
  %188 = trunc i64 %187 to i8
  %189 = getelementptr inbounds i8, i8* %1, i64 22
  store i8 %188, i8* %189, align 1
  %190 = load i64, i64* %161, align 8
  %191 = trunc i64 %190 to i8
  %192 = getelementptr inbounds i8, i8* %1, i64 23
  store i8 %191, i8* %192, align 1
  %193 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %0, i64 0, i32 1, i64 3
  %194 = load i64, i64* %193, align 8
  %195 = lshr i64 %194, 56
  %196 = trunc i64 %195 to i8
  %197 = getelementptr inbounds i8, i8* %1, i64 24
  store i8 %196, i8* %197, align 1
  %198 = load i64, i64* %193, align 8
  %199 = lshr i64 %198, 48
  %200 = trunc i64 %199 to i8
  %201 = getelementptr inbounds i8, i8* %1, i64 25
  store i8 %200, i8* %201, align 1
  %202 = load i64, i64* %193, align 8
  %203 = lshr i64 %202, 40
  %204 = trunc i64 %203 to i8
  %205 = getelementptr inbounds i8, i8* %1, i64 26
  store i8 %204, i8* %205, align 1
  %206 = load i64, i64* %193, align 8
  %207 = lshr i64 %206, 32
  %208 = trunc i64 %207 to i8
  %209 = getelementptr inbounds i8, i8* %1, i64 27
  store i8 %208, i8* %209, align 1
  %210 = load i64, i64* %193, align 8
  %211 = lshr i64 %210, 24
  %212 = trunc i64 %211 to i8
  %213 = getelementptr inbounds i8, i8* %1, i64 28
  store i8 %212, i8* %213, align 1
  %214 = load i64, i64* %193, align 8
  %215 = lshr i64 %214, 16
  %216 = trunc i64 %215 to i8
  %217 = getelementptr inbounds i8, i8* %1, i64 29
  store i8 %216, i8* %217, align 1
  %218 = load i64, i64* %193, align 8
  %219 = lshr i64 %218, 8
  %220 = trunc i64 %219 to i8
  %221 = getelementptr inbounds i8, i8* %1, i64 30
  store i8 %220, i8* %221, align 1
  %222 = load i64, i64* %193, align 8
  %223 = trunc i64 %222 to i8
  %224 = getelementptr inbounds i8, i8* %1, i64 31
  store i8 %223, i8* %224, align 1
  %225 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %0, i64 0, i32 1, i64 4
  %226 = load i64, i64* %225, align 8
  %227 = lshr i64 %226, 56
  %228 = trunc i64 %227 to i8
  %229 = getelementptr inbounds i8, i8* %1, i64 32
  store i8 %228, i8* %229, align 1
  %230 = load i64, i64* %225, align 8
  %231 = lshr i64 %230, 48
  %232 = trunc i64 %231 to i8
  %233 = getelementptr inbounds i8, i8* %1, i64 33
  store i8 %232, i8* %233, align 1
  %234 = load i64, i64* %225, align 8
  %235 = lshr i64 %234, 40
  %236 = trunc i64 %235 to i8
  %237 = getelementptr inbounds i8, i8* %1, i64 34
  store i8 %236, i8* %237, align 1
  %238 = load i64, i64* %225, align 8
  %239 = lshr i64 %238, 32
  %240 = trunc i64 %239 to i8
  %241 = getelementptr inbounds i8, i8* %1, i64 35
  store i8 %240, i8* %241, align 1
  %242 = load i64, i64* %225, align 8
  %243 = lshr i64 %242, 24
  %244 = trunc i64 %243 to i8
  %245 = getelementptr inbounds i8, i8* %1, i64 36
  store i8 %244, i8* %245, align 1
  %246 = load i64, i64* %225, align 8
  %247 = lshr i64 %246, 16
  %248 = trunc i64 %247 to i8
  %249 = getelementptr inbounds i8, i8* %1, i64 37
  store i8 %248, i8* %249, align 1
  %250 = load i64, i64* %225, align 8
  %251 = lshr i64 %250, 8
  %252 = trunc i64 %251 to i8
  %253 = getelementptr inbounds i8, i8* %1, i64 38
  store i8 %252, i8* %253, align 1
  %254 = load i64, i64* %225, align 8
  %255 = trunc i64 %254 to i8
  %256 = getelementptr inbounds i8, i8* %1, i64 39
  store i8 %255, i8* %256, align 1
  %257 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %0, i64 0, i32 1, i64 5
  %258 = load i64, i64* %257, align 8
  %259 = lshr i64 %258, 56
  %260 = trunc i64 %259 to i8
  %261 = getelementptr inbounds i8, i8* %1, i64 40
  store i8 %260, i8* %261, align 1
  %262 = load i64, i64* %257, align 8
  %263 = lshr i64 %262, 48
  %264 = trunc i64 %263 to i8
  %265 = getelementptr inbounds i8, i8* %1, i64 41
  store i8 %264, i8* %265, align 1
  %266 = load i64, i64* %257, align 8
  %267 = lshr i64 %266, 40
  %268 = trunc i64 %267 to i8
  %269 = getelementptr inbounds i8, i8* %1, i64 42
  store i8 %268, i8* %269, align 1
  %270 = load i64, i64* %257, align 8
  %271 = lshr i64 %270, 32
  %272 = trunc i64 %271 to i8
  %273 = getelementptr inbounds i8, i8* %1, i64 43
  store i8 %272, i8* %273, align 1
  %274 = load i64, i64* %257, align 8
  %275 = lshr i64 %274, 24
  %276 = trunc i64 %275 to i8
  %277 = getelementptr inbounds i8, i8* %1, i64 44
  store i8 %276, i8* %277, align 1
  %278 = load i64, i64* %257, align 8
  %279 = lshr i64 %278, 16
  %280 = trunc i64 %279 to i8
  %281 = getelementptr inbounds i8, i8* %1, i64 45
  store i8 %280, i8* %281, align 1
  %282 = load i64, i64* %257, align 8
  %283 = lshr i64 %282, 8
  %284 = trunc i64 %283 to i8
  %285 = getelementptr inbounds i8, i8* %1, i64 46
  store i8 %284, i8* %285, align 1
  %286 = load i64, i64* %257, align 8
  %287 = trunc i64 %286 to i8
  %288 = getelementptr inbounds i8, i8* %1, i64 47
  store i8 %287, i8* %288, align 1
  %289 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %0, i64 0, i32 1, i64 6
  %290 = load i64, i64* %289, align 8
  %291 = lshr i64 %290, 56
  %292 = trunc i64 %291 to i8
  %293 = getelementptr inbounds i8, i8* %1, i64 48
  store i8 %292, i8* %293, align 1
  %294 = load i64, i64* %289, align 8
  %295 = lshr i64 %294, 48
  %296 = trunc i64 %295 to i8
  %297 = getelementptr inbounds i8, i8* %1, i64 49
  store i8 %296, i8* %297, align 1
  %298 = load i64, i64* %289, align 8
  %299 = lshr i64 %298, 40
  %300 = trunc i64 %299 to i8
  %301 = getelementptr inbounds i8, i8* %1, i64 50
  store i8 %300, i8* %301, align 1
  %302 = load i64, i64* %289, align 8
  %303 = lshr i64 %302, 32
  %304 = trunc i64 %303 to i8
  %305 = getelementptr inbounds i8, i8* %1, i64 51
  store i8 %304, i8* %305, align 1
  %306 = load i64, i64* %289, align 8
  %307 = lshr i64 %306, 24
  %308 = trunc i64 %307 to i8
  %309 = getelementptr inbounds i8, i8* %1, i64 52
  store i8 %308, i8* %309, align 1
  %310 = load i64, i64* %289, align 8
  %311 = lshr i64 %310, 16
  %312 = trunc i64 %311 to i8
  %313 = getelementptr inbounds i8, i8* %1, i64 53
  store i8 %312, i8* %313, align 1
  %314 = load i64, i64* %289, align 8
  %315 = lshr i64 %314, 8
  %316 = trunc i64 %315 to i8
  %317 = getelementptr inbounds i8, i8* %1, i64 54
  store i8 %316, i8* %317, align 1
  %318 = load i64, i64* %289, align 8
  %319 = trunc i64 %318 to i8
  %320 = getelementptr inbounds i8, i8* %1, i64 55
  store i8 %319, i8* %320, align 1
  %321 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %0, i64 0, i32 1, i64 7
  %322 = load i64, i64* %321, align 8
  %323 = lshr i64 %322, 56
  %324 = trunc i64 %323 to i8
  %325 = getelementptr inbounds i8, i8* %1, i64 56
  store i8 %324, i8* %325, align 1
  %326 = load i64, i64* %321, align 8
  %327 = lshr i64 %326, 48
  %328 = trunc i64 %327 to i8
  %329 = getelementptr inbounds i8, i8* %1, i64 57
  store i8 %328, i8* %329, align 1
  %330 = load i64, i64* %321, align 8
  %331 = lshr i64 %330, 40
  %332 = trunc i64 %331 to i8
  %333 = getelementptr inbounds i8, i8* %1, i64 58
  store i8 %332, i8* %333, align 1
  %334 = load i64, i64* %321, align 8
  %335 = lshr i64 %334, 32
  %336 = trunc i64 %335 to i8
  %337 = getelementptr inbounds i8, i8* %1, i64 59
  store i8 %336, i8* %337, align 1
  %338 = load i64, i64* %321, align 8
  %339 = lshr i64 %338, 24
  %340 = trunc i64 %339 to i8
  %341 = getelementptr inbounds i8, i8* %1, i64 60
  store i8 %340, i8* %341, align 1
  %342 = load i64, i64* %321, align 8
  %343 = lshr i64 %342, 16
  %344 = trunc i64 %343 to i8
  %345 = getelementptr inbounds i8, i8* %1, i64 61
  store i8 %344, i8* %345, align 1
  %346 = load i64, i64* %321, align 8
  %347 = lshr i64 %346, 8
  %348 = trunc i64 %347 to i8
  %349 = getelementptr inbounds i8, i8* %1, i64 62
  store i8 %348, i8* %349, align 1
  %350 = load i64, i64* %321, align 8
  %351 = trunc i64 %350 to i8
  %352 = getelementptr inbounds i8, i8* %1, i64 63
  store i8 %351, i8* %352, align 1
  call void @llvm.lifetime.end.p0i8(i64 16, i8* nonnull %4) #4
  ret void
}

; Function Attrs: nounwind ssp uwtable
define hidden void @_Z20CRYPT_SHA512GeneratePKhjPh(i8* readonly, i32, i8*) local_unnamed_addr #1 {
  %4 = alloca %struct.CRYPT_sha2_context, align 16
  %5 = bitcast %struct.CRYPT_sha2_context* %4 to i8*
  call void @llvm.lifetime.start.p0i8(i64 200, i8* nonnull %5) #4
  %6 = bitcast %struct.CRYPT_sha2_context* %4 to <2 x i64>*
  store <2 x i64> <i64 0, i64 7640891576956012808>, <2 x i64>* %6, align 16
  %7 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %4, i64 0, i32 1, i64 1
  %8 = bitcast i64* %7 to <2 x i64>*
  store <2 x i64> <i64 -4942790177534073029, i64 4354685564936845355>, <2 x i64>* %8, align 8
  %9 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %4, i64 0, i32 1, i64 3
  %10 = bitcast i64* %9 to <2 x i64>*
  store <2 x i64> <i64 -6534734903238641935, i64 5840696475078001361>, <2 x i64>* %10, align 8
  %11 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %4, i64 0, i32 1, i64 5
  %12 = bitcast i64* %11 to <2 x i64>*
  store <2 x i64> <i64 -7276294671716946913, i64 2270897969802886507>, <2 x i64>* %12, align 8
  %13 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %4, i64 0, i32 1, i64 7
  store i64 6620516959819538809, i64* %13, align 8
  %14 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %4, i64 0, i32 2, i64 0
  call void @llvm.memset.p0i8.i64(i8* align 8 %14, i8 0, i64 128, i1 false) #4
  %15 = icmp eq i32 %1, 0
  br i1 %15, label %34, label %16

16:                                               ; preds = %3
  %17 = getelementptr inbounds %struct.CRYPT_sha2_context, %struct.CRYPT_sha2_context* %4, i64 0, i32 0
  %18 = zext i32 %1 to i64
  store i64 %18, i64* %17, align 16
  %19 = icmp ugt i32 %1, 127
  br i1 %19, label %20, label %31

20:                                               ; preds = %16, %20
  %21 = phi i32 [ %23, %20 ], [ %1, %16 ]
  %22 = phi i8* [ %24, %20 ], [ %0, %16 ]
  call fastcc void @_ZN12_GLOBAL__N_114sha384_processEP18CRYPT_sha2_contextPKh(%struct.CRYPT_sha2_context* nonnull %4, i8* %22) #4
  %23 = add i32 %21, -128
  %24 = getelementptr inbounds i8, i8* %22, i64 128
  %25 = icmp ugt i32 %23, 127
  br i1 %25, label %20, label %26

26:                                               ; preds = %20
  %27 = and i32 %1, 127
  %28 = icmp eq i32 %27, 0
  br i1 %28, label %34, label %29

29:                                               ; preds = %26
  %30 = zext i32 %27 to i64
  br label %31

31:                                               ; preds = %29, %16
  %32 = phi i64 [ %30, %29 ], [ %18, %16 ]
  %33 = phi i8* [ %24, %29 ], [ %0, %16 ]
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 8 %14, i8* align 1 %33, i64 %32, i1 false) #4
  br label %34

34:                                               ; preds = %3, %26, %31
  call void @_Z18CRYPT_SHA512FinishP18CRYPT_sha2_contextPh(%struct.CRYPT_sha2_context* nonnull %4, i8* %2)
  call void @llvm.lifetime.end.p0i8(i64 200, i8* nonnull %5) #4
  ret void
}

attributes #0 = { nofree norecurse nounwind ssp uwtable writeonly "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "min-legal-vector-width"="0" "no-frame-pointer-elim"="false" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "null-pointer-is-valid"="true" "stack-protector-buffer-size"="4" "target-cpu"="x86-64" "target-features"="+cx8,+fxsr,+mmx,+sse,+sse2,+sse3,+x87" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #1 = { nounwind ssp uwtable "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "min-legal-vector-width"="0" "no-frame-pointer-elim"="false" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "null-pointer-is-valid"="true" "stack-protector-buffer-size"="4" "target-cpu"="x86-64" "target-features"="+cx8,+fxsr,+mmx,+sse,+sse2,+sse3,+x87" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #2 = { argmemonly nounwind }
attributes #3 = { nofree norecurse nounwind ssp uwtable "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "min-legal-vector-width"="0" "no-frame-pointer-elim"="false" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "null-pointer-is-valid"="true" "stack-protector-buffer-size"="4" "target-cpu"="x86-64" "target-features"="+cx8,+fxsr,+mmx,+sse,+sse2,+sse3,+x87" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #4 = { nounwind }

!llvm.module.flags = !{!0, !1}

!0 = !{i32 1, !"wchar_size", i32 4}
!1 = !{i32 7, !"PIC Level", i32 2}
